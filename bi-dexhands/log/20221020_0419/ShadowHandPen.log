Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-1ggiwrow
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandPen_ppo_20221020041912
wandb: ⭐️ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: 🚀 View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/1ggiwrow
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:2
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=2, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=2, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:2', seed=None, sim_device='cuda:2', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandPen', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_pen', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 125, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 4, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandPen', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandPen_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 3538
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:2
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 740 steps/s (collection: 18.710s, learning 3.401s)
               Value function loss: 5.6994
                    Surrogate loss: 0.0758
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 22.11s
                        Total time: 22.11s
                               ETA: 2211071.5s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 857 steps/s (collection: 18.651s, learning 0.448s)
               Value function loss: 0.7347
                    Surrogate loss: -0.0267
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 19.10s
                        Total time: 41.21s
                               ETA: 2060453.9s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 791 steps/s (collection: 20.516s, learning 0.193s)
               Value function loss: 0.1694
                    Surrogate loss: -0.0256
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 20.71s
                        Total time: 61.92s
                               ETA: 2063903.5s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 751 steps/s (collection: 21.623s, learning 0.166s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0290
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 21.79s
                        Total time: 83.71s
                               ETA: 2092596.8s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 723 steps/s (collection: 22.466s, learning 0.171s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0182
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 22.64s
                        Total time: 106.34s
                               ETA: 2126769.6s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 726 steps/s (collection: 22.342s, learning 0.221s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0266
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 46.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 22.56s
                        Total time: 128.91s
                               ETA: 2148326.5s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 709 steps/s (collection: 22.909s, learning 0.188s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0359
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 48.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 23.10s
                        Total time: 152.00s
                               ETA: 2171349.7s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 730 steps/s (collection: 22.264s, learning 0.174s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0338
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 56.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 22.44s
                        Total time: 174.44s
                               ETA: 2180367.9s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 740 steps/s (collection: 21.944s, learning 0.170s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0316
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 65.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 22.11s
                        Total time: 196.56s
                               ETA: 2183782.3s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 701 steps/s (collection: 22.687s, learning 0.664s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0360
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 76.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 23.35s
                        Total time: 219.91s
                               ETA: 2198872.5s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 734 steps/s (collection: 22.125s, learning 0.179s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0273
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 22.30s
                        Total time: 242.21s
                               ETA: 2201701.7s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 730 steps/s (collection: 22.251s, learning 0.174s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0299
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 93.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 22.43s
                        Total time: 264.64s
                               ETA: 2205063.8s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.676s, learning 0.191s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0299
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 101.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 21.87s
                        Total time: 286.50s
                               ETA: 2203608.1s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.664s, learning 0.192s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0292
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 109.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 21.86s
                        Total time: 308.36s
                               ETA: 2202281.0s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 735 steps/s (collection: 22.074s, learning 0.216s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0371
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 115.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 22.29s
                        Total time: 330.65s
                               ETA: 2204016.7s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 744 steps/s (collection: 21.833s, learning 0.170s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 120.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 22.00s
                        Total time: 352.65s
                               ETA: 2203743.3s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 738 steps/s (collection: 22.017s, learning 0.177s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0264
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 115.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 22.19s
                        Total time: 374.85s
                               ETA: 2204622.7s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.634s, learning 0.179s)
               Value function loss: 0.1161
                    Surrogate loss: -0.0269
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 109.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 21.81s
                        Total time: 396.66s
                               ETA: 2203286.8s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.689s, learning 0.203s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0437
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 103.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 21.89s
                        Total time: 418.55s
                               ETA: 2202503.7s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 718 steps/s (collection: 22.645s, learning 0.163s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 22.81s
                        Total time: 441.36s
                               ETA: 2206374.5s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.564s, learning 0.213s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0410
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 74.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 21.78s
                        Total time: 463.14s
                               ETA: 2204966.5s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.100s, learning 0.170s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0378
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 21.27s
                        Total time: 484.41s
                               ETA: 2201381.0s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.396s, learning 0.173s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0438
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 21.57s
                        Total time: 505.98s
                               ETA: 2199408.8s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.392s, learning 0.163s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0416
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 94.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 21.55s
                        Total time: 527.53s
                               ETA: 2197536.9s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 750 steps/s (collection: 21.664s, learning 0.172s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 96.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 21.84s
                        Total time: 549.37s
                               ETA: 2196938.6s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.037s, learning 0.166s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0464
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 105.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 22.20s
                        Total time: 571.57s
                               ETA: 2197792.8s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 742 steps/s (collection: 21.875s, learning 0.192s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0495
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 105.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 22.07s
                        Total time: 593.64s
                               ETA: 2198083.5s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.316s, learning 0.170s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0482
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 110.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 21.49s
                        Total time: 615.12s
                               ETA: 2196274.1s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 731 steps/s (collection: 22.210s, learning 0.173s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0478
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 116.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 22.38s
                        Total time: 637.51s
                               ETA: 2197680.4s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.879s, learning 0.208s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0455
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 116.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 21.09s
                        Total time: 658.59s
                               ETA: 2194672.8s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 738 steps/s (collection: 21.917s, learning 0.283s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0503
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 117.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 22.20s
                        Total time: 680.79s
                               ETA: 2195448.6s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.670s, learning 0.213s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0267
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 112.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 21.88s
                        Total time: 702.68s
                               ETA: 2195182.0s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.492s, learning 0.167s)
               Value function loss: 0.0542
                    Surrogate loss: -0.0328
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 101.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 21.66s
                        Total time: 724.34s
                               ETA: 2194252.7s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 784 steps/s (collection: 20.722s, learning 0.165s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0262
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 20.89s
                        Total time: 745.22s
                               ETA: 2191106.2s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.362s, learning 0.172s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0366
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 21.53s
                        Total time: 766.76s
                               ETA: 2189986.0s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.422s, learning 0.279s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0386
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 90.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 21.70s
                        Total time: 788.46s
                               ETA: 2189390.7s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 741 steps/s (collection: 21.912s, learning 0.177s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0436
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 22.09s
                        Total time: 810.55s
                               ETA: 2189874.8s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.133s, learning 0.227s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0535
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 93.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 16.36s
                        Total time: 826.91s
                               ETA: 2175261.7s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.347s, learning 0.248s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0545
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 97.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 11.59s
                        Total time: 838.50s
                               ETA: 2149183.3s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.099s, learning 0.201s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0495
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 104.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 11.30s
                        Total time: 849.80s
                               ETA: 2123672.1s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.909s, learning 0.209s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0517
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 101.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 11.12s
                        Total time: 860.92s
                               ETA: 2098960.0s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.127s, learning 0.176s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 108.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 11.30s
                        Total time: 872.22s
                               ETA: 2075865.0s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.890s, learning 0.180s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0558
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 111.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 11.07s
                        Total time: 883.29s
                               ETA: 2053300.9s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.792s, learning 0.171s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0487
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 111.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 10.96s
                        Total time: 894.25s
                               ETA: 2031519.2s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.918s, learning 0.174s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0399
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 111.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 11.09s
                        Total time: 905.35s
                               ETA: 2010994.2s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.527s, learning 0.334s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0475
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 116.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 10.86s
                        Total time: 916.21s
                               ETA: 1990856.9s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.922s, learning 0.182s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0325
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 118.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 11.10s
                        Total time: 927.31s
                               ETA: 1972093.3s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.037s, learning 0.210s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0399
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 91.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 11.25s
                        Total time: 938.56s
                               ETA: 1954409.5s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.031s, learning 0.219s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 11.25s
                        Total time: 949.81s
                               ETA: 1937452.3s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.516s, learning 0.192s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0525
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 91.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 10.71s
                        Total time: 960.52s
                               ETA: 1920091.1s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.122s, learning 0.190s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0542
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 91.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 11.31s
                        Total time: 971.83s
                               ETA: 1904593.3s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.848s, learning 0.179s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0532
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 92.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 11.03s
                        Total time: 982.86s
                               ETA: 1889144.0s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.536s, learning 0.294s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 90.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 10.83s
                        Total time: 993.69s
                               ETA: 1873905.2s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.490s, learning 0.189s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0557
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 90.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 10.68s
                        Total time: 1004.37s
                               ETA: 1858950.2s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.742s, learning 0.231s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0564
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 97.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 10.97s
                        Total time: 1015.34s
                               ETA: 1845072.8s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.640s, learning 0.175s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0541
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 89.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 10.82s
                        Total time: 1026.15s
                               ETA: 1831408.9s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.952s, learning 0.176s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0524
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 93.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 11.13s
                        Total time: 1037.28s
                               ETA: 1818772.3s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.803s, learning 0.194s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0540
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 96.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 11.00s
                        Total time: 1048.28s
                               ETA: 1806346.7s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.920s, learning 0.185s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 96.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 11.11s
                        Total time: 1059.38s
                               ETA: 1794524.1s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.857s, learning 0.253s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0613
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 99.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 11.11s
                        Total time: 1070.49s
                               ETA: 1783104.7s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.069s, learning 0.170s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0542
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 11.24s
                        Total time: 1081.73s
                               ETA: 1772268.2s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.539s, learning 0.227s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0562
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 11.77s
                        Total time: 1093.50s
                               ETA: 1762631.2s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.622s, learning 0.200s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0574
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 72.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 10.82s
                        Total time: 1104.32s
                               ETA: 1751803.9s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.789s, learning 0.210s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0560
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 76.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 11.00s
                        Total time: 1115.32s
                               ETA: 1741589.5s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.054s, learning 0.173s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0563
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 11.23s
                        Total time: 1126.55s
                               ETA: 1732040.2s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.164s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0550
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 10.94s
                        Total time: 1137.49s
                               ETA: 1722345.4s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.602s, learning 0.161s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0563
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 10.76s
                        Total time: 1148.25s
                               ETA: 1712676.0s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.724s, learning 0.290s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 11.01s
                        Total time: 1159.27s
                               ETA: 1703659.5s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.061s, learning 0.190s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0546
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 11.25s
                        Total time: 1170.52s
                               ETA: 1695246.2s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.014s, learning 0.225s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0558
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 11.24s
                        Total time: 1181.76s
                               ETA: 1687057.0s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.181s, learning 0.282s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0584
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 11.46s
                        Total time: 1193.22s
                               ETA: 1679412.5s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.368s, learning 0.171s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 11.54s
                        Total time: 1204.76s
                               ETA: 1672085.6s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.271s, learning 0.274s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0587
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 10.55s
                        Total time: 1215.30s
                               ETA: 1663599.0s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.576s, learning 0.174s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0574
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 10.75s
                        Total time: 1226.05s
                               ETA: 1655618.4s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.041s, learning 0.235s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0532
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 11.28s
                        Total time: 1237.33s
                               ETA: 1648550.9s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.244s, learning 0.193s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0586
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 11.44s
                        Total time: 1248.77s
                               ETA: 1641880.6s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.129s, learning 0.184s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0597
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 11.31s
                        Total time: 1260.08s
                               ETA: 1635222.4s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.831s, learning 0.186s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0545
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 11.02s
                        Total time: 1271.10s
                               ETA: 1628355.3s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.320s, learning 0.174s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0585
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 11.49s
                        Total time: 1282.59s
                               ETA: 1622265.8s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.920s, learning 0.165s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0610
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 11.08s
                        Total time: 1293.68s
                               ETA: 1615816.6s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.522s, learning 0.186s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0606
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 11.71s
                        Total time: 1305.38s
                               ETA: 1610294.8s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.903s, learning 0.198s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0477
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 11.10s
                        Total time: 1316.48s
                               ETA: 1604167.1s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.901s, learning 0.210s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 11.11s
                        Total time: 1327.59s
                               ETA: 1598200.0s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.685s, learning 0.208s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0499
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 10.89s
                        Total time: 1338.49s
                               ETA: 1592114.8s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.171s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0557
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 10.99s
                        Total time: 1349.47s
                               ETA: 1586283.0s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.575s, learning 0.171s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0545
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 10.75s
                        Total time: 1360.22s
                               ETA: 1580306.4s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.163s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0564
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 11.12s
                        Total time: 1371.34s
                               ETA: 1574895.0s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.313s, learning 0.200s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0552
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 11.51s
                        Total time: 1382.85s
                               ETA: 1570053.8s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.824s, learning 0.177s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0578
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 11.00s
                        Total time: 1393.85s
                               ETA: 1564747.2s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.784s, learning 0.200s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0609
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 10.98s
                        Total time: 1404.84s
                               ETA: 1559539.1s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.890s, learning 0.210s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0540
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 11.10s
                        Total time: 1415.94s
                               ETA: 1554572.8s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.063s, learning 0.159s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 11.22s
                        Total time: 1427.16s
                               ETA: 1549846.0s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.343s, learning 0.192s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0536
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 11.53s
                        Total time: 1438.69s
                               ETA: 1545557.0s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.756s, learning 0.170s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0589
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 10.93s
                        Total time: 1449.62s
                               ETA: 1540711.6s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.179s, learning 0.179s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0592
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 11.36s
                        Total time: 1460.98s
                               ETA: 1536423.1s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.020s, learning 0.172s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0594
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 11.19s
                        Total time: 1472.17s
                               ETA: 1532050.7s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.435s, learning 0.194s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0539
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 10.63s
                        Total time: 1482.80s
                               ETA: 1527188.4s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.096s, learning 0.182s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0556
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 11.28s
                        Total time: 1494.07s
                               ETA: 1523086.8s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.512s, learning 0.164s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0583
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 10.68s
                        Total time: 1504.75s
                               ETA: 1518460.8s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.867s, learning 0.204s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0524
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 11.07s
                        Total time: 1515.82s
                               ETA: 1514321.0s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.809s, learning 0.167s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0595
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 10.98s
                        Total time: 1526.80s
                               ETA: 1510169.3s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.690s, learning 0.172s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0604
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 10.86s
                        Total time: 1537.66s
                               ETA: 1505986.6s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.711s, learning 0.172s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0617
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 10.88s
                        Total time: 1548.54s
                               ETA: 1501906.0s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.827s, learning 0.193s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0524
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 11.02s
                        Total time: 1559.56s
                               ETA: 1498034.9s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.726s, learning 0.194s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0571
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 10.92s
                        Total time: 1570.48s
                               ETA: 1494141.9s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.957s, learning 0.169s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0504
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 11.13s
                        Total time: 1581.61s
                               ETA: 1490517.0s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.985s, learning 0.163s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0473
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 11.15s
                        Total time: 1592.76s
                               ETA: 1486979.1s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.068s, learning 0.185s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0525
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 11.25s
                        Total time: 1604.01s
                               ETA: 1483604.3s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.109s, learning 0.166s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0600
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 11.28s
                        Total time: 1615.28s
                               ETA: 1480311.7s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1463 steps/s (collection: 10.985s, learning 0.213s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0553
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 11.20s
                        Total time: 1626.48s
                               ETA: 1477008.0s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.930s, learning 0.169s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0530
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 76.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 11.10s
                        Total time: 1637.58s
                               ETA: 1473675.6s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.125s, learning 0.209s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0561
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 77.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 11.33s
                        Total time: 1648.92s
                               ETA: 1470611.4s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.887s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0541
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 11.06s
                        Total time: 1659.97s
                               ETA: 1467356.2s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.726s, learning 0.176s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0599
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 10.90s
                        Total time: 1670.87s
                               ETA: 1464022.9s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.949s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0578
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 11.12s
                        Total time: 1681.99s
                               ETA: 1460935.2s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.648s, learning 0.172s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0546
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 10.82s
                        Total time: 1692.81s
                               ETA: 1457643.6s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.776s, learning 0.178s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0598
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 10.95s
                        Total time: 1703.77s
                               ETA: 1454522.1s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.557s, learning 0.194s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0608
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 10.75s
                        Total time: 1714.52s
                               ETA: 1451281.6s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.776s, learning 0.170s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0612
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 10.95s
                        Total time: 1725.46s
                               ETA: 1448259.1s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.526s, learning 0.171s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0599
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 10.70s
                        Total time: 1736.16s
                               ETA: 1445079.8s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.162s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0625
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 11.06s
                        Total time: 1747.22s
                               ETA: 1442250.4s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.793s, learning 0.169s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0606
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 10.96s
                        Total time: 1758.18s
                               ETA: 1439388.6s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.830s, learning 0.174s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0626
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 11.00s
                        Total time: 1769.19s
                               ETA: 1436607.2s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.047s, learning 0.172s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0591
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 89.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 11.22s
                        Total time: 1780.40s
                               ETA: 1434043.5s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.162s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0613
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 11.31s
                        Total time: 1791.71s
                               ETA: 1431590.9s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.714s, learning 0.176s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0619
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 10.89s
                        Total time: 1802.60s
                               ETA: 1428847.0s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.152s, learning 0.193s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0617
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 11.34s
                        Total time: 1813.94s
                               ETA: 1426503.3s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.052s, learning 0.167s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0571
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 11.22s
                        Total time: 1825.16s
                               ETA: 1424098.7s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.054s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 11.22s
                        Total time: 1836.38s
                               ETA: 1421730.0s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.678s, learning 0.185s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0612
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 89.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 10.86s
                        Total time: 1847.25s
                               ETA: 1419125.2s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.854s, learning 0.179s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0584
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 89.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 11.03s
                        Total time: 1858.28s
                               ETA: 1416689.6s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.379s, learning 0.264s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0624
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 10.64s
                        Total time: 1868.92s
                               ETA: 1413995.1s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.616s, learning 0.192s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0578
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 10.81s
                        Total time: 1879.73s
                               ETA: 1411465.2s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.972s, learning 0.175s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0603
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 90.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 11.15s
                        Total time: 1890.88s
                               ETA: 1409225.3s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.821s, learning 0.212s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0618
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 92.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 11.03s
                        Total time: 1901.91s
                               ETA: 1406934.7s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.697s, learning 0.162s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0583
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 10.86s
                        Total time: 1912.77s
                               ETA: 1404549.5s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.540s, learning 0.170s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0530
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 90.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 10.71s
                        Total time: 1923.48s
                               ETA: 1402090.3s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.762s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0535
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 89.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 10.93s
                        Total time: 1934.41s
                               ETA: 1399824.2s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.877s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0521
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 92.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 11.04s
                        Total time: 1945.45s
                               ETA: 1397671.0s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.541s, learning 0.172s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0472
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 91.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 10.71s
                        Total time: 1956.16s
                               ETA: 1395314.9s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.853s, learning 0.169s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0553
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 88.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 11.02s
                        Total time: 1967.18s
                               ETA: 1393211.3s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.575s, learning 0.178s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0609
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 10.75s
                        Total time: 1977.94s
                               ETA: 1390948.0s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.951s, learning 0.212s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0528
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 85.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 11.16s
                        Total time: 1989.10s
                               ETA: 1389002.4s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.165s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0544
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 11.13s
                        Total time: 2000.23s
                               ETA: 1387058.9s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.953s, learning 0.196s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0527
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 87.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 11.15s
                        Total time: 2011.37s
                               ETA: 1385156.7s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.665s, learning 0.189s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0558
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 10.85s
                        Total time: 2022.23s
                               ETA: 1383079.0s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.965s, learning 0.176s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0578
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 11.14s
                        Total time: 2033.37s
                               ETA: 1381224.5s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.846s, learning 0.190s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0607
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 11.04s
                        Total time: 2044.41s
                               ETA: 1379324.3s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.976s, learning 0.170s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0625
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 11.15s
                        Total time: 2055.55s
                               ETA: 1377523.0s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.380s, learning 0.172s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0613
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 10.55s
                        Total time: 2066.10s
                               ETA: 1375350.1s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.895s, learning 0.190s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0629
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 11.09s
                        Total time: 2077.19s
                               ETA: 1373558.4s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.714s, learning 0.210s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 10.92s
                        Total time: 2088.11s
                               ETA: 1371684.2s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.827s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0588
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 10.99s
                        Total time: 2099.10s
                               ETA: 1369877.6s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.794s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0606
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 77.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 10.96s
                        Total time: 2110.06s
                               ETA: 1368072.5s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.864s, learning 0.189s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 76.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 11.05s
                        Total time: 2121.11s
                               ETA: 1366352.7s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.703s, learning 0.189s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0513
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 83.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 10.89s
                        Total time: 2132.01s
                               ETA: 1364551.7s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.872s, learning 0.226s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0468
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 11.10s
                        Total time: 2143.10s
                               ETA: 1362904.6s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.937s, learning 0.181s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0502
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 11.12s
                        Total time: 2154.22s
                               ETA: 1361290.5s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.171s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0513
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 11.08s
                        Total time: 2165.30s
                               ETA: 1359671.0s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.162s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0565
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 11.33s
                        Total time: 2176.63s
                               ETA: 1358229.0s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.033s, learning 0.208s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0607
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 11.24s
                        Total time: 2187.87s
                               ETA: 1356750.2s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.814s, learning 0.176s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0604
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 10.99s
                        Total time: 2198.86s
                               ETA: 1355134.7s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.777s, learning 0.202s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0523
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 86.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 10.98s
                        Total time: 2209.84s
                               ETA: 1353532.0s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.820s, learning 0.173s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0479
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 10.99s
                        Total time: 2220.83s
                               ETA: 1351957.8s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.754s, learning 0.259s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0504
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 11.01s
                        Total time: 2231.84s
                               ETA: 1350414.1s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.418s, learning 0.161s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0545
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 10.58s
                        Total time: 2242.42s
                               ETA: 1348627.4s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.188s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0523
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 11.26s
                        Total time: 2253.68s
                               ETA: 1347269.5s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.082s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0452
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 84.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 11.24s
                        Total time: 2264.93s
                               ETA: 1345918.8s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.811s, learning 0.182s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0608
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 10.99s
                        Total time: 2275.92s
                               ETA: 1344435.3s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.172s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0620
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 81.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 11.21s
                        Total time: 2287.13s
                               ETA: 1343094.8s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.774s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0601
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 10.94s
                        Total time: 2298.06s
                               ETA: 1341611.2s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.257s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0599
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 82.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 11.42s
                        Total time: 2309.49s
                               ETA: 1340428.2s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.068s, learning 0.276s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0622
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 79.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 11.34s
                        Total time: 2320.83s
                               ETA: 1339212.7s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.155s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0518
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 80.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 11.32s
                        Total time: 2332.15s
                               ETA: 1337996.4s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.827s, learning 0.174s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0510
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 77.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 11.00s
                        Total time: 2343.15s
                               ETA: 1336612.9s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.944s, learning 0.174s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0513
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 78.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 11.12s
                        Total time: 2354.27s
                               ETA: 1335311.0s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.697s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0608
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 74.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 10.86s
                        Total time: 2365.13s
                               ETA: 1333880.6s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.263s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0619
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 77.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 11.43s
                        Total time: 2376.56s
                               ETA: 1332783.2s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.043s, learning 0.175s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0582
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 75.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 11.22s
                        Total time: 2387.78s
                               ETA: 1331579.9s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.825s, learning 0.194s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0572
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 75.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 11.02s
                        Total time: 2398.80s
                               ETA: 1330279.1s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.160s, learning 0.175s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0582
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 73.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 11.33s
                        Total time: 2410.13s
                               ETA: 1329167.1s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.070s, learning 0.186s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0535
             Mean action noise std: 0.80
                       Mean reward: 0.00
               Mean episode length: 71.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 11.26s
                        Total time: 2421.39s
                               ETA: 1328023.9s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.707s, learning 0.177s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 71.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 10.88s
                        Total time: 2432.27s
                               ETA: 1326690.2s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1463 steps/s (collection: 10.939s, learning 0.253s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0517
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 70.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 11.19s
                        Total time: 2443.46s
                               ETA: 1325538.3s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.920s, learning 0.166s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0561
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 70.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 11.09s
                        Total time: 2454.55s
                               ETA: 1324341.3s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.932s, learning 0.177s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0510
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 73.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 11.11s
                        Total time: 2465.66s
                               ETA: 1323169.3s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.177s, learning 0.216s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 71.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 11.39s
                        Total time: 2477.05s
                               ETA: 1322161.5s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.573s, learning 0.255s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0490
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 74.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 10.83s
                        Total time: 2487.88s
                               ETA: 1320864.3s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.997s, learning 0.183s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0464
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 11.18s
                        Total time: 2499.06s
                               ETA: 1319766.5s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0626
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 71.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 11.27s
                        Total time: 2510.33s
                               ETA: 1318729.1s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.847s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0514
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 11.01s
                        Total time: 2521.34s
                               ETA: 1317565.6s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.760s, learning 0.209s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 10.97s
                        Total time: 2532.31s
                               ETA: 1316392.1s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.928s, learning 0.180s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0588
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 70.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 11.11s
                        Total time: 2543.42s
                               ETA: 1315302.9s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.048s, learning 0.278s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0595
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 11.33s
                        Total time: 2554.74s
                               ETA: 1314336.7s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.772s, learning 0.188s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 10.96s
                        Total time: 2565.70s
                               ETA: 1313192.9s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.828s, learning 0.177s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 11.01s
                        Total time: 2576.71s
                               ETA: 1312084.0s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.850s, learning 0.228s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0560
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 67.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 11.08s
                        Total time: 2587.79s
                               ETA: 1311022.8s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.975s, learning 0.163s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0527
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 11.14s
                        Total time: 2598.93s
                               ETA: 1310002.6s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.838s, learning 0.163s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0521
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 11.00s
                        Total time: 2609.93s
                               ETA: 1308923.9s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.141s, learning 0.178s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0516
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 67.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 11.32s
                        Total time: 2621.25s
                               ETA: 1308014.4s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.681s, learning 0.172s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 10.85s
                        Total time: 2632.10s
                               ETA: 1306882.5s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.039s, learning 0.215s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0478
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 11.25s
                        Total time: 2643.35s
                               ETA: 1305959.7s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.698s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0464
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 67.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 10.86s
                        Total time: 2654.21s
                               ETA: 1304853.6s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.059s, learning 0.183s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0618
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 68.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 11.24s
                        Total time: 2665.46s
                               ETA: 1303943.9s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.900s, learning 0.178s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0521
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 69.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 11.08s
                        Total time: 2676.53s
                               ETA: 1302963.1s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.569s, learning 0.170s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0521
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 66.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 10.74s
                        Total time: 2687.27s
                               ETA: 1301827.5s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.932s, learning 0.200s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 65.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 11.13s
                        Total time: 2698.40s
                               ETA: 1300891.9s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.765s, learning 0.231s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0534
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 65.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 11.00s
                        Total time: 2709.40s
                               ETA: 1299900.2s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.057s, learning 0.237s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0558
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 62.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 11.29s
                        Total time: 2720.70s
                               ETA: 1299060.4s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.917s, learning 0.161s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 63.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 11.08s
                        Total time: 2731.77s
                               ETA: 1298125.6s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.071s, learning 0.164s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 60.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 11.23s
                        Total time: 2743.01s
                               ETA: 1297273.7s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.919s, learning 0.183s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0522
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 58.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 11.10s
                        Total time: 2754.11s
                               ETA: 1296367.0s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.000s, learning 0.188s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0564
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 58.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 11.19s
                        Total time: 2765.30s
                               ETA: 1295509.2s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.975s, learning 0.159s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0529
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 61.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 11.13s
                        Total time: 2776.43s
                               ETA: 1294634.2s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.799s, learning 0.168s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0556
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 58.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 10.97s
                        Total time: 2787.40s
                               ETA: 1293689.5s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.462s, learning 0.163s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 58.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 10.63s
                        Total time: 2798.02s
                               ETA: 1292595.8s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.442s, learning 0.182s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0510
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 59.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 10.62s
                        Total time: 2808.65s
                               ETA: 1291511.8s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.921s, learning 0.162s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0474
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 58.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 11.08s
                        Total time: 2819.73s
                               ETA: 1290647.5s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.786s, learning 0.163s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0605
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 56.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 10.95s
                        Total time: 2830.68s
                               ETA: 1289730.0s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.647s, learning 0.189s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0602
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 59.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 10.84s
                        Total time: 2841.52s
                               ETA: 1288769.8s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.902s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0501
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 56.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 11.07s
                        Total time: 2852.59s
                               ETA: 1287924.2s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.372s, learning 0.166s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0478
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 56.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 11.54s
                        Total time: 2864.13s
                               ETA: 1287295.8s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1585 steps/s (collection: 10.154s, learning 0.183s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0591
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 57.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 10.34s
                        Total time: 2874.46s
                               ETA: 1286135.2s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.531s, learning 0.177s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 55.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 10.71s
                        Total time: 2885.17s
                               ETA: 1285150.4s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.905s, learning 0.173s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 56.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 11.08s
                        Total time: 2896.25s
                               ETA: 1284338.3s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.121s, learning 0.160s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0595
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 55.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 11.28s
                        Total time: 2907.53s
                               ETA: 1283622.8s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.381s, learning 0.185s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0507
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 54.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 10.57s
                        Total time: 2918.10s
                               ETA: 1282599.6s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.617s, learning 0.181s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0569
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 56.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 10.80s
                        Total time: 2928.89s
                               ETA: 1281686.6s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.531s, learning 0.169s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0623
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 54.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 10.70s
                        Total time: 2939.59s
                               ETA: 1280738.9s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.669s, learning 0.165s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0609
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 53.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 10.83s
                        Total time: 2950.43s
                               ETA: 1279857.4s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.762s, learning 0.282s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0602
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 54.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 11.04s
                        Total time: 2961.47s
                               ETA: 1279074.1s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1462 steps/s (collection: 10.958s, learning 0.248s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0574
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 53.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 11.21s
                        Total time: 2972.68s
                               ETA: 1278367.3s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.002s, learning 0.253s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0571
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 50.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 11.25s
                        Total time: 2983.93s
                               ETA: 1277687.1s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.187s, learning 0.178s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 51.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 11.36s
                        Total time: 2995.30s
                               ETA: 1277059.4s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.045s, learning 0.171s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0584
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 49.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 11.22s
                        Total time: 3006.51s
                               ETA: 1276374.0s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.860s, learning 0.166s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 47.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 11.03s
                        Total time: 3017.54s
                               ETA: 1275613.9s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.131s, learning 0.162s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0474
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 47.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 11.29s
                        Total time: 3028.83s
                               ETA: 1274972.7s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.715s, learning 0.234s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0541
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 45.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 10.95s
                        Total time: 3039.78s
                               ETA: 1274192.5s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.942s, learning 0.184s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0558
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 45.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 11.13s
                        Total time: 3050.91s
                               ETA: 1273492.5s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.960s, learning 0.191s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0540
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 43.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 11.15s
                        Total time: 3062.06s
                               ETA: 1272809.0s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.816s, learning 0.188s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 42.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 11.00s
                        Total time: 3073.06s
                               ETA: 1272070.0s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.548s, learning 0.179s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0608
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 44.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 10.73s
                        Total time: 3083.79s
                               ETA: 1271222.8s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.006s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0570
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 42.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 11.17s
                        Total time: 3094.96s
                               ETA: 1270564.5s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.595s, learning 0.170s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0484
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 43.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 10.77s
                        Total time: 3105.73s
                               ETA: 1269745.8s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.063s, learning 0.162s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0579
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 42.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 11.22s
                        Total time: 3116.95s
                               ETA: 1269120.8s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.772s, learning 0.189s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0494
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 42.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 10.96s
                        Total time: 3127.91s
                               ETA: 1268394.0s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.796s, learning 0.193s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 41.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 10.99s
                        Total time: 3138.90s
                               ETA: 1267683.8s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.038s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 42.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 11.20s
                        Total time: 3150.11s
                               ETA: 1267066.3s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.867s, learning 0.198s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0551
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 41.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 11.07s
                        Total time: 3161.17s
                               ETA: 1266398.1s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.602s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0574
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 41.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 11.76s
                        Total time: 3172.94s
                               ETA: 1266013.8s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.981s, learning 0.174s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0591
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 40.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 11.15s
                        Total time: 3184.09s
                               ETA: 1265390.4s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.747s, learning 0.204s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0588
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 10.95s
                        Total time: 3195.04s
                               ETA: 1264690.9s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.440s, learning 0.190s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 10.63s
                        Total time: 3205.67s
                               ETA: 1263870.3s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.451s, learning 0.184s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 40.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 10.63s
                        Total time: 3216.30s
                               ETA: 1263058.0s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.875s, learning 0.175s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 40.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 11.05s
                        Total time: 3227.36s
                               ETA: 1262414.8s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.620s, learning 0.192s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0630
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 10.81s
                        Total time: 3238.17s
                               ETA: 1261683.8s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.919s, learning 0.259s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0525
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 11.18s
                        Total time: 3249.35s
                               ETA: 1261099.9s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.752s, learning 0.192s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0639
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 10.94s
                        Total time: 3260.29s
                               ETA: 1260430.2s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.109s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 11.27s
                        Total time: 3271.56s
                               ETA: 1259892.5s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.641s, learning 0.274s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 10.92s
                        Total time: 3282.48s
                               ETA: 1259221.5s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.427s, learning 0.174s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 40.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 10.60s
                        Total time: 3293.08s
                               ETA: 1258435.1s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.567s, learning 0.269s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 40.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 11.84s
                        Total time: 3304.91s
                               ETA: 1258125.3s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.842s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 11.01s
                        Total time: 3315.92s
                               ETA: 1257503.9s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.421s, learning 0.192s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0560
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 10.61s
                        Total time: 3326.54s
                               ETA: 1256737.7s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.658s, learning 0.191s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0562
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 10.85s
                        Total time: 3337.39s
                               ETA: 1256066.1s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.834s, learning 0.187s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 11.02s
                        Total time: 3348.41s
                               ETA: 1255463.7s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 930 steps/s (collection: 17.436s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0595
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 17.60s
                        Total time: 3366.01s
                               ETA: 1257324.0s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.213s, learning 0.186s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 21.40s
                        Total time: 3387.41s
                               ETA: 1260583.2s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.741s, learning 0.182s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0606
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 21.92s
                        Total time: 3409.33s
                               ETA: 1264012.5s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.785s, learning 0.186s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 21.97s
                        Total time: 3431.30s
                               ETA: 1267433.9s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.194s, learning 0.171s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0574
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 21.37s
                        Total time: 3452.67s
                               ETA: 1270606.9s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.229s, learning 0.205s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 39.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 21.43s
                        Total time: 3474.10s
                               ETA: 1273781.6s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.289s, learning 0.199s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 21.49s
                        Total time: 3495.59s
                               ETA: 1276952.7s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.960s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 21.13s
                        Total time: 3516.72s
                               ETA: 1279968.8s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.501s, learning 0.199s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 21.70s
                        Total time: 3538.41s
                               ETA: 1283170.8s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.142s, learning 0.176s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0600
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 21.32s
                        Total time: 3559.73s
                               ETA: 1286211.3s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.097s, learning 0.278s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 38.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 21.37s
                        Total time: 3581.11s
                               ETA: 1289250.3s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 734 steps/s (collection: 22.108s, learning 0.207s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0605
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 22.31s
                        Total time: 3603.42s
                               ETA: 1292604.6s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.136s, learning 0.201s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 21.34s
                        Total time: 3624.76s
                               ETA: 1295585.3s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.009s, learning 0.203s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0598
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 21.21s
                        Total time: 3645.97s
                               ETA: 1298499.8s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.309s, learning 0.186s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0610
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 21.50s
                        Total time: 3667.47s
                               ETA: 1301494.0s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.088s, learning 0.233s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0589
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 21.32s
                        Total time: 3688.79s
                               ETA: 1304405.1s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 744 steps/s (collection: 21.799s, learning 0.204s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 22.00s
                        Total time: 3710.79s
                               ETA: 1307535.8s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.208s, learning 0.172s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 21.38s
                        Total time: 3732.17s
                               ETA: 1310425.6s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.968s, learning 0.185s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0631
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 37.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 21.15s
                        Total time: 3753.32s
                               ETA: 1313215.7s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.537s, learning 0.199s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0587
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 36.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 21.74s
                        Total time: 3775.06s
                               ETA: 1316189.3s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.190s, learning 0.170s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 35.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 21.36s
                        Total time: 3796.42s
                               ETA: 1319011.4s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.506s, learning 0.180s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0584
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 34.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 21.69s
                        Total time: 3818.11s
                               ETA: 1321926.4s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.425s, learning 0.199s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0606
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 34.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 21.62s
                        Total time: 3839.73s
                               ETA: 1324799.9s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 773 steps/s (collection: 20.962s, learning 0.212s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 34.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 21.17s
                        Total time: 3860.90s
                               ETA: 1327498.7s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.226s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0624
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 33.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 21.39s
                        Total time: 3882.29s
                               ETA: 1330252.8s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.107s, learning 0.161s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0649
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 21.27s
                        Total time: 3903.56s
                               ETA: 1332946.3s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 779 steps/s (collection: 20.847s, learning 0.162s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 21.01s
                        Total time: 3924.57s
                               ETA: 1335533.0s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.350s, learning 0.177s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 33.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 21.53s
                        Total time: 3946.10s
                               ETA: 1338277.5s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.119s, learning 0.177s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0598
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 21.30s
                        Total time: 3967.39s
                               ETA: 1340925.4s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.457s, learning 0.193s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0628
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 21.65s
                        Total time: 3989.04s
                               ETA: 1343674.3s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.479s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 21.65s
                        Total time: 4010.69s
                               ETA: 1346404.4s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.544s, learning 0.158s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0634
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 21.70s
                        Total time: 4032.39s
                               ETA: 1349133.6s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.397s, learning 0.161s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0595
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 21.56s
                        Total time: 4053.95s
                               ETA: 1351796.4s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 782 steps/s (collection: 20.755s, learning 0.195s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 30.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 20.95s
                        Total time: 4074.90s
                               ETA: 1354239.2s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 729 steps/s (collection: 22.291s, learning 0.166s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0615
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 30.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 22.46s
                        Total time: 4097.36s
                               ETA: 1357164.9s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.851s, learning 0.196s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 21.05s
                        Total time: 4118.41s
                               ETA: 1359605.6s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.604s, learning 0.279s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0606
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 21.88s
                        Total time: 4140.29s
                               ETA: 1362305.1s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.413s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0626
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 30.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 21.59s
                        Total time: 4161.87s
                               ETA: 1364889.6s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.901s, learning 0.228s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0616
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 11.13s
                        Total time: 4173.00s
                               ETA: 1364038.9s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.727s, learning 0.163s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 10.89s
                        Total time: 4183.89s
                               ETA: 1363115.7s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.700s, learning 0.189s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 10.89s
                        Total time: 4194.78s
                               ETA: 1362197.9s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.941s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0623
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 11.11s
                        Total time: 4205.89s
                               ETA: 1361357.3s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.924s, learning 0.161s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 11.08s
                        Total time: 4216.98s
                               ETA: 1360514.2s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.652s, learning 0.201s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0638
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 30.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 10.85s
                        Total time: 4227.83s
                               ETA: 1359602.0s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.971s, learning 0.160s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 11.13s
                        Total time: 4238.96s
                               ETA: 1358784.7s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.060s, learning 0.194s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 11.25s
                        Total time: 4250.22s
                               ETA: 1358012.1s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.547s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 10.71s
                        Total time: 4260.92s
                               ETA: 1357069.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.513s, learning 0.184s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 10.70s
                        Total time: 4271.62s
                               ETA: 1356130.4s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.836s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 11.00s
                        Total time: 4282.62s
                               ETA: 1355292.0s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.763s, learning 0.174s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0633
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 10.94s
                        Total time: 4293.55s
                               ETA: 1354439.8s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.406s, learning 0.255s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 10.66s
                        Total time: 4304.22s
                               ETA: 1353505.9s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.052s, learning 0.178s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 11.23s
                        Total time: 4315.45s
                               ETA: 1352756.5s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.598s, learning 0.176s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0652
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 31.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 10.77s
                        Total time: 4326.22s
                               ETA: 1351869.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.509s, learning 0.164s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0622
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 32.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 10.67s
                        Total time: 4336.89s
                               ETA: 1350955.8s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.964s, learning 0.164s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0615
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 30.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 11.13s
                        Total time: 4348.02s
                               ETA: 1350189.2s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.622s, learning 0.168s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 29.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 10.79s
                        Total time: 4358.81s
                               ETA: 1349322.6s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.708s, learning 0.198s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 29.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 10.91s
                        Total time: 4369.72s
                               ETA: 1348497.1s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.780s, learning 0.172s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0568
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 29.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 10.95s
                        Total time: 4380.67s
                               ETA: 1347691.2s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.109s, learning 0.159s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0533
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 28.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 11.27s
                        Total time: 4391.94s
                               ETA: 1346986.8s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.870s, learning 0.282s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0570
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 28.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 11.15s
                        Total time: 4403.09s
                               ETA: 1346251.2s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.557s, learning 0.218s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0591
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 27.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 10.78s
                        Total time: 4413.86s
                               ETA: 1345405.1s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.117s, learning 0.187s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0584
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 27.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 11.30s
                        Total time: 4425.17s
                               ETA: 1344724.8s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.857s, learning 0.162s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 26.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 11.02s
                        Total time: 4436.19s
                               ETA: 1343962.3s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.115s, learning 0.314s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 26.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 11.43s
                        Total time: 4447.62s
                               ETA: 1343328.3s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.436s, learning 0.192s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0569
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 10.63s
                        Total time: 4458.24s
                               ETA: 1342456.7s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.938s, learning 0.171s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0575
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 11.11s
                        Total time: 4469.35s
                               ETA: 1341734.6s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.305s, learning 0.242s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0584
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 26.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 11.55s
                        Total time: 4480.90s
                               ETA: 1341147.9s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.712s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0606
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 10.87s
                        Total time: 4491.77s
                               ETA: 1340364.0s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.310s, learning 0.172s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 26.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 11.48s
                        Total time: 4503.26s
                               ETA: 1339765.5s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1565 steps/s (collection: 10.301s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0579
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 26.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 10.46s
                        Total time: 4513.72s
                               ETA: 1338868.4s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.166s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 11.33s
                        Total time: 4525.05s
                               ETA: 1338232.1s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.163s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 11.24s
                        Total time: 4536.28s
                               ETA: 1337573.3s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.723s, learning 0.227s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 10.95s
                        Total time: 4547.23s
                               ETA: 1336833.4s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.692s, learning 0.218s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 10.91s
                        Total time: 4558.15s
                               ETA: 1336086.3s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.398s, learning 0.178s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 10.58s
                        Total time: 4568.72s
                               ETA: 1335245.6s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.795s, learning 0.168s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0582
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 10.96s
                        Total time: 4579.68s
                               ETA: 1334522.9s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1463 steps/s (collection: 10.974s, learning 0.220s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0609
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 11.19s
                        Total time: 4590.88s
                               ETA: 1333871.2s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.353s, learning 0.197s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0600
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 24.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 10.55s
                        Total time: 4601.43s
                               ETA: 1333036.7s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.571s, learning 0.212s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0609
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 10.78s
                        Total time: 4612.21s
                               ETA: 1332274.3s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.167s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0612
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 10.72s
                        Total time: 4622.94s
                               ETA: 1331499.3s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.096s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0550
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 25.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 11.26s
                        Total time: 4634.20s
                               ETA: 1330882.2s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.556s, learning 0.266s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 24.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 10.82s
                        Total time: 4645.02s
                               ETA: 1330143.5s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.164s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0548
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 24.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 10.70s
                        Total time: 4655.72s
                               ETA: 1329373.9s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.554s, learning 0.196s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0536
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 10.75s
                        Total time: 4666.47s
                               ETA: 1328623.1s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.570s, learning 0.209s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 10.78s
                        Total time: 4677.25s
                               ETA: 1327884.6s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.070s, learning 0.159s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0551
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 11.23s
                        Total time: 4688.47s
                               ETA: 1327277.8s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.681s, learning 0.166s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0529
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 10.85s
                        Total time: 4699.32s
                               ETA: 1326566.6s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.719s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 10.89s
                        Total time: 4710.21s
                               ETA: 1325870.9s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.571s, learning 0.159s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0530
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 10.73s
                        Total time: 4720.94s
                               ETA: 1325134.7s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.914s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0572
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 11.07s
                        Total time: 4732.02s
                               ETA: 1324499.0s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.158s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0610
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 11.33s
                        Total time: 4743.35s
                               ETA: 1323938.5s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.159s, learning 0.212s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0611
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 11.37s
                        Total time: 4754.72s
                               ETA: 1323392.2s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.944s, learning 0.165s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0562
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 11.11s
                        Total time: 4765.83s
                               ETA: 1322775.8s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.234s, learning 0.284s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 11.52s
                        Total time: 4777.35s
                               ETA: 1322276.3s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.940s, learning 0.227s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0587
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 11.17s
                        Total time: 4788.51s
                               ETA: 1321682.3s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.167s, learning 0.158s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 11.33s
                        Total time: 4799.84s
                               ETA: 1321135.2s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.987s, learning 0.178s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0561
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 11.17s
                        Total time: 4811.00s
                               ETA: 1320547.2s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.886s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 11.05s
                        Total time: 4822.05s
                               ETA: 1319930.2s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.619s, learning 0.170s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 10.79s
                        Total time: 4832.84s
                               ETA: 1319245.9s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.126s, learning 0.263s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 11.39s
                        Total time: 4844.23s
                               ETA: 1318728.6s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.182s, learning 0.196s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0572
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 11.38s
                        Total time: 4855.61s
                               ETA: 1318210.9s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.163s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 10.91s
                        Total time: 4866.52s
                               ETA: 1317570.2s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.190s, learning 0.178s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 11.37s
                        Total time: 4877.89s
                               ETA: 1317055.8s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.967s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 11.14s
                        Total time: 4889.02s
                               ETA: 1316482.0s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.665s, learning 0.204s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 12.87s
                        Total time: 4901.89s
                               ETA: 1316376.2s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.609s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 21.78s
                        Total time: 4923.67s
                               ETA: 1318657.7s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.534s, learning 0.180s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 21.71s
                        Total time: 4945.39s
                               ETA: 1320909.1s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.877s, learning 0.248s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 22.12s
                        Total time: 4967.51s
                               ETA: 1323257.7s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.613s, learning 0.177s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0616
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 21.79s
                        Total time: 4989.30s
                               ETA: 1325504.5s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.264s, learning 0.204s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0588
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 21.47s
                        Total time: 5010.77s
                               ETA: 1327654.2s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 741 steps/s (collection: 21.897s, learning 0.191s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0561
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 22.09s
                        Total time: 5032.86s
                               ETA: 1329956.1s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 793 steps/s (collection: 20.467s, learning 0.174s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0587
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 20.64s
                        Total time: 5053.50s
                               ETA: 1331864.5s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.529s, learning 0.176s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0627
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 21.70s
                        Total time: 5075.20s
                               ETA: 1334042.2s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.486s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 21.65s
                        Total time: 5096.85s
                               ETA: 1336193.9s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.134s, learning 0.195s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0570
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 21.33s
                        Total time: 5118.18s
                               ETA: 1338250.3s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.313s, learning 0.208s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 21.52s
                        Total time: 5139.70s
                               ETA: 1340345.8s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 772 steps/s (collection: 20.970s, learning 0.225s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 21.20s
                        Total time: 5160.90s
                               ETA: 1342345.8s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.468s, learning 0.195s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0561
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 21.66s
                        Total time: 5182.56s
                               ETA: 1344456.2s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 780 steps/s (collection: 20.796s, learning 0.190s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 20.99s
                        Total time: 5203.55s
                               ETA: 1346380.6s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.773s, learning 0.171s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0606
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 21.94s
                        Total time: 5225.49s
                               ETA: 1348542.1s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.180s, learning 0.228s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 21.41s
                        Total time: 5246.90s
                               ETA: 1350554.4s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.135s, learning 0.271s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0570
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 21.41s
                        Total time: 5268.30s
                               ETA: 1352555.7s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.975s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 22.14s
                        Total time: 5290.44s
                               ETA: 1354734.2s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.800s, learning 0.171s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0574
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 21.97s
                        Total time: 5312.41s
                               ETA: 1356858.8s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.383s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 21.55s
                        Total time: 5333.96s
                               ETA: 1358865.0s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.372s, learning 0.197s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0595
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 21.57s
                        Total time: 5355.53s
                               ETA: 1360865.6s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.248s, learning 0.174s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 21.42s
                        Total time: 5376.95s
                               ETA: 1362818.6s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.732s, learning 0.162s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 21.89s
                        Total time: 5398.85s
                               ETA: 1364881.0s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.024s, learning 0.325s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0591
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 21.35s
                        Total time: 5420.20s
                               ETA: 1366795.4s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.270s, learning 0.197s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0579
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 21.47s
                        Total time: 5441.66s
                               ETA: 1368729.6s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 781 steps/s (collection: 20.774s, learning 0.180s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0580
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 20.95s
                        Total time: 5462.62s
                               ETA: 1370525.3s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.937s, learning 0.227s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0557
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 21.16s
                        Total time: 5483.78s
                               ETA: 1372364.4s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.229s, learning 0.192s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 21.42s
                        Total time: 5505.20s
                               ETA: 1374258.5s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.899s, learning 0.168s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0556
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 21.07s
                        Total time: 5526.27s
                               ETA: 1376054.8s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 782 steps/s (collection: 20.745s, learning 0.201s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0504
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 20.95s
                        Total time: 5547.21s
                               ETA: 1377812.0s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.925s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0523
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 21.10s
                        Total time: 5568.31s
                               ETA: 1379597.6s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.242s, learning 0.211s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0588
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 21.45s
                        Total time: 5589.76s
                               ETA: 1381462.4s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.773s, learning 0.180s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0576
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 21.95s
                        Total time: 5611.72s
                               ETA: 1383441.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.059s, learning 0.157s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 21.22s
                        Total time: 5632.93s
                               ETA: 1385228.6s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 784 steps/s (collection: 20.733s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 20.90s
                        Total time: 5653.83s
                               ETA: 1386929.2s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.091s, learning 0.183s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 21.27s
                        Total time: 5675.10s
                               ETA: 1388713.2s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.098s, learning 0.199s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 21.30s
                        Total time: 5696.40s
                               ETA: 1390494.3s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.906s, learning 0.188s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0550
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 16.09s
                        Total time: 5712.50s
                               ETA: 1390999.5s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.562s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 10.74s
                        Total time: 5723.23s
                               ETA: 1390200.7s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.865s, learning 0.192s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 11.06s
                        Total time: 5734.29s
                               ETA: 1389483.6s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1456 steps/s (collection: 10.929s, learning 0.316s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 11.25s
                        Total time: 5745.53s
                               ETA: 1388815.4s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.147s, learning 0.184s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 11.33s
                        Total time: 5756.86s
                               ETA: 1388171.1s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.475s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0550
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 11.64s
                        Total time: 5768.51s
                               ETA: 1387604.7s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.071s, learning 0.241s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 11.31s
                        Total time: 5779.82s
                               ETA: 1386961.7s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 11.13s
                        Total time: 5790.95s
                               ETA: 1386277.4s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.613s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 10.77s
                        Total time: 5801.72s
                               ETA: 1385512.2s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.761s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 19.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 10.93s
                        Total time: 5812.65s
                               ETA: 1384787.1s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.721s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0568
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 10.89s
                        Total time: 5823.54s
                               ETA: 1384057.6s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.750s, learning 0.194s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 10.94s
                        Total time: 5834.49s
                               ETA: 1383343.3s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0615
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 11.23s
                        Total time: 5845.72s
                               ETA: 1382699.9s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0576
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 11.15s
                        Total time: 5856.87s
                               ETA: 1382040.8s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.756s, learning 0.203s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0562
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 10.96s
                        Total time: 5867.83s
                               ETA: 1381339.6s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.910s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0530
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 11.09s
                        Total time: 5878.92s
                               ETA: 1380671.8s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.644s, learning 0.259s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0572
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 10.90s
                        Total time: 5889.82s
                               ETA: 1379963.9s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.998s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0609
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 11.18s
                        Total time: 5901.00s
                               ETA: 1379323.1s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.981s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0551
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 11.15s
                        Total time: 5912.14s
                               ETA: 1378678.3s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1453 steps/s (collection: 10.938s, learning 0.332s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 20.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 11.27s
                        Total time: 5923.41s
                               ETA: 1378065.2s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.134s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 11.30s
                        Total time: 5934.71s
                               ETA: 1377461.6s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.726s, learning 0.177s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 10.90s
                        Total time: 5945.61s
                               ETA: 1376769.2s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.646s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0514
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 10.81s
                        Total time: 5956.43s
                               ETA: 1376059.1s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.802s, learning 0.161s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0605
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 10.96s
                        Total time: 5967.39s
                               ETA: 1375386.8s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.999s, learning 0.168s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 11.17s
                        Total time: 5978.56s
                               ETA: 1374764.4s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.858s, learning 0.159s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 11.02s
                        Total time: 5989.57s
                               ETA: 1374110.5s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.954s, learning 0.207s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 11.16s
                        Total time: 6000.74s
                               ETA: 1373492.5s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.768s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 10.95s
                        Total time: 6011.68s
                               ETA: 1372828.3s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1562 steps/s (collection: 10.325s, learning 0.159s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 10.48s
                        Total time: 6022.17s
                               ETA: 1372061.7s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.125s, learning 0.254s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 11.38s
                        Total time: 6033.55s
                               ETA: 1371502.0s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.065s, learning 0.214s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 11.28s
                        Total time: 6044.82s
                               ETA: 1370922.1s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.683s, learning 0.160s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0557
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 10.84s
                        Total time: 6055.67s
                               ETA: 1370246.1s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.470s, learning 0.220s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0547
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 10.69s
                        Total time: 6066.36s
                               ETA: 1369538.6s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.840s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0621
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 11.01s
                        Total time: 6077.36s
                               ETA: 1368905.6s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.394s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0629
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 10.56s
                        Total time: 6087.92s
                               ETA: 1368174.1s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.007s, learning 0.182s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0640
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 11.19s
                        Total time: 6099.11s
                               ETA: 1367587.8s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.868s, learning 0.182s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 11.05s
                        Total time: 6110.16s
                               ETA: 1366973.0s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.284s, learning 0.258s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 11.54s
                        Total time: 6121.70s
                               ETA: 1366470.7s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 10.85s
                        Total time: 6132.56s
                               ETA: 1365817.4s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.717s, learning 0.183s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0605
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 21.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 10.90s
                        Total time: 6143.46s
                               ETA: 1365177.3s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.852s, learning 0.184s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0654
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 11.04s
                        Total time: 6154.49s
                               ETA: 1364570.1s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.066s, learning 0.187s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 11.25s
                        Total time: 6165.74s
                               ETA: 1364013.4s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0624
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 11.08s
                        Total time: 6176.82s
                               ETA: 1363420.3s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.237s, learning 0.321s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0630
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 12.56s
                        Total time: 6189.38s
                               ETA: 1363155.9s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.403s, learning 0.193s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 21.60s
                        Total time: 6210.97s
                               ETA: 1364878.8s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.921s, learning 0.175s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0638
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 21.10s
                        Total time: 6232.07s
                               ETA: 1366484.4s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.287s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0571
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 21.46s
                        Total time: 6253.53s
                               ETA: 1368162.3s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 770 steps/s (collection: 20.990s, learning 0.262s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0603
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 21.25s
                        Total time: 6274.78s
                               ETA: 1369787.6s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.103s, learning 0.182s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 21.29s
                        Total time: 6296.07s
                               ETA: 1371413.0s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 779 steps/s (collection: 20.839s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0602
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 21.01s
                        Total time: 6317.07s
                               ETA: 1372970.5s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.315s, learning 0.192s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0624
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 22.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 21.51s
                        Total time: 6338.58s
                               ETA: 1374629.6s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.613s, learning 0.182s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 21.80s
                        Total time: 6360.38s
                               ETA: 1376343.9s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.237s, learning 0.179s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0597
             Mean action noise std: 0.79
                       Mean reward: 0.00
               Mean episode length: 23.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 21.42s
                        Total time: 6381.79s
                               ETA: 1377968.7s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.590s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 23.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 21.75s
                        Total time: 6403.55s
                               ETA: 1379659.4s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.695s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0651
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 21.87s
                        Total time: 6425.42s
                               ETA: 1381368.2s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.747s, learning 0.193s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0578
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 21.94s
                        Total time: 6447.36s
                               ETA: 1383083.8s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.359s, learning 0.202s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0637
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 21.56s
                        Total time: 6468.92s
                               ETA: 1384710.9s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 771 steps/s (collection: 21.036s, learning 0.209s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0634
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 21.24s
                        Total time: 6490.17s
                               ETA: 1386263.3s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.458s, learning 0.193s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 21.65s
                        Total time: 6511.82s
                               ETA: 1387895.6s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.275s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 21.44s
                        Total time: 6533.25s
                               ETA: 1389475.2s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.325s, learning 0.175s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 21.50s
                        Total time: 6554.75s
                               ETA: 1391061.6s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.398s, learning 0.171s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0520
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 21.57s
                        Total time: 6576.32s
                               ETA: 1392655.4s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 750 steps/s (collection: 21.626s, learning 0.193s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 21.82s
                        Total time: 6598.14s
                               ETA: 1394295.4s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.313s, learning 0.198s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0506
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 21.51s
                        Total time: 6619.65s
                               ETA: 1395863.3s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.289s, learning 0.189s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 21.48s
                        Total time: 6641.13s
                               ETA: 1397417.8s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.706s, learning 0.181s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0628
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 21.89s
                        Total time: 6663.02s
                               ETA: 1399051.4s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.447s, learning 0.166s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 21.61s
                        Total time: 6684.63s
                               ETA: 1400620.6s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.501s, learning 0.183s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 21.68s
                        Total time: 6706.32s
                               ETA: 1402197.8s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.520s, learning 0.170s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 21.69s
                        Total time: 6728.01s
                               ETA: 1403769.7s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.094s, learning 0.211s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 21.30s
                        Total time: 6749.31s
                               ETA: 1405254.7s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.302s, learning 0.179s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 21.48s
                        Total time: 6770.79s
                               ETA: 1406769.9s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.299s, learning 0.180s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 21.48s
                        Total time: 6792.27s
                               ETA: 1408278.3s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.972s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 21.14s
                        Total time: 6813.41s
                               ETA: 1409709.6s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 739 steps/s (collection: 21.978s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0579
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 22.14s
                        Total time: 6835.55s
                               ETA: 1411342.7s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.464s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0614
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 21.66s
                        Total time: 6857.21s
                               ETA: 1412869.6s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.565s, learning 0.203s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 21.77s
                        Total time: 6878.98s
                               ETA: 1414412.1s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.387s, learning 0.240s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0603
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 21.63s
                        Total time: 6900.61s
                               ETA: 1415919.1s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.413s, learning 0.199s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0604
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 21.61s
                        Total time: 6922.22s
                               ETA: 1417416.9s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.824s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 21.99s
                        Total time: 6944.21s
                               ETA: 1418985.9s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.507s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 21.67s
                        Total time: 6965.88s
                               ETA: 1420482.7s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.623s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 21.78s
                        Total time: 6987.66s
                               ETA: 1421996.6s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.216s, learning 0.271s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 16.49s
                        Total time: 7004.15s
                               ETA: 1422428.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.856s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 11.02s
                        Total time: 7015.17s
                               ETA: 1421750.6s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.118s, learning 0.183s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0629
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 11.30s
                        Total time: 7026.47s
                               ETA: 1421132.3s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.985s, learning 0.182s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0607
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 11.17s
                        Total time: 7037.64s
                               ETA: 1420489.5s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.493s, learning 0.189s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 10.68s
                        Total time: 7048.32s
                               ETA: 1419751.5s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.651s, learning 0.187s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0560
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 10.84s
                        Total time: 7059.16s
                               ETA: 1419047.6s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.000s, learning 0.170s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0611
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 11.17s
                        Total time: 7070.33s
                               ETA: 1418413.4s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1466 steps/s (collection: 10.993s, learning 0.182s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0562
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 11.18s
                        Total time: 7081.50s
                               ETA: 1417782.7s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.842s, learning 0.199s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 11.04s
                        Total time: 7092.55s
                               ETA: 1417127.6s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.003s, learning 0.172s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0598
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 11.17s
                        Total time: 7103.72s
                               ETA: 1416501.8s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.939s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 11.12s
                        Total time: 7114.84s
                               ETA: 1415867.1s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.679s, learning 0.175s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0546
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 10.85s
                        Total time: 7125.69s
                               ETA: 1415182.5s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.842s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 11.00s
                        Total time: 7136.69s
                               ETA: 1414529.8s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.784s, learning 0.185s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0577
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 10.97s
                        Total time: 7147.66s
                               ETA: 1413873.2s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.759s, learning 0.183s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0586
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 10.94s
                        Total time: 7158.61s
                               ETA: 1413213.8s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.483s, learning 0.183s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0535
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 10.67s
                        Total time: 7169.27s
                               ETA: 1412502.5s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1462 steps/s (collection: 10.991s, learning 0.215s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0586
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 11.21s
                        Total time: 7180.48s
                               ETA: 1411900.2s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.983s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0535
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 11.14s
                        Total time: 7191.62s
                               ETA: 1411288.3s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.844s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0593
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 11.01s
                        Total time: 7202.63s
                               ETA: 1410652.9s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.927s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0558
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 11.10s
                        Total time: 7213.73s
                               ETA: 1410036.9s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.849s, learning 0.211s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 11.06s
                        Total time: 7224.79s
                               ETA: 1409415.6s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.125s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0504
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 11.30s
                        Total time: 7236.09s
                               ETA: 1408843.1s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.600s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 10.77s
                        Total time: 7246.86s
                               ETA: 1408170.4s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.414s, learning 0.243s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0540
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 11.66s
                        Total time: 7258.52s
                               ETA: 1407672.0s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.732s, learning 0.185s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0591
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 11.92s
                        Total time: 7270.44s
                               ETA: 1407225.8s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.028s, learning 0.187s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0554
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 11.21s
                        Total time: 7281.65s
                               ETA: 1406645.6s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.467s, learning 0.238s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0581
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 10.70s
                        Total time: 7292.36s
                               ETA: 1405969.3s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.293s, learning 0.189s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 11.48s
                        Total time: 7303.84s
                               ETA: 1405445.1s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.713s, learning 0.180s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 10.89s
                        Total time: 7314.73s
                               ETA: 1404809.8s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.697s, learning 0.190s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0566
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 10.89s
                        Total time: 7325.62s
                               ETA: 1404175.8s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.069s, learning 0.190s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0545
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 11.26s
                        Total time: 7336.88s
                               ETA: 1403615.3s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.831s, learning 0.193s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0612
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 11.02s
                        Total time: 7347.90s
                               ETA: 1403012.1s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.634s, learning 0.178s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 10.81s
                        Total time: 7358.71s
                               ETA: 1402370.6s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.030s, learning 0.197s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 11.23s
                        Total time: 7369.94s
                               ETA: 1401810.6s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.915s, learning 0.187s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0602
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 11.10s
                        Total time: 7381.04s
                               ETA: 1401229.0s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.422s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 12.58s
                        Total time: 7393.63s
                               ETA: 1400930.3s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.486s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 21.66s
                        Total time: 7415.29s
                               ETA: 1402349.3s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 784 steps/s (collection: 20.723s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 20.89s
                        Total time: 7436.18s
                               ETA: 1403617.5s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.352s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0565
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 21.52s
                        Total time: 7457.70s
                               ETA: 1404999.5s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.061s, learning 0.193s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0520
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 21.25s
                        Total time: 7478.95s
                               ETA: 1406326.0s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.884s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 21.06s
                        Total time: 7500.01s
                               ETA: 1407610.2s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.425s, learning 0.171s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 21.60s
                        Total time: 7521.61s
                               ETA: 1408990.7s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.302s, learning 0.172s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0590
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 21.47s
                        Total time: 7543.08s
                               ETA: 1410343.1s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.966s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 21.14s
                        Total time: 7564.22s
                               ETA: 1411627.8s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.560s, learning 0.192s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0636
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 21.75s
                        Total time: 7585.97s
                               ETA: 1413021.8s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 781 steps/s (collection: 20.796s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0571
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 20.96s
                        Total time: 7606.93s
                               ETA: 1414263.4s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.175s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 21.35s
                        Total time: 7628.28s
                               ETA: 1415573.1s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.276s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0515
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 21.44s
                        Total time: 7649.73s
                               ETA: 1416894.8s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.111s, learning 0.192s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0635
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 21.30s
                        Total time: 7671.03s
                               ETA: 1418185.4s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.248s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0573
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 21.41s
                        Total time: 7692.44s
                               ETA: 1419490.8s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.685s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 21.85s
                        Total time: 7714.29s
                               ETA: 1420872.0s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.430s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0571
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 21.60s
                        Total time: 7735.89s
                               ETA: 1422201.9s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 771 steps/s (collection: 21.069s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0571
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 21.23s
                        Total time: 7757.12s
                               ETA: 1423459.9s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.915s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 21.08s
                        Total time: 7778.20s
                               ETA: 1424685.7s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.111s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 21.28s
                        Total time: 7799.48s
                               ETA: 1425942.9s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.643s, learning 0.225s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 21.87s
                        Total time: 7821.35s
                               ETA: 1427302.8s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.073s, learning 0.193s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0577
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 21.27s
                        Total time: 7842.62s
                               ETA: 1428548.1s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.479s, learning 0.177s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 21.66s
                        Total time: 7864.27s
                               ETA: 1429859.7s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.289s, learning 0.182s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 21.47s
                        Total time: 7885.74s
                               ETA: 1431132.9s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.179s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0629
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 21.35s
                        Total time: 7907.09s
                               ETA: 1432378.5s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 780 steps/s (collection: 20.839s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 21.00s
                        Total time: 7928.09s
                               ETA: 1433557.3s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.400s, learning 0.198s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 21.60s
                        Total time: 7949.69s
                               ETA: 1434839.3s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.634s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0593
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 21.80s
                        Total time: 7971.49s
                               ETA: 1436152.6s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.201s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 21.37s
                        Total time: 7992.85s
                               ETA: 1437384.0s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.617s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 21.78s
                        Total time: 8014.63s
                               ETA: 1438684.8s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.292s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0581
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 21.46s
                        Total time: 8036.10s
                               ETA: 1439923.4s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.446s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 21.62s
                        Total time: 8057.71s
                               ETA: 1441185.5s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.284s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0631
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 21.45s
                        Total time: 8079.16s
                               ETA: 1442413.2s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.350s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 21.55s
                        Total time: 8100.71s
                               ETA: 1443653.7s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.747s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 21.92s
                        Total time: 8122.63s
                               ETA: 1444956.2s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.715s, learning 0.187s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 21.90s
                        Total time: 8144.53s
                               ETA: 1446250.8s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 744 steps/s (collection: 21.859s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0580
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 22.02s
                        Total time: 8166.55s
                               ETA: 1447561.6s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.905s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 21.08s
                        Total time: 8187.63s
                               ETA: 1448701.0s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.340s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0593
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 16.51s
                        Total time: 8204.14s
                               ETA: 1449029.4s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.606s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0619
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 10.77s
                        Total time: 8214.91s
                               ETA: 1448344.1s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.801s, learning 0.212s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0621
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 11.01s
                        Total time: 8225.92s
                               ETA: 1447704.3s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.185s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0606
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 11.36s
                        Total time: 8237.28s
                               ETA: 1447127.0s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.754s, learning 0.184s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0541
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 10.94s
                        Total time: 8248.22s
                               ETA: 1446478.4s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.943s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0578
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 11.11s
                        Total time: 8259.33s
                               ETA: 1445861.9s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.073s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0578
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 11.23s
                        Total time: 8270.56s
                               ETA: 1445269.6s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.318s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 11.48s
                        Total time: 8282.04s
                               ETA: 1444722.6s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.606s, learning 0.194s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0585
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 10.80s
                        Total time: 8292.84s
                               ETA: 1444058.6s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.634s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 10.81s
                        Total time: 8303.65s
                               ETA: 1443397.8s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.431s, learning 0.186s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0582
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 11.62s
                        Total time: 8315.27s
                               ETA: 1442880.0s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.655s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0602
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 10.82s
                        Total time: 8326.09s
                               ETA: 1442226.8s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.777s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 10.94s
                        Total time: 8337.04s
                               ETA: 1441596.6s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.804s, learning 0.190s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 10.99s
                        Total time: 8348.03s
                               ETA: 1440977.1s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.526s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0507
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 10.70s
                        Total time: 8358.73s
                               ETA: 1440308.9s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1464 steps/s (collection: 10.980s, learning 0.204s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 11.18s
                        Total time: 8369.91s
                               ETA: 1439726.4s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.831s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0554
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 11.01s
                        Total time: 8380.92s
                               ETA: 1439115.4s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.554s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 10.72s
                        Total time: 8391.64s
                               ETA: 1438457.7s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.538s, learning 0.175s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0505
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 10.71s
                        Total time: 8402.36s
                               ETA: 1437800.6s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.962s, learning 0.177s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0633
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 11.14s
                        Total time: 8413.49s
                               ETA: 1437218.6s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1571 steps/s (collection: 10.227s, learning 0.195s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 10.42s
                        Total time: 8423.92s
                               ETA: 1436516.2s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.880s, learning 0.177s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 11.06s
                        Total time: 8434.97s
                               ETA: 1435924.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.587s, learning 0.198s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0630
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 10.79s
                        Total time: 8445.76s
                               ETA: 1435288.2s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.318s, learning 0.203s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0623
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 11.52s
                        Total time: 8457.28s
                               ETA: 1434779.0s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.297s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 11.48s
                        Total time: 8468.76s
                               ETA: 1434263.9s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.057s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 11.22s
                        Total time: 8479.98s
                               ETA: 1433708.0s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.575s, learning 0.221s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0630
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 10.80s
                        Total time: 8490.78s
                               ETA: 1433081.7s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.869s, learning 0.162s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0649
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 11.03s
                        Total time: 8501.81s
                               ETA: 1432497.0s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.905s, learning 0.161s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0647
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 11.07s
                        Total time: 8512.87s
                               ETA: 1431920.1s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.005s, learning 0.166s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 11.17s
                        Total time: 8524.04s
                               ETA: 1431362.8s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.615s, learning 0.166s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0645
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 10.78s
                        Total time: 8534.83s
                               ETA: 1430742.0s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.209s, learning 0.179s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0582
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 11.39s
                        Total time: 8546.21s
                               ETA: 1430224.8s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.168s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0649
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 11.03s
                        Total time: 8557.25s
                               ETA: 1429649.8s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.112s, learning 0.200s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0531
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 11.31s
                        Total time: 8568.56s
                               ETA: 1429123.3s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.152s, learning 0.187s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 11.34s
                        Total time: 8579.90s
                               ETA: 1428603.0s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.200s, learning 0.192s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0528
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 11.39s
                        Total time: 8591.29s
                               ETA: 1428093.4s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.627s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 12.79s
                        Total time: 8604.08s
                               ETA: 1427818.0s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.491s, learning 0.180s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0581
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 21.67s
                        Total time: 8625.75s
                               ETA: 1429014.1s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.410s, learning 0.197s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0552
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 21.61s
                        Total time: 8647.36s
                               ETA: 1430195.6s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.331s, learning 0.199s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0587
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 21.53s
                        Total time: 8668.89s
                               ETA: 1431360.4s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.791s, learning 0.186s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0542
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 21.98s
                        Total time: 8690.87s
                               ETA: 1432595.0s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 783 steps/s (collection: 20.725s, learning 0.179s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 20.90s
                        Total time: 8711.77s
                               ETA: 1433648.9s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.216s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0554
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 21.39s
                        Total time: 8733.16s
                               ETA: 1434779.3s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.254s, learning 0.201s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0566
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 21.46s
                        Total time: 8754.62s
                               ETA: 1435916.4s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.492s, learning 0.261s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 21.75s
                        Total time: 8776.37s
                               ETA: 1437098.3s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 750 steps/s (collection: 21.619s, learning 0.210s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0523
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 21.83s
                        Total time: 8798.20s
                               ETA: 1438288.8s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.903s, learning 0.214s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0564
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 22.12s
                        Total time: 8820.32s
                               ETA: 1439522.2s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.225s, learning 0.262s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0547
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 21.49s
                        Total time: 8841.80s
                               ETA: 1440648.8s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.771s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 21.94s
                        Total time: 8863.75s
                               ETA: 1441845.8s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.461s, learning 0.191s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0585
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 21.65s
                        Total time: 8885.40s
                               ETA: 1442991.8s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 735 steps/s (collection: 22.101s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 22.27s
                        Total time: 8907.67s
                               ETA: 1444234.5s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.113s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0548
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 21.27s
                        Total time: 8928.95s
                               ETA: 1445311.5s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.743s, learning 0.160s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 21.90s
                        Total time: 8950.85s
                               ETA: 1446486.5s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.377s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 21.55s
                        Total time: 8972.40s
                               ETA: 1447601.1s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 773 steps/s (collection: 21.004s, learning 0.182s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0554
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 21.19s
                        Total time: 8993.59s
                               ETA: 1448652.9s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.688s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 21.85s
                        Total time: 9015.44s
                               ETA: 1449807.9s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.991s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 21.16s
                        Total time: 9036.59s
                               ETA: 1450847.9s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.088s, learning 0.194s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 21.28s
                        Total time: 9057.88s
                               ETA: 1451904.4s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 773 steps/s (collection: 20.888s, learning 0.306s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0496
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 21.19s
                        Total time: 9079.07s
                               ETA: 1452943.6s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.521s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 21.69s
                        Total time: 9100.76s
                               ETA: 1454058.4s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 735 steps/s (collection: 22.030s, learning 0.252s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 22.28s
                        Total time: 9123.04s
                               ETA: 1455264.1s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.605s, learning 0.160s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0558
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 21.76s
                        Total time: 9144.81s
                               ETA: 1456383.5s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.248s, learning 0.298s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 21.55s
                        Total time: 9166.35s
                               ETA: 1457464.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.151s, learning 0.178s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0581
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 21.33s
                        Total time: 9187.68s
                               ETA: 1458507.4s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.093s, learning 0.270s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0545
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 21.36s
                        Total time: 9209.04s
                               ETA: 1459552.5s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.781s, learning 0.176s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0585
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 21.96s
                        Total time: 9231.00s
                               ETA: 1460688.1s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.923s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0556
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 21.10s
                        Total time: 9252.10s
                               ETA: 1461684.9s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.429s, learning 0.225s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 21.65s
                        Total time: 9273.75s
                               ETA: 1462765.5s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.115s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 21.29s
                        Total time: 9295.04s
                               ETA: 1463784.7s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.376s, learning 0.195s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 21.57s
                        Total time: 9316.61s
                               ETA: 1464845.4s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.788s, learning 0.190s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 21.98s
                        Total time: 9338.59s
                               ETA: 1465966.6s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.622s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 21.79s
                        Total time: 9360.38s
                               ETA: 1467055.0s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.239s, learning 0.208s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 21.45s
                        Total time: 9381.83s
                               ETA: 1468086.1s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.581s, learning 0.198s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0508
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 21.78s
                        Total time: 9403.61s
                               ETA: 1469165.7s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.359s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 16.53s
                        Total time: 9420.14s
                               ETA: 1469423.4s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.026s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0608
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 11.20s
                        Total time: 9431.34s
                               ETA: 1468849.7s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.114s, learning 0.219s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 11.33s
                        Total time: 9442.67s
                               ETA: 1468298.4s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.802s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 10.97s
                        Total time: 9453.64s
                               ETA: 1467693.0s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.886s, learning 0.254s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 11.14s
                        Total time: 9464.78s
                               ETA: 1467115.3s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.604s, learning 0.184s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0544
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 10.79s
                        Total time: 9475.57s
                               ETA: 1466485.0s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.054s, learning 0.181s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 11.24s
                        Total time: 9486.81s
                               ETA: 1465925.7s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.035s, learning 0.330s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 11.36s
                        Total time: 9498.17s
                               ETA: 1465388.1s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.603s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0624
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 10.78s
                        Total time: 9508.95s
                               ETA: 1464761.4s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.163s, learning 0.234s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0544
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 11.40s
                        Total time: 9520.35s
                               ETA: 1464232.0s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.827s, learning 0.335s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0568
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 11.16s
                        Total time: 9531.51s
                               ETA: 1463668.3s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.809s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 10.97s
                        Total time: 9542.48s
                               ETA: 1463077.2s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.884s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0641
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 11.05s
                        Total time: 9553.53s
                               ETA: 1462500.0s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.139s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 11.31s
                        Total time: 9564.84s
                               ETA: 1461964.3s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.018s, learning 0.200s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 11.22s
                        Total time: 9576.06s
                               ETA: 1461415.9s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.620s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0590
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 10.78s
                        Total time: 9586.84s
                               ETA: 1460802.4s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.440s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0621
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 10.61s
                        Total time: 9597.45s
                               ETA: 1460164.9s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.828s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 10.99s
                        Total time: 9608.44s
                               ETA: 1459586.9s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.804s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0636
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 10.97s
                        Total time: 9619.41s
                               ETA: 1459008.1s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.444s, learning 0.160s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0582
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 10.60s
                        Total time: 9630.02s
                               ETA: 1458375.2s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.253s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0608
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 11.42s
                        Total time: 9641.44s
                               ETA: 1457867.9s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.917s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 11.08s
                        Total time: 9652.52s
                               ETA: 1457310.6s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.158s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0630
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 10.77s
                        Total time: 9663.29s
                               ETA: 1456708.4s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.207s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0619
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 11.38s
                        Total time: 9674.67s
                               ETA: 1456199.5s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.248s, learning 0.175s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 11.42s
                        Total time: 9686.10s
                               ETA: 1455698.6s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1152 steps/s (collection: 14.041s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 14.21s
                        Total time: 9700.31s
                               ETA: 1455617.8s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.792s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0597
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 21.96s
                        Total time: 9722.27s
                               ETA: 1456697.6s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.046s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0608
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 21.22s
                        Total time: 9743.48s
                               ETA: 1457663.3s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.345s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0633
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 21.51s
                        Total time: 9764.99s
                               ETA: 1458669.7s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.193s, learning 0.196s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0623
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 21.39s
                        Total time: 9786.38s
                               ETA: 1459655.0s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.264s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 21.44s
                        Total time: 9807.82s
                               ETA: 1460644.4s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.373s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0654
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 21.55s
                        Total time: 9829.36s
                               ETA: 1461647.2s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.395s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0552
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 21.59s
                        Total time: 9850.96s
                               ETA: 1462653.5s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.726s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0635
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 21.90s
                        Total time: 9872.86s
                               ETA: 1463702.4s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 768 steps/s (collection: 20.996s, learning 0.316s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 21.31s
                        Total time: 9894.17s
                               ETA: 1464661.3s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.406s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 21.58s
                        Total time: 9915.74s
                               ETA: 1465656.3s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 798 steps/s (collection: 20.334s, learning 0.184s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0593
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 20.52s
                        Total time: 9936.26s
                               ETA: 1466491.9s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 773 steps/s (collection: 20.983s, learning 0.204s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 21.19s
                        Total time: 9957.45s
                               ETA: 1467423.8s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.033s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0627
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 21.21s
                        Total time: 9978.66s
                               ETA: 1468355.5s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.886s, learning 0.247s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0620
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 21.13s
                        Total time: 9999.79s
                               ETA: 1469273.7s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.181s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0612
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 21.34s
                        Total time: 10021.13s
                               ETA: 1470220.1s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.149s, learning 0.198s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0593
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 21.35s
                        Total time: 10042.48s
                               ETA: 1471164.1s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.203s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 21.37s
                        Total time: 10063.85s
                               ETA: 1472107.9s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.946s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0547
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 21.11s
                        Total time: 10084.96s
                               ETA: 1473011.9s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.457s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0624
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 21.63s
                        Total time: 10106.59s
                               ETA: 1473988.6s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.564s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 21.73s
                        Total time: 10128.31s
                               ETA: 1474976.4s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 773 steps/s (collection: 21.024s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 21.19s
                        Total time: 10149.51s
                               ETA: 1475883.8s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.883s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 21.05s
                        Total time: 10170.56s
                               ETA: 1476768.4s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 782 steps/s (collection: 20.685s, learning 0.262s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 20.95s
                        Total time: 10191.51s
                               ETA: 1477634.6s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 741 steps/s (collection: 21.898s, learning 0.213s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0533
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 22.11s
                        Total time: 10213.62s
                               ETA: 1478666.7s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.955s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0608
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 22.13s
                        Total time: 10235.75s
                               ETA: 1479698.5s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 795 steps/s (collection: 20.429s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 20.60s
                        Total time: 10256.35s
                               ETA: 1480506.7s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.451s, learning 0.234s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 21.68s
                        Total time: 10278.03s
                               ETA: 1481468.7s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.647s, learning 0.266s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0609
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 21.91s
                        Total time: 10299.94s
                               ETA: 1482460.6s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.788s, learning 0.157s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 21.95s
                        Total time: 10321.89s
                               ETA: 1483454.3s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.818s, learning 0.158s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0611
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 21.98s
                        Total time: 10343.87s
                               ETA: 1484449.5s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.224s, learning 0.239s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 21.46s
                        Total time: 10365.33s
                               ETA: 1485368.2s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.698s, learning 0.237s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 21.93s
                        Total time: 10387.26s
                               ETA: 1486351.7s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.757s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 21.93s
                        Total time: 10409.19s
                               ETA: 1487331.1s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.500s, learning 0.264s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 21.76s
                        Total time: 10430.95s
                               ETA: 1488284.3s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.700s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 21.87s
                        Total time: 10452.83s
                               ETA: 1489250.4s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 754 steps/s (collection: 21.520s, learning 0.203s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0616
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 21.72s
                        Total time: 10474.55s
                               ETA: 1490192.4s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.903s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0618
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 21.07s
                        Total time: 10495.62s
                               ETA: 1491038.9s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1063 steps/s (collection: 15.244s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 15.41s
                        Total time: 10511.03s
                               ETA: 1491079.5s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.865s, learning 0.180s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0602
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 11.04s
                        Total time: 10522.07s
                               ETA: 1490502.0s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.030s, learning 0.195s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 11.23s
                        Total time: 10533.30s
                               ETA: 1489951.6s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.952s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0607
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 11.11s
                        Total time: 10544.41s
                               ETA: 1489387.0s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.259s, learning 0.185s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 11.44s
                        Total time: 10555.86s
                               ETA: 1488870.5s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.113s, learning 0.203s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0586
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 11.32s
                        Total time: 10567.17s
                               ETA: 1488337.4s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0607
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 11.12s
                        Total time: 10578.29s
                               ETA: 1487777.9s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.597s, learning 0.199s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0588
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 10.80s
                        Total time: 10589.08s
                               ETA: 1487174.8s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.855s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 11.02s
                        Total time: 10600.10s
                               ETA: 1486604.6s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.840s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0526
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 11.01s
                        Total time: 10611.11s
                               ETA: 1486034.7s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.131s, learning 0.242s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 11.37s
                        Total time: 10622.49s
                               ETA: 1485517.3s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.127s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0525
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 11.30s
                        Total time: 10633.78s
                               ETA: 1484990.4s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.516s, learning 0.159s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 10.68s
                        Total time: 10644.46s
                               ETA: 1484378.5s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.627s, learning 0.181s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0609
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 10.81s
                        Total time: 10655.27s
                               ETA: 1483786.8s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.782s, learning 0.191s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 10.97s
                        Total time: 10666.24s
                               ETA: 1483219.5s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.424s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0624
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 10.59s
                        Total time: 10676.83s
                               ETA: 1482601.1s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.942s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0524
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 11.11s
                        Total time: 10687.94s
                               ETA: 1482056.6s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.871s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 11.03s
                        Total time: 10698.98s
                               ETA: 1481502.6s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.880s, learning 0.191s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 11.07s
                        Total time: 10710.05s
                               ETA: 1480955.1s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.037s, learning 0.211s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0582
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 11.25s
                        Total time: 10721.30s
                               ETA: 1480433.7s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.243s, learning 0.195s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0621
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 11.44s
                        Total time: 10732.73s
                               ETA: 1479939.8s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.537s, learning 0.242s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0557
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 11.78s
                        Total time: 10744.51s
                               ETA: 1479494.2s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.816s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0606
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 10.98s
                        Total time: 10755.49s
                               ETA: 1478939.6s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.586s, learning 0.213s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0554
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 10.80s
                        Total time: 10766.29s
                               ETA: 1478362.1s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 11.30s
                        Total time: 10777.59s
                               ETA: 1477855.3s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.524s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 10.69s
                        Total time: 10788.28s
                               ETA: 1477265.1s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.152s, learning 0.189s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0587
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 11.34s
                        Total time: 10799.62s
                               ETA: 1476766.2s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.380s, learning 0.255s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0581
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 12.63s
                        Total time: 10812.25s
                               ETA: 1476445.3s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.290s, learning 0.159s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 21.45s
                        Total time: 10833.70s
                               ETA: 1477327.2s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.512s, learning 0.222s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0587
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 21.73s
                        Total time: 10855.44s
                               ETA: 1478245.5s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.402s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 21.57s
                        Total time: 10877.01s
                               ETA: 1479139.1s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.560s, learning 0.177s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0557
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 21.74s
                        Total time: 10898.75s
                               ETA: 1480052.6s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.169s, learning 0.181s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0580
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 21.35s
                        Total time: 10920.10s
                               ETA: 1480911.2s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.493s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 21.66s
                        Total time: 10941.76s
                               ETA: 1481809.3s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.279s, learning 0.190s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 21.47s
                        Total time: 10963.23s
                               ETA: 1482679.2s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 773 steps/s (collection: 21.023s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 21.19s
                        Total time: 10984.42s
                               ETA: 1483509.2s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.196s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0590
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 14.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 21.36s
                        Total time: 11005.78s
                               ETA: 1484359.9s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.361s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0597
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 21.53s
                        Total time: 11027.31s
                               ETA: 1485231.0s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.530s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0559
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 21.69s
                        Total time: 11049.01s
                               ETA: 1486121.4s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.269s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 21.47s
                        Total time: 11070.47s
                               ETA: 1486978.8s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.059s, learning 0.159s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0608
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 22.22s
                        Total time: 11092.69s
                               ETA: 1487934.8s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.684s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0582
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 21.86s
                        Total time: 11114.55s
                               ETA: 1488839.6s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.399s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 21.57s
                        Total time: 11136.12s
                               ETA: 1489703.1s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 783 steps/s (collection: 20.759s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 20.92s
                        Total time: 11157.04s
                               ETA: 1490478.2s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.266s, learning 0.238s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 21.50s
                        Total time: 11178.54s
                               ETA: 1491328.6s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.194s, learning 0.191s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 21.39s
                        Total time: 11199.93s
                               ETA: 1492161.0s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.352s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 21.51s
                        Total time: 11221.44s
                               ETA: 1493008.2s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.911s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0588
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 21.08s
                        Total time: 11242.52s
                               ETA: 1493795.3s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.390s, learning 0.175s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0556
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 21.57s
                        Total time: 11264.09s
                               ETA: 1494644.7s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 774 steps/s (collection: 21.002s, learning 0.157s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 21.16s
                        Total time: 11285.25s
                               ETA: 1495438.1s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.994s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 21.16s
                        Total time: 11306.40s
                               ETA: 1496228.9s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.312s, learning 0.182s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0612
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 21.49s
                        Total time: 11327.90s
                               ETA: 1497062.2s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.516s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0503
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 21.69s
                        Total time: 11349.59s
                               ETA: 1497918.9s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.196s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0607
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 21.37s
                        Total time: 11370.96s
                               ETA: 1498731.8s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.501s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0614
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 21.67s
                        Total time: 11392.63s
                               ETA: 1499581.5s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.445s, learning 0.168s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 21.61s
                        Total time: 11414.24s
                               ETA: 1500421.2s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.148s, learning 0.165s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0605
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 21.31s
                        Total time: 11435.56s
                               ETA: 1501219.3s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 786 steps/s (collection: 20.671s, learning 0.169s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0627
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 20.84s
                        Total time: 11456.40s
                               ETA: 1501953.3s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.441s, learning 0.173s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0488
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 21.61s
                        Total time: 11478.01s
                               ETA: 1502786.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.683s, learning 0.173s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 15.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 21.86s
                        Total time: 11499.87s
                               ETA: 1503649.1s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.365s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0516
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 21.53s
                        Total time: 11521.40s
                               ETA: 1504467.0s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 756 steps/s (collection: 21.380s, learning 0.266s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0604
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 21.65s
                        Total time: 11543.04s
                               ETA: 1505297.7s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 771 steps/s (collection: 21.070s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0612
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 21.24s
                        Total time: 11564.29s
                               ETA: 1506073.7s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.296s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 21.46s
                        Total time: 11585.75s
                               ETA: 1506876.5s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 773 steps/s (collection: 21.003s, learning 0.191s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 21.19s
                        Total time: 11606.94s
                               ETA: 1507641.8s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.049s, learning 0.237s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 16.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 16.29s
                        Total time: 11623.23s
                               ETA: 1507768.5s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.904s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0542
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 11.10s
                        Total time: 11634.33s
                               ETA: 1507223.0s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.876s, learning 0.217s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 11.09s
                        Total time: 11645.42s
                               ETA: 1506678.1s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.759s, learning 0.199s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0610
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 10.96s
                        Total time: 11656.38s
                               ETA: 1506117.1s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.088s, learning 0.215s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 11.30s
                        Total time: 11667.69s
                               ETA: 1505601.9s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.772s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0600
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 10.94s
                        Total time: 11678.63s
                               ETA: 1505041.1s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.701s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0630
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 10.88s
                        Total time: 11689.50s
                               ETA: 1504473.8s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.803s, learning 0.220s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 11.02s
                        Total time: 11700.52s
                               ETA: 1503926.6s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.331s, learning 0.192s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0636
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 18.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 11.52s
                        Total time: 11712.05s
                               ETA: 1503445.1s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.530s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 17.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 10.70s
                        Total time: 11722.74s
                               ETA: 1502858.6s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.723s, learning 0.179s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0632
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 10.90s
                        Total time: 11733.64s
                               ETA: 1502300.2s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.819s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0605
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 10.98s
                        Total time: 11744.63s
                               ETA: 1501753.6s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.878s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0668
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 11.05s
                        Total time: 11755.68s
                               ETA: 1501217.0s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.711s, learning 0.190s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0621
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 10.90s
                        Total time: 11766.58s
                               ETA: 1500662.6s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.128s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0641
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 19.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 11.30s
                        Total time: 11777.88s
                               ETA: 1500160.4s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.664s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 10.83s
                        Total time: 11788.72s
                               ETA: 1499600.2s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.027s, learning 0.193s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0626
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 20.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 11.22s
                        Total time: 11799.94s
                               ETA: 1499090.5s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.075s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0538
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 11.25s
                        Total time: 11811.18s
                               ETA: 1498585.4s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.032s, learning 0.240s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0599
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 21.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 11.27s
                        Total time: 11822.45s
                               ETA: 1498084.7s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.828s, learning 0.161s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0653
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 23.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 10.99s
                        Total time: 11833.44s
                               ETA: 1497549.4s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.847s, learning 0.176s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0543
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 22.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 11.02s
                        Total time: 11844.47s
                               ETA: 1497019.9s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.006s, learning 0.192s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0621
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 24.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 11.20s
                        Total time: 11855.67s
                               ETA: 1496513.8s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.617s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0615
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 24.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 10.78s
                        Total time: 11866.45s
                               ETA: 1495956.4s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.612s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0643
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 25.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 10.78s
                        Total time: 11877.23s
                               ETA: 1495400.7s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.166s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0537
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 25.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 10.89s
                        Total time: 11888.12s
                               ETA: 1494859.8s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.416s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0634
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 26.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 10.58s
                        Total time: 11898.71s
                               ETA: 1494281.7s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.877s, learning 0.199s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0546
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 26.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 11.08s
                        Total time: 11909.78s
                               ETA: 1493766.7s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.123s, learning 0.171s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0605
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 27.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 11.29s
                        Total time: 11921.08s
                               ETA: 1493280.4s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.594s, learning 0.190s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0530
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 29.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 10.78s
                        Total time: 11931.86s
                               ETA: 1492731.4s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.664s, learning 0.210s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0613
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 29.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 10.87s
                        Total time: 11942.73s
                               ETA: 1492195.0s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0564
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 30.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 11.43s
                        Total time: 11954.16s
                               ETA: 1491729.3s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.041s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0622
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 29.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 11.22s
                        Total time: 11965.38s
                               ETA: 1491238.5s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1464 steps/s (collection: 10.985s, learning 0.200s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0625
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 31.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 11.18s
                        Total time: 11976.57s
                               ETA: 1490744.5s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.016s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0631
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 30.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 11.20s
                        Total time: 11987.76s
                               ETA: 1490253.2s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.074s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0639
             Mean action noise std: 0.78
                       Mean reward: 0.00
               Mean episode length: 31.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 11.24s
                        Total time: 11999.00s
                               ETA: 1489768.5s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.858s, learning 0.170s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0592
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 31.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 11.03s
                        Total time: 12010.03s
                               ETA: 1489258.8s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.290s, learning 0.209s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0604
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 35.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 12.50s
                        Total time: 12022.53s
                               ETA: 1488932.5s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 771 steps/s (collection: 21.081s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0572
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 35.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 21.25s
                        Total time: 12043.78s
                               ETA: 1489689.1s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.464s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0621
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 34.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 21.62s
                        Total time: 12065.40s
                               ETA: 1490490.1s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.393s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0635
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 36.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 21.55s
                        Total time: 12086.95s
                               ETA: 1491280.5s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.394s, learning 0.163s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 37.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 21.56s
                        Total time: 12108.51s
                               ETA: 1492069.2s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.172s, learning 0.185s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0616
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 36.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 21.36s
                        Total time: 12129.87s
                               ETA: 1492831.5s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 737 steps/s (collection: 21.971s, learning 0.232s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0577
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 37.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 22.20s
                        Total time: 12152.07s
                               ETA: 1493695.7s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.285s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0575
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 38.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 21.45s
                        Total time: 12173.52s
                               ETA: 1494465.4s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.307s, learning 0.264s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0618
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 38.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 21.57s
                        Total time: 12195.09s
                               ETA: 1495247.9s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.937s, learning 0.187s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0615
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 39.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 21.12s
                        Total time: 12216.22s
                               ETA: 1495973.6s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.985s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0603
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 39.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 21.15s
                        Total time: 12237.37s
                               ETA: 1496701.2s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.176s, learning 0.185s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0589
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 39.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 21.36s
                        Total time: 12258.73s
                               ETA: 1497452.2s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.227s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0556
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 39.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 21.39s
                        Total time: 12280.12s
                               ETA: 1498205.4s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.529s, learning 0.163s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0611
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 40.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 21.69s
                        Total time: 12301.82s
                               ETA: 1498993.0s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.441s, learning 0.182s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0563
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 41.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 21.62s
                        Total time: 12323.44s
                               ETA: 1499770.2s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.315s, learning 0.220s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0581
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 42.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 21.54s
                        Total time: 12344.98s
                               ETA: 1500534.8s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.854s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0570
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 41.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 21.03s
                        Total time: 12366.01s
                               ETA: 1501236.4s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.268s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0592
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 44.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 21.44s
                        Total time: 12387.45s
                               ETA: 1501985.8s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.972s, learning 0.193s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0598
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 44.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 21.16s
                        Total time: 12408.61s
                               ETA: 1502699.8s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.137s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0536
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 44.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 21.30s
                        Total time: 12429.91s
                               ETA: 1503428.3s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.469s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0613
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 44.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 21.63s
                        Total time: 12451.54s
                               ETA: 1504195.1s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 762 steps/s (collection: 21.290s, learning 0.211s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0524
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 46.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 21.50s
                        Total time: 12473.05s
                               ETA: 1504944.2s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.459s, learning 0.231s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 46.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 21.69s
                        Total time: 12494.73s
                               ETA: 1505714.2s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.196s, learning 0.192s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0572
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 47.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 21.39s
                        Total time: 12516.12s
                               ETA: 1506445.9s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.299s, learning 0.161s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0621
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 48.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 21.46s
                        Total time: 12537.58s
                               ETA: 1507184.5s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.716s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0601
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 51.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 21.88s
                        Total time: 12559.46s
                               ETA: 1507972.0s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.691s, learning 0.196s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0574
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 50.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 21.89s
                        Total time: 12581.35s
                               ETA: 1508758.1s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.365s, learning 0.224s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0512
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 51.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 21.59s
                        Total time: 12602.94s
                               ETA: 1509506.5s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.175s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0592
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 51.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 21.34s
                        Total time: 12624.28s
                               ETA: 1510223.6s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.386s, learning 0.218s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0521
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 51.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 21.60s
                        Total time: 12645.89s
                               ETA: 1510970.1s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.736s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0582
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 53.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 21.91s
                        Total time: 12667.79s
                               ETA: 1511750.8s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.075s, learning 0.188s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0610
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 53.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 21.26s
                        Total time: 12689.05s
                               ETA: 1512453.0s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.498s, learning 0.180s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0560
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 53.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 21.68s
                        Total time: 12710.73s
                               ETA: 1513202.8s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.149s, learning 0.172s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0568
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 56.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 21.32s
                        Total time: 12732.05s
                               ETA: 1513908.3s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.135s, learning 0.161s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0515
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 57.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 21.30s
                        Total time: 12753.35s
                               ETA: 1514609.2s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.236s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0556
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 60.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 21.40s
                        Total time: 12774.75s
                               ETA: 1515320.5s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.224s, learning 0.197s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0540
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 56.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 21.42s
                        Total time: 12796.17s
                               ETA: 1516032.7s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.527s, learning 0.158s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0552
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 64.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 21.68s
                        Total time: 12817.85s
                               ETA: 1516774.3s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.277s, learning 0.171s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0552
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 61.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 16.45s
                        Total time: 12834.30s
                               ETA: 1516895.1s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.177s, learning 0.195s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 64.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 11.37s
                        Total time: 12845.67s
                               ETA: 1516416.5s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.410s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0571
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 65.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 11.58s
                        Total time: 12857.25s
                               ETA: 1515963.2s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.783s, learning 0.175s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0490
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 67.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 10.96s
                        Total time: 12868.21s
                               ETA: 1515438.0s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.105s, learning 0.196s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0567
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 65.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 11.30s
                        Total time: 12879.51s
                               ETA: 1514954.4s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.573s, learning 0.194s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0568
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 66.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 10.77s
                        Total time: 12890.28s
                               ETA: 1514409.2s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0559
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 71.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 11.54s
                        Total time: 12901.82s
                               ETA: 1513956.4s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.925s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 72.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 11.10s
                        Total time: 12912.92s
                               ETA: 1513452.2s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.025s, learning 0.177s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 74.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 11.20s
                        Total time: 12924.12s
                               ETA: 1512961.4s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.426s, learning 0.179s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0580
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 70.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 10.61s
                        Total time: 12934.73s
                               ETA: 1512402.0s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.627s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 75.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 10.80s
                        Total time: 12945.53s
                               ETA: 1511867.0s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.563s, learning 0.178s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0597
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 74.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 10.74s
                        Total time: 12956.27s
                               ETA: 1511326.1s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.624s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 75.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 10.80s
                        Total time: 12967.07s
                               ETA: 1510793.2s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.952s, learning 0.186s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0604
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 76.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 11.14s
                        Total time: 12978.21s
                               ETA: 1510300.9s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.163s, learning 0.188s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0583
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 79.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 11.35s
                        Total time: 12989.56s
                               ETA: 1509834.5s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.720s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 73.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 10.89s
                        Total time: 13000.45s
                               ETA: 1509315.2s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.820s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0569
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 79.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 10.98s
                        Total time: 13011.43s
                               ETA: 1508808.2s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.384s, learning 0.168s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0548
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 79.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 10.55s
                        Total time: 13021.98s
                               ETA: 1508252.6s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.891s, learning 0.159s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0543
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 78.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 11.05s
                        Total time: 13033.03s
                               ETA: 1507755.7s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.946s, learning 0.160s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0443
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 83.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 11.11s
                        Total time: 13044.14s
                               ETA: 1507266.6s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.630s, learning 0.161s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0483
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 84.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 10.79s
                        Total time: 13054.93s
                               ETA: 1506742.2s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.662s, learning 0.156s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0536
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 86.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 10.82s
                        Total time: 13065.75s
                               ETA: 1506222.1s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.753s, learning 0.159s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0501
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 85.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 10.91s
                        Total time: 13076.66s
                               ETA: 1505714.0s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.829s, learning 0.162s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 84.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 10.99s
                        Total time: 13087.65s
                               ETA: 1505216.2s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0532
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 91.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 11.14s
                        Total time: 13098.79s
                               ETA: 1504736.7s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.682s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0537
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 94.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 10.84s
                        Total time: 13109.63s
                               ETA: 1504224.2s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.848s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0553
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 91.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 11.01s
                        Total time: 13120.64s
                               ETA: 1503731.9s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.856s, learning 0.178s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0567
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 91.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 11.03s
                        Total time: 13131.68s
                               ETA: 1503243.6s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.810s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0573
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 96.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 10.97s
                        Total time: 13142.65s
                               ETA: 1502749.4s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.655s, learning 0.209s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 94.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 10.86s
                        Total time: 13153.52s
                               ETA: 1502243.7s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.789s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0569
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 95.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 10.96s
                        Total time: 13164.48s
                               ETA: 1501750.3s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.559s, learning 0.275s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0511
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 100.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 10.83s
                        Total time: 13175.31s
                               ETA: 1501243.6s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.493s, learning 0.196s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0573
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 101.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 10.69s
                        Total time: 13186.00s
                               ETA: 1500721.4s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.084s, learning 0.176s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 103.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 11.26s
                        Total time: 13197.26s
                               ETA: 1500265.3s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.038s, learning 0.185s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0589
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 107.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 11.22s
                        Total time: 13208.48s
                               ETA: 1499805.9s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.718s, learning 0.186s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0620
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 107.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 10.90s
                        Total time: 13219.39s
                               ETA: 1499311.5s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.524s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0570
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 106.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 10.69s
                        Total time: 13230.08s
                               ETA: 1498794.2s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.856s, learning 0.262s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0578
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 107.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 11.12s
                        Total time: 13241.20s
                               ETA: 1498326.3s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0612
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 112.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 10.93s
                        Total time: 13252.13s
                               ETA: 1497837.8s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.645s, learning 0.184s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0574
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 109.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 10.83s
                        Total time: 13262.96s
                               ETA: 1497339.3s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.578s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0561
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 109.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 10.74s
                        Total time: 13273.70s
                               ETA: 1496832.4s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.463s, learning 0.175s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0575
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 10.64s
                        Total time: 13284.34s
                               ETA: 1496314.6s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.730s, learning 0.221s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0581
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 10.95s
                        Total time: 13295.29s
                               ETA: 1495833.2s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.550s, learning 0.179s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0569
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 112.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 10.73s
                        Total time: 13306.02s
                               ETA: 1495327.9s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.562s, learning 0.177s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0584
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 112.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 10.74s
                        Total time: 13316.76s
                               ETA: 1494824.8s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.460s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0533
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 113.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 10.63s
                        Total time: 13327.39s
                               ETA: 1494310.6s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.725s, learning 0.250s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0552
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 10.98s
                        Total time: 13338.36s
                               ETA: 1493836.2s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.705s, learning 0.183s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0564
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 10.89s
                        Total time: 13349.25s
                               ETA: 1493353.2s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.818s, learning 0.215s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0516
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 114.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 11.03s
                        Total time: 13360.28s
                               ETA: 1492887.4s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.767s, learning 0.181s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0477
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 10.95s
                        Total time: 13371.23s
                               ETA: 1492413.1s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.821s, learning 0.247s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 114.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 11.07s
                        Total time: 13382.30s
                               ETA: 1491953.3s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.887s, learning 0.180s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0477
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 116.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 11.07s
                        Total time: 13393.37s
                               ETA: 1491494.3s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.635s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 113.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 10.81s
                        Total time: 13404.17s
                               ETA: 1491007.4s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.781s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0523
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 117.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 10.95s
                        Total time: 13415.13s
                               ETA: 1490538.0s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.104s, learning 0.186s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0505
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 11.29s
                        Total time: 13426.42s
                               ETA: 1490106.8s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.123s, learning 0.174s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0533
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 114.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 11.30s
                        Total time: 13437.71s
                               ETA: 1489677.3s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.903s, learning 0.169s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0484
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 11.07s
                        Total time: 13448.79s
                               ETA: 1489223.9s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1462 steps/s (collection: 10.994s, learning 0.209s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0493
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 116.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 11.20s
                        Total time: 13459.99s
                               ETA: 1488786.0s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.753s, learning 0.200s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0514
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 117.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 10.95s
                        Total time: 13470.94s
                               ETA: 1488321.4s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.497s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0460
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 10.66s
                        Total time: 13481.61s
                               ETA: 1487825.8s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.667s, learning 0.195s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0476
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 113.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 10.86s
                        Total time: 13492.47s
                               ETA: 1487353.2s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.891s, learning 0.211s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0523
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 114.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 11.10s
                        Total time: 13503.57s
                               ETA: 1486908.0s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.515s, learning 0.187s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 10.70s
                        Total time: 13514.27s
                               ETA: 1486419.8s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.907s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0500
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 11.07s
                        Total time: 13525.34s
                               ETA: 1485973.1s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.529s, learning 0.175s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0523
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 116.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 10.70s
                        Total time: 13536.04s
                               ETA: 1485487.1s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.879s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0562
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 115.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 11.04s
                        Total time: 13547.09s
                               ETA: 1485039.5s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.110s, learning 0.201s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 116.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 11.31s
                        Total time: 13558.40s
                               ETA: 1484622.0s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.729s, learning 0.170s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0496
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.85
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 10.90s
                        Total time: 13569.30s
                               ETA: 1484160.5s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.942s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 11.11s
                        Total time: 13580.40s
                               ETA: 1483722.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0497
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 10.71s
                        Total time: 13591.11s
                               ETA: 1483242.5s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.587s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0503
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 114.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 10.76s
                        Total time: 13601.87s
                               ETA: 1482768.8s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.368s, learning 0.166s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0489
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 113.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 11.53s
                        Total time: 13613.41s
                               ETA: 1482380.4s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.722s, learning 0.187s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0524
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 117.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 10.91s
                        Total time: 13624.32s
                               ETA: 1481924.9s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.636s, learning 0.173s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0499
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 10.81s
                        Total time: 13635.13s
                               ETA: 1481459.4s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.905s, learning 0.222s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 11.13s
                        Total time: 13646.25s
                               ETA: 1481029.4s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.984s, learning 0.197s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0559
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 11.18s
                        Total time: 13657.43s
                               ETA: 1480606.2s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.784s, learning 0.173s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0556
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 10.96s
                        Total time: 13668.39s
                               ETA: 1480159.6s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.088s, learning 0.194s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0560
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 11.28s
                        Total time: 13679.67s
                               ETA: 1479749.2s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.059s, learning 0.226s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0564
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 11.28s
                        Total time: 13690.96s
                               ETA: 1479339.9s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.732s, learning 0.196s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0501
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 10.93s
                        Total time: 13701.88s
                               ETA: 1478893.0s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.832s, learning 0.213s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0578
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 11.04s
                        Total time: 13712.93s
                               ETA: 1478459.6s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.028s, learning 0.193s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0500
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 11.22s
                        Total time: 13724.15s
                               ETA: 1478046.1s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.042s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0552
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 121.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 11.22s
                        Total time: 13735.37s
                               ETA: 1477633.1s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.682s, learning 0.211s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0545
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 121.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 10.89s
                        Total time: 13746.26s
                               ETA: 1477186.1s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.053s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 11.22s
                        Total time: 13757.48s
                               ETA: 1476775.1s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1464 steps/s (collection: 10.951s, learning 0.234s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0589
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 11.18s
                        Total time: 13768.66s
                               ETA: 1476361.2s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.857s, learning 0.174s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 11.03s
                        Total time: 13779.69s
                               ETA: 1475931.8s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.482s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0541
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 10.64s
                        Total time: 13790.34s
                               ETA: 1475461.9s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.767s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0594
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 118.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 10.94s
                        Total time: 13801.27s
                               ETA: 1475024.2s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.887s, learning 0.259s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0561
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 122.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 11.15s
                        Total time: 13812.42s
                               ETA: 1474609.8s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1464 steps/s (collection: 10.998s, learning 0.190s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0521
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 121.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 11.19s
                        Total time: 13823.61s
                               ETA: 1474200.8s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.259s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0571
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 123.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 11.43s
                        Total time: 13835.03s
                               ETA: 1473818.0s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.145s, learning 0.271s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0506
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 122.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 11.42s
                        Total time: 13846.45s
                               ETA: 1473434.8s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.528s, learning 0.203s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0562
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 121.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 10.73s
                        Total time: 13857.18s
                               ETA: 1472979.7s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.051s, learning 0.209s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0505
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 117.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 11.26s
                        Total time: 13868.44s
                               ETA: 1472581.7s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.714s, learning 0.172s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 122.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 10.89s
                        Total time: 13879.33s
                               ETA: 1472144.8s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.663s, learning 0.177s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0509
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 119.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 10.84s
                        Total time: 13890.17s
                               ETA: 1471704.1s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.594s, learning 0.180s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0547
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 122.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 10.77s
                        Total time: 13900.94s
                               ETA: 1471257.2s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.511s, learning 0.189s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 122.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 10.70s
                        Total time: 13911.64s
                               ETA: 1470803.5s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.872s, learning 0.165s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0481
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 120.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 11.04s
                        Total time: 13922.68s
                               ETA: 1470386.3s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.187s, learning 0.190s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0512
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 11.38s
                        Total time: 13934.06s
                               ETA: 1470005.9s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.832s, learning 0.179s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0514
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 11.01s
                        Total time: 13945.07s
                               ETA: 1469587.5s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.805s, learning 0.177s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0527
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 10.98s
                        Total time: 13956.05s
                               ETA: 1469167.0s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.760s, learning 0.174s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0532
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 124.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 10.93s
                        Total time: 13966.98s
                               ETA: 1468742.4s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.501s, learning 0.180s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0477
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 10.68s
                        Total time: 13977.66s
                               ETA: 1468292.1s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1452 steps/s (collection: 10.988s, learning 0.289s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0486
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 11.28s
                        Total time: 13988.94s
                               ETA: 1467905.1s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1468 steps/s (collection: 11.000s, learning 0.159s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0509
             Mean action noise std: 0.77
                       Mean reward: 0.00
               Mean episode length: 117.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 11.16s
                        Total time: 14000.10s
                               ETA: 1467506.7s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.242s, learning 0.234s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0459
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 11.48s
                        Total time: 14011.58s
                               ETA: 1467142.3s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.797s, learning 0.201s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0496
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 11.00s
                        Total time: 14022.57s
                               ETA: 1466728.6s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.724s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 119.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 10.90s
                        Total time: 14033.47s
                               ETA: 1466305.1s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.142s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 119.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 11.31s
                        Total time: 14044.78s
                               ETA: 1465925.6s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.033s, learning 0.200s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0570
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 11.23s
                        Total time: 14056.01s
                               ETA: 1465538.9s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.175s, learning 0.177s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0480
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 118.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 11.35s
                        Total time: 14067.36s
                               ETA: 1465165.4s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.813s, learning 0.185s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0555
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 11.00s
                        Total time: 14078.36s
                               ETA: 1464755.9s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.755s, learning 0.255s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0526
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 119.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 11.01s
                        Total time: 14089.37s
                               ETA: 1464348.4s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0550
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 11.37s
                        Total time: 14100.74s
                               ETA: 1463978.7s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.149s, learning 0.276s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0560
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 11.43s
                        Total time: 14112.16s
                               ETA: 1463616.0s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.069s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0540
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 11.24s
                        Total time: 14123.40s
                               ETA: 1463234.3s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.823s, learning 0.171s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0602
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 10.99s
                        Total time: 14134.39s
                               ETA: 1462828.3s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.933s, learning 0.160s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0538
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 11.09s
                        Total time: 14145.48s
                               ETA: 1462433.4s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0592
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 11.38s
                        Total time: 14156.87s
                               ETA: 1462069.1s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.755s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0574
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 10.92s
                        Total time: 14167.78s
                               ETA: 1461657.7s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.023s, learning 0.166s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0537
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 119.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 11.19s
                        Total time: 14178.97s
                               ETA: 1461275.2s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.634s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0586
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 10.80s
                        Total time: 14189.77s
                               ETA: 1460853.0s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.630s, learning 0.197s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0581
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 10.83s
                        Total time: 14200.60s
                               ETA: 1460434.8s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.018s, learning 0.196s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0605
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 11.21s
                        Total time: 14211.81s
                               ETA: 1460057.1s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.814s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0566
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 122.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 10.98s
                        Total time: 14222.79s
                               ETA: 1459656.0s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.685s, learning 0.189s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 10.87s
                        Total time: 14233.66s
                               ETA: 1459245.0s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.863s, learning 0.163s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0578
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 11.03s
                        Total time: 14244.69s
                               ETA: 1458850.4s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.412s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0591
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 10.58s
                        Total time: 14255.27s
                               ETA: 1458410.9s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1467 steps/s (collection: 11.002s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0599
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 11.16s
                        Total time: 14266.43s
                               ETA: 1458032.1s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.888s, learning 0.168s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0596
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 11.06s
                        Total time: 14277.49s
                               ETA: 1457643.0s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.750s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0603
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.31
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 10.91s
                        Total time: 14288.40s
                               ETA: 1457240.2s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.824s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0611
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 10.99s
                        Total time: 14299.39s
                               ETA: 1456845.5s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.725s, learning 0.192s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0601
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 10.92s
                        Total time: 14310.30s
                               ETA: 1456444.6s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.165s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0595
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 122.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 10.87s
                        Total time: 14321.17s
                               ETA: 1456039.9s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.579s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0630
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 10.75s
                        Total time: 14331.92s
                               ETA: 1455623.5s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.015s, learning 0.223s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0604
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 11.24s
                        Total time: 14343.16s
                               ETA: 1455257.6s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.574s, learning 0.274s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0598
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 124.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 10.85s
                        Total time: 14354.01s
                               ETA: 1454852.9s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.998s, learning 0.162s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0515
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 11.16s
                        Total time: 14365.17s
                               ETA: 1454480.6s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.051s, learning 0.205s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0585
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 120.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 11.26s
                        Total time: 14376.42s
                               ETA: 1454118.8s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.798s, learning 0.170s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 10.97s
                        Total time: 14387.39s
                               ETA: 1453728.7s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.694s, learning 0.158s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 122.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 10.85s
                        Total time: 14398.25s
                               ETA: 1453327.5s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.565s, learning 0.189s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0518
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 121.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 10.75s
                        Total time: 14409.00s
                               ETA: 1452917.3s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.122s, learning 0.203s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0541
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 124.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 11.32s
                        Total time: 14420.32s
                               ETA: 1452565.3s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.707s, learning 0.169s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0524
             Mean action noise std: 0.77
                       Mean reward: 0.01
               Mean episode length: 123.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 10.88s
                        Total time: 14431.20s
                               ETA: 1452169.0s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.164s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 123.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 10.78s
                        Total time: 14441.98s
                               ETA: 1451763.5s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1462 steps/s (collection: 10.985s, learning 0.218s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0528
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 122.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 11.20s
                        Total time: 14453.18s
                               ETA: 1451401.5s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.673s, learning 0.161s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0491
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 121.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 10.83s
                        Total time: 14464.02s
                               ETA: 1451003.1s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.796s, learning 0.167s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0504
             Mean action noise std: 0.77
                       Mean reward: 0.03
               Mean episode length: 122.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 10.96s
                        Total time: 14474.98s
                               ETA: 1450618.6s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.910s, learning 0.190s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0512
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 123.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 11.10s
                        Total time: 14486.08s
                               ETA: 1450248.4s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.575s, learning 0.174s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0482
             Mean action noise std: 0.77
                       Mean reward: 0.03
               Mean episode length: 121.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 10.75s
                        Total time: 14496.83s
                               ETA: 1449844.0s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.900s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0426
             Mean action noise std: 0.77
                       Mean reward: 0.03
               Mean episode length: 122.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 11.07s
                        Total time: 14507.90s
                               ETA: 1449472.2s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.084s, learning 0.201s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0306
             Mean action noise std: 0.77
                       Mean reward: 0.03
               Mean episode length: 121.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 11.28s
                        Total time: 14519.18s
                               ETA: 1449122.7s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.165s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0314
             Mean action noise std: 0.77
                       Mean reward: 0.07
               Mean episode length: 122.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 10.87s
                        Total time: 14530.05s
                               ETA: 1448732.5s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.017s, learning 0.275s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0313
             Mean action noise std: 0.77
                       Mean reward: 0.07
               Mean episode length: 120.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 11.29s
                        Total time: 14541.34s
                               ETA: 1448385.1s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0343
             Mean action noise std: 0.77
                       Mean reward: 0.09
               Mean episode length: 121.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 11.49s
                        Total time: 14552.83s
                               ETA: 1448057.7s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.730s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0320
             Mean action noise std: 0.77
                       Mean reward: 0.12
               Mean episode length: 119.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 10.90s
                        Total time: 14563.73s
                               ETA: 1447672.5s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.881s, learning 0.168s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0339
             Mean action noise std: 0.77
                       Mean reward: 0.14
               Mean episode length: 121.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 11.05s
                        Total time: 14574.78s
                               ETA: 1447303.0s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.162s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0338
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 121.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 10.92s
                        Total time: 14585.70s
                               ETA: 1446921.9s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.696s, learning 0.162s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0386
             Mean action noise std: 0.77
                       Mean reward: 0.18
               Mean episode length: 118.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 10.86s
                        Total time: 14596.56s
                               ETA: 1446534.9s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.622s, learning 0.262s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 0.27
               Mean episode length: 121.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 10.88s
                        Total time: 14607.44s
                               ETA: 1446151.2s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.947s, learning 0.172s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0436
             Mean action noise std: 0.77
                       Mean reward: 0.38
               Mean episode length: 119.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 11.12s
                        Total time: 14618.56s
                               ETA: 1445791.6s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.809s, learning 0.179s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0393
             Mean action noise std: 0.77
                       Mean reward: 0.40
               Mean episode length: 117.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 10.99s
                        Total time: 14629.55s
                               ETA: 1445419.8s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.942s, learning 0.159s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0450
             Mean action noise std: 0.77
                       Mean reward: 0.52
               Mean episode length: 119.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 11.10s
                        Total time: 14640.65s
                               ETA: 1445059.8s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.855s, learning 0.162s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0454
             Mean action noise std: 0.77
                       Mean reward: 0.72
               Mean episode length: 119.75
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 11.02s
                        Total time: 14651.67s
                               ETA: 1444692.2s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.652s, learning 0.187s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0453
             Mean action noise std: 0.77
                       Mean reward: 0.77
               Mean episode length: 119.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 10.84s
                        Total time: 14662.50s
                               ETA: 1444307.8s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.714s, learning 0.383s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0475
             Mean action noise std: 0.77
                       Mean reward: 1.01
               Mean episode length: 116.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 11.10s
                        Total time: 14673.60s
                               ETA: 1443949.5s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.719s, learning 0.168s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 1.02
               Mean episode length: 116.93
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 10.89s
                        Total time: 14684.49s
                               ETA: 1443571.3s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.897s, learning 0.176s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0475
             Mean action noise std: 0.77
                       Mean reward: 1.05
               Mean episode length: 114.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.07s
                        Total time: 14695.56s
                               ETA: 1443212.1s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.004s, learning 0.273s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0510
             Mean action noise std: 0.77
                       Mean reward: 1.46
               Mean episode length: 119.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 11.28s
                        Total time: 14706.84s
                               ETA: 1442873.6s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.159s, learning 0.226s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0466
             Mean action noise std: 0.77
                       Mean reward: 1.60
               Mean episode length: 118.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 11.39s
                        Total time: 14718.22s
                               ETA: 1442546.3s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.620s, learning 0.232s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0494
             Mean action noise std: 0.77
                       Mean reward: 1.66
               Mean episode length: 117.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 10.85s
                        Total time: 14729.08s
                               ETA: 1442167.6s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.975s, learning 0.185s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0478
             Mean action noise std: 0.77
                       Mean reward: 2.00
               Mean episode length: 115.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 11.16s
                        Total time: 14740.24s
                               ETA: 1441819.6s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.066s, learning 0.229s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0500
             Mean action noise std: 0.77
                       Mean reward: 1.83
               Mean episode length: 112.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 11.30s
                        Total time: 14751.53s
                               ETA: 1441485.5s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.430s, learning 0.188s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0494
             Mean action noise std: 0.77
                       Mean reward: 2.34
               Mean episode length: 117.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 10.62s
                        Total time: 14762.15s
                               ETA: 1441085.9s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.690s, learning 0.195s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 2.36
               Mean episode length: 112.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 10.88s
                        Total time: 14773.04s
                               ETA: 1440713.1s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.116s, learning 0.171s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0509
             Mean action noise std: 0.77
                       Mean reward: 2.71
               Mean episode length: 115.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 11.29s
                        Total time: 14784.32s
                               ETA: 1440380.2s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.935s, learning 0.193s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0489
             Mean action noise std: 0.77
                       Mean reward: 2.48
               Mean episode length: 112.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 11.13s
                        Total time: 14795.45s
                               ETA: 1440032.4s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.162s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0518
             Mean action noise std: 0.77
                       Mean reward: 2.73
               Mean episode length: 109.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 10.73s
                        Total time: 14806.19s
                               ETA: 1439647.1s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.965s, learning 0.164s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0465
             Mean action noise std: 0.77
                       Mean reward: 2.87
               Mean episode length: 112.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 11.13s
                        Total time: 14817.32s
                               ETA: 1439300.8s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.088s, learning 0.160s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0498
             Mean action noise std: 0.77
                       Mean reward: 3.00
               Mean episode length: 113.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 11.25s
                        Total time: 14828.56s
                               ETA: 1438966.7s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.971s, learning 0.162s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0501
             Mean action noise std: 0.77
                       Mean reward: 3.50
               Mean episode length: 115.29
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 11.13s
                        Total time: 14839.70s
                               ETA: 1438622.0s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.818s, learning 0.203s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0498
             Mean action noise std: 0.77
                       Mean reward: 3.37
               Mean episode length: 114.77
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 11.02s
                        Total time: 14850.72s
                               ETA: 1438267.2s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.867s, learning 0.184s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0483
             Mean action noise std: 0.77
                       Mean reward: 3.51
               Mean episode length: 112.02
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 11.05s
                        Total time: 14861.77s
                               ETA: 1437915.9s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.869s, learning 0.188s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0526
             Mean action noise std: 0.77
                       Mean reward: 3.24
               Mean episode length: 109.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 11.06s
                        Total time: 14872.82s
                               ETA: 1437565.9s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.918s, learning 0.200s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0499
             Mean action noise std: 0.77
                       Mean reward: 3.38
               Mean episode length: 109.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 11.12s
                        Total time: 14883.94s
                               ETA: 1437222.4s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.739s, learning 0.176s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0504
             Mean action noise std: 0.77
                       Mean reward: 3.62
               Mean episode length: 108.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 10.92s
                        Total time: 14894.86s
                               ETA: 1436860.1s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.809s, learning 0.181s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0495
             Mean action noise std: 0.77
                       Mean reward: 3.81
               Mean episode length: 112.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 10.99s
                        Total time: 14905.85s
                               ETA: 1436505.7s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.615s, learning 0.206s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0492
             Mean action noise std: 0.77
                       Mean reward: 4.22
               Mean episode length: 114.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 10.82s
                        Total time: 14916.67s
                               ETA: 1436135.7s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.700s, learning 0.190s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0507
             Mean action noise std: 0.77
                       Mean reward: 3.97
               Mean episode length: 109.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 10.89s
                        Total time: 14927.56s
                               ETA: 1435772.9s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.862s, learning 0.190s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0497
             Mean action noise std: 0.77
                       Mean reward: 4.41
               Mean episode length: 112.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 11.05s
                        Total time: 14938.61s
                               ETA: 1435426.3s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.673s, learning 0.171s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0480
             Mean action noise std: 0.77
                       Mean reward: 4.34
               Mean episode length: 114.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 10.84s
                        Total time: 14949.45s
                               ETA: 1435060.5s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.039s, learning 0.171s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0462
             Mean action noise std: 0.77
                       Mean reward: 4.25
               Mean episode length: 112.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 11.21s
                        Total time: 14960.66s
                               ETA: 1434730.6s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.165s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0467
             Mean action noise std: 0.77
                       Mean reward: 4.78
               Mean episode length: 111.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 11.03s
                        Total time: 14971.69s
                               ETA: 1434383.8s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.770s, learning 0.173s)
               Value function loss: 0.0473
                    Surrogate loss: -0.0492
             Mean action noise std: 0.77
                       Mean reward: 4.84
               Mean episode length: 114.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 10.94s
                        Total time: 14982.63s
                               ETA: 1434029.4s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.791s, learning 0.270s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0450
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 112.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 11.06s
                        Total time: 14993.70s
                               ETA: 1433687.1s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.910s, learning 0.193s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0490
             Mean action noise std: 0.77
                       Mean reward: 4.98
               Mean episode length: 114.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 11.10s
                        Total time: 15004.80s
                               ETA: 1433349.5s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.166s)
               Value function loss: 0.0491
                    Surrogate loss: -0.0443
             Mean action noise std: 0.77
                       Mean reward: 5.03
               Mean episode length: 113.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 10.77s
                        Total time: 15015.57s
                               ETA: 1432980.5s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.774s, learning 0.192s)
               Value function loss: 0.0435
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 5.14
               Mean episode length: 115.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 10.97s
                        Total time: 15026.54s
                               ETA: 1432631.1s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.144s, learning 0.170s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0493
             Mean action noise std: 0.77
                       Mean reward: 5.34
               Mean episode length: 113.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 11.31s
                        Total time: 15037.85s
                               ETA: 1432315.3s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.576s, learning 0.183s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0504
             Mean action noise std: 0.77
                       Mean reward: 5.00
               Mean episode length: 108.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 10.76s
                        Total time: 15048.61s
                               ETA: 1431947.4s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.014s, learning 0.208s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0515
             Mean action noise std: 0.77
                       Mean reward: 5.36
               Mean episode length: 113.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 11.22s
                        Total time: 15059.83s
                               ETA: 1431624.2s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.859s, learning 0.170s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0499
             Mean action noise std: 0.77
                       Mean reward: 5.69
               Mean episode length: 114.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 11.03s
                        Total time: 15070.86s
                               ETA: 1431283.2s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.874s, learning 0.172s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 5.72
               Mean episode length: 115.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 11.05s
                        Total time: 15081.91s
                               ETA: 1430944.6s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.377s, learning 0.186s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0465
             Mean action noise std: 0.77
                       Mean reward: 5.43
               Mean episode length: 115.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 11.56s
                        Total time: 15093.47s
                               ETA: 1430655.6s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.436s, learning 0.190s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0498
             Mean action noise std: 0.77
                       Mean reward: 5.97
               Mean episode length: 113.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 11.63s
                        Total time: 15105.09s
                               ETA: 1430372.9s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.768s, learning 0.198s)
               Value function loss: 0.0455
                    Surrogate loss: -0.0505
             Mean action noise std: 0.77
                       Mean reward: 5.44
               Mean episode length: 115.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 10.97s
                        Total time: 15116.06s
                               ETA: 1430028.5s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.829s, learning 0.182s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0457
             Mean action noise std: 0.77
                       Mean reward: 5.79
               Mean episode length: 114.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 11.01s
                        Total time: 15127.07s
                               ETA: 1429688.8s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.453s, learning 0.211s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0492
             Mean action noise std: 0.77
                       Mean reward: 6.12
               Mean episode length: 115.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 10.66s
                        Total time: 15137.74s
                               ETA: 1429317.1s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.598s, learning 0.221s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0490
             Mean action noise std: 0.77
                       Mean reward: 6.43
               Mean episode length: 115.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 10.82s
                        Total time: 15148.55s
                               ETA: 1428960.7s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.881s, learning 0.192s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0497
             Mean action noise std: 0.77
                       Mean reward: 6.45
               Mean episode length: 116.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 11.07s
                        Total time: 15159.63s
                               ETA: 1428628.8s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.712s, learning 0.229s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0470
             Mean action noise std: 0.77
                       Mean reward: 6.14
               Mean episode length: 114.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 10.94s
                        Total time: 15170.57s
                               ETA: 1428285.2s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.893s, learning 0.176s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0475
             Mean action noise std: 0.77
                       Mean reward: 6.49
               Mean episode length: 118.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 11.07s
                        Total time: 15181.64s
                               ETA: 1427954.3s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.922s, learning 0.178s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0482
             Mean action noise std: 0.77
                       Mean reward: 6.19
               Mean episode length: 114.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 11.10s
                        Total time: 15192.74s
                               ETA: 1427626.8s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.688s, learning 0.165s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0508
             Mean action noise std: 0.77
                       Mean reward: 6.45
               Mean episode length: 117.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 10.85s
                        Total time: 15203.59s
                               ETA: 1427276.8s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.169s)
               Value function loss: 0.0481
                    Surrogate loss: -0.0503
             Mean action noise std: 0.77
                       Mean reward: 6.45
               Mean episode length: 118.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 11.33s
                        Total time: 15214.92s
                               ETA: 1426972.2s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.162s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0470
             Mean action noise std: 0.77
                       Mean reward: 6.92
               Mean episode length: 118.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 11.19s
                        Total time: 15226.11s
                               ETA: 1426654.7s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.566s, learning 0.184s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0460
             Mean action noise std: 0.77
                       Mean reward: 6.50
               Mean episode length: 115.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 10.75s
                        Total time: 15236.86s
                               ETA: 1426296.9s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.674s, learning 0.165s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0489
             Mean action noise std: 0.77
                       Mean reward: 6.94
               Mean episode length: 116.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 10.84s
                        Total time: 15247.70s
                               ETA: 1425948.0s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.601s, learning 0.214s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0476
             Mean action noise std: 0.77
                       Mean reward: 6.46
               Mean episode length: 113.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 10.82s
                        Total time: 15258.51s
                               ETA: 1425597.5s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.303s, learning 0.172s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0471
             Mean action noise std: 0.77
                       Mean reward: 6.55
               Mean episode length: 113.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 11.48s
                        Total time: 15269.99s
                               ETA: 1425309.4s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.810s, learning 0.188s)
               Value function loss: 0.0479
                    Surrogate loss: -0.0490
             Mean action noise std: 0.77
                       Mean reward: 6.91
               Mean episode length: 116.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 11.00s
                        Total time: 15280.99s
                               ETA: 1424977.3s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.780s, learning 0.170s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0449
             Mean action noise std: 0.77
                       Mean reward: 7.04
               Mean episode length: 113.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 10.95s
                        Total time: 15291.94s
                               ETA: 1424641.2s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.170s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0453
             Mean action noise std: 0.77
                       Mean reward: 6.67
               Mean episode length: 116.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 11.23s
                        Total time: 15303.17s
                               ETA: 1424332.3s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.725s, learning 0.273s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0441
             Mean action noise std: 0.77
                       Mean reward: 6.70
               Mean episode length: 115.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 11.00s
                        Total time: 15314.17s
                               ETA: 1424002.0s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.712s, learning 0.162s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0435
             Mean action noise std: 0.77
                       Mean reward: 6.56
               Mean episode length: 113.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 10.87s
                        Total time: 15325.04s
                               ETA: 1423660.7s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.092s, learning 0.172s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0418
             Mean action noise std: 0.77
                       Mean reward: 7.06
               Mean episode length: 115.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 11.26s
                        Total time: 15336.31s
                               ETA: 1423356.2s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.958s, learning 0.191s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0468
             Mean action noise std: 0.77
                       Mean reward: 6.44
               Mean episode length: 111.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 11.15s
                        Total time: 15347.46s
                               ETA: 1423041.6s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.017s, learning 0.183s)
               Value function loss: 0.0578
                    Surrogate loss: -0.0468
             Mean action noise std: 0.77
                       Mean reward: 6.27
               Mean episode length: 112.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 11.20s
                        Total time: 15358.66s
                               ETA: 1422732.3s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.995s, learning 0.187s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0478
             Mean action noise std: 0.77
                       Mean reward: 6.74
               Mean episode length: 114.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 11.18s
                        Total time: 15369.84s
                               ETA: 1422421.9s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.810s, learning 0.197s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0458
             Mean action noise std: 0.76
                       Mean reward: 6.46
               Mean episode length: 112.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 11.01s
                        Total time: 15380.85s
                               ETA: 1422095.9s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.767s, learning 0.164s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0493
             Mean action noise std: 0.76
                       Mean reward: 7.34
               Mean episode length: 113.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 10.93s
                        Total time: 15391.78s
                               ETA: 1421763.4s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.948s, learning 0.159s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0485
             Mean action noise std: 0.76
                       Mean reward: 6.29
               Mean episode length: 110.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 11.11s
                        Total time: 15402.88s
                               ETA: 1421447.8s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.022s, learning 0.170s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0456
             Mean action noise std: 0.76
                       Mean reward: 6.38
               Mean episode length: 110.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 11.19s
                        Total time: 15414.08s
                               ETA: 1421140.6s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.781s, learning 0.161s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0494
             Mean action noise std: 0.76
                       Mean reward: 6.52
               Mean episode length: 111.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 10.94s
                        Total time: 15425.02s
                               ETA: 1420810.9s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.930s, learning 0.167s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0482
             Mean action noise std: 0.76
                       Mean reward: 6.83
               Mean episode length: 112.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 11.10s
                        Total time: 15436.12s
                               ETA: 1420496.0s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.514s, learning 0.172s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 7.05
               Mean episode length: 112.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 10.69s
                        Total time: 15446.80s
                               ETA: 1420144.0s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.133s, learning 0.175s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0484
             Mean action noise std: 0.76
                       Mean reward: 7.29
               Mean episode length: 114.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 11.31s
                        Total time: 15458.11s
                               ETA: 1419849.7s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.735s, learning 0.160s)
               Value function loss: 0.0662
                    Surrogate loss: -0.0472
             Mean action noise std: 0.76
                       Mean reward: 7.23
               Mean episode length: 113.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 10.89s
                        Total time: 15469.00s
                               ETA: 1419518.0s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.934s, learning 0.189s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0428
             Mean action noise std: 0.76
                       Mean reward: 7.31
               Mean episode length: 113.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 11.12s
                        Total time: 15480.13s
                               ETA: 1419207.8s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.097s, learning 0.190s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 6.75
               Mean episode length: 111.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 11.29s
                        Total time: 15491.42s
                               ETA: 1418913.2s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.486s, learning 0.212s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0423
             Mean action noise std: 0.76
                       Mean reward: 6.88
               Mean episode length: 110.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 11.70s
                        Total time: 15503.11s
                               ETA: 1418656.8s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.040s, learning 0.158s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0439
             Mean action noise std: 0.76
                       Mean reward: 6.99
               Mean episode length: 110.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 11.20s
                        Total time: 15514.31s
                               ETA: 1418355.1s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.116s, learning 0.258s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0448
             Mean action noise std: 0.76
                       Mean reward: 6.69
               Mean episode length: 107.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 11.37s
                        Total time: 15525.69s
                               ETA: 1418070.1s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.247s, learning 0.170s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0456
             Mean action noise std: 0.76
                       Mean reward: 6.97
               Mean episode length: 109.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 11.42s
                        Total time: 15537.10s
                               ETA: 1417789.4s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.877s, learning 0.224s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0463
             Mean action noise std: 0.76
                       Mean reward: 6.96
               Mean episode length: 108.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 11.10s
                        Total time: 15548.21s
                               ETA: 1417480.4s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.025s, learning 0.180s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0452
             Mean action noise std: 0.76
                       Mean reward: 7.33
               Mean episode length: 109.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 11.21s
                        Total time: 15559.41s
                               ETA: 1417181.5s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.053s, learning 0.180s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0427
             Mean action noise std: 0.76
                       Mean reward: 7.46
               Mean episode length: 110.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 11.23s
                        Total time: 15570.64s
                               ETA: 1416885.6s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.930s, learning 0.174s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0460
             Mean action noise std: 0.76
                       Mean reward: 7.50
               Mean episode length: 113.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 11.10s
                        Total time: 15581.75s
                               ETA: 1416578.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.927s, learning 0.226s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0446
             Mean action noise std: 0.76
                       Mean reward: 7.73
               Mean episode length: 107.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 11.15s
                        Total time: 15592.90s
                               ETA: 1416276.4s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.161s, learning 0.159s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0444
             Mean action noise std: 0.76
                       Mean reward: 8.25
               Mean episode length: 113.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 11.32s
                        Total time: 15604.22s
                               ETA: 1415990.0s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.745s, learning 0.160s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0461
             Mean action noise std: 0.76
                       Mean reward: 7.93
               Mean episode length: 112.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 10.91s
                        Total time: 15615.13s
                               ETA: 1415666.5s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.974s, learning 0.188s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0473
             Mean action noise std: 0.76
                       Mean reward: 8.10
               Mean episode length: 115.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 11.16s
                        Total time: 15626.29s
                               ETA: 1415366.9s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.433s, learning 0.193s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0471
             Mean action noise std: 0.76
                       Mean reward: 8.39
               Mean episode length: 115.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 11.63s
                        Total time: 15637.92s
                               ETA: 1415109.7s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.091s, learning 0.189s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0492
             Mean action noise std: 0.76
                       Mean reward: 8.75
               Mean episode length: 119.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 11.28s
                        Total time: 15649.19s
                               ETA: 1414821.7s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.831s, learning 0.162s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0482
             Mean action noise std: 0.76
                       Mean reward: 9.48
               Mean episode length: 119.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 10.99s
                        Total time: 15660.19s
                               ETA: 1414508.3s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.618s, learning 0.168s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0475
             Mean action noise std: 0.76
                       Mean reward: 9.22
               Mean episode length: 119.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 10.79s
                        Total time: 15670.97s
                               ETA: 1414176.7s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.712s, learning 0.159s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0484
             Mean action noise std: 0.76
                       Mean reward: 8.80
               Mean episode length: 118.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 10.87s
                        Total time: 15681.84s
                               ETA: 1413853.4s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.803s, learning 0.207s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0478
             Mean action noise std: 0.76
                       Mean reward: 9.35
               Mean episode length: 115.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 12.01s
                        Total time: 15693.85s
                               ETA: 1413633.3s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.044s, learning 0.252s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0485
             Mean action noise std: 0.76
                       Mean reward: 8.97
               Mean episode length: 122.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 11.30s
                        Total time: 15705.15s
                               ETA: 1413349.3s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.655s, learning 0.218s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0486
             Mean action noise std: 0.76
                       Mean reward: 9.43
               Mean episode length: 118.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 10.87s
                        Total time: 15716.02s
                               ETA: 1413027.7s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.932s, learning 0.268s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0496
             Mean action noise std: 0.76
                       Mean reward: 9.87
               Mean episode length: 117.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 11.20s
                        Total time: 15727.22s
                               ETA: 1412736.1s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.588s, learning 0.181s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0433
             Mean action noise std: 0.76
                       Mean reward: 9.99
               Mean episode length: 120.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 10.77s
                        Total time: 15737.99s
                               ETA: 1412406.2s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.062s, learning 0.187s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0446
             Mean action noise std: 0.76
                       Mean reward: 9.46
               Mean episode length: 115.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 11.25s
                        Total time: 15749.24s
                               ETA: 1412120.0s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.171s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0485
             Mean action noise std: 0.76
                       Mean reward: 10.17
               Mean episode length: 118.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 11.44s
                        Total time: 15760.68s
                               ETA: 1411851.5s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.010s, learning 0.163s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0444
             Mean action noise std: 0.76
                       Mean reward: 9.60
               Mean episode length: 120.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 11.17s
                        Total time: 15771.85s
                               ETA: 1411559.5s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.124s, learning 0.164s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0433
             Mean action noise std: 0.76
                       Mean reward: 10.64
               Mean episode length: 123.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 11.29s
                        Total time: 15783.14s
                               ETA: 1411278.3s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.952s, learning 0.182s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0474
             Mean action noise std: 0.76
                       Mean reward: 10.74
               Mean episode length: 123.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 11.13s
                        Total time: 15794.28s
                               ETA: 1410983.8s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.196s, learning 0.161s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0472
             Mean action noise std: 0.76
                       Mean reward: 10.51
               Mean episode length: 118.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 11.36s
                        Total time: 15805.63s
                               ETA: 1410709.8s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.130s, learning 0.172s)
               Value function loss: 0.0913
                    Surrogate loss: -0.0474
             Mean action noise std: 0.76
                       Mean reward: 11.36
               Mean episode length: 123.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 11.30s
                        Total time: 15816.93s
                               ETA: 1410431.3s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.973s, learning 0.227s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0476
             Mean action noise std: 0.76
                       Mean reward: 10.98
               Mean episode length: 118.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 11.20s
                        Total time: 15828.13s
                               ETA: 1410144.2s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.964s, learning 0.167s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0473
             Mean action noise std: 0.76
                       Mean reward: 11.55
               Mean episode length: 119.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 11.13s
                        Total time: 15839.27s
                               ETA: 1409851.4s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.050s, learning 0.178s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0527
             Mean action noise std: 0.76
                       Mean reward: 11.95
               Mean episode length: 121.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 11.23s
                        Total time: 15850.49s
                               ETA: 1409567.8s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.173s, learning 0.212s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0482
             Mean action noise std: 0.76
                       Mean reward: 11.41
               Mean episode length: 120.32
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 11.38s
                        Total time: 15861.88s
                               ETA: 1409298.7s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.693s, learning 0.178s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0500
             Mean action noise std: 0.76
                       Mean reward: 10.93
               Mean episode length: 123.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 10.87s
                        Total time: 15872.75s
                               ETA: 1408984.3s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.875s, learning 0.182s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 11.74
               Mean episode length: 122.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 11.06s
                        Total time: 15883.81s
                               ETA: 1408687.0s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.999s, learning 0.159s)
               Value function loss: 0.0943
                    Surrogate loss: -0.0466
             Mean action noise std: 0.76
                       Mean reward: 10.78
               Mean episode length: 121.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 11.16s
                        Total time: 15894.96s
                               ETA: 1408399.2s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.100s, learning 0.168s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0426
             Mean action noise std: 0.76
                       Mean reward: 11.70
               Mean episode length: 122.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 11.27s
                        Total time: 15906.23s
                               ETA: 1408121.6s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.649s, learning 0.162s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0464
             Mean action noise std: 0.76
                       Mean reward: 11.87
               Mean episode length: 121.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 10.81s
                        Total time: 15917.04s
                               ETA: 1407804.1s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.136s, learning 0.199s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0467
             Mean action noise std: 0.76
                       Mean reward: 11.28
               Mean episode length: 122.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 11.34s
                        Total time: 15928.38s
                               ETA: 1407533.5s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.264s, learning 0.160s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0441
             Mean action noise std: 0.76
                       Mean reward: 11.54
               Mean episode length: 121.22
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 11.42s
                        Total time: 15939.80s
                               ETA: 1407271.0s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.814s, learning 0.158s)
               Value function loss: 0.1055
                    Surrogate loss: -0.0439
             Mean action noise std: 0.76
                       Mean reward: 11.75
               Mean episode length: 122.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 10.97s
                        Total time: 15950.77s
                               ETA: 1406969.3s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.868s, learning 0.219s)
               Value function loss: 0.1082
                    Surrogate loss: -0.0479
             Mean action noise std: 0.76
                       Mean reward: 11.48
               Mean episode length: 120.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 11.09s
                        Total time: 15961.86s
                               ETA: 1406678.2s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.022s, learning 0.216s)
               Value function loss: 0.1044
                    Surrogate loss: -0.0459
             Mean action noise std: 0.76
                       Mean reward: 11.55
               Mean episode length: 120.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 11.24s
                        Total time: 15973.10s
                               ETA: 1406400.9s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.862s, learning 0.162s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0453
             Mean action noise std: 0.76
                       Mean reward: 12.24
               Mean episode length: 121.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 11.02s
                        Total time: 15984.12s
                               ETA: 1406105.3s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.929s, learning 0.165s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0478
             Mean action noise std: 0.76
                       Mean reward: 12.39
               Mean episode length: 122.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 11.09s
                        Total time: 15995.22s
                               ETA: 1405816.3s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.063s, learning 0.162s)
               Value function loss: 0.0961
                    Surrogate loss: -0.0431
             Mean action noise std: 0.76
                       Mean reward: 11.80
               Mean episode length: 118.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 11.22s
                        Total time: 16006.44s
                               ETA: 1405539.2s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.832s, learning 0.164s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0460
             Mean action noise std: 0.76
                       Mean reward: 12.67
               Mean episode length: 123.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 11.00s
                        Total time: 16017.44s
                               ETA: 1405242.6s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.686s, learning 0.166s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0441
             Mean action noise std: 0.76
                       Mean reward: 12.54
               Mean episode length: 121.29
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 10.85s
                        Total time: 16028.29s
                               ETA: 1404933.9s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.492s, learning 0.164s)
               Value function loss: 0.0910
                    Surrogate loss: -0.0456
             Mean action noise std: 0.76
                       Mean reward: 12.59
               Mean episode length: 120.49
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 10.66s
                        Total time: 16038.95s
                               ETA: 1404608.5s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.012s, learning 0.228s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0471
             Mean action noise std: 0.76
                       Mean reward: 12.76
               Mean episode length: 123.29
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 11.24s
                        Total time: 16050.19s
                               ETA: 1404334.7s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.181s, learning 0.163s)
               Value function loss: 0.0962
                    Surrogate loss: -0.0463
             Mean action noise std: 0.76
                       Mean reward: 12.08
               Mean episode length: 121.81
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 11.34s
                        Total time: 16061.53s
                               ETA: 1404070.6s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.652s, learning 0.159s)
               Value function loss: 0.0907
                    Surrogate loss: -0.0489
             Mean action noise std: 0.76
                       Mean reward: 12.71
               Mean episode length: 123.31
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 10.81s
                        Total time: 16072.34s
                               ETA: 1403760.2s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.792s, learning 0.195s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0432
             Mean action noise std: 0.76
                       Mean reward: 13.11
               Mean episode length: 123.18
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 10.99s
                        Total time: 16083.33s
                               ETA: 1403465.8s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.936s, learning 0.167s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0437
             Mean action noise std: 0.76
                       Mean reward: 13.76
               Mean episode length: 123.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 11.10s
                        Total time: 16094.43s
                               ETA: 1403182.0s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.786s, learning 0.197s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 13.06
               Mean episode length: 122.59
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 10.98s
                        Total time: 16105.42s
                               ETA: 1402888.2s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.048s, learning 0.164s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 13.55
               Mean episode length: 122.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 11.21s
                        Total time: 16116.63s
                               ETA: 1402614.9s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.162s)
               Value function loss: 0.1167
                    Surrogate loss: -0.0456
             Mean action noise std: 0.76
                       Mean reward: 14.39
               Mean episode length: 122.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 11.03s
                        Total time: 16127.66s
                               ETA: 1402325.9s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.506s, learning 0.208s)
               Value function loss: 0.0958
                    Surrogate loss: -0.0479
             Mean action noise std: 0.76
                       Mean reward: 14.18
               Mean episode length: 123.15
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 11.71s
                        Total time: 16139.37s
                               ETA: 1402097.0s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.006s, learning 0.188s)
               Value function loss: 0.1204
                    Surrogate loss: -0.0430
             Mean action noise std: 0.76
                       Mean reward: 14.47
               Mean episode length: 120.31
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 11.19s
                        Total time: 16150.56s
                               ETA: 1401823.5s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.165s)
               Value function loss: 0.1167
                    Surrogate loss: -0.0453
             Mean action noise std: 0.76
                       Mean reward: 14.70
               Mean episode length: 123.61
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 11.07s
                        Total time: 16161.63s
                               ETA: 1401539.6s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.819s, learning 0.232s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0413
             Mean action noise std: 0.76
                       Mean reward: 13.77
               Mean episode length: 117.94
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 11.05s
                        Total time: 16172.68s
                               ETA: 1401254.6s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.914s, learning 0.159s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 13.34
               Mean episode length: 113.59
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 11.07s
                        Total time: 16183.76s
                               ETA: 1400971.9s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.775s, learning 0.160s)
               Value function loss: 0.1361
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 14.21
               Mean episode length: 115.05
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 10.94s
                        Total time: 16194.69s
                               ETA: 1400677.9s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.785s, learning 0.203s)
               Value function loss: 0.1228
                    Surrogate loss: -0.0400
             Mean action noise std: 0.76
                       Mean reward: 14.91
               Mean episode length: 118.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 10.99s
                        Total time: 16205.68s
                               ETA: 1400388.9s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.873s, learning 0.198s)
               Value function loss: 0.1258
                    Surrogate loss: -0.0401
             Mean action noise std: 0.76
                       Mean reward: 14.55
               Mean episode length: 116.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 11.07s
                        Total time: 16216.75s
                               ETA: 1400107.6s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.969s, learning 0.223s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0452
             Mean action noise std: 0.76
                       Mean reward: 15.40
               Mean episode length: 120.71
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 11.19s
                        Total time: 16227.94s
                               ETA: 1399837.1s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.778s, learning 0.174s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0396
             Mean action noise std: 0.76
                       Mean reward: 14.91
               Mean episode length: 118.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 10.95s
                        Total time: 16238.89s
                               ETA: 1399546.4s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.299s, learning 0.216s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 14.98
               Mean episode length: 118.74
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 11.52s
                        Total time: 16250.41s
                               ETA: 1399304.7s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.597s, learning 0.168s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 15.03
               Mean episode length: 118.98
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 10.77s
                        Total time: 16261.17s
                               ETA: 1398998.8s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.296s, learning 0.167s)
               Value function loss: 0.1488
                    Surrogate loss: -0.0386
             Mean action noise std: 0.76
                       Mean reward: 15.20
               Mean episode length: 120.59
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 11.46s
                        Total time: 16272.64s
                               ETA: 1398753.5s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.092s, learning 0.229s)
               Value function loss: 0.1353
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 15.69
               Mean episode length: 119.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 11.32s
                        Total time: 16283.96s
                               ETA: 1398496.4s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.807s, learning 0.161s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 15.79
               Mean episode length: 120.37
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 10.97s
                        Total time: 16294.93s
                               ETA: 1398209.4s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.449s, learning 0.164s)
               Value function loss: 0.1641
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 15.38
               Mean episode length: 120.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 10.61s
                        Total time: 16305.54s
                               ETA: 1397892.4s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.311s, learning 0.174s)
               Value function loss: 0.1294
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 16.06
               Mean episode length: 123.34
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 11.49s
                        Total time: 16317.02s
                               ETA: 1397650.7s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.596s, learning 0.174s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 15.80
               Mean episode length: 123.19
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 10.77s
                        Total time: 16327.79s
                               ETA: 1397348.2s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.116s, learning 0.170s)
               Value function loss: 0.1764
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 15.97
               Mean episode length: 122.87
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 11.29s
                        Total time: 16339.08s
                               ETA: 1397090.4s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.686s, learning 0.175s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 16.22
               Mean episode length: 124.10
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 10.86s
                        Total time: 16349.94s
                               ETA: 1396796.7s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.893s, learning 0.172s)
               Value function loss: 0.1621
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 16.09
               Mean episode length: 121.51
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 11.07s
                        Total time: 16361.01s
                               ETA: 1396520.8s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.167s)
               Value function loss: 0.1362
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 16.44
               Mean episode length: 123.87
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 10.88s
                        Total time: 16371.89s
                               ETA: 1396229.4s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.153s, learning 0.161s)
               Value function loss: 0.1505
                    Surrogate loss: -0.0369
             Mean action noise std: 0.76
                       Mean reward: 16.95
               Mean episode length: 124.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 11.31s
                        Total time: 16383.20s
                               ETA: 1395975.7s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.247s, learning 0.167s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0420
             Mean action noise std: 0.76
                       Mean reward: 16.90
               Mean episode length: 123.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 11.41s
                        Total time: 16394.61s
                               ETA: 1395730.9s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.506s, learning 0.172s)
               Value function loss: 0.1434
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 16.19
               Mean episode length: 123.35
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 10.68s
                        Total time: 16405.29s
                               ETA: 1395423.9s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.900s, learning 0.172s)
               Value function loss: 0.1454
                    Surrogate loss: -0.0341
             Mean action noise std: 0.76
                       Mean reward: 15.68
               Mean episode length: 121.39
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 11.07s
                        Total time: 16416.36s
                               ETA: 1395150.9s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.161s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 16.81
               Mean episode length: 124.98
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 11.20s
                        Total time: 16427.56s
                               ETA: 1394888.9s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.212s, learning 0.176s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 16.16
               Mean episode length: 123.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 11.39s
                        Total time: 16438.95s
                               ETA: 1394643.6s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.101s, learning 0.187s)
               Value function loss: 0.1369
                    Surrogate loss: -0.0419
             Mean action noise std: 0.76
                       Mean reward: 17.12
               Mean episode length: 124.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 11.29s
                        Total time: 16450.24s
                               ETA: 1394390.2s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.105s, learning 0.164s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0433
             Mean action noise std: 0.76
                       Mean reward: 16.36
               Mean episode length: 123.70
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 11.27s
                        Total time: 16461.51s
                               ETA: 1394135.7s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.154s, learning 0.185s)
               Value function loss: 0.1598
                    Surrogate loss: -0.0422
             Mean action noise std: 0.76
                       Mean reward: 16.68
               Mean episode length: 122.77
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 11.34s
                        Total time: 16472.84s
                               ETA: 1393887.5s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.818s, learning 0.161s)
               Value function loss: 0.1601
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 16.02
               Mean episode length: 121.27
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 10.98s
                        Total time: 16483.82s
                               ETA: 1393609.2s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.102s, learning 0.297s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 16.63
               Mean episode length: 122.92
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 11.40s
                        Total time: 16495.22s
                               ETA: 1393366.9s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.102s, learning 0.225s)
               Value function loss: 0.1417
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 17.52
               Mean episode length: 123.90
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 11.33s
                        Total time: 16506.55s
                               ETA: 1393118.8s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.880s, learning 0.161s)
               Value function loss: 0.1623
                    Surrogate loss: -0.0411
             Mean action noise std: 0.76
                       Mean reward: 17.21
               Mean episode length: 124.05
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 11.04s
                        Total time: 16517.59s
                               ETA: 1392847.2s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.822s, learning 0.172s)
               Value function loss: 0.1514
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 16.55
               Mean episode length: 121.20
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 10.99s
                        Total time: 16528.58s
                               ETA: 1392571.9s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.068s, learning 0.162s)
               Value function loss: 0.1855
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 16.64
               Mean episode length: 120.73
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 11.23s
                        Total time: 16539.81s
                               ETA: 1392317.0s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.201s, learning 0.190s)
               Value function loss: 0.1578
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 16.73
               Mean episode length: 122.66
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 11.39s
                        Total time: 16551.20s
                               ETA: 1392076.0s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.935s, learning 0.172s)
               Value function loss: 0.1366
                    Surrogate loss: -0.0417
             Mean action noise std: 0.76
                       Mean reward: 16.64
               Mean episode length: 123.98
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 11.11s
                        Total time: 16562.31s
                               ETA: 1391811.6s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.178s, learning 0.172s)
               Value function loss: 0.1752
                    Surrogate loss: -0.0386
             Mean action noise std: 0.76
                       Mean reward: 16.80
               Mean episode length: 122.96
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 11.35s
                        Total time: 16573.66s
                               ETA: 1391568.1s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.644s, learning 0.176s)
               Value function loss: 0.1439
                    Surrogate loss: -0.0436
             Mean action noise std: 0.76
                       Mean reward: 17.93
               Mean episode length: 123.85
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 10.82s
                        Total time: 16584.48s
                               ETA: 1391280.4s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.501s, learning 0.165s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0444
             Mean action noise std: 0.76
                       Mean reward: 17.29
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 10.67s
                        Total time: 16595.15s
                               ETA: 1390980.2s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.423s, learning 0.194s)
               Value function loss: 0.1704
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 16.98
               Mean episode length: 121.47
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 11.62s
                        Total time: 16606.76s
                               ETA: 1390760.2s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.993s, learning 0.162s)
               Value function loss: 0.1659
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 17.60
               Mean episode length: 124.26
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 11.15s
                        Total time: 16617.92s
                               ETA: 1390501.9s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.206s, learning 0.245s)
               Value function loss: 0.1787
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 18.78
               Mean episode length: 124.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 11.45s
                        Total time: 16629.37s
                               ETA: 1390268.8s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.955s, learning 0.171s)
               Value function loss: 0.1727
                    Surrogate loss: -0.0387
             Mean action noise std: 0.76
                       Mean reward: 17.59
               Mean episode length: 123.93
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 11.13s
                        Total time: 16640.50s
                               ETA: 1390008.8s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.100s, learning 0.170s)
               Value function loss: 0.2040
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 17.75
               Mean episode length: 122.64
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 11.27s
                        Total time: 16651.77s
                               ETA: 1389761.4s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.958s, learning 0.191s)
               Value function loss: 0.1917
                    Surrogate loss: -0.0416
             Mean action noise std: 0.76
                       Mean reward: 17.47
               Mean episode length: 122.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 11.15s
                        Total time: 16662.91s
                               ETA: 1389504.3s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.891s, learning 0.174s)
               Value function loss: 0.1744
                    Surrogate loss: -0.0357
             Mean action noise std: 0.76
                       Mean reward: 18.50
               Mean episode length: 124.93
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 11.07s
                        Total time: 16673.98s
                               ETA: 1389240.5s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.930s, learning 0.164s)
               Value function loss: 0.2290
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 18.12
               Mean episode length: 122.72
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 11.09s
                        Total time: 16685.07s
                               ETA: 1388979.7s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.598s, learning 0.179s)
               Value function loss: 0.1717
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 18.09
               Mean episode length: 124.23
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 10.78s
                        Total time: 16695.85s
                               ETA: 1388692.8s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.908s, learning 0.174s)
               Value function loss: 0.2341
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 18.32
               Mean episode length: 121.70
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 11.08s
                        Total time: 16706.93s
                               ETA: 1388431.9s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.924s, learning 0.160s)
               Value function loss: 0.2217
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 18.18
               Mean episode length: 122.58
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 11.08s
                        Total time: 16718.02s
                               ETA: 1388171.4s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.965s, learning 0.188s)
               Value function loss: 0.1743
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 19.00
               Mean episode length: 124.07
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 11.15s
                        Total time: 16729.17s
                               ETA: 1387917.1s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.131s, learning 0.167s)
               Value function loss: 0.1743
                    Surrogate loss: -0.0391
             Mean action noise std: 0.76
                       Mean reward: 18.80
               Mean episode length: 123.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 11.30s
                        Total time: 16740.47s
                               ETA: 1387675.2s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.031s, learning 0.175s)
               Value function loss: 0.2039
                    Surrogate loss: -0.0310
             Mean action noise std: 0.76
                       Mean reward: 18.87
               Mean episode length: 122.69
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 11.21s
                        Total time: 16751.67s
                               ETA: 1387426.1s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.072s, learning 0.161s)
               Value function loss: 0.2003
                    Surrogate loss: -0.0327
             Mean action noise std: 0.76
                       Mean reward: 18.05
               Mean episode length: 121.34
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 11.23s
                        Total time: 16762.91s
                               ETA: 1387179.6s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.832s, learning 0.248s)
               Value function loss: 0.2129
                    Surrogate loss: -0.0305
             Mean action noise std: 0.76
                       Mean reward: 18.52
               Mean episode length: 122.36
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 11.08s
                        Total time: 16773.99s
                               ETA: 1386920.8s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.808s, learning 0.190s)
               Value function loss: 0.2110
                    Surrogate loss: -0.0360
             Mean action noise std: 0.76
                       Mean reward: 18.14
               Mean episode length: 120.10
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 11.00s
                        Total time: 16784.98s
                               ETA: 1386655.8s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.767s, learning 0.193s)
               Value function loss: 0.2366
                    Surrogate loss: -0.0324
             Mean action noise std: 0.76
                       Mean reward: 18.63
               Mean episode length: 123.04
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 10.96s
                        Total time: 16795.94s
                               ETA: 1386388.0s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 789 steps/s (collection: 20.578s, learning 0.178s)
               Value function loss: 0.2171
                    Surrogate loss: -0.0342
             Mean action noise std: 0.76
                       Mean reward: 18.78
               Mean episode length: 122.02
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 20.76s
                        Total time: 16816.70s
                               ETA: 1386928.6s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 742 steps/s (collection: 21.851s, learning 0.213s)
               Value function loss: 0.2268
                    Surrogate loss: -0.0366
             Mean action noise std: 0.76
                       Mean reward: 19.28
               Mean episode length: 124.36
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 22.06s
                        Total time: 16838.76s
                               ETA: 1387576.0s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.568s, learning 0.176s)
               Value function loss: 0.2766
                    Surrogate loss: -0.0363
             Mean action noise std: 0.76
                       Mean reward: 19.08
               Mean episode length: 123.06
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 21.74s
                        Total time: 16860.51s
                               ETA: 1388196.0s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.375s, learning 0.160s)
               Value function loss: 0.2261
                    Surrogate loss: -0.0381
             Mean action noise std: 0.76
                       Mean reward: 18.92
               Mean episode length: 121.86
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 21.53s
                        Total time: 16882.04s
                               ETA: 1388797.6s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.593s, learning 0.170s)
               Value function loss: 0.2018
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 19.20
               Mean episode length: 123.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 21.76s
                        Total time: 16903.81s
                               ETA: 1389417.0s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.441s, learning 0.303s)
               Value function loss: 0.2913
                    Surrogate loss: -0.0248
             Mean action noise std: 0.76
                       Mean reward: 19.40
               Mean episode length: 123.35
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 21.74s
                        Total time: 16925.55s
                               ETA: 1390033.7s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.396s, learning 0.276s)
               Value function loss: 0.2005
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 19.10
               Mean episode length: 121.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 21.67s
                        Total time: 16947.22s
                               ETA: 1390643.5s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.369s, learning 0.164s)
               Value function loss: 0.2333
                    Surrogate loss: -0.0333
             Mean action noise std: 0.76
                       Mean reward: 19.78
               Mean episode length: 124.40
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 21.53s
                        Total time: 16968.76s
                               ETA: 1391240.8s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 772 steps/s (collection: 21.025s, learning 0.179s)
               Value function loss: 0.2183
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 19.09
               Mean episode length: 121.22
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 21.20s
                        Total time: 16989.96s
                               ETA: 1391810.1s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.256s, learning 0.163s)
               Value function loss: 0.2235
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 19.04
               Mean episode length: 122.22
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 21.42s
                        Total time: 17011.38s
                               ETA: 1392396.1s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.596s, learning 0.166s)
               Value function loss: 0.2234
                    Surrogate loss: -0.0343
             Mean action noise std: 0.75
                       Mean reward: 19.70
               Mean episode length: 122.27
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 21.76s
                        Total time: 17033.14s
                               ETA: 1393009.1s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.423s, learning 0.168s)
               Value function loss: 0.2055
                    Surrogate loss: -0.0303
             Mean action noise std: 0.75
                       Mean reward: 19.71
               Mean episode length: 123.61
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 21.59s
                        Total time: 17054.73s
                               ETA: 1393607.1s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 748 steps/s (collection: 21.695s, learning 0.189s)
               Value function loss: 0.2364
                    Surrogate loss: -0.0318
             Mean action noise std: 0.75
                       Mean reward: 19.03
               Mean episode length: 120.34
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 21.88s
                        Total time: 17076.62s
                               ETA: 1394228.0s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.189s, learning 0.228s)
               Value function loss: 0.2300
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 20.21
               Mean episode length: 122.97
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 21.42s
                        Total time: 17098.03s
                               ETA: 1394809.7s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.268s, learning 0.164s)
               Value function loss: 0.2556
                    Surrogate loss: -0.0322
             Mean action noise std: 0.75
                       Mean reward: 19.70
               Mean episode length: 122.03
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 21.43s
                        Total time: 17119.46s
                               ETA: 1395391.6s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.474s, learning 0.157s)
               Value function loss: 0.2655
                    Surrogate loss: -0.0347
             Mean action noise std: 0.75
                       Mean reward: 19.78
               Mean episode length: 124.58
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 21.63s
                        Total time: 17141.09s
                               ETA: 1395988.8s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.398s, learning 0.183s)
               Value function loss: 0.2518
                    Surrogate loss: -0.0287
             Mean action noise std: 0.75
                       Mean reward: 19.67
               Mean episode length: 121.12
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 21.58s
                        Total time: 17162.68s
                               ETA: 1396580.9s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.334s, learning 0.237s)
               Value function loss: 0.2708
                    Surrogate loss: -0.0343
             Mean action noise std: 0.75
                       Mean reward: 20.77
               Mean episode length: 124.14
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 21.57s
                        Total time: 17184.25s
                               ETA: 1397171.2s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.619s, learning 0.172s)
               Value function loss: 0.3032
                    Surrogate loss: -0.0320
             Mean action noise std: 0.75
                       Mean reward: 20.14
               Mean episode length: 122.21
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 21.79s
                        Total time: 17206.04s
                               ETA: 1397778.4s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 740 steps/s (collection: 21.947s, learning 0.170s)
               Value function loss: 0.2825
                    Surrogate loss: -0.0341
             Mean action noise std: 0.75
                       Mean reward: 19.25
               Mean episode length: 121.47
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 22.12s
                        Total time: 17228.16s
                               ETA: 1398410.9s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.546s, learning 0.208s)
               Value function loss: 0.3174
                    Surrogate loss: -0.0314
             Mean action noise std: 0.75
                       Mean reward: 19.86
               Mean episode length: 122.11
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 21.75s
                        Total time: 17249.91s
                               ETA: 1399013.0s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 768 steps/s (collection: 21.138s, learning 0.186s)
               Value function loss: 0.2661
                    Surrogate loss: -0.0332
             Mean action noise std: 0.75
                       Mean reward: 20.23
               Mean episode length: 122.24
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 21.32s
                        Total time: 17271.23s
                               ETA: 1399579.1s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.372s, learning 0.170s)
               Value function loss: 0.3212
                    Surrogate loss: -0.0295
             Mean action noise std: 0.75
                       Mean reward: 19.98
               Mean episode length: 121.47
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 21.54s
                        Total time: 17292.78s
                               ETA: 1400162.0s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 775 steps/s (collection: 20.970s, learning 0.170s)
               Value function loss: 0.3219
                    Surrogate loss: -0.0317
             Mean action noise std: 0.75
                       Mean reward: 19.47
               Mean episode length: 121.58
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 21.14s
                        Total time: 17313.92s
                               ETA: 1400711.3s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 762 steps/s (collection: 21.310s, learning 0.164s)
               Value function loss: 0.2789
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 19.53
               Mean episode length: 120.05
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 21.47s
                        Total time: 17335.39s
                               ETA: 1401286.7s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 738 steps/s (collection: 22.031s, learning 0.165s)
               Value function loss: 0.3181
                    Surrogate loss: -0.0357
             Mean action noise std: 0.75
                       Mean reward: 19.36
               Mean episode length: 120.81
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 22.20s
                        Total time: 17357.58s
                               ETA: 1401919.5s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.504s, learning 0.166s)
               Value function loss: 0.3348
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 19.47
               Mean episode length: 121.03
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 21.67s
                        Total time: 17379.25s
                               ETA: 1402508.7s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.506s, learning 0.252s)
               Value function loss: 0.3046
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 19.76
               Mean episode length: 119.60
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 21.76s
                        Total time: 17401.01s
                               ETA: 1403104.0s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 730 steps/s (collection: 22.250s, learning 0.166s)
               Value function loss: 0.2555
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 19.81
               Mean episode length: 119.99
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 22.42s
                        Total time: 17423.43s
                               ETA: 1403751.4s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.273s, learning 0.160s)
               Value function loss: 0.3377
                    Surrogate loss: -0.0331
             Mean action noise std: 0.75
                       Mean reward: 19.08
               Mean episode length: 118.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 21.43s
                        Total time: 17444.86s
                               ETA: 1404318.5s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.744s, learning 0.291s)
               Value function loss: 0.2990
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 20.55
               Mean episode length: 122.57
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 22.03s
                        Total time: 17466.90s
                               ETA: 1404933.1s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.730s, learning 0.198s)
               Value function loss: 0.3262
                    Surrogate loss: -0.0335
             Mean action noise std: 0.75
                       Mean reward: 20.80
               Mean episode length: 123.21
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 21.93s
                        Total time: 17488.83s
                               ETA: 1405538.1s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.393s, learning 0.238s)
               Value function loss: 0.3397
                    Surrogate loss: -0.0329
             Mean action noise std: 0.75
                       Mean reward: 20.21
               Mean episode length: 120.66
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 21.63s
                        Total time: 17510.46s
                               ETA: 1406118.1s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.318s, learning 0.296s)
               Value function loss: 0.3906
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 20.82
               Mean episode length: 124.24
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 21.61s
                        Total time: 17532.07s
                               ETA: 1406695.8s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.677s, learning 0.166s)
               Value function loss: 0.3596
                    Surrogate loss: -0.0324
             Mean action noise std: 0.75
                       Mean reward: 19.66
               Mean episode length: 120.58
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 21.84s
                        Total time: 17553.91s
                               ETA: 1407290.9s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.859s, learning 0.243s)
               Value function loss: 0.3554
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 20.12
               Mean episode length: 120.14
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 21.10s
                        Total time: 17575.01s
                               ETA: 1407825.7s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 763 steps/s (collection: 21.253s, learning 0.196s)
               Value function loss: 0.4244
                    Surrogate loss: -0.0311
             Mean action noise std: 0.75
                       Mean reward: 20.16
               Mean episode length: 122.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 21.45s
                        Total time: 17596.46s
                               ETA: 1408387.3s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.522s, learning 0.204s)
               Value function loss: 0.3496
                    Surrogate loss: -0.0319
             Mean action noise std: 0.75
                       Mean reward: 20.07
               Mean episode length: 121.76
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 18.73s
                        Total time: 17615.19s
                               ETA: 1408730.2s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.814s, learning 0.163s)
               Value function loss: 0.3896
                    Surrogate loss: -0.0258
             Mean action noise std: 0.75
                       Mean reward: 20.95
               Mean episode length: 123.15
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 10.98s
                        Total time: 17626.17s
                               ETA: 1408453.3s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.918s, learning 0.179s)
               Value function loss: 0.3328
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 20.23
               Mean episode length: 123.24
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 11.10s
                        Total time: 17637.26s
                               ETA: 1408186.5s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.867s, learning 0.168s)
               Value function loss: 0.2899
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 20.34
               Mean episode length: 123.65
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 11.03s
                        Total time: 17648.30s
                               ETA: 1407915.0s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.553s, learning 0.247s)
               Value function loss: 0.3440
                    Surrogate loss: -0.0345
             Mean action noise std: 0.75
                       Mean reward: 20.05
               Mean episode length: 121.77
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 10.80s
                        Total time: 17659.10s
                               ETA: 1407625.3s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.999s, learning 0.177s)
               Value function loss: 0.3401
                    Surrogate loss: -0.0293
             Mean action noise std: 0.75
                       Mean reward: 20.61
               Mean episode length: 122.41
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 11.18s
                        Total time: 17670.27s
                               ETA: 1407366.0s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.255s, learning 0.167s)
               Value function loss: 0.3685
                    Surrogate loss: -0.0332
             Mean action noise std: 0.75
                       Mean reward: 19.57
               Mean episode length: 120.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 11.42s
                        Total time: 17681.69s
                               ETA: 1407126.6s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.831s, learning 0.203s)
               Value function loss: 0.3570
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 20.53
               Mean episode length: 121.04
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 11.03s
                        Total time: 17692.73s
                               ETA: 1406856.8s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1460 steps/s (collection: 10.996s, learning 0.226s)
               Value function loss: 0.4258
                    Surrogate loss: -0.0347
             Mean action noise std: 0.75
                       Mean reward: 20.22
               Mean episode length: 120.63
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 11.22s
                        Total time: 17703.95s
                               ETA: 1406602.3s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.333s, learning 0.159s)
               Value function loss: 0.3890
                    Surrogate loss: -0.0342
             Mean action noise std: 0.75
                       Mean reward: 20.37
               Mean episode length: 122.25
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 11.49s
                        Total time: 17715.44s
                               ETA: 1406369.7s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.897s, learning 0.162s)
               Value function loss: 0.3462
                    Surrogate loss: -0.0319
             Mean action noise std: 0.75
                       Mean reward: 20.67
               Mean episode length: 123.31
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 11.06s
                        Total time: 17726.50s
                               ETA: 1406103.1s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.640s, learning 0.165s)
               Value function loss: 0.3956
                    Surrogate loss: -0.0335
             Mean action noise std: 0.75
                       Mean reward: 20.65
               Mean episode length: 121.65
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 10.80s
                        Total time: 17737.31s
                               ETA: 1405816.7s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.067s, learning 0.161s)
               Value function loss: 0.3708
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 21.21
               Mean episode length: 123.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 11.23s
                        Total time: 17748.53s
                               ETA: 1405564.4s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.176s)
               Value function loss: 0.3974
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 20.82
               Mean episode length: 123.14
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 11.21s
                        Total time: 17759.75s
                               ETA: 1405311.0s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.882s, learning 0.192s)
               Value function loss: 0.3530
                    Surrogate loss: -0.0327
             Mean action noise std: 0.75
                       Mean reward: 21.40
               Mean episode length: 122.58
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 11.07s
                        Total time: 17770.82s
                               ETA: 1405047.2s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.791s, learning 0.185s)
               Value function loss: 0.3897
                    Surrogate loss: -0.0330
             Mean action noise std: 0.75
                       Mean reward: 20.56
               Mean episode length: 123.25
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 10.98s
                        Total time: 17781.80s
                               ETA: 1404776.1s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.757s, learning 0.162s)
               Value function loss: 0.3554
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 21.10
               Mean episode length: 123.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 10.92s
                        Total time: 17792.71s
                               ETA: 1404500.9s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.727s, learning 0.161s)
               Value function loss: 0.4346
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 21.28
               Mean episode length: 123.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 10.89s
                        Total time: 17803.60s
                               ETA: 1404223.6s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.732s, learning 0.190s)
               Value function loss: 0.3475
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 20.31
               Mean episode length: 123.07
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 10.92s
                        Total time: 17814.52s
                               ETA: 1403949.5s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.249s, learning 0.171s)
               Value function loss: 0.4074
                    Surrogate loss: -0.0297
             Mean action noise std: 0.75
                       Mean reward: 20.95
               Mean episode length: 123.53
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 11.42s
                        Total time: 17825.94s
                               ETA: 1403714.9s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1568 steps/s (collection: 10.280s, learning 0.167s)
               Value function loss: 0.4049
                    Surrogate loss: -0.0334
             Mean action noise std: 0.75
                       Mean reward: 21.46
               Mean episode length: 123.60
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 10.45s
                        Total time: 17836.39s
                               ETA: 1403404.2s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.685s, learning 0.204s)
               Value function loss: 0.3594
                    Surrogate loss: -0.0297
             Mean action noise std: 0.75
                       Mean reward: 21.08
               Mean episode length: 123.40
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 10.89s
                        Total time: 17847.28s
                               ETA: 1403128.7s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1455 steps/s (collection: 10.970s, learning 0.286s)
               Value function loss: 0.3408
                    Surrogate loss: -0.0290
             Mean action noise std: 0.75
                       Mean reward: 20.85
               Mean episode length: 124.08
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 11.26s
                        Total time: 17858.54s
                               ETA: 1402882.5s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.863s, learning 0.185s)
               Value function loss: 0.4326
                    Surrogate loss: -0.0328
             Mean action noise std: 0.75
                       Mean reward: 21.23
               Mean episode length: 124.24
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 11.05s
                        Total time: 17869.58s
                               ETA: 1402620.3s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.164s)
               Value function loss: 0.4284
                    Surrogate loss: -0.0331
             Mean action noise std: 0.75
                       Mean reward: 20.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 11.36s
                        Total time: 17880.94s
                               ETA: 1402382.8s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.877s, learning 0.178s)
               Value function loss: 0.4356
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 21.54
               Mean episode length: 123.40
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 11.05s
                        Total time: 17892.00s
                               ETA: 1402121.9s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.141s, learning 0.171s)
               Value function loss: 0.4592
                    Surrogate loss: -0.0309
             Mean action noise std: 0.75
                       Mean reward: 22.06
               Mean episode length: 123.50
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 11.31s
                        Total time: 17903.31s
                               ETA: 1401881.5s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.073s, learning 0.167s)
               Value function loss: 0.4152
                    Surrogate loss: -0.0321
             Mean action noise std: 0.75
                       Mean reward: 21.56
               Mean episode length: 124.11
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 11.24s
                        Total time: 17914.55s
                               ETA: 1401636.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.941s, learning 0.175s)
               Value function loss: 0.4585
                    Surrogate loss: -0.0333
             Mean action noise std: 0.75
                       Mean reward: 20.57
               Mean episode length: 121.76
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 11.12s
                        Total time: 17925.66s
                               ETA: 1401381.1s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.156s, learning 0.199s)
               Value function loss: 0.3990
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 21.84
               Mean episode length: 123.30
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 11.35s
                        Total time: 17937.02s
                               ETA: 1401145.2s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.157s, learning 0.180s)
               Value function loss: 0.4581
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 21.98
               Mean episode length: 124.48
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 11.34s
                        Total time: 17948.36s
                               ETA: 1400908.3s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.057s, learning 0.208s)
               Value function loss: 0.3914
                    Surrogate loss: -0.0363
             Mean action noise std: 0.75
                       Mean reward: 21.68
               Mean episode length: 123.40
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 11.26s
                        Total time: 17959.62s
                               ETA: 1400666.0s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.572s, learning 0.184s)
               Value function loss: 0.4242
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 21.98
               Mean episode length: 124.96
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 10.76s
                        Total time: 17970.38s
                               ETA: 1400384.5s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.901s, learning 0.215s)
               Value function loss: 0.3872
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 22.11
               Mean episode length: 124.78
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 11.12s
                        Total time: 17981.49s
                               ETA: 1400131.5s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.887s, learning 0.177s)
               Value function loss: 0.3842
                    Surrogate loss: -0.0305
             Mean action noise std: 0.75
                       Mean reward: 21.93
               Mean episode length: 124.17
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 11.06s
                        Total time: 17992.56s
                               ETA: 1399874.8s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.697s, learning 0.203s)
               Value function loss: 0.4313
                    Surrogate loss: -0.0315
             Mean action noise std: 0.75
                       Mean reward: 21.99
               Mean episode length: 124.48
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 10.90s
                        Total time: 18003.46s
                               ETA: 1399605.8s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.713s, learning 0.162s)
               Value function loss: 0.4162
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 22.06
               Mean episode length: 124.21
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 10.88s
                        Total time: 18014.33s
                               ETA: 1399335.2s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.894s, learning 0.176s)
               Value function loss: 0.4201
                    Surrogate loss: -0.0336
             Mean action noise std: 0.75
                       Mean reward: 21.79
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 11.07s
                        Total time: 18025.40s
                               ETA: 1399080.2s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.334s, learning 0.181s)
               Value function loss: 0.3688
                    Surrogate loss: -0.0336
             Mean action noise std: 0.75
                       Mean reward: 22.10
               Mean episode length: 124.36
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 10.51s
                        Total time: 18035.92s
                               ETA: 1398782.4s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.155s, learning 0.194s)
               Value function loss: 0.4784
                    Surrogate loss: -0.0308
             Mean action noise std: 0.75
                       Mean reward: 22.15
               Mean episode length: 124.76
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 11.35s
                        Total time: 18047.27s
                               ETA: 1398549.8s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.890s, learning 0.202s)
               Value function loss: 0.4633
                    Surrogate loss: -0.0284
             Mean action noise std: 0.75
                       Mean reward: 21.89
               Mean episode length: 122.17
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 11.09s
                        Total time: 18058.36s
                               ETA: 1398297.6s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.885s, learning 0.192s)
               Value function loss: 0.4470
                    Surrogate loss: -0.0275
             Mean action noise std: 0.75
                       Mean reward: 22.32
               Mean episode length: 124.09
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 11.08s
                        Total time: 18069.43s
                               ETA: 1398044.6s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.889s, learning 0.261s)
               Value function loss: 0.4456
                    Surrogate loss: -0.0266
             Mean action noise std: 0.75
                       Mean reward: 21.76
               Mean episode length: 123.55
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 11.15s
                        Total time: 18080.58s
                               ETA: 1397797.7s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.969s, learning 0.192s)
               Value function loss: 0.5304
                    Surrogate loss: -0.0270
             Mean action noise std: 0.75
                       Mean reward: 22.37
               Mean episode length: 124.73
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 11.16s
                        Total time: 18091.74s
                               ETA: 1397551.9s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.437s, learning 0.216s)
               Value function loss: 0.4510
                    Surrogate loss: -0.0317
             Mean action noise std: 0.75
                       Mean reward: 22.24
               Mean episode length: 124.49
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 10.65s
                        Total time: 18102.40s
                               ETA: 1397267.3s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.823s, learning 0.194s)
               Value function loss: 0.4241
                    Surrogate loss: -0.0336
             Mean action noise std: 0.75
                       Mean reward: 22.25
               Mean episode length: 123.85
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 11.02s
                        Total time: 18113.42s
                               ETA: 1397011.3s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.228s, learning 0.191s)
               Value function loss: 0.5049
                    Surrogate loss: -0.0310
             Mean action noise std: 0.75
                       Mean reward: 22.27
               Mean episode length: 124.71
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 11.42s
                        Total time: 18124.84s
                               ETA: 1396786.7s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.012s, learning 0.170s)
               Value function loss: 0.3877
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 22.77
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 11.18s
                        Total time: 18136.02s
                               ETA: 1396544.1s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.929s, learning 0.213s)
               Value function loss: 0.4826
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 21.77
               Mean episode length: 122.57
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 11.14s
                        Total time: 18147.16s
                               ETA: 1396298.8s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.934s, learning 0.184s)
               Value function loss: 0.3920
                    Surrogate loss: -0.0326
             Mean action noise std: 0.75
                       Mean reward: 22.90
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 11.12s
                        Total time: 18158.28s
                               ETA: 1396051.9s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.923s, learning 0.195s)
               Value function loss: 0.4110
                    Surrogate loss: -0.0319
             Mean action noise std: 0.75
                       Mean reward: 21.68
               Mean episode length: 123.08
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 11.12s
                        Total time: 18169.40s
                               ETA: 1395805.5s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.765s, learning 0.170s)
               Value function loss: 0.4354
                    Surrogate loss: -0.0294
             Mean action noise std: 0.75
                       Mean reward: 23.11
               Mean episode length: 124.92
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 10.93s
                        Total time: 18180.33s
                               ETA: 1395545.4s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.885s, learning 0.168s)
               Value function loss: 0.3736
                    Surrogate loss: -0.0300
             Mean action noise std: 0.75
                       Mean reward: 22.74
               Mean episode length: 124.91
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 11.05s
                        Total time: 18191.38s
                               ETA: 1395294.6s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.428s, learning 0.173s)
               Value function loss: 0.4133
                    Surrogate loss: -0.0332
             Mean action noise std: 0.75
                       Mean reward: 22.36
               Mean episode length: 124.86
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 10.60s
                        Total time: 18201.98s
                               ETA: 1395009.6s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.708s, learning 0.204s)
               Value function loss: 0.4033
                    Surrogate loss: -0.0301
             Mean action noise std: 0.75
                       Mean reward: 22.75
               Mean episode length: 124.65
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 10.91s
                        Total time: 18212.90s
                               ETA: 1394748.9s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.570s, learning 0.175s)
               Value function loss: 0.5131
                    Surrogate loss: -0.0328
             Mean action noise std: 0.75
                       Mean reward: 22.43
               Mean episode length: 124.46
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 10.74s
                        Total time: 18223.64s
                               ETA: 1394475.8s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.768s, learning 0.162s)
               Value function loss: 0.4618
                    Surrogate loss: -0.0325
             Mean action noise std: 0.75
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 10.93s
                        Total time: 18234.57s
                               ETA: 1394217.2s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.344s, learning 0.174s)
               Value function loss: 0.4320
                    Surrogate loss: -0.0333
             Mean action noise std: 0.75
                       Mean reward: 23.20
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 11.52s
                        Total time: 18246.09s
                               ETA: 1394003.9s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.984s, learning 0.167s)
               Value function loss: 0.4515
                    Surrogate loss: -0.0373
             Mean action noise std: 0.75
                       Mean reward: 22.85
               Mean episode length: 124.86
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 11.15s
                        Total time: 18257.24s
                               ETA: 1393763.0s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.167s, learning 0.172s)
               Value function loss: 0.4828
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 22.75
               Mean episode length: 124.94
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 11.34s
                        Total time: 18268.58s
                               ETA: 1393536.7s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.152s, learning 0.202s)
               Value function loss: 0.4132
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 23.20
               Mean episode length: 124.98
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 11.35s
                        Total time: 18279.93s
                               ETA: 1393311.9s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.311s, learning 0.191s)
               Value function loss: 0.4182
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 23.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 11.50s
                        Total time: 18291.43s
                               ETA: 1393098.7s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.493s, learning 0.170s)
               Value function loss: 0.3982
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 23.17
               Mean episode length: 124.47
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 11.66s
                        Total time: 18303.10s
                               ETA: 1392898.1s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.983s, learning 0.191s)
               Value function loss: 0.3981
                    Surrogate loss: -0.0298
             Mean action noise std: 0.75
                       Mean reward: 23.24
               Mean episode length: 124.82
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 11.17s
                        Total time: 18314.27s
                               ETA: 1392660.6s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.162s)
               Value function loss: 0.4364
                    Surrogate loss: -0.0311
             Mean action noise std: 0.75
                       Mean reward: 23.16
               Mean episode length: 124.88
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 11.39s
                        Total time: 18325.66s
                               ETA: 1392439.7s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.821s, learning 0.164s)
               Value function loss: 0.3977
                    Surrogate loss: -0.0323
             Mean action noise std: 0.75
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 10.99s
                        Total time: 18336.64s
                               ETA: 1392188.6s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.970s, learning 0.158s)
               Value function loss: 0.4383
                    Surrogate loss: -0.0329
             Mean action noise std: 0.75
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 11.13s
                        Total time: 18347.77s
                               ETA: 1391948.6s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.424s, learning 0.197s)
               Value function loss: 0.4354
                    Surrogate loss: -0.0350
             Mean action noise std: 0.75
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 11.62s
                        Total time: 18359.39s
                               ETA: 1391746.4s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.073s, learning 0.175s)
               Value function loss: 0.3861
                    Surrogate loss: -0.0298
             Mean action noise std: 0.75
                       Mean reward: 23.85
               Mean episode length: 124.12
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 11.25s
                        Total time: 18370.64s
                               ETA: 1391516.2s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.170s)
               Value function loss: 0.3435
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 23.72
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 11.59s
                        Total time: 18382.23s
                               ETA: 1391312.1s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.734s, learning 0.172s)
               Value function loss: 0.4921
                    Surrogate loss: -0.0364
             Mean action noise std: 0.75
                       Mean reward: 24.34
               Mean episode length: 124.40
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 10.91s
                        Total time: 18393.14s
                               ETA: 1391056.7s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.832s, learning 0.206s)
               Value function loss: 0.4762
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 23.73
               Mean episode length: 124.27
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 11.04s
                        Total time: 18404.17s
                               ETA: 1390811.6s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.044s, learning 0.165s)
               Value function loss: 0.4336
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 24.75
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.21s
                        Total time: 18415.38s
                               ETA: 1390579.8s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.117s, learning 0.265s)
               Value function loss: 0.4319
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 24.67
               Mean episode length: 124.09
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 11.38s
                        Total time: 18426.77s
                               ETA: 1390361.5s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.998s, learning 0.167s)
               Value function loss: 0.4320
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 24.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 11.16s
                        Total time: 18437.93s
                               ETA: 1390127.0s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.162s)
               Value function loss: 0.4243
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 24.88
               Mean episode length: 124.08
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 11.34s
                        Total time: 18449.27s
                               ETA: 1389906.2s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.999s, learning 0.186s)
               Value function loss: 0.3962
                    Surrogate loss: -0.0393
             Mean action noise std: 0.75
                       Mean reward: 25.58
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 11.18s
                        Total time: 18460.46s
                               ETA: 1389673.9s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.234s, learning 0.180s)
               Value function loss: 0.4540
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 25.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 11.41s
                        Total time: 18471.87s
                               ETA: 1389459.1s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.674s, learning 0.172s)
               Value function loss: 0.4100
                    Surrogate loss: -0.0384
             Mean action noise std: 0.75
                       Mean reward: 25.03
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 10.85s
                        Total time: 18482.72s
                               ETA: 1389202.0s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.514s, learning 0.168s)
               Value function loss: 0.4550
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.68s
                        Total time: 18494.40s
                               ETA: 1389008.1s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.118s, learning 0.155s)
               Value function loss: 0.3781
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 25.98
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 11.27s
                        Total time: 18505.67s
                               ETA: 1388783.8s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.827s, learning 0.305s)
               Value function loss: 0.3881
                    Surrogate loss: -0.0330
             Mean action noise std: 0.75
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 11.13s
                        Total time: 18516.80s
                               ETA: 1388549.1s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.846s, learning 0.209s)
               Value function loss: 0.4661
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 26.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 11.06s
                        Total time: 18527.86s
                               ETA: 1388309.2s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.972s, learning 0.190s)
               Value function loss: 0.4381
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 26.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.16s
                        Total time: 18539.02s
                               ETA: 1388077.5s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.320s, learning 0.217s)
               Value function loss: 0.4362
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 26.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 11.54s
                        Total time: 18550.56s
                               ETA: 1387874.3s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.914s, learning 0.178s)
               Value function loss: 0.4502
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 27.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 11.09s
                        Total time: 18561.65s
                               ETA: 1387638.0s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.295s, learning 0.216s)
               Value function loss: 0.5367
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 27.77
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 11.51s
                        Total time: 18573.16s
                               ETA: 1387433.4s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.202s, learning 0.184s)
               Value function loss: 0.5151
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 27.14
               Mean episode length: 124.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 11.39s
                        Total time: 18584.55s
                               ETA: 1387219.7s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.504s, learning 0.161s)
               Value function loss: 0.4809
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 28.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.67s
                        Total time: 18596.21s
                               ETA: 1387027.2s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.785s, learning 0.210s)
               Value function loss: 0.4962
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 27.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 11.00s
                        Total time: 18607.21s
                               ETA: 1386785.1s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.926s, learning 0.197s)
               Value function loss: 0.5660
                    Surrogate loss: -0.0365
             Mean action noise std: 0.75
                       Mean reward: 28.51
               Mean episode length: 123.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 11.12s
                        Total time: 18618.33s
                               ETA: 1386552.7s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1456 steps/s (collection: 10.971s, learning 0.279s)
               Value function loss: 0.5654
                    Surrogate loss: -0.0331
             Mean action noise std: 0.75
                       Mean reward: 28.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 11.25s
                        Total time: 18629.58s
                               ETA: 1386330.1s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.307s, learning 0.164s)
               Value function loss: 0.4513
                    Surrogate loss: -0.0357
             Mean action noise std: 0.75
                       Mean reward: 27.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 11.47s
                        Total time: 18641.05s
                               ETA: 1386124.4s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.756s, learning 0.175s)
               Value function loss: 0.5137
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 28.09
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 10.93s
                        Total time: 18651.98s
                               ETA: 1385878.8s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.052s, learning 0.158s)
               Value function loss: 0.4573
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 27.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 11.21s
                        Total time: 18663.19s
                               ETA: 1385654.2s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.767s, learning 0.245s)
               Value function loss: 0.5466
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 28.99
               Mean episode length: 124.15
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 11.01s
                        Total time: 18674.20s
                               ETA: 1385415.3s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.755s, learning 0.212s)
               Value function loss: 0.4732
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 28.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 10.97s
                        Total time: 18685.17s
                               ETA: 1385173.3s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.044s, learning 0.192s)
               Value function loss: 0.5111
                    Surrogate loss: -0.0302
             Mean action noise std: 0.75
                       Mean reward: 27.90
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 11.24s
                        Total time: 18696.41s
                               ETA: 1384951.7s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.188s, learning 0.162s)
               Value function loss: 0.5109
                    Surrogate loss: -0.0333
             Mean action noise std: 0.75
                       Mean reward: 28.02
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 11.35s
                        Total time: 18707.76s
                               ETA: 1384738.8s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.458s, learning 0.162s)
               Value function loss: 0.4317
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 28.14
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 11.62s
                        Total time: 18719.38s
                               ETA: 1384546.2s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.955s, learning 0.169s)
               Value function loss: 0.4975
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 28.12
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 11.12s
                        Total time: 18730.50s
                               ETA: 1384317.2s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.765s, learning 0.182s)
               Value function loss: 0.5372
                    Surrogate loss: -0.0252
             Mean action noise std: 0.75
                       Mean reward: 28.05
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 10.95s
                        Total time: 18741.45s
                               ETA: 1384075.5s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.227s, learning 0.225s)
               Value function loss: 0.6083
                    Surrogate loss: -0.0296
             Mean action noise std: 0.75
                       Mean reward: 29.16
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 11.45s
                        Total time: 18752.90s
                               ETA: 1383871.4s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.698s, learning 0.212s)
               Value function loss: 0.5513
                    Surrogate loss: -0.0335
             Mean action noise std: 0.75
                       Mean reward: 29.15
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 10.91s
                        Total time: 18763.81s
                               ETA: 1383627.5s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.928s, learning 0.166s)
               Value function loss: 0.5311
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 28.66
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 11.09s
                        Total time: 18774.90s
                               ETA: 1383397.7s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.162s)
               Value function loss: 0.6460
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 29.41
               Mean episode length: 124.13
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 11.20s
                        Total time: 18786.10s
                               ETA: 1383175.6s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.193s, learning 0.164s)
               Value function loss: 0.6352
                    Surrogate loss: -0.0322
             Mean action noise std: 0.75
                       Mean reward: 29.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 11.36s
                        Total time: 18797.46s
                               ETA: 1382965.7s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.160s)
               Value function loss: 0.5089
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 29.06
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 11.36s
                        Total time: 18808.81s
                               ETA: 1382756.2s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.132s, learning 0.185s)
               Value function loss: 0.5009
                    Surrogate loss: -0.0363
             Mean action noise std: 0.75
                       Mean reward: 28.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 11.32s
                        Total time: 18820.13s
                               ETA: 1382543.9s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.867s, learning 0.194s)
               Value function loss: 0.4495
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 29.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 11.06s
                        Total time: 18831.19s
                               ETA: 1382313.2s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.025s, learning 0.168s)
               Value function loss: 0.4531
                    Surrogate loss: -0.0293
             Mean action noise std: 0.75
                       Mean reward: 29.12
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 11.19s
                        Total time: 18842.39s
                               ETA: 1382092.5s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.203s, learning 0.173s)
               Value function loss: 0.4632
                    Surrogate loss: -0.0331
             Mean action noise std: 0.75
                       Mean reward: 29.34
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 11.38s
                        Total time: 18853.76s
                               ETA: 1381885.5s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.029s, learning 0.192s)
               Value function loss: 0.4280
                    Surrogate loss: -0.0288
             Mean action noise std: 0.74
                       Mean reward: 29.68
               Mean episode length: 124.07
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 11.22s
                        Total time: 18864.98s
                               ETA: 1381667.5s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.143s, learning 0.187s)
               Value function loss: 0.4598
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 30.15
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 11.33s
                        Total time: 18876.31s
                               ETA: 1381457.7s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.561s, learning 0.165s)
               Value function loss: 0.4953
                    Surrogate loss: -0.0296
             Mean action noise std: 0.74
                       Mean reward: 30.29
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 10.73s
                        Total time: 18887.04s
                               ETA: 1381204.1s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.997s, learning 0.214s)
               Value function loss: 0.4699
                    Surrogate loss: -0.0326
             Mean action noise std: 0.74
                       Mean reward: 29.82
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 11.21s
                        Total time: 18898.25s
                               ETA: 1380986.2s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.160s)
               Value function loss: 0.4556
                    Surrogate loss: -0.0346
             Mean action noise std: 0.74
                       Mean reward: 29.73
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 11.36s
                        Total time: 18909.61s
                               ETA: 1380779.5s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.887s, learning 0.162s)
               Value function loss: 0.5482
                    Surrogate loss: -0.0334
             Mean action noise std: 0.74
                       Mean reward: 30.08
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 11.05s
                        Total time: 18920.66s
                               ETA: 1380550.4s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.847s, learning 0.279s)
               Value function loss: 0.6209
                    Surrogate loss: -0.0287
             Mean action noise std: 0.74
                       Mean reward: 29.93
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 11.13s
                        Total time: 18931.79s
                               ETA: 1380327.3s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.948s, learning 0.186s)
               Value function loss: 0.6252
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 29.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 11.13s
                        Total time: 18942.92s
                               ETA: 1380105.0s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.920s, learning 0.173s)
               Value function loss: 0.6577
                    Surrogate loss: -0.0291
             Mean action noise std: 0.74
                       Mean reward: 29.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 11.09s
                        Total time: 18954.01s
                               ETA: 1379880.1s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.788s, learning 0.186s)
               Value function loss: 0.7013
                    Surrogate loss: -0.0306
             Mean action noise std: 0.74
                       Mean reward: 29.81
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 10.97s
                        Total time: 18964.99s
                               ETA: 1379646.9s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.899s, learning 0.245s)
               Value function loss: 0.7041
                    Surrogate loss: -0.0295
             Mean action noise std: 0.74
                       Mean reward: 29.35
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 11.14s
                        Total time: 18976.13s
                               ETA: 1379426.3s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.939s, learning 0.261s)
               Value function loss: 0.6172
                    Surrogate loss: -0.0274
             Mean action noise std: 0.74
                       Mean reward: 28.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 11.20s
                        Total time: 18987.33s
                               ETA: 1379210.1s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.826s, learning 0.167s)
               Value function loss: 0.7767
                    Surrogate loss: -0.0283
             Mean action noise std: 0.74
                       Mean reward: 29.11
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 10.99s
                        Total time: 18998.32s
                               ETA: 1378979.2s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.843s, learning 0.170s)
               Value function loss: 0.6652
                    Surrogate loss: -0.0299
             Mean action noise std: 0.74
                       Mean reward: 29.93
               Mean episode length: 124.69
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 11.01s
                        Total time: 19009.34s
                               ETA: 1378750.1s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.105s, learning 0.170s)
               Value function loss: 0.7168
                    Surrogate loss: -0.0318
             Mean action noise std: 0.74
                       Mean reward: 29.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 11.28s
                        Total time: 19020.61s
                               ETA: 1378540.3s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.162s)
               Value function loss: 0.7305
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 29.67
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 11.43s
                        Total time: 19032.04s
                               ETA: 1378341.8s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.261s, learning 0.160s)
               Value function loss: 0.6866
                    Surrogate loss: -0.0268
             Mean action noise std: 0.74
                       Mean reward: 28.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 11.42s
                        Total time: 19043.46s
                               ETA: 1378143.1s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.580s, learning 0.164s)
               Value function loss: 0.7911
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 29.36
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 11.74s
                        Total time: 19055.21s
                               ETA: 1377968.0s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.903s, learning 0.231s)
               Value function loss: 0.7984
                    Surrogate loss: -0.0212
             Mean action noise std: 0.74
                       Mean reward: 29.54
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 11.13s
                        Total time: 19066.34s
                               ETA: 1377749.1s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.862s, learning 0.200s)
               Value function loss: 0.8799
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 29.39
               Mean episode length: 124.72
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 11.06s
                        Total time: 19077.40s
                               ETA: 1377525.3s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.495s, learning 0.181s)
               Value function loss: 0.8593
                    Surrogate loss: -0.0268
             Mean action noise std: 0.74
                       Mean reward: 29.31
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 10.68s
                        Total time: 19088.08s
                               ETA: 1377274.0s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.953s, learning 0.164s)
               Value function loss: 0.9597
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 29.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 11.12s
                        Total time: 19099.19s
                               ETA: 1377054.7s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.908s, learning 0.159s)
               Value function loss: 1.0100
                    Surrogate loss: -0.0221
             Mean action noise std: 0.74
                       Mean reward: 29.11
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 11.07s
                        Total time: 19110.26s
                               ETA: 1376832.2s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.168s)
               Value function loss: 0.8530
                    Surrogate loss: -0.0270
             Mean action noise std: 0.74
                       Mean reward: 29.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 11.42s
                        Total time: 19121.68s
                               ETA: 1376635.3s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.779s, learning 0.219s)
               Value function loss: 1.0252
                    Surrogate loss: -0.0221
             Mean action noise std: 0.74
                       Mean reward: 29.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 11.00s
                        Total time: 19132.68s
                               ETA: 1376408.4s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.873s, learning 0.168s)
               Value function loss: 0.9201
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 28.94
               Mean episode length: 124.10
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 11.04s
                        Total time: 19143.72s
                               ETA: 1376184.9s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.067s, learning 0.162s)
               Value function loss: 0.9340
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 29.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 11.23s
                        Total time: 19154.95s
                               ETA: 1375975.3s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.765s, learning 0.262s)
               Value function loss: 0.7456
                    Surrogate loss: -0.0278
             Mean action noise std: 0.74
                       Mean reward: 29.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 11.03s
                        Total time: 19165.97s
                               ETA: 1375751.5s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.727s, learning 0.160s)
               Value function loss: 0.7996
                    Surrogate loss: -0.0257
             Mean action noise std: 0.74
                       Mean reward: 29.78
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 10.89s
                        Total time: 19176.86s
                               ETA: 1375517.9s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.395s, learning 0.201s)
               Value function loss: 0.7184
                    Surrogate loss: -0.0307
             Mean action noise std: 0.74
                       Mean reward: 28.18
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 11.60s
                        Total time: 19188.46s
                               ETA: 1375335.4s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.908s, learning 0.202s)
               Value function loss: 0.8040
                    Surrogate loss: -0.0272
             Mean action noise std: 0.74
                       Mean reward: 28.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 11.11s
                        Total time: 19199.57s
                               ETA: 1375118.4s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.349s, learning 0.169s)
               Value function loss: 0.7134
                    Surrogate loss: -0.0275
             Mean action noise std: 0.74
                       Mean reward: 28.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 11.52s
                        Total time: 19211.08s
                               ETA: 1374930.9s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.034s, learning 0.190s)
               Value function loss: 0.7841
                    Surrogate loss: -0.0286
             Mean action noise std: 0.74
                       Mean reward: 28.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 11.22s
                        Total time: 19222.31s
                               ETA: 1374722.6s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.102s, learning 0.167s)
               Value function loss: 0.8308
                    Surrogate loss: -0.0275
             Mean action noise std: 0.74
                       Mean reward: 27.97
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 11.27s
                        Total time: 19233.58s
                               ETA: 1374517.8s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.165s)
               Value function loss: 0.6807
                    Surrogate loss: -0.0288
             Mean action noise std: 0.74
                       Mean reward: 28.56
               Mean episode length: 124.73
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 10.96s
                        Total time: 19244.54s
                               ETA: 1374291.5s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.082s, learning 0.254s)
               Value function loss: 0.6534
                    Surrogate loss: -0.0284
             Mean action noise std: 0.74
                       Mean reward: 28.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 11.34s
                        Total time: 19255.88s
                               ETA: 1374092.1s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.229s, learning 0.198s)
               Value function loss: 0.7952
                    Surrogate loss: -0.0287
             Mean action noise std: 0.74
                       Mean reward: 28.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 11.43s
                        Total time: 19267.30s
                               ETA: 1373899.4s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.171s)
               Value function loss: 0.8574
                    Surrogate loss: -0.0264
             Mean action noise std: 0.74
                       Mean reward: 27.40
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 11.54s
                        Total time: 19278.84s
                               ETA: 1373714.9s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.047s, learning 0.160s)
               Value function loss: 0.8094
                    Surrogate loss: -0.0261
             Mean action noise std: 0.74
                       Mean reward: 29.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 11.21s
                        Total time: 19290.05s
                               ETA: 1373507.0s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.392s, learning 0.167s)
               Value function loss: 0.8028
                    Surrogate loss: -0.0291
             Mean action noise std: 0.74
                       Mean reward: 28.78
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 11.56s
                        Total time: 19301.61s
                               ETA: 1373324.6s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.033s, learning 0.188s)
               Value function loss: 0.7987
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 27.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 11.22s
                        Total time: 19312.83s
                               ETA: 1373118.3s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.519s, learning 0.159s)
               Value function loss: 0.8416
                    Surrogate loss: -0.0278
             Mean action noise std: 0.74
                       Mean reward: 28.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 11.68s
                        Total time: 19324.50s
                               ETA: 1372944.7s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.918s, learning 0.167s)
               Value function loss: 0.6864
                    Surrogate loss: -0.0277
             Mean action noise std: 0.74
                       Mean reward: 29.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 11.08s
                        Total time: 19335.59s
                               ETA: 1372729.3s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.150s, learning 0.160s)
               Value function loss: 0.7580
                    Surrogate loss: -0.0284
             Mean action noise std: 0.74
                       Mean reward: 28.29
               Mean episode length: 124.11
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 11.31s
                        Total time: 19346.90s
                               ETA: 1372530.2s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.837s, learning 0.163s)
               Value function loss: 0.6395
                    Surrogate loss: -0.0321
             Mean action noise std: 0.74
                       Mean reward: 28.37
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 11.00s
                        Total time: 19357.90s
                               ETA: 1372309.4s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.840s, learning 0.168s)
               Value function loss: 0.7375
                    Surrogate loss: -0.0303
             Mean action noise std: 0.74
                       Mean reward: 28.97
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 11.01s
                        Total time: 19368.91s
                               ETA: 1372089.4s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.734s, learning 0.168s)
               Value function loss: 0.6696
                    Surrogate loss: -0.0267
             Mean action noise std: 0.74
                       Mean reward: 28.53
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 10.90s
                        Total time: 19379.81s
                               ETA: 1371862.2s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.862s, learning 0.168s)
               Value function loss: 0.5932
                    Surrogate loss: -0.0322
             Mean action noise std: 0.74
                       Mean reward: 29.34
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 11.03s
                        Total time: 19390.84s
                               ETA: 1371644.4s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.218s, learning 0.255s)
               Value function loss: 0.7082
                    Surrogate loss: -0.0263
             Mean action noise std: 0.74
                       Mean reward: 29.87
               Mean episode length: 124.10
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 11.47s
                        Total time: 19402.31s
                               ETA: 1371458.2s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.216s, learning 0.187s)
               Value function loss: 0.6336
                    Surrogate loss: -0.0304
             Mean action noise std: 0.74
                       Mean reward: 29.12
               Mean episode length: 124.10
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 11.40s
                        Total time: 19413.71s
                               ETA: 1371267.3s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.077s, learning 0.209s)
               Value function loss: 0.6317
                    Surrogate loss: -0.0295
             Mean action noise std: 0.74
                       Mean reward: 29.41
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 11.29s
                        Total time: 19425.00s
                               ETA: 1371068.5s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.788s, learning 0.162s)
               Value function loss: 0.5597
                    Surrogate loss: -0.0307
             Mean action noise std: 0.74
                       Mean reward: 29.79
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 10.95s
                        Total time: 19435.95s
                               ETA: 1370846.2s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.491s, learning 0.160s)
               Value function loss: 0.6881
                    Surrogate loss: -0.0295
             Mean action noise std: 0.74
                       Mean reward: 29.36
               Mean episode length: 124.07
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 10.65s
                        Total time: 19446.60s
                               ETA: 1370603.0s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.546s, learning 0.169s)
               Value function loss: 0.6966
                    Surrogate loss: -0.0311
             Mean action noise std: 0.74
                       Mean reward: 29.35
               Mean episode length: 124.29
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 11.72s
                        Total time: 19458.32s
                               ETA: 1370435.3s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.473s, learning 0.164s)
               Value function loss: 0.6350
                    Surrogate loss: -0.0311
             Mean action noise std: 0.74
                       Mean reward: 29.70
               Mean episode length: 124.89
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 10.64s
                        Total time: 19468.95s
                               ETA: 1370191.8s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.841s, learning 0.164s)
               Value function loss: 0.6765
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 30.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 11.01s
                        Total time: 19479.96s
                               ETA: 1369974.6s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.765s, learning 0.185s)
               Value function loss: 0.7401
                    Surrogate loss: -0.0287
             Mean action noise std: 0.74
                       Mean reward: 30.45
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 10.95s
                        Total time: 19490.91s
                               ETA: 1369753.9s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.921s, learning 0.169s)
               Value function loss: 0.6943
                    Surrogate loss: -0.0264
             Mean action noise std: 0.74
                       Mean reward: 30.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 11.09s
                        Total time: 19502.00s
                               ETA: 1369543.2s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.200s, learning 0.171s)
               Value function loss: 0.6028
                    Surrogate loss: -0.0294
             Mean action noise std: 0.74
                       Mean reward: 30.08
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 11.37s
                        Total time: 19513.37s
                               ETA: 1369352.5s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.182s, learning 0.158s)
               Value function loss: 0.7145
                    Surrogate loss: -0.0266
             Mean action noise std: 0.74
                       Mean reward: 30.41
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 11.34s
                        Total time: 19524.71s
                               ETA: 1369159.9s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.160s)
               Value function loss: 0.5815
                    Surrogate loss: -0.0304
             Mean action noise std: 0.74
                       Mean reward: 30.72
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 11.17s
                        Total time: 19535.88s
                               ETA: 1368955.5s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.053s, learning 0.166s)
               Value function loss: 0.7054
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 30.30
               Mean episode length: 124.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 11.22s
                        Total time: 19547.10s
                               ETA: 1368755.0s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.166s)
               Value function loss: 0.6328
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 30.58
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 10.94s
                        Total time: 19558.04s
                               ETA: 1368535.3s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.205s, learning 0.171s)
               Value function loss: 0.7166
                    Surrogate loss: -0.0280
             Mean action noise std: 0.74
                       Mean reward: 30.76
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 11.38s
                        Total time: 19569.42s
                               ETA: 1368346.3s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.849s, learning 0.161s)
               Value function loss: 0.6295
                    Surrogate loss: -0.0288
             Mean action noise std: 0.74
                       Mean reward: 30.68
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 11.01s
                        Total time: 19580.43s
                               ETA: 1368131.9s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.647s, learning 0.183s)
               Value function loss: 0.6291
                    Surrogate loss: -0.0272
             Mean action noise std: 0.74
                       Mean reward: 31.05
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 11.83s
                        Total time: 19592.26s
                               ETA: 1367975.2s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.280s, learning 0.169s)
               Value function loss: 0.6795
                    Surrogate loss: -0.0304
             Mean action noise std: 0.74
                       Mean reward: 31.31
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 11.45s
                        Total time: 19603.71s
                               ETA: 1367792.0s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.168s, learning 0.163s)
               Value function loss: 0.6841
                    Surrogate loss: -0.0268
             Mean action noise std: 0.74
                       Mean reward: 31.23
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 11.33s
                        Total time: 19615.04s
                               ETA: 1367600.9s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.689s, learning 0.181s)
               Value function loss: 0.9096
                    Surrogate loss: -0.0204
             Mean action noise std: 0.74
                       Mean reward: 31.31
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 11.87s
                        Total time: 19626.91s
                               ETA: 1367447.6s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.804s, learning 0.176s)
               Value function loss: 0.7914
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 30.59
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 10.98s
                        Total time: 19637.89s
                               ETA: 1367232.5s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.504s, learning 0.160s)
               Value function loss: 0.7378
                    Surrogate loss: -0.0258
             Mean action noise std: 0.74
                       Mean reward: 31.06
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 11.66s
                        Total time: 19649.55s
                               ETA: 1367065.3s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.020s, learning 0.168s)
               Value function loss: 0.7701
                    Surrogate loss: -0.0305
             Mean action noise std: 0.74
                       Mean reward: 31.76
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 11.19s
                        Total time: 19660.74s
                               ETA: 1366865.2s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.038s, learning 0.168s)
               Value function loss: 0.8514
                    Surrogate loss: -0.0243
             Mean action noise std: 0.74
                       Mean reward: 30.68
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 11.21s
                        Total time: 19671.95s
                               ETA: 1366666.6s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.794s, learning 0.163s)
               Value function loss: 0.7249
                    Surrogate loss: -0.0257
             Mean action noise std: 0.74
                       Mean reward: 31.66
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 10.96s
                        Total time: 19682.90s
                               ETA: 1366451.0s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.178s, learning 0.158s)
               Value function loss: 0.6705
                    Surrogate loss: -0.0278
             Mean action noise std: 0.74
                       Mean reward: 30.79
               Mean episode length: 124.25
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 11.34s
                        Total time: 19694.24s
                               ETA: 1366262.0s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.042s, learning 0.200s)
               Value function loss: 0.6197
                    Surrogate loss: -0.0300
             Mean action noise std: 0.74
                       Mean reward: 31.44
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 11.24s
                        Total time: 19705.48s
                               ETA: 1366066.7s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.740s, learning 0.183s)
               Value function loss: 0.6458
                    Surrogate loss: -0.0311
             Mean action noise std: 0.74
                       Mean reward: 31.83
               Mean episode length: 124.04
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 10.92s
                        Total time: 19716.41s
                               ETA: 1365849.5s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.915s, learning 0.174s)
               Value function loss: 0.7837
                    Surrogate loss: -0.0221
             Mean action noise std: 0.74
                       Mean reward: 31.70
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 11.09s
                        Total time: 19727.50s
                               ETA: 1365644.2s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.049s, learning 0.161s)
               Value function loss: 0.6853
                    Surrogate loss: -0.0262
             Mean action noise std: 0.74
                       Mean reward: 31.57
               Mean episode length: 124.94
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 11.21s
                        Total time: 19738.71s
                               ETA: 1365447.5s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.000s, learning 0.285s)
               Value function loss: 0.7776
                    Surrogate loss: -0.0241
             Mean action noise std: 0.74
                       Mean reward: 31.91
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 11.29s
                        Total time: 19749.99s
                               ETA: 1365256.2s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.429s, learning 0.164s)
               Value function loss: 0.8340
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: 31.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 11.59s
                        Total time: 19761.58s
                               ETA: 1365086.5s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.332s, learning 0.218s)
               Value function loss: 0.7027
                    Surrogate loss: -0.0278
             Mean action noise std: 0.74
                       Mean reward: 32.15
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 11.55s
                        Total time: 19773.13s
                               ETA: 1364913.9s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.231s, learning 0.170s)
               Value function loss: 0.6441
                    Surrogate loss: -0.0229
             Mean action noise std: 0.74
                       Mean reward: 31.88
               Mean episode length: 124.10
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 11.40s
                        Total time: 19784.53s
                               ETA: 1364731.3s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.804s, learning 0.193s)
               Value function loss: 0.8914
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 31.80
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 11.00s
                        Total time: 19795.53s
                               ETA: 1364521.2s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.860s, learning 0.164s)
               Value function loss: 0.8679
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 32.02
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 11.02s
                        Total time: 19806.55s
                               ETA: 1364313.2s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.851s, learning 0.184s)
               Value function loss: 0.9107
                    Surrogate loss: -0.0232
             Mean action noise std: 0.74
                       Mean reward: 32.24
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 11.03s
                        Total time: 19817.59s
                               ETA: 1364106.1s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.859s, learning 0.185s)
               Value function loss: 0.9517
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 32.01
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 11.04s
                        Total time: 19828.63s
                               ETA: 1363900.0s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.148s, learning 0.269s)
               Value function loss: 1.0103
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 31.51
               Mean episode length: 124.05
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 11.42s
                        Total time: 19840.05s
                               ETA: 1363719.8s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.160s)
               Value function loss: 1.0170
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 31.71
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 11.12s
                        Total time: 19851.17s
                               ETA: 1363519.2s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.159s)
               Value function loss: 0.8659
                    Surrogate loss: -0.0243
             Mean action noise std: 0.74
                       Mean reward: 32.12
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 11.04s
                        Total time: 19862.21s
                               ETA: 1363313.8s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.238s, learning 0.188s)
               Value function loss: 0.6683
                    Surrogate loss: -0.0298
             Mean action noise std: 0.74
                       Mean reward: 32.37
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 11.43s
                        Total time: 19873.63s
                               ETA: 1363134.9s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.483s, learning 0.174s)
               Value function loss: 0.5926
                    Surrogate loss: -0.0280
             Mean action noise std: 0.74
                       Mean reward: 32.55
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 11.66s
                        Total time: 19885.29s
                               ETA: 1362972.2s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.816s, learning 0.295s)
               Value function loss: 0.7063
                    Surrogate loss: -0.0296
             Mean action noise std: 0.74
                       Mean reward: 32.18
               Mean episode length: 124.11
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 11.11s
                        Total time: 19896.40s
                               ETA: 1362772.3s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.878s, learning 0.163s)
               Value function loss: 0.6034
                    Surrogate loss: -0.0257
             Mean action noise std: 0.74
                       Mean reward: 32.17
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 11.04s
                        Total time: 19907.44s
                               ETA: 1362567.8s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.915s, learning 0.185s)
               Value function loss: 0.6152
                    Surrogate loss: -0.0295
             Mean action noise std: 0.74
                       Mean reward: 31.64
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 11.10s
                        Total time: 19918.54s
                               ETA: 1362367.6s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.806s, learning 0.293s)
               Value function loss: 0.7021
                    Surrogate loss: -0.0281
             Mean action noise std: 0.74
                       Mean reward: 32.91
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 11.10s
                        Total time: 19929.64s
                               ETA: 1362167.6s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.835s, learning 0.162s)
               Value function loss: 0.6790
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 32.20
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 11.00s
                        Total time: 19940.64s
                               ETA: 1361960.9s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.089s, learning 0.170s)
               Value function loss: 0.6671
                    Surrogate loss: -0.0286
             Mean action noise std: 0.74
                       Mean reward: 32.19
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 11.26s
                        Total time: 19951.90s
                               ETA: 1361772.4s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.001s, learning 0.182s)
               Value function loss: 0.6588
                    Surrogate loss: -0.0260
             Mean action noise std: 0.74
                       Mean reward: 31.84
               Mean episode length: 124.31
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 11.18s
                        Total time: 19963.08s
                               ETA: 1361578.9s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.104s, learning 0.173s)
               Value function loss: 0.8575
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 32.09
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 11.28s
                        Total time: 19974.36s
                               ETA: 1361392.1s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.257s, learning 0.163s)
               Value function loss: 0.7784
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 32.03
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 11.42s
                        Total time: 19985.78s
                               ETA: 1361215.2s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.017s, learning 0.160s)
               Value function loss: 0.7724
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 31.28
               Mean episode length: 124.34
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 11.18s
                        Total time: 19996.95s
                               ETA: 1361022.0s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.688s, learning 0.198s)
               Value function loss: 0.8643
                    Surrogate loss: -0.0212
             Mean action noise std: 0.74
                       Mean reward: 30.59
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 10.89s
                        Total time: 20007.84s
                               ETA: 1360809.3s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.817s, learning 0.204s)
               Value function loss: 0.9794
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 31.14
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 11.02s
                        Total time: 20018.86s
                               ETA: 1360606.1s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.069s, learning 0.198s)
               Value function loss: 0.7409
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 30.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 11.27s
                        Total time: 20030.13s
                               ETA: 1360419.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.127s, learning 0.165s)
               Value function loss: 0.7108
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 29.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 11.29s
                        Total time: 20041.42s
                               ETA: 1360235.5s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.854s, learning 0.162s)
               Value function loss: 0.7783
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 29.89
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 11.02s
                        Total time: 20052.44s
                               ETA: 1360032.8s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.090s, learning 0.164s)
               Value function loss: 0.6437
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 28.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 11.25s
                        Total time: 20063.69s
                               ETA: 1359846.4s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1458 steps/s (collection: 10.943s, learning 0.294s)
               Value function loss: 0.6923
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 28.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 11.24s
                        Total time: 20074.93s
                               ETA: 1359659.1s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.780s, learning 0.171s)
               Value function loss: 0.5652
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 28.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 10.95s
                        Total time: 20085.88s
                               ETA: 1359452.6s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.114s, learning 0.168s)
               Value function loss: 0.6781
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 28.58
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 11.28s
                        Total time: 20097.16s
                               ETA: 1359268.9s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.958s, learning 0.213s)
               Value function loss: 0.7075
                    Surrogate loss: -0.0269
             Mean action noise std: 0.74
                       Mean reward: 26.98
               Mean episode length: 124.61
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 11.17s
                        Total time: 20108.33s
                               ETA: 1359077.8s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.082s, learning 0.166s)
               Value function loss: 0.6643
                    Surrogate loss: -0.0236
             Mean action noise std: 0.74
                       Mean reward: 28.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 11.25s
                        Total time: 20119.58s
                               ETA: 1358892.2s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.005s, learning 0.256s)
               Value function loss: 0.6778
                    Surrogate loss: -0.0264
             Mean action noise std: 0.74
                       Mean reward: 28.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 11.26s
                        Total time: 20130.84s
                               ETA: 1358707.8s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.771s, learning 0.241s)
               Value function loss: 0.7860
                    Surrogate loss: -0.0263
             Mean action noise std: 0.74
                       Mean reward: 27.39
               Mean episode length: 124.09
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 11.01s
                        Total time: 20141.85s
                               ETA: 1358506.7s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.466s, learning 0.166s)
               Value function loss: 0.7926
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 27.43
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 11.63s
                        Total time: 20153.49s
                               ETA: 1358347.7s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.910s, learning 0.184s)
               Value function loss: 0.7856
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 28.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 11.09s
                        Total time: 20164.58s
                               ETA: 1358152.6s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.166s)
               Value function loss: 0.7846
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 28.20
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.43s
                        Total time: 20176.01s
                               ETA: 1357980.6s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.695s, learning 0.188s)
               Value function loss: 0.8206
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 28.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 10.88s
                        Total time: 20186.89s
                               ETA: 1357771.9s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.067s, learning 0.281s)
               Value function loss: 0.7656
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 27.68
               Mean episode length: 124.53
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 11.35s
                        Total time: 20198.24s
                               ETA: 1357594.8s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.997s, learning 0.188s)
               Value function loss: 0.6683
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 27.25
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 11.18s
                        Total time: 20209.43s
                               ETA: 1357406.8s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.940s, learning 0.189s)
               Value function loss: 0.7583
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 28.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 11.13s
                        Total time: 20220.56s
                               ETA: 1357215.4s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.832s, learning 0.163s)
               Value function loss: 0.6204
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 26.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 10.99s
                        Total time: 20231.55s
                               ETA: 1357015.2s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.898s, learning 0.169s)
               Value function loss: 0.7421
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 27.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 11.07s
                        Total time: 20242.62s
                               ETA: 1356820.1s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.964s, learning 0.194s)
               Value function loss: 0.7244
                    Surrogate loss: -0.0214
             Mean action noise std: 0.74
                       Mean reward: 27.93
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 11.16s
                        Total time: 20253.78s
                               ETA: 1356631.3s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.164s)
               Value function loss: 0.5216
                    Surrogate loss: -0.0295
             Mean action noise std: 0.74
                       Mean reward: 28.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 11.39s
                        Total time: 20265.17s
                               ETA: 1356458.4s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.083s, learning 0.176s)
               Value function loss: 0.7184
                    Surrogate loss: -0.0268
             Mean action noise std: 0.74
                       Mean reward: 28.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 11.26s
                        Total time: 20276.43s
                               ETA: 1356276.9s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.653s, learning 0.292s)
               Value function loss: 0.7280
                    Surrogate loss: -0.0289
             Mean action noise std: 0.74
                       Mean reward: 27.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 10.94s
                        Total time: 20287.37s
                               ETA: 1356074.6s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.009s, learning 0.177s)
               Value function loss: 0.7102
                    Surrogate loss: -0.0227
             Mean action noise std: 0.74
                       Mean reward: 28.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 11.19s
                        Total time: 20298.56s
                               ETA: 1355888.6s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.163s)
               Value function loss: 0.6146
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 28.04
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 11.48s
                        Total time: 20310.04s
                               ETA: 1355722.4s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.835s, learning 0.173s)
               Value function loss: 0.7008
                    Surrogate loss: -0.0242
             Mean action noise std: 0.74
                       Mean reward: 28.05
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 11.01s
                        Total time: 20321.04s
                               ETA: 1355525.0s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.271s, learning 0.195s)
               Value function loss: 0.7298
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 28.93
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 11.47s
                        Total time: 20332.51s
                               ETA: 1355358.4s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.024s, learning 0.156s)
               Value function loss: 0.6509
                    Surrogate loss: -0.0265
             Mean action noise std: 0.74
                       Mean reward: 27.77
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 11.18s
                        Total time: 20343.69s
                               ETA: 1355173.0s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.844s, learning 0.220s)
               Value function loss: 0.6435
                    Surrogate loss: -0.0265
             Mean action noise std: 0.74
                       Mean reward: 28.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 11.06s
                        Total time: 20354.75s
                               ETA: 1354980.1s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.592s, learning 0.200s)
               Value function loss: 0.7433
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 28.58
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 10.79s
                        Total time: 20365.55s
                               ETA: 1354769.4s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.675s, learning 0.156s)
               Value function loss: 0.6083
                    Surrogate loss: -0.0262
             Mean action noise std: 0.74
                       Mean reward: 28.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 10.83s
                        Total time: 20376.38s
                               ETA: 1354561.6s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.343s, learning 0.169s)
               Value function loss: 0.6222
                    Surrogate loss: -0.0286
             Mean action noise std: 0.74
                       Mean reward: 29.41
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 11.51s
                        Total time: 20387.89s
                               ETA: 1354399.2s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.867s, learning 0.194s)
               Value function loss: 0.7179
                    Surrogate loss: -0.0206
             Mean action noise std: 0.74
                       Mean reward: 29.04
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 11.06s
                        Total time: 20398.95s
                               ETA: 1354207.1s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.900s, learning 0.160s)
               Value function loss: 0.6049
                    Surrogate loss: -0.0270
             Mean action noise std: 0.74
                       Mean reward: 28.73
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 11.06s
                        Total time: 20410.01s
                               ETA: 1354015.2s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.465s, learning 0.239s)
               Value function loss: 0.6389
                    Surrogate loss: -0.0254
             Mean action noise std: 0.74
                       Mean reward: 30.27
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 11.70s
                        Total time: 20421.71s
                               ETA: 1353866.2s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.667s, learning 0.184s)
               Value function loss: 0.5157
                    Surrogate loss: -0.0269
             Mean action noise std: 0.74
                       Mean reward: 29.81
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 10.85s
                        Total time: 20432.57s
                               ETA: 1353660.9s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.022s, learning 0.176s)
               Value function loss: 0.4827
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 29.42
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 11.20s
                        Total time: 20443.76s
                               ETA: 1353478.8s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.042s, learning 0.168s)
               Value function loss: 0.6126
                    Surrogate loss: -0.0248
             Mean action noise std: 0.74
                       Mean reward: 29.41
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 11.21s
                        Total time: 20454.97s
                               ETA: 1353297.7s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.632s, learning 0.168s)
               Value function loss: 0.5833
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 29.20
               Mean episode length: 123.37
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 10.80s
                        Total time: 20465.77s
                               ETA: 1353089.8s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.930s, learning 0.191s)
               Value function loss: 0.5668
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 30.79
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 11.12s
                        Total time: 20476.89s
                               ETA: 1352903.3s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.081s, learning 0.166s)
               Value function loss: 0.5646
                    Surrogate loss: -0.0213
             Mean action noise std: 0.74
                       Mean reward: 31.57
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 11.25s
                        Total time: 20488.14s
                               ETA: 1352725.3s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.881s, learning 0.233s)
               Value function loss: 0.7908
                    Surrogate loss: -0.0236
             Mean action noise std: 0.74
                       Mean reward: 31.55
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 11.11s
                        Total time: 20499.25s
                               ETA: 1352538.8s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.008s, learning 0.178s)
               Value function loss: 0.7305
                    Surrogate loss: -0.0240
             Mean action noise std: 0.74
                       Mean reward: 31.94
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 11.19s
                        Total time: 20510.44s
                               ETA: 1352357.3s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.333s, learning 0.171s)
               Value function loss: 0.6797
                    Surrogate loss: -0.0288
             Mean action noise std: 0.74
                       Mean reward: 31.59
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 11.50s
                        Total time: 20521.94s
                               ETA: 1352197.1s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1459 steps/s (collection: 10.929s, learning 0.297s)
               Value function loss: 0.7880
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: 30.30
               Mean episode length: 124.07
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 11.23s
                        Total time: 20533.17s
                               ETA: 1352018.7s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.809s, learning 0.186s)
               Value function loss: 0.8914
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 31.55
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 11.00s
                        Total time: 20544.17s
                               ETA: 1351825.3s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.574s, learning 0.166s)
               Value function loss: 0.8350
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 31.41
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 10.74s
                        Total time: 20554.91s
                               ETA: 1351615.4s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.608s, learning 0.163s)
               Value function loss: 0.7581
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 31.38
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 10.77s
                        Total time: 20565.68s
                               ETA: 1351407.8s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.994s, learning 0.185s)
               Value function loss: 0.6927
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: 31.62
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 11.18s
                        Total time: 20576.86s
                               ETA: 1351227.2s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.229s, learning 0.157s)
               Value function loss: 0.6642
                    Surrogate loss: -0.0258
             Mean action noise std: 0.74
                       Mean reward: 31.78
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 11.39s
                        Total time: 20588.24s
                               ETA: 1351060.5s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.099s, learning 0.160s)
               Value function loss: 0.7308
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: 32.50
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 11.26s
                        Total time: 20599.50s
                               ETA: 1350885.6s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.858s, learning 0.165s)
               Value function loss: 0.6483
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 32.03
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.02s
                        Total time: 20610.52s
                               ETA: 1350695.4s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.041s, learning 0.168s)
               Value function loss: 0.6962
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 32.96
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 11.21s
                        Total time: 20621.73s
                               ETA: 1350517.7s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.146s, learning 0.176s)
               Value function loss: 0.7684
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 33.06
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 11.32s
                        Total time: 20633.05s
                               ETA: 1350347.6s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.801s, learning 0.171s)
               Value function loss: 0.6973
                    Surrogate loss: -0.0250
             Mean action noise std: 0.74
                       Mean reward: 32.73
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 10.97s
                        Total time: 20644.02s
                               ETA: 1350154.8s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.750s, learning 0.267s)
               Value function loss: 0.7035
                    Surrogate loss: -0.0278
             Mean action noise std: 0.74
                       Mean reward: 32.85
               Mean episode length: 124.85
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 11.02s
                        Total time: 20655.04s
                               ETA: 1349965.3s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.169s)
               Value function loss: 0.8245
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 33.34
               Mean episode length: 124.25
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 10.98s
                        Total time: 20666.03s
                               ETA: 1349773.8s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.890s, learning 0.194s)
               Value function loss: 0.8631
                    Surrogate loss: -0.0231
             Mean action noise std: 0.74
                       Mean reward: 32.73
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 11.08s
                        Total time: 20677.11s
                               ETA: 1349589.1s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.948s, learning 0.228s)
               Value function loss: 0.9176
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: 33.60
               Mean episode length: 124.04
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 11.18s
                        Total time: 20688.29s
                               ETA: 1349410.6s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.200s, learning 0.191s)
               Value function loss: 0.8035
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 32.63
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 11.39s
                        Total time: 20699.68s
                               ETA: 1349246.4s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.883s, learning 0.175s)
               Value function loss: 0.8444
                    Surrogate loss: -0.0223
             Mean action noise std: 0.74
                       Mean reward: 32.57
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 11.06s
                        Total time: 20710.74s
                               ETA: 1349060.6s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.720s, learning 0.164s)
               Value function loss: 0.8478
                    Surrogate loss: -0.0204
             Mean action noise std: 0.74
                       Mean reward: 32.91
               Mean episode length: 124.86
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 10.88s
                        Total time: 20721.62s
                               ETA: 1348863.8s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.143s, learning 0.167s)
               Value function loss: 0.6921
                    Surrogate loss: -0.0270
             Mean action noise std: 0.74
                       Mean reward: 32.66
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 11.31s
                        Total time: 20732.93s
                               ETA: 1348694.9s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.068s, learning 0.233s)
               Value function loss: 0.7879
                    Surrogate loss: -0.0287
             Mean action noise std: 0.74
                       Mean reward: 32.02
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 11.30s
                        Total time: 20744.23s
                               ETA: 1348525.7s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.605s, learning 0.225s)
               Value function loss: 0.7223
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 33.29
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 10.83s
                        Total time: 20755.06s
                               ETA: 1348326.1s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.991s, learning 0.162s)
               Value function loss: 0.8361
                    Surrogate loss: -0.0206
             Mean action noise std: 0.74
                       Mean reward: 32.04
               Mean episode length: 124.78
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 11.15s
                        Total time: 20766.22s
                               ETA: 1348147.6s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.139s, learning 0.162s)
               Value function loss: 0.7318
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 32.43
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 11.30s
                        Total time: 20777.52s
                               ETA: 1347979.1s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.622s, learning 0.286s)
               Value function loss: 0.6986
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 32.94
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 10.91s
                        Total time: 20788.42s
                               ETA: 1347785.2s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.755s, learning 0.180s)
               Value function loss: 0.7447
                    Surrogate loss: -0.0228
             Mean action noise std: 0.74
                       Mean reward: 32.22
               Mean episode length: 124.38
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 10.94s
                        Total time: 20799.36s
                               ETA: 1347593.3s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.637s, learning 0.297s)
               Value function loss: 0.7914
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 32.99
               Mean episode length: 124.24
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 10.93s
                        Total time: 20810.29s
                               ETA: 1347401.5s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.863s, learning 0.173s)
               Value function loss: 0.6489
                    Surrogate loss: -0.0252
             Mean action noise std: 0.73
                       Mean reward: 32.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 11.04s
                        Total time: 20821.33s
                               ETA: 1347216.7s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.063s, learning 0.159s)
               Value function loss: 0.5950
                    Surrogate loss: -0.0232
             Mean action noise std: 0.73
                       Mean reward: 32.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 11.22s
                        Total time: 20832.55s
                               ETA: 1347044.0s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.167s)
               Value function loss: 0.8132
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 32.81
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 10.80s
                        Total time: 20843.35s
                               ETA: 1346844.3s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.864s, learning 0.175s)
               Value function loss: 0.8529
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 32.38
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 11.04s
                        Total time: 20854.39s
                               ETA: 1346660.3s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.822s, learning 0.172s)
               Value function loss: 0.7473
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 32.86
               Mean episode length: 124.14
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 10.99s
                        Total time: 20865.38s
                               ETA: 1346473.6s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.671s, learning 0.187s)
               Value function loss: 0.7733
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 31.34
               Mean episode length: 124.22
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 10.86s
                        Total time: 20876.24s
                               ETA: 1346278.3s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.825s, learning 0.195s)
               Value function loss: 0.8149
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 32.93
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 11.02s
                        Total time: 20887.26s
                               ETA: 1346093.8s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.686s, learning 0.203s)
               Value function loss: 0.7524
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 32.26
               Mean episode length: 124.04
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 10.89s
                        Total time: 20898.15s
                               ETA: 1345901.1s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.875s, learning 0.162s)
               Value function loss: 0.6500
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 32.08
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 11.04s
                        Total time: 20909.19s
                               ETA: 1345718.1s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.658s, learning 0.174s)
               Value function loss: 0.8454
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 32.59
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 10.83s
                        Total time: 20920.02s
                               ETA: 1345522.2s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.347s, learning 0.175s)
               Value function loss: 0.7648
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 31.54
               Mean episode length: 124.14
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 11.52s
                        Total time: 20931.54s
                               ETA: 1345370.8s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.726s, learning 0.171s)
               Value function loss: 0.8232
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 31.84
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 10.90s
                        Total time: 20942.44s
                               ETA: 1345179.5s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.043s, learning 0.178s)
               Value function loss: 0.6383
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 31.19
               Mean episode length: 124.70
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 11.22s
                        Total time: 20953.66s
                               ETA: 1345009.3s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.833s, learning 0.166s)
               Value function loss: 0.6308
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 32.08
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 11.00s
                        Total time: 20964.66s
                               ETA: 1344825.0s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.010s, learning 0.193s)
               Value function loss: 0.6536
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 31.06
               Mean episode length: 124.14
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 11.20s
                        Total time: 20975.87s
                               ETA: 1344654.0s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.046s, learning 0.281s)
               Value function loss: 0.5357
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 31.22
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 11.33s
                        Total time: 20987.19s
                               ETA: 1344491.2s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.163s)
               Value function loss: 0.5704
                    Surrogate loss: -0.0258
             Mean action noise std: 0.73
                       Mean reward: 31.91
               Mean episode length: 124.24
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 11.30s
                        Total time: 20998.50s
                               ETA: 1344327.0s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.912s, learning 0.179s)
               Value function loss: 0.6008
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 32.03
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 11.09s
                        Total time: 21009.59s
                               ETA: 1344149.4s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.879s, learning 0.169s)
               Value function loss: 0.7561
                    Surrogate loss: -0.0235
             Mean action noise std: 0.73
                       Mean reward: 32.34
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 11.05s
                        Total time: 21020.64s
                               ETA: 1343969.3s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.964s, learning 0.173s)
               Value function loss: 0.6718
                    Surrogate loss: -0.0238
             Mean action noise std: 0.73
                       Mean reward: 31.50
               Mean episode length: 124.13
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 11.14s
                        Total time: 21031.77s
                               ETA: 1343795.1s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.840s, learning 0.261s)
               Value function loss: 0.5958
                    Surrogate loss: -0.0243
             Mean action noise std: 0.73
                       Mean reward: 33.36
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 11.10s
                        Total time: 21042.87s
                               ETA: 1343618.8s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.990s, learning 0.163s)
               Value function loss: 0.6204
                    Surrogate loss: -0.0234
             Mean action noise std: 0.73
                       Mean reward: 32.78
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 11.15s
                        Total time: 21054.03s
                               ETA: 1343446.0s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.949s, learning 0.174s)
               Value function loss: 0.6736
                    Surrogate loss: -0.0275
             Mean action noise std: 0.73
                       Mean reward: 32.28
               Mean episode length: 123.77
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 11.12s
                        Total time: 21065.15s
                               ETA: 1343271.6s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.619s, learning 0.254s)
               Value function loss: 0.7053
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 31.95
               Mean episode length: 124.52
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 10.87s
                        Total time: 21076.02s
                               ETA: 1343081.5s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.620s, learning 0.163s)
               Value function loss: 0.5780
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 32.49
               Mean episode length: 124.19
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 10.78s
                        Total time: 21086.81s
                               ETA: 1342885.8s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.724s, learning 0.209s)
               Value function loss: 0.5607
                    Surrogate loss: -0.0251
             Mean action noise std: 0.73
                       Mean reward: 32.55
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 10.93s
                        Total time: 21097.74s
                               ETA: 1342699.9s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.147s, learning 0.175s)
               Value function loss: 0.5719
                    Surrogate loss: -0.0242
             Mean action noise std: 0.73
                       Mean reward: 32.38
               Mean episode length: 124.08
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 11.32s
                        Total time: 21109.06s
                               ETA: 1342538.9s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.168s)
               Value function loss: 0.5790
                    Surrogate loss: -0.0268
             Mean action noise std: 0.73
                       Mean reward: 32.51
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 11.38s
                        Total time: 21120.44s
                               ETA: 1342382.0s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.720s, learning 0.187s)
               Value function loss: 0.5193
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 33.48
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 10.91s
                        Total time: 21131.35s
                               ETA: 1342195.1s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.456s, learning 0.168s)
               Value function loss: 0.5477
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 33.60
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 11.62s
                        Total time: 21142.97s
                               ETA: 1342053.9s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.152s, learning 0.222s)
               Value function loss: 0.5701
                    Surrogate loss: -0.0225
             Mean action noise std: 0.73
                       Mean reward: 34.04
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 11.37s
                        Total time: 21154.35s
                               ETA: 1341897.1s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.122s, learning 0.172s)
               Value function loss: 0.6034
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 32.22
               Mean episode length: 124.59
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 11.29s
                        Total time: 21165.64s
                               ETA: 1341735.4s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.801s, learning 0.198s)
               Value function loss: 0.5085
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 34.24
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 11.00s
                        Total time: 21176.64s
                               ETA: 1341555.2s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.863s, learning 0.227s)
               Value function loss: 0.6667
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 34.00
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 11.09s
                        Total time: 21187.73s
                               ETA: 1341381.0s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.736s, learning 0.162s)
               Value function loss: 0.7807
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 34.80
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 10.90s
                        Total time: 21198.63s
                               ETA: 1341194.8s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.770s, learning 0.161s)
               Value function loss: 0.7378
                    Surrogate loss: -0.0212
             Mean action noise std: 0.73
                       Mean reward: 34.38
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 10.93s
                        Total time: 21209.56s
                               ETA: 1341010.9s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.061s, learning 0.170s)
               Value function loss: 0.7406
                    Surrogate loss: -0.0257
             Mean action noise std: 0.73
                       Mean reward: 34.11
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 11.23s
                        Total time: 21220.79s
                               ETA: 1340846.2s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.787s, learning 0.157s)
               Value function loss: 0.8027
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 33.88
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 10.94s
                        Total time: 21231.74s
                               ETA: 1340663.6s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.658s, learning 0.159s)
               Value function loss: 0.7675
                    Surrogate loss: -0.0192
             Mean action noise std: 0.73
                       Mean reward: 33.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 10.82s
                        Total time: 21242.55s
                               ETA: 1340473.1s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.204s, learning 0.194s)
               Value function loss: 0.6555
                    Surrogate loss: -0.0240
             Mean action noise std: 0.73
                       Mean reward: 34.12
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 11.40s
                        Total time: 21253.95s
                               ETA: 1340319.6s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.524s, learning 0.178s)
               Value function loss: 0.8119
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 34.23
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 10.70s
                        Total time: 21264.65s
                               ETA: 1340122.3s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.923s, learning 0.182s)
               Value function loss: 0.6600
                    Surrogate loss: -0.0286
             Mean action noise std: 0.73
                       Mean reward: 33.23
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 11.11s
                        Total time: 21275.76s
                               ETA: 1339950.7s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.095s, learning 0.229s)
               Value function loss: 0.7169
                    Surrogate loss: -0.0243
             Mean action noise std: 0.73
                       Mean reward: 34.45
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 11.32s
                        Total time: 21287.08s
                               ETA: 1339793.1s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.797s, learning 0.205s)
               Value function loss: 0.7763
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 33.37
               Mean episode length: 123.81
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 11.00s
                        Total time: 21298.08s
                               ETA: 1339615.4s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.829s, learning 0.188s)
               Value function loss: 0.8320
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 33.33
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 11.02s
                        Total time: 21309.10s
                               ETA: 1339438.9s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.099s, learning 0.163s)
               Value function loss: 0.8089
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 33.30
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 11.26s
                        Total time: 21320.36s
                               ETA: 1339277.9s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.902s, learning 0.192s)
               Value function loss: 0.7924
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 33.63
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 11.09s
                        Total time: 21331.46s
                               ETA: 1339106.6s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.025s, learning 0.207s)
               Value function loss: 0.5616
                    Surrogate loss: -0.0288
             Mean action noise std: 0.73
                       Mean reward: 31.88
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 11.23s
                        Total time: 21342.69s
                               ETA: 1338944.2s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.163s)
               Value function loss: 0.6962
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 33.23
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 11.13s
                        Total time: 21353.81s
                               ETA: 1338775.3s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.061s, learning 0.170s)
               Value function loss: 0.9333
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 32.80
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 11.23s
                        Total time: 21365.05s
                               ETA: 1338613.2s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.082s, learning 0.212s)
               Value function loss: 0.7471
                    Surrogate loss: -0.0255
             Mean action noise std: 0.73
                       Mean reward: 32.38
               Mean episode length: 124.31
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 11.29s
                        Total time: 21376.34s
                               ETA: 1338455.3s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.026s, learning 0.183s)
               Value function loss: 0.7068
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 32.97
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 11.21s
                        Total time: 21387.55s
                               ETA: 1338292.2s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.167s)
               Value function loss: 0.6722
                    Surrogate loss: -0.0243
             Mean action noise std: 0.73
                       Mean reward: 32.47
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 11.00s
                        Total time: 21398.55s
                               ETA: 1338116.5s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.663s, learning 0.182s)
               Value function loss: 0.8345
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 33.03
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 10.84s
                        Total time: 21409.40s
                               ETA: 1337931.0s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.857s, learning 0.177s)
               Value function loss: 0.7136
                    Surrogate loss: -0.0231
             Mean action noise std: 0.73
                       Mean reward: 32.96
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 11.03s
                        Total time: 21420.43s
                               ETA: 1337757.6s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.166s)
               Value function loss: 0.6475
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 32.53
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 10.76s
                        Total time: 21431.20s
                               ETA: 1337567.6s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.714s, learning 0.196s)
               Value function loss: 0.7443
                    Surrogate loss: -0.0219
             Mean action noise std: 0.73
                       Mean reward: 32.35
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 10.91s
                        Total time: 21442.11s
                               ETA: 1337386.9s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.214s, learning 0.177s)
               Value function loss: 0.8086
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 32.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 11.39s
                        Total time: 21453.50s
                               ETA: 1337236.4s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.047s, learning 0.319s)
               Value function loss: 0.8074
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 32.38
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 11.37s
                        Total time: 21464.86s
                               ETA: 1337084.4s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.164s)
               Value function loss: 0.7498
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 32.96
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 11.24s
                        Total time: 21476.10s
                               ETA: 1336924.8s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.898s, learning 0.171s)
               Value function loss: 0.8144
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 33.43
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 11.07s
                        Total time: 21487.17s
                               ETA: 1336754.7s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.033s, learning 0.175s)
               Value function loss: 0.7560
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 32.40
               Mean episode length: 124.21
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 11.21s
                        Total time: 21498.38s
                               ETA: 1336593.6s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.923s, learning 0.175s)
               Value function loss: 0.6254
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 32.71
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 11.10s
                        Total time: 21509.48s
                               ETA: 1336425.7s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.768s, learning 0.157s)
               Value function loss: 0.6495
                    Surrogate loss: -0.0239
             Mean action noise std: 0.73
                       Mean reward: 32.67
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 10.93s
                        Total time: 21520.40s
                               ETA: 1336247.3s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.161s)
               Value function loss: 0.7678
                    Surrogate loss: -0.0217
             Mean action noise std: 0.73
                       Mean reward: 32.72
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 11.07s
                        Total time: 21531.48s
                               ETA: 1336078.3s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.045s, learning 0.302s)
               Value function loss: 0.8753
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 32.61
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 11.35s
                        Total time: 21542.82s
                               ETA: 1335926.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.969s, learning 0.166s)
               Value function loss: 1.0037
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 32.90
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 11.14s
                        Total time: 21553.96s
                               ETA: 1335761.8s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.162s)
               Value function loss: 1.0820
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 33.15
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 11.12s
                        Total time: 21565.08s
                               ETA: 1335596.2s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.681s, learning 0.221s)
               Value function loss: 1.0483
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 32.82
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 10.90s
                        Total time: 21575.98s
                               ETA: 1335417.4s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.942s, learning 0.159s)
               Value function loss: 1.1227
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 31.99
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 11.10s
                        Total time: 21587.08s
                               ETA: 1335251.1s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.629s, learning 0.163s)
               Value function loss: 0.8840
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 33.12
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 10.79s
                        Total time: 21597.87s
                               ETA: 1335065.9s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.179s)
               Value function loss: 0.8838
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 32.96
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 11.21s
                        Total time: 21609.08s
                               ETA: 1334907.0s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.954s, learning 0.176s)
               Value function loss: 0.6767
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 32.71
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 11.13s
                        Total time: 21620.21s
                               ETA: 1334743.1s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.027s, learning 0.177s)
               Value function loss: 0.7783
                    Surrogate loss: -0.0234
             Mean action noise std: 0.73
                       Mean reward: 32.34
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 11.20s
                        Total time: 21631.42s
                               ETA: 1334583.9s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.870s, learning 0.165s)
               Value function loss: 0.6916
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 32.47
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 11.03s
                        Total time: 21642.45s
                               ETA: 1334414.5s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.926s, learning 0.218s)
               Value function loss: 0.6215
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 32.13
               Mean episode length: 124.28
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 11.14s
                        Total time: 21653.60s
                               ETA: 1334252.1s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.930s, learning 0.186s)
               Value function loss: 0.6236
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 32.10
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 11.12s
                        Total time: 21664.71s
                               ETA: 1334088.0s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.829s, learning 0.219s)
               Value function loss: 0.6873
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 30.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 11.05s
                        Total time: 21675.76s
                               ETA: 1333920.1s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.616s, learning 0.176s)
               Value function loss: 0.6272
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 32.70
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 10.79s
                        Total time: 21686.55s
                               ETA: 1333736.6s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.102s, learning 0.187s)
               Value function loss: 0.5273
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 30.60
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 11.29s
                        Total time: 21697.84s
                               ETA: 1333583.8s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.899s, learning 0.181s)
               Value function loss: 0.6995
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 31.02
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 11.08s
                        Total time: 21708.92s
                               ETA: 1333418.4s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.677s, learning 0.175s)
               Value function loss: 0.6791
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 32.13
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 10.85s
                        Total time: 21719.77s
                               ETA: 1333239.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1564 steps/s (collection: 10.285s, learning 0.185s)
               Value function loss: 0.6306
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 31.30
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 10.47s
                        Total time: 21730.24s
                               ETA: 1333036.7s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.765s, learning 0.166s)
               Value function loss: 0.6437
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 31.94
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 10.93s
                        Total time: 21741.18s
                               ETA: 1332862.8s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.973s, learning 0.207s)
               Value function loss: 0.7247
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 31.50
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 11.18s
                        Total time: 21752.36s
                               ETA: 1332704.2s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.785s, learning 0.174s)
               Value function loss: 0.6026
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 31.31
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 10.96s
                        Total time: 21763.31s
                               ETA: 1332532.4s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.135s, learning 0.160s)
               Value function loss: 0.6629
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 32.56
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 11.29s
                        Total time: 21774.61s
                               ETA: 1332381.3s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.483s, learning 0.192s)
               Value function loss: 0.8079
                    Surrogate loss: -0.0251
             Mean action noise std: 0.73
                       Mean reward: 31.33
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 10.67s
                        Total time: 21785.28s
                               ETA: 1332192.4s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.587s, learning 0.181s)
               Value function loss: 0.7575
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 31.54
               Mean episode length: 124.16
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 10.77s
                        Total time: 21796.05s
                               ETA: 1332009.5s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.641s, learning 0.178s)
               Value function loss: 0.7934
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 30.64
               Mean episode length: 124.24
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 10.82s
                        Total time: 21806.87s
                               ETA: 1331830.0s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.897s, learning 0.192s)
               Value function loss: 0.7053
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 31.94
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 11.09s
                        Total time: 21817.96s
                               ETA: 1331667.0s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.753s, learning 0.200s)
               Value function loss: 0.6629
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 31.59
               Mean episode length: 124.62
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 10.95s
                        Total time: 21828.91s
                               ETA: 1331496.0s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.044s, learning 0.183s)
               Value function loss: 0.7352
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 31.20
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 11.23s
                        Total time: 21840.14s
                               ETA: 1331341.9s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.106s, learning 0.174s)
               Value function loss: 0.7344
                    Surrogate loss: -0.0218
             Mean action noise std: 0.73
                       Mean reward: 33.27
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 11.28s
                        Total time: 21851.42s
                               ETA: 1331191.2s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.836s, learning 0.178s)
               Value function loss: 0.6571
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 31.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 11.01s
                        Total time: 21862.43s
                               ETA: 1331024.5s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.792s, learning 0.194s)
               Value function loss: 0.6059
                    Surrogate loss: -0.0273
             Mean action noise std: 0.73
                       Mean reward: 30.49
               Mean episode length: 124.16
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 10.99s
                        Total time: 21873.42s
                               ETA: 1330856.2s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.965s, learning 0.184s)
               Value function loss: 0.8325
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 31.88
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 11.15s
                        Total time: 21884.57s
                               ETA: 1330698.1s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.816s, learning 0.180s)
               Value function loss: 0.7896
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 31.05
               Mean episode length: 124.12
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 12.00s
                        Total time: 21896.57s
                               ETA: 1330591.6s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.802s, learning 0.178s)
               Value function loss: 0.6244
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 30.66
               Mean episode length: 124.12
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 10.98s
                        Total time: 21907.55s
                               ETA: 1330423.6s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.959s, learning 0.173s)
               Value function loss: 0.7239
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 31.98
               Mean episode length: 124.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 11.13s
                        Total time: 21918.68s
                               ETA: 1330265.0s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.156s, learning 0.185s)
               Value function loss: 0.6812
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 31.84
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 11.34s
                        Total time: 21930.02s
                               ETA: 1330119.2s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.892s, learning 0.203s)
               Value function loss: 0.5981
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 31.61
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 11.10s
                        Total time: 21941.11s
                               ETA: 1329958.6s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.966s, learning 0.188s)
               Value function loss: 0.5024
                    Surrogate loss: -0.0249
             Mean action noise std: 0.73
                       Mean reward: 32.19
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 11.15s
                        Total time: 21952.27s
                               ETA: 1329801.8s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.861s, learning 0.180s)
               Value function loss: 0.5613
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 31.33
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 11.04s
                        Total time: 21963.31s
                               ETA: 1329638.4s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.250s, learning 0.208s)
               Value function loss: 0.4965
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 31.85
               Mean episode length: 124.16
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 11.46s
                        Total time: 21974.77s
                               ETA: 1329500.3s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.812s, learning 0.200s)
               Value function loss: 0.6133
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 31.37
               Mean episode length: 124.45
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 11.01s
                        Total time: 21985.78s
                               ETA: 1329335.5s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.775s, learning 0.176s)
               Value function loss: 0.4400
                    Surrogate loss: -0.0256
             Mean action noise std: 0.73
                       Mean reward: 31.39
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 10.95s
                        Total time: 21996.73s
                               ETA: 1329167.1s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.882s, learning 0.193s)
               Value function loss: 0.4939
                    Surrogate loss: -0.0284
             Mean action noise std: 0.73
                       Mean reward: 31.65
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 11.08s
                        Total time: 22007.80s
                               ETA: 1329006.5s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.030s, learning 0.182s)
               Value function loss: 0.5148
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 31.65
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 11.21s
                        Total time: 22019.02s
                               ETA: 1328854.3s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.813s, learning 0.257s)
               Value function loss: 0.4015
                    Surrogate loss: -0.0253
             Mean action noise std: 0.73
                       Mean reward: 31.91
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 11.07s
                        Total time: 22030.09s
                               ETA: 1328693.8s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.307s, learning 0.176s)
               Value function loss: 0.4071
                    Surrogate loss: -0.0253
             Mean action noise std: 0.73
                       Mean reward: 31.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 11.48s
                        Total time: 22041.57s
                               ETA: 1328558.3s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.042s, learning 0.194s)
               Value function loss: 0.5737
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 30.93
               Mean episode length: 123.39
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 11.24s
                        Total time: 22052.80s
                               ETA: 1328408.0s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.161s, learning 0.174s)
               Value function loss: 0.6237
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 30.69
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 11.33s
                        Total time: 22064.14s
                               ETA: 1328263.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.423s, learning 0.160s)
               Value function loss: 0.7591
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 31.48
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 11.58s
                        Total time: 22075.72s
                               ETA: 1328134.8s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.724s, learning 0.238s)
               Value function loss: 0.6195
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 31.05
               Mean episode length: 124.52
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 10.96s
                        Total time: 22086.68s
                               ETA: 1327968.6s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.313s, learning 0.263s)
               Value function loss: 0.6380
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 31.17
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 11.58s
                        Total time: 22098.26s
                               ETA: 1327839.5s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.960s, learning 0.205s)
               Value function loss: 0.6865
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 31.84
               Mean episode length: 124.04
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 11.16s
                        Total time: 22109.42s
                               ETA: 1327685.8s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.753s, learning 0.166s)
               Value function loss: 0.5248
                    Surrogate loss: -0.0254
             Mean action noise std: 0.73
                       Mean reward: 30.77
               Mean episode length: 124.06
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 10.92s
                        Total time: 22120.34s
                               ETA: 1327517.5s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.092s, learning 0.163s)
               Value function loss: 0.6327
                    Surrogate loss: -0.0228
             Mean action noise std: 0.73
                       Mean reward: 31.29
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 11.25s
                        Total time: 22131.60s
                               ETA: 1327369.6s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.808s, learning 0.166s)
               Value function loss: 0.4885
                    Surrogate loss: -0.0312
             Mean action noise std: 0.73
                       Mean reward: 31.33
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 10.97s
                        Total time: 22142.57s
                               ETA: 1327204.9s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.276s, learning 0.191s)
               Value function loss: 0.5658
                    Surrogate loss: -0.0264
             Mean action noise std: 0.73
                       Mean reward: 31.89
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 11.47s
                        Total time: 22154.04s
                               ETA: 1327070.1s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.123s, learning 0.196s)
               Value function loss: 0.5219
                    Surrogate loss: -0.0240
             Mean action noise std: 0.73
                       Mean reward: 32.27
               Mean episode length: 124.20
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 11.32s
                        Total time: 22165.36s
                               ETA: 1326926.4s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.874s, learning 0.281s)
               Value function loss: 0.5608
                    Surrogate loss: -0.0266
             Mean action noise std: 0.73
                       Mean reward: 30.95
               Mean episode length: 123.65
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 11.16s
                        Total time: 22176.51s
                               ETA: 1326773.2s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.003s, learning 0.196s)
               Value function loss: 0.6440
                    Surrogate loss: -0.0252
             Mean action noise std: 0.73
                       Mean reward: 31.40
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 11.20s
                        Total time: 22187.71s
                               ETA: 1326622.8s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.013s, learning 0.194s)
               Value function loss: 0.5761
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 31.32
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 11.21s
                        Total time: 22198.92s
                               ETA: 1326473.1s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.834s, learning 0.187s)
               Value function loss: 0.5364
                    Surrogate loss: -0.0230
             Mean action noise std: 0.73
                       Mean reward: 32.13
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 11.02s
                        Total time: 22209.94s
                               ETA: 1326312.4s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.978s, learning 0.173s)
               Value function loss: 0.5384
                    Surrogate loss: -0.0255
             Mean action noise std: 0.73
                       Mean reward: 31.30
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 11.15s
                        Total time: 22221.09s
                               ETA: 1326159.6s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.336s, learning 0.208s)
               Value function loss: 0.6460
                    Surrogate loss: -0.0245
             Mean action noise std: 0.73
                       Mean reward: 31.91
               Mean episode length: 124.27
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 11.54s
                        Total time: 22232.63s
                               ETA: 1326030.4s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.945s, learning 0.190s)
               Value function loss: 0.7258
                    Surrogate loss: -0.0231
             Mean action noise std: 0.73
                       Mean reward: 32.57
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 11.14s
                        Total time: 22243.77s
                               ETA: 1325877.0s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.941s, learning 0.164s)
               Value function loss: 0.7285
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 31.97
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 11.10s
                        Total time: 22254.87s
                               ETA: 1325721.9s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.941s, learning 0.166s)
               Value function loss: 0.6908
                    Surrogate loss: -0.0230
             Mean action noise std: 0.73
                       Mean reward: 31.15
               Mean episode length: 124.27
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 11.11s
                        Total time: 22265.98s
                               ETA: 1325567.2s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.877s, learning 0.166s)
               Value function loss: 0.8310
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 31.75
               Mean episode length: 124.34
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 11.04s
                        Total time: 22277.02s
                               ETA: 1325408.8s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.935s, learning 0.190s)
               Value function loss: 0.7239
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 31.93
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 11.12s
                        Total time: 22288.15s
                               ETA: 1325255.5s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.159s, learning 0.165s)
               Value function loss: 0.6709
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 31.87
               Mean episode length: 124.43
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 11.32s
                        Total time: 22299.47s
                               ETA: 1325114.2s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.055s, learning 0.163s)
               Value function loss: 0.8606
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 32.41
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 11.22s
                        Total time: 22310.69s
                               ETA: 1324966.7s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.199s, learning 0.167s)
               Value function loss: 0.6272
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 31.85
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 11.37s
                        Total time: 22322.06s
                               ETA: 1324828.3s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.987s, learning 0.190s)
               Value function loss: 0.7928
                    Surrogate loss: -0.0219
             Mean action noise std: 0.73
                       Mean reward: 32.94
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 11.18s
                        Total time: 22333.23s
                               ETA: 1324678.7s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.084s, learning 0.166s)
               Value function loss: 0.7431
                    Surrogate loss: -0.0230
             Mean action noise std: 0.73
                       Mean reward: 31.72
               Mean episode length: 124.51
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 11.25s
                        Total time: 22344.48s
                               ETA: 1324533.6s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.335s, learning 0.173s)
               Value function loss: 0.7781
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 31.58
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 11.51s
                        Total time: 22355.99s
                               ETA: 1324404.0s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.946s, learning 0.194s)
               Value function loss: 0.8124
                    Surrogate loss: -0.0244
             Mean action noise std: 0.73
                       Mean reward: 32.55
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 11.14s
                        Total time: 22367.13s
                               ETA: 1324252.8s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.566s, learning 0.172s)
               Value function loss: 0.8538
                    Surrogate loss: -0.0232
             Mean action noise std: 0.73
                       Mean reward: 32.01
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 10.74s
                        Total time: 22377.87s
                               ETA: 1324077.8s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.634s, learning 0.168s)
               Value function loss: 0.9801
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 32.20
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 10.80s
                        Total time: 22388.67s
                               ETA: 1323906.9s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.666s, learning 0.238s)
               Value function loss: 0.8860
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 31.66
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 10.90s
                        Total time: 22399.58s
                               ETA: 1323742.2s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.211s, learning 0.158s)
               Value function loss: 1.0893
                    Surrogate loss: -0.0270
             Mean action noise std: 0.73
                       Mean reward: 31.11
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 11.37s
                        Total time: 22410.94s
                               ETA: 1323605.2s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.494s, learning 0.176s)
               Value function loss: 1.1387
                    Surrogate loss: -0.0235
             Mean action noise std: 0.73
                       Mean reward: 30.50
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 12.67s
                        Total time: 22423.61s
                               ETA: 1323545.1s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.385s, learning 0.225s)
               Value function loss: 0.9853
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 30.77
               Mean episode length: 124.95
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 21.61s
                        Total time: 22445.22s
                               ETA: 1324012.4s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 774 steps/s (collection: 20.994s, learning 0.172s)
               Value function loss: 0.9436
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 29.66
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 21.17s
                        Total time: 22466.39s
                               ETA: 1324452.9s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.436s, learning 0.166s)
               Value function loss: 0.8617
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 30.75
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 21.60s
                        Total time: 22487.99s
                               ETA: 1324918.7s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 780 steps/s (collection: 20.826s, learning 0.169s)
               Value function loss: 0.6267
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 31.33
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 21.00s
                        Total time: 22508.99s
                               ETA: 1325348.1s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.462s, learning 0.171s)
               Value function loss: 0.5761
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 31.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 21.63s
                        Total time: 22530.62s
                               ETA: 1325814.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.156s, learning 0.187s)
               Value function loss: 0.5264
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 31.35
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 21.34s
                        Total time: 22551.96s
                               ETA: 1326263.2s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 742 steps/s (collection: 21.841s, learning 0.225s)
               Value function loss: 0.4811
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 30.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 22.07s
                        Total time: 22574.03s
                               ETA: 1326753.9s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 744 steps/s (collection: 21.827s, learning 0.180s)
               Value function loss: 0.5403
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 30.79
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 22.01s
                        Total time: 22596.04s
                               ETA: 1327240.4s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.393s, learning 0.170s)
               Value function loss: 0.4626
                    Surrogate loss: -0.0258
             Mean action noise std: 0.73
                       Mean reward: 31.20
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 21.56s
                        Total time: 22617.60s
                               ETA: 1327700.4s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 754 steps/s (collection: 21.548s, learning 0.166s)
               Value function loss: 0.4580
                    Surrogate loss: -0.0264
             Mean action noise std: 0.73
                       Mean reward: 30.39
               Mean episode length: 124.03
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 21.71s
                        Total time: 22639.31s
                               ETA: 1328168.6s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 766 steps/s (collection: 21.209s, learning 0.172s)
               Value function loss: 0.4493
                    Surrogate loss: -0.0236
             Mean action noise std: 0.73
                       Mean reward: 30.30
               Mean episode length: 124.04
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 21.38s
                        Total time: 22660.69s
                               ETA: 1328616.7s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.607s, learning 0.178s)
               Value function loss: 0.4365
                    Surrogate loss: -0.0254
             Mean action noise std: 0.73
                       Mean reward: 31.47
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 21.79s
                        Total time: 22682.48s
                               ETA: 1329087.9s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.738s, learning 0.186s)
               Value function loss: 0.4301
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 30.68
               Mean episode length: 123.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 21.92s
                        Total time: 22704.40s
                               ETA: 1329566.6s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 761 steps/s (collection: 21.306s, learning 0.203s)
               Value function loss: 0.5089
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 30.83
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 21.51s
                        Total time: 22725.91s
                               ETA: 1330020.5s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 761 steps/s (collection: 21.335s, learning 0.187s)
               Value function loss: 0.5164
                    Surrogate loss: -0.0212
             Mean action noise std: 0.73
                       Mean reward: 30.63
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 21.52s
                        Total time: 22747.43s
                               ETA: 1330474.6s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.250s, learning 0.174s)
               Value function loss: 0.5206
                    Surrogate loss: -0.0229
             Mean action noise std: 0.73
                       Mean reward: 29.75
               Mean episode length: 123.98
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 21.42s
                        Total time: 22768.86s
                               ETA: 1330922.4s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 762 steps/s (collection: 21.299s, learning 0.180s)
               Value function loss: 0.4745
                    Surrogate loss: -0.0232
             Mean action noise std: 0.73
                       Mean reward: 30.28
               Mean episode length: 124.49
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 21.48s
                        Total time: 22790.34s
                               ETA: 1331372.8s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 763 steps/s (collection: 21.259s, learning 0.188s)
               Value function loss: 0.5076
                    Surrogate loss: -0.0242
             Mean action noise std: 0.73
                       Mean reward: 30.58
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 21.45s
                        Total time: 22811.78s
                               ETA: 1331820.7s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 761 steps/s (collection: 21.330s, learning 0.172s)
               Value function loss: 0.5593
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 31.23
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 21.50s
                        Total time: 22833.29s
                               ETA: 1332271.4s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.484s, learning 0.199s)
               Value function loss: 0.4062
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 30.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 21.68s
                        Total time: 22854.97s
                               ETA: 1332732.1s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.397s, learning 0.168s)
               Value function loss: 0.5623
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 30.29
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 21.56s
                        Total time: 22876.53s
                               ETA: 1333185.3s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.734s, learning 0.306s)
               Value function loss: 0.4230
                    Surrogate loss: -0.0212
             Mean action noise std: 0.73
                       Mean reward: 30.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 22.04s
                        Total time: 22898.57s
                               ETA: 1333665.6s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.117s, learning 0.228s)
               Value function loss: 0.4905
                    Surrogate loss: -0.0231
             Mean action noise std: 0.73
                       Mean reward: 30.50
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 21.34s
                        Total time: 22919.92s
                               ETA: 1334104.8s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 754 steps/s (collection: 21.543s, learning 0.169s)
               Value function loss: 0.4299
                    Surrogate loss: -0.0209
             Mean action noise std: 0.73
                       Mean reward: 29.10
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 21.71s
                        Total time: 22941.63s
                               ETA: 1334564.9s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 741 steps/s (collection: 21.920s, learning 0.170s)
               Value function loss: 0.4260
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 30.33
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 22.09s
                        Total time: 22963.72s
                               ETA: 1335046.4s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.173s, learning 0.176s)
               Value function loss: 0.4597
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 29.56
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 21.35s
                        Total time: 22985.07s
                               ETA: 1335484.2s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.502s, learning 0.187s)
               Value function loss: 0.4480
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 30.38
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 21.69s
                        Total time: 23006.76s
                               ETA: 1335941.2s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 754 steps/s (collection: 21.550s, learning 0.161s)
               Value function loss: 0.4293
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 30.25
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 21.71s
                        Total time: 23028.47s
                               ETA: 1336398.9s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.598s, learning 0.212s)
               Value function loss: 0.4678
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 30.53
               Mean episode length: 124.62
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 21.81s
                        Total time: 23050.28s
                               ETA: 1336861.8s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.934s, learning 0.172s)
               Value function loss: 0.5304
                    Surrogate loss: -0.0253
             Mean action noise std: 0.73
                       Mean reward: 30.27
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 21.11s
                        Total time: 23071.39s
                               ETA: 1337283.3s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 761 steps/s (collection: 21.351s, learning 0.165s)
               Value function loss: 0.5587
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 29.84
               Mean episode length: 124.13
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 21.52s
                        Total time: 23092.90s
                               ETA: 1337728.1s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.433s, learning 0.182s)
               Value function loss: 0.4806
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 30.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 21.61s
                        Total time: 23114.52s
                               ETA: 1338178.0s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.583s, learning 0.195s)
               Value function loss: 0.5840
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 30.69
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 21.78s
                        Total time: 23136.29s
                               ETA: 1338636.8s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.188s, learning 0.172s)
               Value function loss: 0.6493
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 30.08
               Mean episode length: 124.88
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 21.36s
                        Total time: 23157.65s
                               ETA: 1339070.9s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 741 steps/s (collection: 21.906s, learning 0.178s)
               Value function loss: 0.5956
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 30.30
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 22.08s
                        Total time: 23179.74s
                               ETA: 1339546.3s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 763 steps/s (collection: 21.284s, learning 0.162s)
               Value function loss: 0.5137
                    Surrogate loss: -0.0253
             Mean action noise std: 0.73
                       Mean reward: 30.22
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 21.45s
                        Total time: 23201.18s
                               ETA: 1339984.2s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.749s, learning 0.173s)
               Value function loss: 0.6597
                    Surrogate loss: -0.0244
             Mean action noise std: 0.73
                       Mean reward: 31.07
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 21.92s
                        Total time: 23223.11s
                               ETA: 1340449.1s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.989s, learning 0.199s)
               Value function loss: 0.5275
                    Surrogate loss: -0.0209
             Mean action noise std: 0.73
                       Mean reward: 29.98
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 16.19s
                        Total time: 23239.29s
                               ETA: 1340582.6s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.696s, learning 0.163s)
               Value function loss: 0.6484
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 29.99
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 10.86s
                        Total time: 23250.15s
                               ETA: 1340408.8s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.943s, learning 0.188s)
               Value function loss: 0.5508
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 30.09
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 11.13s
                        Total time: 23261.28s
                               ETA: 1340250.8s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.086s, learning 0.304s)
               Value function loss: 0.6031
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 31.25
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 11.39s
                        Total time: 23272.67s
                               ETA: 1340107.9s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.240s, learning 0.172s)
               Value function loss: 0.6166
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 31.45
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 11.41s
                        Total time: 23284.09s
                               ETA: 1339966.4s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.798s, learning 0.206s)
               Value function loss: 0.5331
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 30.72
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 11.00s
                        Total time: 23295.09s
                               ETA: 1339801.6s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.944s, learning 0.180s)
               Value function loss: 0.5951
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 30.46
               Mean episode length: 124.58
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 11.12s
                        Total time: 23306.21s
                               ETA: 1339643.9s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.019s, learning 0.175s)
               Value function loss: 0.6712
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 30.56
               Mean episode length: 124.13
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 11.19s
                        Total time: 23317.41s
                               ETA: 1339490.3s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.157s, learning 0.184s)
               Value function loss: 0.6604
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 30.28
               Mean episode length: 123.51
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 11.34s
                        Total time: 23328.75s
                               ETA: 1339345.4s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.779s, learning 0.196s)
               Value function loss: 0.7316
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 30.94
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 10.97s
                        Total time: 23339.72s
                               ETA: 1339179.7s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.670s, learning 0.193s)
               Value function loss: 0.7163
                    Surrogate loss: -0.0278
             Mean action noise std: 0.73
                       Mean reward: 30.64
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 10.86s
                        Total time: 23350.59s
                               ETA: 1339007.6s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.543s, learning 0.308s)
               Value function loss: 0.7245
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 30.96
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 11.85s
                        Total time: 23362.44s
                               ETA: 1338892.4s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.911s, learning 0.166s)
               Value function loss: 0.7940
                    Surrogate loss: -0.0227
             Mean action noise std: 0.73
                       Mean reward: 29.87
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 11.08s
                        Total time: 23373.51s
                               ETA: 1338733.0s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.838s, learning 0.161s)
               Value function loss: 0.6650
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 30.37
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 11.00s
                        Total time: 23384.51s
                               ETA: 1338569.2s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.930s, learning 0.189s)
               Value function loss: 0.7050
                    Surrogate loss: -0.0263
             Mean action noise std: 0.73
                       Mean reward: 31.12
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 11.12s
                        Total time: 23395.63s
                               ETA: 1338412.6s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.099s, learning 0.209s)
               Value function loss: 0.5438
                    Surrogate loss: -0.0246
             Mean action noise std: 0.73
                       Mean reward: 31.07
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 11.31s
                        Total time: 23406.94s
                               ETA: 1338266.9s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.642s, learning 0.271s)
               Value function loss: 0.6819
                    Surrogate loss: -0.0288
             Mean action noise std: 0.73
                       Mean reward: 31.43
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 10.91s
                        Total time: 23417.85s
                               ETA: 1338098.7s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.531s, learning 0.191s)
               Value function loss: 0.7261
                    Surrogate loss: -0.0244
             Mean action noise std: 0.73
                       Mean reward: 31.32
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 10.72s
                        Total time: 23428.57s
                               ETA: 1337919.9s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.739s, learning 0.160s)
               Value function loss: 0.5602
                    Surrogate loss: -0.0257
             Mean action noise std: 0.73
                       Mean reward: 31.39
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 10.90s
                        Total time: 23439.47s
                               ETA: 1337751.3s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.760s, learning 0.167s)
               Value function loss: 0.7091
                    Surrogate loss: -0.0283
             Mean action noise std: 0.73
                       Mean reward: 31.51
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 10.93s
                        Total time: 23450.40s
                               ETA: 1337584.6s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1460 steps/s (collection: 10.974s, learning 0.245s)
               Value function loss: 0.7064
                    Surrogate loss: -0.0259
             Mean action noise std: 0.73
                       Mean reward: 31.40
               Mean episode length: 124.24
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 11.22s
                        Total time: 23461.62s
                               ETA: 1337434.7s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.515s, learning 0.192s)
               Value function loss: 0.7236
                    Surrogate loss: -0.0246
             Mean action noise std: 0.73
                       Mean reward: 31.87
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 10.71s
                        Total time: 23472.33s
                               ETA: 1337255.8s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.193s, learning 0.203s)
               Value function loss: 0.6485
                    Surrogate loss: -0.0240
             Mean action noise std: 0.73
                       Mean reward: 32.49
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 11.40s
                        Total time: 23483.72s
                               ETA: 1337116.3s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.932s, learning 0.187s)
               Value function loss: 0.8763
                    Surrogate loss: -0.0258
             Mean action noise std: 0.73
                       Mean reward: 32.62
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 11.12s
                        Total time: 23494.84s
                               ETA: 1336961.2s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.311s, learning 0.168s)
               Value function loss: 0.8551
                    Surrogate loss: -0.0248
             Mean action noise std: 0.73
                       Mean reward: 32.33
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 11.48s
                        Total time: 23506.32s
                               ETA: 1336826.7s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.087s, learning 0.171s)
               Value function loss: 0.8927
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 32.51
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 11.26s
                        Total time: 23517.58s
                               ETA: 1336679.8s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.166s)
               Value function loss: 0.8804
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 32.44
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 11.12s
                        Total time: 23528.70s
                               ETA: 1336525.4s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.731s, learning 0.196s)
               Value function loss: 1.0234
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 32.73
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 10.93s
                        Total time: 23539.63s
                               ETA: 1336360.0s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.705s, learning 0.184s)
               Value function loss: 0.8820
                    Surrogate loss: -0.0227
             Mean action noise std: 0.73
                       Mean reward: 32.86
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 10.89s
                        Total time: 23550.52s
                               ETA: 1336192.7s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.547s, learning 0.169s)
               Value function loss: 0.8047
                    Surrogate loss: -0.0259
             Mean action noise std: 0.73
                       Mean reward: 32.69
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 11.72s
                        Total time: 23562.23s
                               ETA: 1336072.4s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.740s, learning 0.162s)
               Value function loss: 0.9638
                    Surrogate loss: -0.0209
             Mean action noise std: 0.73
                       Mean reward: 33.70
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 10.90s
                        Total time: 23573.13s
                               ETA: 1335906.1s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.434s, learning 0.165s)
               Value function loss: 0.7874
                    Surrogate loss: -0.0263
             Mean action noise std: 0.73
                       Mean reward: 33.56
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 10.60s
                        Total time: 23583.73s
                               ETA: 1335722.9s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.627s, learning 0.169s)
               Value function loss: 0.8628
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 33.01
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 10.80s
                        Total time: 23594.53s
                               ETA: 1335550.9s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.729s, learning 0.160s)
               Value function loss: 0.6567
                    Surrogate loss: -0.0232
             Mean action noise std: 0.73
                       Mean reward: 33.47
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 10.89s
                        Total time: 23605.42s
                               ETA: 1335384.5s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.843s, learning 0.163s)
               Value function loss: 0.6542
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 33.32
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 11.01s
                        Total time: 23616.42s
                               ETA: 1335224.8s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.911s, learning 0.188s)
               Value function loss: 0.7426
                    Surrogate loss: -0.0256
             Mean action noise std: 0.73
                       Mean reward: 33.41
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 11.10s
                        Total time: 23627.52s
                               ETA: 1335070.6s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.969s, learning 0.164s)
               Value function loss: 0.7571
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 33.21
               Mean episode length: 124.63
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 11.13s
                        Total time: 23638.66s
                               ETA: 1334918.4s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.931s, learning 0.265s)
               Value function loss: 0.6762
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 32.72
               Mean episode length: 124.38
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 11.20s
                        Total time: 23649.85s
                               ETA: 1334770.0s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.191s, learning 0.167s)
               Value function loss: 0.7648
                    Surrogate loss: -0.0234
             Mean action noise std: 0.73
                       Mean reward: 33.26
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 11.36s
                        Total time: 23661.21s
                               ETA: 1334630.8s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.713s, learning 0.170s)
               Value function loss: 0.9102
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 33.48
               Mean episode length: 124.13
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 10.88s
                        Total time: 23672.09s
                               ETA: 1334465.0s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.637s, learning 0.161s)
               Value function loss: 0.8466
                    Surrogate loss: -0.0236
             Mean action noise std: 0.73
                       Mean reward: 32.94
               Mean episode length: 124.12
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 10.80s
                        Total time: 23682.89s
                               ETA: 1334294.6s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.916s, learning 0.175s)
               Value function loss: 0.7618
                    Surrogate loss: -0.0236
             Mean action noise std: 0.73
                       Mean reward: 33.74
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 11.09s
                        Total time: 23693.98s
                               ETA: 1334140.9s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.962s, learning 0.163s)
               Value function loss: 0.9077
                    Surrogate loss: -0.0261
             Mean action noise std: 0.73
                       Mean reward: 33.06
               Mean episode length: 124.96
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 11.13s
                        Total time: 23705.11s
                               ETA: 1333989.3s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.397s, learning 0.203s)
               Value function loss: 0.9913
                    Surrogate loss: -0.0223
             Mean action noise std: 0.73
                       Mean reward: 33.73
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 11.60s
                        Total time: 23716.71s
                               ETA: 1333864.5s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.903s, learning 0.292s)
               Value function loss: 0.7835
                    Surrogate loss: -0.0241
             Mean action noise std: 0.73
                       Mean reward: 32.88
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 11.19s
                        Total time: 23727.90s
                               ETA: 1333717.0s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.086s, learning 0.180s)
               Value function loss: 0.7422
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 33.16
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 11.27s
                        Total time: 23739.17s
                               ETA: 1333573.8s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.771s, learning 0.196s)
               Value function loss: 0.7981
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 33.71
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 10.97s
                        Total time: 23750.13s
                               ETA: 1333413.9s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.461s, learning 0.214s)
               Value function loss: 0.6934
                    Surrogate loss: -0.0247
             Mean action noise std: 0.73
                       Mean reward: 33.42
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 10.67s
                        Total time: 23760.81s
                               ETA: 1333237.8s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.315s, learning 0.195s)
               Value function loss: 0.8150
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 32.96
               Mean episode length: 124.13
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 11.51s
                        Total time: 23772.32s
                               ETA: 1333108.7s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.980s, learning 0.163s)
               Value function loss: 0.7451
                    Surrogate loss: -0.0263
             Mean action noise std: 0.73
                       Mean reward: 33.16
               Mean episode length: 124.10
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 11.14s
                        Total time: 23783.46s
                               ETA: 1332959.2s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.948s, learning 0.165s)
               Value function loss: 0.8443
                    Surrogate loss: -0.0240
             Mean action noise std: 0.73
                       Mean reward: 33.14
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 11.11s
                        Total time: 23794.57s
                               ETA: 1332808.2s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.766s, learning 0.160s)
               Value function loss: 0.8635
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 33.29
               Mean episode length: 124.97
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 10.93s
                        Total time: 23805.50s
                               ETA: 1332646.9s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.068s, learning 0.184s)
               Value function loss: 0.7967
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 33.12
               Mean episode length: 124.24
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 11.25s
                        Total time: 23816.75s
                               ETA: 1332503.9s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.759s, learning 0.161s)
               Value function loss: 0.8030
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 33.48
               Mean episode length: 124.11
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 10.92s
                        Total time: 23827.67s
                               ETA: 1332342.5s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.749s, learning 0.214s)
               Value function loss: 0.9649
                    Surrogate loss: -0.0224
             Mean action noise std: 0.73
                       Mean reward: 33.53
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 10.96s
                        Total time: 23838.64s
                               ETA: 1332183.8s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.032s, learning 0.170s)
               Value function loss: 0.9040
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 32.48
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 11.20s
                        Total time: 23849.84s
                               ETA: 1332038.5s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.166s, learning 0.199s)
               Value function loss: 1.0215
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 33.37
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 11.36s
                        Total time: 23861.20s
                               ETA: 1331902.5s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.037s, learning 0.159s)
               Value function loss: 0.9885
                    Surrogate loss: -0.0225
             Mean action noise std: 0.73
                       Mean reward: 33.32
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 11.20s
                        Total time: 23872.40s
                               ETA: 1331757.2s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.893s, learning 0.167s)
               Value function loss: 0.9927
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 33.46
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 11.06s
                        Total time: 23883.46s
                               ETA: 1331604.5s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.830s, learning 0.231s)
               Value function loss: 1.0707
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 32.84
               Mean episode length: 124.43
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 11.06s
                        Total time: 23894.52s
                               ETA: 1331452.0s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.196s, learning 0.169s)
               Value function loss: 0.8298
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 33.18
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 11.36s
                        Total time: 23905.89s
                               ETA: 1331316.6s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.698s, learning 0.164s)
               Value function loss: 1.0231
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 33.13
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 10.86s
                        Total time: 23916.75s
                               ETA: 1331153.3s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.167s)
               Value function loss: 0.8009
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 33.85
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 11.29s
                        Total time: 23928.04s
                               ETA: 1331013.9s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.940s, learning 0.173s)
               Value function loss: 0.8714
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 31.96
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 11.11s
                        Total time: 23939.15s
                               ETA: 1330865.0s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.676s, learning 0.173s)
               Value function loss: 0.6445
                    Surrogate loss: -0.0245
             Mean action noise std: 0.73
                       Mean reward: 32.03
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 10.85s
                        Total time: 23950.00s
                               ETA: 1330701.4s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.755s, learning 0.188s)
               Value function loss: 0.6623
                    Surrogate loss: -0.0238
             Mean action noise std: 0.73
                       Mean reward: 32.63
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 10.94s
                        Total time: 23960.94s
                               ETA: 1330543.4s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.464s, learning 0.175s)
               Value function loss: 0.7178
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 31.54
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 11.64s
                        Total time: 23972.58s
                               ETA: 1330424.1s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.152s, learning 0.167s)
               Value function loss: 0.6580
                    Surrogate loss: -0.0219
             Mean action noise std: 0.73
                       Mean reward: 32.72
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 11.32s
                        Total time: 23983.90s
                               ETA: 1330287.1s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.079s, learning 0.186s)
               Value function loss: 0.6415
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 32.37
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 11.27s
                        Total time: 23995.17s
                               ETA: 1330147.4s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.107s, learning 0.262s)
               Value function loss: 0.6726
                    Surrogate loss: -0.0231
             Mean action noise std: 0.73
                       Mean reward: 33.12
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 11.37s
                        Total time: 24006.53s
                               ETA: 1330013.4s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.937s, learning 0.179s)
               Value function loss: 0.7971
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 32.35
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 11.12s
                        Total time: 24017.65s
                               ETA: 1329865.7s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.934s, learning 0.185s)
               Value function loss: 0.7323
                    Surrogate loss: -0.0230
             Mean action noise std: 0.73
                       Mean reward: 32.63
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 11.12s
                        Total time: 24028.77s
                               ETA: 1329718.3s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.164s)
               Value function loss: 0.7126
                    Surrogate loss: -0.0225
             Mean action noise std: 0.73
                       Mean reward: 32.76
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 11.34s
                        Total time: 24040.11s
                               ETA: 1329583.0s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.028s, learning 0.190s)
               Value function loss: 0.7629
                    Surrogate loss: -0.0259
             Mean action noise std: 0.73
                       Mean reward: 33.62
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 11.22s
                        Total time: 24051.32s
                               ETA: 1329441.4s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.014s, learning 0.161s)
               Value function loss: 0.8411
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 33.17
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 11.18s
                        Total time: 24062.50s
                               ETA: 1329297.5s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.164s)
               Value function loss: 0.8346
                    Surrogate loss: -0.0217
             Mean action noise std: 0.73
                       Mean reward: 32.81
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 10.95s
                        Total time: 24073.45s
                               ETA: 1329141.1s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.634s, learning 0.176s)
               Value function loss: 0.7166
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 33.37
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 10.81s
                        Total time: 24084.26s
                               ETA: 1328977.4s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.933s, learning 0.167s)
               Value function loss: 0.9002
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 33.40
               Mean episode length: 124.22
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 11.10s
                        Total time: 24095.36s
                               ETA: 1328829.8s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.938s, learning 0.197s)
               Value function loss: 0.7253
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 33.78
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 11.14s
                        Total time: 24106.49s
                               ETA: 1328684.3s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.858s, learning 0.173s)
               Value function loss: 0.8503
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 32.40
               Mean episode length: 124.04
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 11.03s
                        Total time: 24117.52s
                               ETA: 1328533.2s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.207s, learning 0.192s)
               Value function loss: 0.6484
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 33.52
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 11.40s
                        Total time: 24128.92s
                               ETA: 1328402.6s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.425s, learning 0.177s)
               Value function loss: 0.7907
                    Surrogate loss: -0.0220
             Mean action noise std: 0.73
                       Mean reward: 33.19
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 11.60s
                        Total time: 24140.52s
                               ETA: 1328283.2s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.935s, learning 0.178s)
               Value function loss: 0.8775
                    Surrogate loss: -0.0222
             Mean action noise std: 0.73
                       Mean reward: 33.14
               Mean episode length: 124.20
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 11.11s
                        Total time: 24151.64s
                               ETA: 1328137.1s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.758s, learning 0.179s)
               Value function loss: 0.7805
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 33.19
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 10.94s
                        Total time: 24162.57s
                               ETA: 1327981.5s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.870s, learning 0.176s)
               Value function loss: 0.8780
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 33.72
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 11.05s
                        Total time: 24173.62s
                               ETA: 1327832.0s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.989s, learning 0.170s)
               Value function loss: 0.9548
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 34.19
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 11.16s
                        Total time: 24184.78s
                               ETA: 1327688.9s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.749s, learning 0.175s)
               Value function loss: 1.0410
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 33.73
               Mean episode length: 124.37
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 11.92s
                        Total time: 24196.70s
                               ETA: 1327587.9s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.433s, learning 0.161s)
               Value function loss: 1.0196
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 34.02
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 11.59s
                        Total time: 24208.30s
                               ETA: 1327468.8s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.808s, learning 0.198s)
               Value function loss: 0.9407
                    Surrogate loss: -0.0217
             Mean action noise std: 0.73
                       Mean reward: 33.15
               Mean episode length: 124.12
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 11.01s
                        Total time: 24219.30s
                               ETA: 1327317.8s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.945s, learning 0.167s)
               Value function loss: 1.0536
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 33.99
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 11.11s
                        Total time: 24230.41s
                               ETA: 1327172.6s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.285s, learning 0.167s)
               Value function loss: 1.1107
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 34.70
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 11.45s
                        Total time: 24241.87s
                               ETA: 1327046.2s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.915s, learning 0.169s)
               Value function loss: 0.9001
                    Surrogate loss: -0.0248
             Mean action noise std: 0.73
                       Mean reward: 33.65
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 11.08s
                        Total time: 24252.95s
                               ETA: 1326899.8s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.798s, learning 0.178s)
               Value function loss: 0.9154
                    Surrogate loss: -0.0213
             Mean action noise std: 0.73
                       Mean reward: 34.73
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 10.98s
                        Total time: 24263.93s
                               ETA: 1326747.7s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.098s, learning 0.211s)
               Value function loss: 0.9284
                    Surrogate loss: -0.0224
             Mean action noise std: 0.73
                       Mean reward: 34.68
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 11.31s
                        Total time: 24275.24s
                               ETA: 1326613.9s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.854s, learning 0.166s)
               Value function loss: 0.9746
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 33.63
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 11.02s
                        Total time: 24286.25s
                               ETA: 1326464.4s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.243s, learning 0.163s)
               Value function loss: 1.1047
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 35.17
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 11.41s
                        Total time: 24297.66s
                               ETA: 1326336.2s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.165s)
               Value function loss: 0.8631
                    Surrogate loss: -0.0230
             Mean action noise std: 0.73
                       Mean reward: 34.52
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 11.01s
                        Total time: 24308.68s
                               ETA: 1326186.8s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.009s, learning 0.185s)
               Value function loss: 1.0032
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 34.17
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 11.19s
                        Total time: 24319.87s
                               ETA: 1326047.3s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.200s, learning 0.166s)
               Value function loss: 1.0141
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 33.57
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 11.37s
                        Total time: 24331.24s
                               ETA: 1325917.3s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.168s)
               Value function loss: 0.9233
                    Surrogate loss: -0.0229
             Mean action noise std: 0.73
                       Mean reward: 34.53
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 11.40s
                        Total time: 24342.64s
                               ETA: 1325789.3s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.170s)
               Value function loss: 0.8948
                    Surrogate loss: -0.0224
             Mean action noise std: 0.72
                       Mean reward: 34.45
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 11.39s
                        Total time: 24354.03s
                               ETA: 1325661.1s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.164s)
               Value function loss: 1.2378
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 34.08
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 10.91s
                        Total time: 24364.94s
                               ETA: 1325506.9s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.690s, learning 0.241s)
               Value function loss: 1.1549
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 33.92
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 11.93s
                        Total time: 24376.87s
                               ETA: 1325408.1s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.462s, learning 0.256s)
               Value function loss: 1.0660
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 34.28
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 11.72s
                        Total time: 24388.59s
                               ETA: 1325297.9s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.888s, learning 0.183s)
               Value function loss: 0.9970
                    Surrogate loss: -0.0214
             Mean action noise std: 0.72
                       Mean reward: 34.55
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 11.07s
                        Total time: 24399.66s
                               ETA: 1325152.7s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.878s, learning 0.197s)
               Value function loss: 1.0237
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 34.05
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 11.08s
                        Total time: 24410.74s
                               ETA: 1325007.8s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.914s, learning 0.188s)
               Value function loss: 1.0033
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 33.51
               Mean episode length: 124.07
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 11.10s
                        Total time: 24421.84s
                               ETA: 1324864.5s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.173s)
               Value function loss: 0.8194
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 33.77
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 11.51s
                        Total time: 24433.35s
                               ETA: 1324743.7s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.076s, learning 0.202s)
               Value function loss: 0.9310
                    Surrogate loss: -0.0259
             Mean action noise std: 0.72
                       Mean reward: 33.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 11.28s
                        Total time: 24444.63s
                               ETA: 1324610.3s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.325s, learning 0.176s)
               Value function loss: 0.7534
                    Surrogate loss: -0.0257
             Mean action noise std: 0.72
                       Mean reward: 33.87
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 11.50s
                        Total time: 24456.13s
                               ETA: 1324489.1s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.169s)
               Value function loss: 0.8371
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 33.28
               Mean episode length: 124.29
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 11.39s
                        Total time: 24467.52s
                               ETA: 1324362.0s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.110s, learning 0.184s)
               Value function loss: 0.8115
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 33.93
               Mean episode length: 124.39
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 11.29s
                        Total time: 24478.82s
                               ETA: 1324229.8s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.160s, learning 0.162s)
               Value function loss: 0.7425
                    Surrogate loss: -0.0253
             Mean action noise std: 0.72
                       Mean reward: 33.82
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 11.32s
                        Total time: 24490.14s
                               ETA: 1324099.3s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.276s, learning 0.169s)
               Value function loss: 0.7998
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 33.62
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 11.44s
                        Total time: 24501.58s
                               ETA: 1323975.6s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.109s, learning 0.188s)
               Value function loss: 0.6287
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 33.21
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 11.30s
                        Total time: 24512.88s
                               ETA: 1323843.9s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.104s, learning 0.178s)
               Value function loss: 0.6429
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 33.03
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 11.28s
                        Total time: 24524.16s
                               ETA: 1323711.6s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.490s, learning 0.178s)
               Value function loss: 0.7017
                    Surrogate loss: -0.0224
             Mean action noise std: 0.72
                       Mean reward: 32.69
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 10.67s
                        Total time: 24534.83s
                               ETA: 1323546.3s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.849s, learning 0.173s)
               Value function loss: 0.8614
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 34.03
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 11.02s
                        Total time: 24545.85s
                               ETA: 1323400.2s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.209s, learning 0.192s)
               Value function loss: 0.7572
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 33.21
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 11.40s
                        Total time: 24557.25s
                               ETA: 1323274.7s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.872s, learning 0.210s)
               Value function loss: 0.7390
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 32.67
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 11.08s
                        Total time: 24568.34s
                               ETA: 1323132.2s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.278s, learning 0.170s)
               Value function loss: 0.9163
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 32.81
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 11.45s
                        Total time: 24579.78s
                               ETA: 1323009.6s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.717s, learning 0.173s)
               Value function loss: 1.1870
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 32.24
               Mean episode length: 124.10
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 10.89s
                        Total time: 24590.67s
                               ETA: 1322857.0s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.910s, learning 0.164s)
               Value function loss: 0.8948
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 33.14
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 11.07s
                        Total time: 24601.75s
                               ETA: 1322714.5s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.390s, learning 0.163s)
               Value function loss: 0.7027
                    Surrogate loss: -0.0219
             Mean action noise std: 0.72
                       Mean reward: 32.38
               Mean episode length: 124.19
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 11.55s
                        Total time: 24613.30s
                               ETA: 1322597.9s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.173s, learning 0.164s)
               Value function loss: 0.8579
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 33.64
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 11.34s
                        Total time: 24624.64s
                               ETA: 1322469.7s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.922s, learning 0.177s)
               Value function loss: 0.6043
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 33.58
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 11.10s
                        Total time: 24635.74s
                               ETA: 1322329.0s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.883s, learning 0.192s)
               Value function loss: 0.7522
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 32.65
               Mean episode length: 123.37
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 11.08s
                        Total time: 24646.81s
                               ETA: 1322187.1s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.964s, learning 0.177s)
               Value function loss: 0.6628
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 33.18
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 11.14s
                        Total time: 24657.95s
                               ETA: 1322048.8s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.036s, learning 0.166s)
               Value function loss: 0.6618
                    Surrogate loss: -0.0230
             Mean action noise std: 0.72
                       Mean reward: 33.30
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 11.20s
                        Total time: 24669.16s
                               ETA: 1321914.0s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.976s, learning 0.192s)
               Value function loss: 0.6749
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 33.82
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 11.17s
                        Total time: 24680.32s
                               ETA: 1321777.5s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.714s, learning 0.190s)
               Value function loss: 0.5708
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 33.34
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 10.90s
                        Total time: 24691.23s
                               ETA: 1321627.0s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.692s, learning 0.239s)
               Value function loss: 0.6190
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 32.82
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 10.93s
                        Total time: 24702.16s
                               ETA: 1321478.1s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.959s, learning 0.168s)
               Value function loss: 0.6959
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 33.52
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 11.13s
                        Total time: 24713.29s
                               ETA: 1321339.8s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.706s, learning 0.165s)
               Value function loss: 0.6612
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 33.59
               Mean episode length: 124.89
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 10.87s
                        Total time: 24724.16s
                               ETA: 1321187.9s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.812s, learning 0.172s)
               Value function loss: 0.7317
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 33.10
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 10.98s
                        Total time: 24735.14s
                               ETA: 1321042.3s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.185s, learning 0.173s)
               Value function loss: 0.6055
                    Surrogate loss: -0.0248
             Mean action noise std: 0.72
                       Mean reward: 32.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 11.36s
                        Total time: 24746.50s
                               ETA: 1320916.7s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.150s, learning 0.219s)
               Value function loss: 0.7151
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 32.62
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.37s
                        Total time: 24757.87s
                               ETA: 1320791.9s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.159s)
               Value function loss: 0.8152
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 33.47
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 11.33s
                        Total time: 24769.20s
                               ETA: 1320665.4s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.797s, learning 0.169s)
               Value function loss: 0.6324
                    Surrogate loss: -0.0232
             Mean action noise std: 0.72
                       Mean reward: 32.37
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 10.97s
                        Total time: 24780.17s
                               ETA: 1320519.3s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.335s, learning 0.168s)
               Value function loss: 0.7403
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 33.45
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 11.50s
                        Total time: 24791.67s
                               ETA: 1320402.0s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.556s, learning 0.163s)
               Value function loss: 0.6145
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 34.51
               Mean episode length: 124.84
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 11.72s
                        Total time: 24803.39s
                               ETA: 1320296.3s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.919s, learning 0.240s)
               Value function loss: 0.6938
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 33.64
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 11.16s
                        Total time: 24814.55s
                               ETA: 1320161.0s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.819s, learning 0.172s)
               Value function loss: 0.9046
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 34.56
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 10.99s
                        Total time: 24825.54s
                               ETA: 1320016.8s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.025s, learning 0.200s)
               Value function loss: 0.8309
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 33.36
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 11.23s
                        Total time: 24836.77s
                               ETA: 1319885.2s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.791s, learning 0.171s)
               Value function loss: 0.8956
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 34.60
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 10.96s
                        Total time: 24847.73s
                               ETA: 1319739.8s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.989s, learning 0.167s)
               Value function loss: 0.8328
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 35.63
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 11.16s
                        Total time: 24858.88s
                               ETA: 1319604.8s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.162s)
               Value function loss: 0.8672
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 34.69
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 11.06s
                        Total time: 24869.94s
                               ETA: 1319464.7s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.073s, learning 0.170s)
               Value function loss: 0.8140
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 35.07
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 11.24s
                        Total time: 24881.19s
                               ETA: 1319334.6s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.702s, learning 0.191s)
               Value function loss: 1.1381
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 35.23
               Mean episode length: 124.59
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 10.89s
                        Total time: 24892.08s
                               ETA: 1319186.1s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.834s, learning 0.198s)
               Value function loss: 1.1190
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 35.53
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 11.03s
                        Total time: 24903.11s
                               ETA: 1319045.0s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.365s, learning 0.173s)
               Value function loss: 1.2399
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 35.36
               Mean episode length: 124.86
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 11.54s
                        Total time: 24914.65s
                               ETA: 1318931.0s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.262s, learning 0.161s)
               Value function loss: 1.3314
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 35.37
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 11.42s
                        Total time: 24926.07s
                               ETA: 1318811.0s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.150s, learning 0.187s)
               Value function loss: 1.6293
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 35.64
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 11.34s
                        Total time: 24937.41s
                               ETA: 1318686.5s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.741s, learning 0.168s)
               Value function loss: 1.2108
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 35.33
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 10.91s
                        Total time: 24948.32s
                               ETA: 1318539.5s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.166s)
               Value function loss: 1.0324
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 35.13
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 11.17s
                        Total time: 24959.49s
                               ETA: 1318406.6s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.966s, learning 0.198s)
               Value function loss: 1.3292
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 35.64
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 11.16s
                        Total time: 24970.66s
                               ETA: 1318273.4s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.150s, learning 0.171s)
               Value function loss: 0.9985
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 35.11
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 11.32s
                        Total time: 24981.98s
                               ETA: 1318148.5s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.579s, learning 0.159s)
               Value function loss: 1.1752
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 35.61
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 10.74s
                        Total time: 24992.72s
                               ETA: 1317993.1s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.463s, learning 0.191s)
               Value function loss: 1.1191
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 34.56
               Mean episode length: 123.17
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 11.65s
                        Total time: 25004.37s
                               ETA: 1317886.0s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.890s, learning 0.156s)
               Value function loss: 1.1344
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 35.95
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 11.05s
                        Total time: 25015.42s
                               ETA: 1317747.1s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.162s)
               Value function loss: 1.4092
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 36.12
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 10.96s
                        Total time: 25026.38s
                               ETA: 1317603.8s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.039s, learning 0.162s)
               Value function loss: 1.3453
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 35.57
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 11.20s
                        Total time: 25037.58s
                               ETA: 1317473.3s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.167s)
               Value function loss: 1.2378
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 36.12
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 11.23s
                        Total time: 25048.81s
                               ETA: 1317344.3s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.645s, learning 0.185s)
               Value function loss: 1.1218
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 36.25
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 10.83s
                        Total time: 25059.64s
                               ETA: 1317194.6s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.672s, learning 0.159s)
               Value function loss: 1.4083
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 35.81
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 10.83s
                        Total time: 25070.47s
                               ETA: 1317045.0s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.892s, learning 0.185s)
               Value function loss: 1.4363
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 36.14
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 11.08s
                        Total time: 25081.54s
                               ETA: 1316908.5s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.057s, learning 0.163s)
               Value function loss: 1.2253
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 36.06
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 11.22s
                        Total time: 25092.76s
                               ETA: 1316779.7s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.001s, learning 0.179s)
               Value function loss: 1.3992
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 35.56
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 11.18s
                        Total time: 25103.94s
                               ETA: 1316648.8s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.409s, learning 0.193s)
               Value function loss: 1.1965
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 34.72
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 11.60s
                        Total time: 25115.55s
                               ETA: 1316540.3s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.120s, learning 0.201s)
               Value function loss: 1.1348
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 35.02
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 11.32s
                        Total time: 25126.87s
                               ETA: 1316417.1s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.618s, learning 0.181s)
               Value function loss: 0.8942
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 35.44
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 10.80s
                        Total time: 25137.67s
                               ETA: 1316266.6s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.162s)
               Value function loss: 1.0203
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 35.89
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 11.01s
                        Total time: 25148.68s
                               ETA: 1316127.6s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.826s, learning 0.172s)
               Value function loss: 0.8669
                    Surrogate loss: -0.0224
             Mean action noise std: 0.72
                       Mean reward: 34.79
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 11.00s
                        Total time: 25159.68s
                               ETA: 1315987.8s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.487s, learning 0.183s)
               Value function loss: 1.0477
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 35.47
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 10.67s
                        Total time: 25170.35s
                               ETA: 1315831.1s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.869s, learning 0.160s)
               Value function loss: 0.8981
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 35.48
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 11.03s
                        Total time: 25181.37s
                               ETA: 1315693.3s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.284s, learning 0.264s)
               Value function loss: 0.9952
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 34.81
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 11.55s
                        Total time: 25192.92s
                               ETA: 1315582.7s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.420s, learning 0.186s)
               Value function loss: 0.9845
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 34.76
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 11.61s
                        Total time: 25204.53s
                               ETA: 1315475.3s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.045s, learning 0.168s)
               Value function loss: 0.8130
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 35.22
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 11.21s
                        Total time: 25215.74s
                               ETA: 1315347.5s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.732s, learning 0.192s)
               Value function loss: 0.8337
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 35.75
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 10.92s
                        Total time: 25226.67s
                               ETA: 1315204.7s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.336s, learning 0.160s)
               Value function loss: 1.0359
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 36.11
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 11.50s
                        Total time: 25238.16s
                               ETA: 1315091.8s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.083s, learning 0.277s)
               Value function loss: 1.0069
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 35.96
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 11.36s
                        Total time: 25249.52s
                               ETA: 1314972.0s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.087s, learning 0.193s)
               Value function loss: 0.9847
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 36.37
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 11.28s
                        Total time: 25260.80s
                               ETA: 1314848.2s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.157s, learning 0.247s)
               Value function loss: 1.0262
                    Surrogate loss: -0.0228
             Mean action noise std: 0.72
                       Mean reward: 35.07
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 11.40s
                        Total time: 25272.20s
                               ETA: 1314730.8s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.961s, learning 0.166s)
               Value function loss: 0.9706
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 36.27
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 11.13s
                        Total time: 25283.33s
                               ETA: 1314599.2s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.875s, learning 0.170s)
               Value function loss: 0.9843
                    Surrogate loss: -0.0213
             Mean action noise std: 0.72
                       Mean reward: 35.22
               Mean episode length: 124.07
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 11.05s
                        Total time: 25294.38s
                               ETA: 1314463.5s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.962s, learning 0.160s)
               Value function loss: 0.8915
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 34.89
               Mean episode length: 124.07
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 11.12s
                        Total time: 25305.50s
                               ETA: 1314332.0s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.661s, learning 0.194s)
               Value function loss: 0.9985
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 36.53
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 10.86s
                        Total time: 25316.35s
                               ETA: 1314186.7s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.789s, learning 0.193s)
               Value function loss: 0.8435
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 36.22
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 10.98s
                        Total time: 25327.34s
                               ETA: 1314048.1s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.623s, learning 0.187s)
               Value function loss: 0.9889
                    Surrogate loss: -0.0234
             Mean action noise std: 0.72
                       Mean reward: 36.24
               Mean episode length: 124.28
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 10.81s
                        Total time: 25338.15s
                               ETA: 1313900.7s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.722s, learning 0.171s)
               Value function loss: 0.9976
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 36.56
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 10.89s
                        Total time: 25349.04s
                               ETA: 1313757.8s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.205s, learning 0.247s)
               Value function loss: 0.9707
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 36.61
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 11.45s
                        Total time: 25360.49s
                               ETA: 1313644.0s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.917s, learning 0.184s)
               Value function loss: 0.9390
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 36.98
               Mean episode length: 124.45
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 11.10s
                        Total time: 25371.59s
                               ETA: 1313512.1s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.790s, learning 0.158s)
               Value function loss: 0.9653
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 36.45
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 10.95s
                        Total time: 25382.54s
                               ETA: 1313372.4s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.967s, learning 0.160s)
               Value function loss: 0.9767
                    Surrogate loss: -0.0218
             Mean action noise std: 0.72
                       Mean reward: 35.96
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 11.13s
                        Total time: 25393.67s
                               ETA: 1313242.2s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.032s, learning 0.168s)
               Value function loss: 0.8636
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 35.97
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 11.20s
                        Total time: 25404.87s
                               ETA: 1313115.8s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.025s, learning 0.207s)
               Value function loss: 1.1600
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 37.25
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 11.23s
                        Total time: 25416.10s
                               ETA: 1312991.1s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.771s, learning 0.216s)
               Value function loss: 0.9209
                    Surrogate loss: -0.0264
             Mean action noise std: 0.72
                       Mean reward: 36.41
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 10.99s
                        Total time: 25427.09s
                               ETA: 1312854.0s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.253s, learning 0.210s)
               Value function loss: 0.8884
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 35.71
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 11.46s
                        Total time: 25438.55s
                               ETA: 1312741.5s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.951s, learning 0.198s)
               Value function loss: 0.8982
                    Surrogate loss: -0.0218
             Mean action noise std: 0.72
                       Mean reward: 36.17
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 11.15s
                        Total time: 25449.70s
                               ETA: 1312613.0s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.641s, learning 0.183s)
               Value function loss: 0.9510
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 35.54
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 10.82s
                        Total time: 25460.52s
                               ETA: 1312467.8s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.856s, learning 0.241s)
               Value function loss: 0.8924
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 36.07
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 11.10s
                        Total time: 25471.62s
                               ETA: 1312336.8s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.118s, learning 0.178s)
               Value function loss: 0.7715
                    Surrogate loss: -0.0241
             Mean action noise std: 0.72
                       Mean reward: 36.18
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 11.30s
                        Total time: 25482.91s
                               ETA: 1312216.2s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.090s, learning 0.213s)
               Value function loss: 0.9807
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 35.74
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 11.30s
                        Total time: 25494.22s
                               ETA: 1312096.1s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.428s, learning 0.162s)
               Value function loss: 0.8806
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 36.09
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 10.59s
                        Total time: 25504.81s
                               ETA: 1311939.5s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.814s, learning 0.160s)
               Value function loss: 1.0772
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 35.43
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 10.97s
                        Total time: 25515.78s
                               ETA: 1311802.7s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.919s, learning 0.199s)
               Value function loss: 0.8282
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 35.20
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 11.12s
                        Total time: 25526.90s
                               ETA: 1311673.4s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.013s, learning 0.169s)
               Value function loss: 0.9265
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 34.85
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 11.18s
                        Total time: 25538.08s
                               ETA: 1311547.6s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.502s, learning 0.163s)
               Value function loss: 0.9137
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 35.39
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 10.66s
                        Total time: 25548.75s
                               ETA: 1311395.3s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.218s, learning 0.162s)
               Value function loss: 0.9164
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 34.83
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 11.38s
                        Total time: 25560.13s
                               ETA: 1311279.9s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.856s, learning 0.164s)
               Value function loss: 0.8656
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 35.37
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 11.02s
                        Total time: 25571.15s
                               ETA: 1311146.1s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.310s, learning 0.182s)
               Value function loss: 1.0069
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 35.65
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 11.49s
                        Total time: 25582.64s
                               ETA: 1311036.6s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.854s, learning 0.163s)
               Value function loss: 1.0805
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 33.92
               Mean episode length: 124.50
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 11.02s
                        Total time: 25593.65s
                               ETA: 1310903.0s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.062s, learning 0.162s)
               Value function loss: 1.0444
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 34.75
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 11.22s
                        Total time: 25604.88s
                               ETA: 1310780.0s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.042s, learning 0.193s)
               Value function loss: 1.0501
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 35.34
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 11.23s
                        Total time: 25616.11s
                               ETA: 1310657.7s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.509s, learning 0.166s)
               Value function loss: 1.1486
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 35.07
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 11.67s
                        Total time: 25627.79s
                               ETA: 1310558.1s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.811s, learning 0.194s)
               Value function loss: 1.3583
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 35.78
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 11.00s
                        Total time: 25638.79s
                               ETA: 1310424.2s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.885s, learning 0.175s)
               Value function loss: 1.0888
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 35.12
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 11.06s
                        Total time: 25649.85s
                               ETA: 1310293.4s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.914s, learning 0.176s)
               Value function loss: 1.0172
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 35.11
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 11.09s
                        Total time: 25660.94s
                               ETA: 1310164.2s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.366s, learning 0.190s)
               Value function loss: 1.1256
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 35.38
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 11.56s
                        Total time: 25672.50s
                               ETA: 1310058.8s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.836s, learning 0.198s)
               Value function loss: 1.0477
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 35.13
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 11.03s
                        Total time: 25683.53s
                               ETA: 1309927.0s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.951s, learning 0.251s)
               Value function loss: 1.2347
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 35.89
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 11.20s
                        Total time: 25694.74s
                               ETA: 1309803.8s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.138s, learning 0.167s)
               Value function loss: 0.9485
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 35.81
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 11.30s
                        Total time: 25706.04s
                               ETA: 1309686.0s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.021s, learning 0.162s)
               Value function loss: 1.0054
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 35.47
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 11.18s
                        Total time: 25717.22s
                               ETA: 1309562.1s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.895s, learning 0.174s)
               Value function loss: 1.0540
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 36.01
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 11.07s
                        Total time: 25728.29s
                               ETA: 1309432.6s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.849s, learning 0.163s)
               Value function loss: 0.8797
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 34.46
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 11.01s
                        Total time: 25739.30s
                               ETA: 1309300.2s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.972s, learning 0.160s)
               Value function loss: 0.8968
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 35.13
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 11.13s
                        Total time: 25750.44s
                               ETA: 1309174.1s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.865s, learning 0.170s)
               Value function loss: 1.2222
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 34.97
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 11.04s
                        Total time: 25761.47s
                               ETA: 1309043.1s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.211s, learning 0.187s)
               Value function loss: 1.1218
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 35.82
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 11.40s
                        Total time: 25772.87s
                               ETA: 1308930.7s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.026s, learning 0.176s)
               Value function loss: 1.0338
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 36.04
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 11.20s
                        Total time: 25784.07s
                               ETA: 1308808.6s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.942s, learning 0.170s)
               Value function loss: 1.0475
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 34.82
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 11.11s
                        Total time: 25795.18s
                               ETA: 1308681.9s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.162s)
               Value function loss: 1.0687
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 34.91
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 10.96s
                        Total time: 25806.14s
                               ETA: 1308547.6s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.087s, learning 0.201s)
               Value function loss: 1.0525
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 35.14
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 11.29s
                        Total time: 25817.43s
                               ETA: 1308430.1s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.130s, learning 0.199s)
               Value function loss: 0.9175
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 34.78
               Mean episode length: 123.98
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 11.33s
                        Total time: 25828.76s
                               ETA: 1308314.7s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.785s, learning 0.175s)
               Value function loss: 1.0331
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 33.49
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 10.96s
                        Total time: 25839.72s
                               ETA: 1308180.8s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.924s, learning 0.174s)
               Value function loss: 0.8464
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 35.38
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 11.10s
                        Total time: 25850.82s
                               ETA: 1308054.0s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.557s, learning 0.164s)
               Value function loss: 0.9466
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 34.93
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 10.72s
                        Total time: 25861.54s
                               ETA: 1307908.3s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.064s, learning 0.164s)
               Value function loss: 0.8381
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 35.14
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 11.23s
                        Total time: 25872.77s
                               ETA: 1307788.4s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.314s, learning 0.218s)
               Value function loss: 0.7903
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 34.42
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 11.53s
                        Total time: 25884.30s
                               ETA: 1307683.8s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.251s, learning 0.175s)
               Value function loss: 0.8669
                    Surrogate loss: -0.0208
             Mean action noise std: 0.72
                       Mean reward: 35.15
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 11.43s
                        Total time: 25895.72s
                               ETA: 1307574.0s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.151s, learning 0.183s)
               Value function loss: 0.8496
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 33.97
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 11.33s
                        Total time: 25907.06s
                               ETA: 1307459.7s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.235s, learning 0.161s)
               Value function loss: 0.7496
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 34.89
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 11.40s
                        Total time: 25918.45s
                               ETA: 1307348.7s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.099s, learning 0.178s)
               Value function loss: 0.8965
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 34.00
               Mean episode length: 124.39
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 11.28s
                        Total time: 25929.73s
                               ETA: 1307231.7s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.853s, learning 0.166s)
               Value function loss: 1.0058
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 35.06
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 11.02s
                        Total time: 25940.75s
                               ETA: 1307101.9s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.014s, learning 0.233s)
               Value function loss: 0.9567
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 34.78
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 11.25s
                        Total time: 25952.00s
                               ETA: 1306983.6s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.119s, learning 0.181s)
               Value function loss: 0.7375
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 35.28
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 11.30s
                        Total time: 25963.30s
                               ETA: 1306868.1s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.018s, learning 0.171s)
               Value function loss: 0.8458
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 34.24
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 11.19s
                        Total time: 25974.49s
                               ETA: 1306747.2s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.195s, learning 0.169s)
               Value function loss: 0.9877
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 34.49
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 11.36s
                        Total time: 25985.85s
                               ETA: 1306635.2s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.951s, learning 0.188s)
               Value function loss: 0.7452
                    Surrogate loss: -0.0227
             Mean action noise std: 0.72
                       Mean reward: 33.76
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 11.14s
                        Total time: 25996.99s
                               ETA: 1306512.0s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.169s)
               Value function loss: 0.7392
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 34.78
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 11.35s
                        Total time: 26008.34s
                               ETA: 1306399.2s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.755s, learning 0.171s)
               Value function loss: 0.8306
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 34.14
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 10.93s
                        Total time: 26019.26s
                               ETA: 1306265.5s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.132s, learning 0.211s)
               Value function loss: 0.6938
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 35.07
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 11.34s
                        Total time: 26030.60s
                               ETA: 1306152.8s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.115s, learning 0.192s)
               Value function loss: 0.8893
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 34.92
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 11.31s
                        Total time: 26041.91s
                               ETA: 1306038.4s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.055s, learning 0.211s)
               Value function loss: 0.6451
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 35.09
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 11.27s
                        Total time: 26053.18s
                               ETA: 1305922.1s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.234s, learning 0.171s)
               Value function loss: 0.6206
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 34.84
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 11.41s
                        Total time: 26064.58s
                               ETA: 1305812.9s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.227s, learning 0.177s)
               Value function loss: 0.6929
                    Surrogate loss: -0.0195
             Mean action noise std: 0.72
                       Mean reward: 34.38
               Mean episode length: 124.27
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 11.40s
                        Total time: 26075.98s
                               ETA: 1305703.7s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.891s, learning 0.164s)
               Value function loss: 0.5952
                    Surrogate loss: -0.0218
             Mean action noise std: 0.72
                       Mean reward: 35.40
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 11.06s
                        Total time: 26087.04s
                               ETA: 1305577.1s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.878s, learning 0.190s)
               Value function loss: 0.6431
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 11.07s
                        Total time: 26098.11s
                               ETA: 1305451.3s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.350s, learning 0.280s)
               Value function loss: 0.8007
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 35.06
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 11.63s
                        Total time: 26109.74s
                               ETA: 1305353.8s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.974s, learning 0.168s)
               Value function loss: 0.7969
                    Surrogate loss: -0.0219
             Mean action noise std: 0.72
                       Mean reward: 34.49
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 11.14s
                        Total time: 26120.88s
                               ETA: 1305231.9s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.337s, learning 0.290s)
               Value function loss: 0.8539
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 35.21
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 11.63s
                        Total time: 26132.51s
                               ETA: 1305134.3s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.853s, learning 0.191s)
               Value function loss: 0.7956
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 35.32
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 11.04s
                        Total time: 26143.55s
                               ETA: 1305007.7s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.878s, learning 0.182s)
               Value function loss: 0.9918
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 34.91
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 11.06s
                        Total time: 26154.61s
                               ETA: 1304882.1s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.355s, learning 0.190s)
               Value function loss: 0.9521
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 34.76
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 11.54s
                        Total time: 26166.16s
                               ETA: 1304780.8s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.649s, learning 0.175s)
               Value function loss: 0.8031
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 34.58
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 11.82s
                        Total time: 26177.98s
                               ETA: 1304693.4s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.199s, learning 0.169s)
               Value function loss: 0.9770
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 34.53
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 11.37s
                        Total time: 26189.35s
                               ETA: 1304583.4s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.107s, learning 0.164s)
               Value function loss: 0.6935
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 34.40
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 11.27s
                        Total time: 26200.62s
                               ETA: 1304468.8s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.167s)
               Value function loss: 0.8803
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 34.93
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 11.38s
                        Total time: 26211.99s
                               ETA: 1304359.4s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.909s, learning 0.217s)
               Value function loss: 0.8478
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 33.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 11.13s
                        Total time: 26223.12s
                               ETA: 1304237.7s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.536s, learning 0.173s)
               Value function loss: 0.7261
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 33.55
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 11.71s
                        Total time: 26234.83s
                               ETA: 1304145.1s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.052s, learning 0.163s)
               Value function loss: 0.8759
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 33.92
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 11.22s
                        Total time: 26246.04s
                               ETA: 1304028.0s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.169s)
               Value function loss: 0.7232
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 33.49
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 11.38s
                        Total time: 26257.43s
                               ETA: 1303919.5s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.309s, learning 0.164s)
               Value function loss: 0.6653
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 32.54
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 11.47s
                        Total time: 26268.90s
                               ETA: 1303815.4s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.995s, learning 0.207s)
               Value function loss: 0.6284
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 33.43
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 11.20s
                        Total time: 26280.10s
                               ETA: 1303698.0s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.161s)
               Value function loss: 0.8923
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 32.63
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 11.24s
                        Total time: 26291.34s
                               ETA: 1303582.3s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.172s, learning 0.194s)
               Value function loss: 0.9333
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 32.03
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 11.37s
                        Total time: 26302.71s
                               ETA: 1303473.3s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.399s, learning 0.160s)
               Value function loss: 0.7780
                    Surrogate loss: -0.0232
             Mean action noise std: 0.72
                       Mean reward: 31.83
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 11.56s
                        Total time: 26314.26s
                               ETA: 1303373.8s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.253s, learning 0.170s)
               Value function loss: 0.7851
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 32.08
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 11.42s
                        Total time: 26325.69s
                               ETA: 1303267.8s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.828s, learning 0.168s)
               Value function loss: 0.8542
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 31.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 11.00s
                        Total time: 26336.68s
                               ETA: 1303140.7s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.966s, learning 0.167s)
               Value function loss: 0.7352
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 31.94
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 11.13s
                        Total time: 26347.82s
                               ETA: 1303020.5s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.390s, learning 0.184s)
               Value function loss: 0.6704
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 31.63
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 11.57s
                        Total time: 26359.39s
                               ETA: 1302922.2s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.759s, learning 0.181s)
               Value function loss: 0.8107
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 32.14
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 10.94s
                        Total time: 26370.33s
                               ETA: 1302792.7s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.214s, learning 0.192s)
               Value function loss: 0.6639
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 31.86
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 11.41s
                        Total time: 26381.74s
                               ETA: 1302686.3s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.659s, learning 0.193s)
               Value function loss: 0.8178
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 31.46
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 10.85s
                        Total time: 26392.59s
                               ETA: 1302552.6s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.770s, learning 0.176s)
               Value function loss: 0.6838
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 31.32
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 10.95s
                        Total time: 26403.53s
                               ETA: 1302423.8s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.838s, learning 0.181s)
               Value function loss: 0.6402
                    Surrogate loss: -0.0258
             Mean action noise std: 0.72
                       Mean reward: 30.65
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 11.02s
                        Total time: 26414.55s
                               ETA: 1302298.6s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1459 steps/s (collection: 10.963s, learning 0.263s)
               Value function loss: 0.8033
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 31.43
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 11.23s
                        Total time: 26425.78s
                               ETA: 1302183.7s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.326s, learning 0.192s)
               Value function loss: 0.7477
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 30.77
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 11.52s
                        Total time: 26437.30s
                               ETA: 1302083.4s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.918s, learning 0.241s)
               Value function loss: 0.7103
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 30.62
               Mean episode length: 124.36
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 11.16s
                        Total time: 26448.46s
                               ETA: 1301965.5s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.158s, learning 0.176s)
               Value function loss: 0.7194
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 31.20
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 11.33s
                        Total time: 26459.79s
                               ETA: 1301856.3s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.933s, learning 0.161s)
               Value function loss: 0.8622
                    Surrogate loss: -0.0219
             Mean action noise std: 0.72
                       Mean reward: 31.89
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 11.09s
                        Total time: 26470.89s
                               ETA: 1301735.3s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.983s, learning 0.172s)
               Value function loss: 0.7635
                    Surrogate loss: -0.0231
             Mean action noise std: 0.72
                       Mean reward: 31.21
               Mean episode length: 124.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 11.15s
                        Total time: 26482.04s
                               ETA: 1301617.5s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.888s, learning 0.164s)
               Value function loss: 0.7322
                    Surrogate loss: -0.0262
             Mean action noise std: 0.72
                       Mean reward: 31.54
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 11.05s
                        Total time: 26493.09s
                               ETA: 1301494.7s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.211s, learning 0.267s)
               Value function loss: 0.8974
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 31.12
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 11.48s
                        Total time: 26504.57s
                               ETA: 1301392.9s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.133s, learning 0.217s)
               Value function loss: 0.9121
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 31.10
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 11.35s
                        Total time: 26515.92s
                               ETA: 1301285.0s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.117s, learning 0.197s)
               Value function loss: 0.6959
                    Surrogate loss: -0.0275
             Mean action noise std: 0.72
                       Mean reward: 31.08
               Mean episode length: 124.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 11.31s
                        Total time: 26527.23s
                               ETA: 1301175.4s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.432s, learning 0.274s)
               Value function loss: 0.5771
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 31.74
               Mean episode length: 124.83
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 11.71s
                        Total time: 26538.94s
                               ETA: 1301085.1s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.137s, learning 0.189s)
               Value function loss: 0.6641
                    Surrogate loss: -0.0250
             Mean action noise std: 0.72
                       Mean reward: 31.62
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 11.33s
                        Total time: 26550.27s
                               ETA: 1300976.3s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.961s, learning 0.226s)
               Value function loss: 0.6124
                    Surrogate loss: -0.0246
             Mean action noise std: 0.72
                       Mean reward: 31.17
               Mean episode length: 124.58
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 11.19s
                        Total time: 26561.45s
                               ETA: 1300860.8s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.308s, learning 0.163s)
               Value function loss: 0.6868
                    Surrogate loss: -0.0248
             Mean action noise std: 0.72
                       Mean reward: 30.21
               Mean episode length: 123.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 11.47s
                        Total time: 26572.92s
                               ETA: 1300759.2s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.650s, learning 0.162s)
               Value function loss: 0.5748
                    Surrogate loss: -0.0243
             Mean action noise std: 0.72
                       Mean reward: 31.17
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 10.81s
                        Total time: 26583.74s
                               ETA: 1300625.5s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.911s, learning 0.175s)
               Value function loss: 0.5986
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 31.12
               Mean episode length: 124.72
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 11.09s
                        Total time: 26594.82s
                               ETA: 1300505.4s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.207s, learning 0.166s)
               Value function loss: 0.5840
                    Surrogate loss: -0.0269
             Mean action noise std: 0.72
                       Mean reward: 31.53
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 11.37s
                        Total time: 26606.20s
                               ETA: 1300399.4s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.684s, learning 0.190s)
               Value function loss: 0.5285
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 31.31
               Mean episode length: 124.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 10.87s
                        Total time: 26617.07s
                               ETA: 1300269.1s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.100s, learning 0.219s)
               Value function loss: 0.4946
                    Surrogate loss: -0.0219
             Mean action noise std: 0.72
                       Mean reward: 30.93
               Mean episode length: 124.21
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 11.32s
                        Total time: 26628.39s
                               ETA: 1300160.6s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.255s, learning 0.189s)
               Value function loss: 0.6791
                    Surrogate loss: -0.0256
             Mean action noise std: 0.72
                       Mean reward: 31.42
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 11.44s
                        Total time: 26639.83s
                               ETA: 1300058.3s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.171s)
               Value function loss: 0.6989
                    Surrogate loss: -0.0230
             Mean action noise std: 0.72
                       Mean reward: 30.47
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 10.78s
                        Total time: 26650.61s
                               ETA: 1299923.6s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.927s, learning 0.188s)
               Value function loss: 0.6064
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 31.55
               Mean episode length: 124.52
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 11.11s
                        Total time: 26661.72s
                               ETA: 1299805.4s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.155s, learning 0.192s)
               Value function loss: 0.7125
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 30.40
               Mean episode length: 124.66
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 11.35s
                        Total time: 26673.07s
                               ETA: 1299698.7s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.908s, learning 0.266s)
               Value function loss: 0.6929
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 31.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 11.17s
                        Total time: 26684.25s
                               ETA: 1299583.8s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.205s, learning 0.198s)
               Value function loss: 0.7496
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 30.84
               Mean episode length: 124.95
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 11.40s
                        Total time: 26695.65s
                               ETA: 1299480.0s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.019s, learning 0.194s)
               Value function loss: 0.6291
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 30.36
               Mean episode length: 124.97
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 11.21s
                        Total time: 26706.86s
                               ETA: 1299367.0s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.103s, learning 0.187s)
               Value function loss: 0.7092
                    Surrogate loss: -0.0235
             Mean action noise std: 0.72
                       Mean reward: 30.49
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 11.29s
                        Total time: 26718.15s
                               ETA: 1299258.0s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.758s, learning 0.185s)
               Value function loss: 0.6163
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 30.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 10.94s
                        Total time: 26729.09s
                               ETA: 1299132.1s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.100s, learning 0.219s)
               Value function loss: 0.6959
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 29.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 11.32s
                        Total time: 26740.41s
                               ETA: 1299024.6s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.168s)
               Value function loss: 0.6844
                    Surrogate loss: -0.0245
             Mean action noise std: 0.72
                       Mean reward: 29.55
               Mean episode length: 124.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 11.51s
                        Total time: 26751.92s
                               ETA: 1298926.4s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.970s, learning 0.190s)
               Value function loss: 0.7350
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 30.08
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 11.16s
                        Total time: 26763.08s
                               ETA: 1298811.4s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.802s, learning 0.184s)
               Value function loss: 0.9063
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 30.21
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 10.99s
                        Total time: 26774.07s
                               ETA: 1298688.1s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.040s, learning 0.189s)
               Value function loss: 0.8210
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 29.27
               Mean episode length: 124.96
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 11.23s
                        Total time: 26785.30s
                               ETA: 1298576.6s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.893s, learning 0.316s)
               Value function loss: 0.5661
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 29.66
               Mean episode length: 124.68
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 11.21s
                        Total time: 26796.51s
                               ETA: 1298464.3s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.113s, learning 0.207s)
               Value function loss: 0.5603
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 28.91
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 11.32s
                        Total time: 26807.83s
                               ETA: 1298357.4s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.708s, learning 0.168s)
               Value function loss: 0.7465
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 29.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 10.88s
                        Total time: 26818.70s
                               ETA: 1298229.2s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.397s, learning 0.170s)
               Value function loss: 0.7897
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 29.35
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 11.57s
                        Total time: 26830.27s
                               ETA: 1298134.5s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.093s, learning 0.160s)
               Value function loss: 0.6989
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 28.58
               Mean episode length: 124.88
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 11.25s
                        Total time: 26841.52s
                               ETA: 1298024.6s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.167s)
               Value function loss: 0.6237
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 28.53
               Mean episode length: 124.38
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 11.49s
                        Total time: 26853.01s
                               ETA: 1297926.4s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.192s, learning 0.211s)
               Value function loss: 0.7106
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 28.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 11.40s
                        Total time: 26864.41s
                               ETA: 1297824.0s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.034s, learning 0.169s)
               Value function loss: 0.6625
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 29.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 11.20s
                        Total time: 26875.61s
                               ETA: 1297712.0s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.986s, learning 0.181s)
               Value function loss: 0.5367
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 29.65
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 11.17s
                        Total time: 26886.78s
                               ETA: 1297598.5s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.569s, learning 0.194s)
               Value function loss: 0.5994
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 27.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 10.76s
                        Total time: 26897.54s
                               ETA: 1297465.5s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.887s, learning 0.218s)
               Value function loss: 0.4881
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 27.99
               Mean episode length: 124.78
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 11.10s
                        Total time: 26908.65s
                               ETA: 1297349.1s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.007s, learning 0.245s)
               Value function loss: 0.5960
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 28.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 11.25s
                        Total time: 26919.90s
                               ETA: 1297240.0s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.284s, learning 0.190s)
               Value function loss: 0.5182
                    Surrogate loss: -0.0218
             Mean action noise std: 0.72
                       Mean reward: 27.26
               Mean episode length: 124.42
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 11.47s
                        Total time: 26931.38s
                               ETA: 1297141.7s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.801s, learning 0.183s)
               Value function loss: 0.5741
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 27.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 10.98s
                        Total time: 26942.36s
                               ETA: 1297019.8s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.683s, learning 0.165s)
               Value function loss: 0.5096
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 27.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 10.85s
                        Total time: 26953.21s
                               ETA: 1296891.5s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.634s, learning 0.171s)
               Value function loss: 0.4457
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 26.83
               Mean episode length: 124.46
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 10.80s
                        Total time: 26964.01s
                               ETA: 1296761.2s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.679s, learning 0.176s)
               Value function loss: 0.4570
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 26.29
               Mean episode length: 124.86
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 10.86s
                        Total time: 26974.87s
                               ETA: 1296633.4s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.716s, learning 0.208s)
               Value function loss: 0.4707
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 26.82
               Mean episode length: 124.93
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 10.92s
                        Total time: 26985.79s
                               ETA: 1296509.1s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.511s, learning 0.164s)
               Value function loss: 0.5752
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 27.35
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 10.68s
                        Total time: 26996.47s
                               ETA: 1296373.0s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.413s, learning 0.193s)
               Value function loss: 0.5895
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: 27.51
               Mean episode length: 124.09
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 10.61s
                        Total time: 27007.07s
                               ETA: 1296233.6s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.658s, learning 0.189s)
               Value function loss: 0.5269
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 27.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 10.85s
                        Total time: 27017.92s
                               ETA: 1296106.0s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.258s, learning 0.174s)
               Value function loss: 0.6355
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 27.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 11.43s
                        Total time: 27029.35s
                               ETA: 1296006.5s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.599s, learning 0.179s)
               Value function loss: 0.7172
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 27.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 10.78s
                        Total time: 27040.13s
                               ETA: 1295875.7s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.588s, learning 0.212s)
               Value function loss: 0.4752
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 27.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 10.80s
                        Total time: 27050.93s
                               ETA: 1295746.1s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.957s, learning 0.185s)
               Value function loss: 0.4683
                    Surrogate loss: -0.0219
             Mean action noise std: 0.72
                       Mean reward: 27.54
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 11.14s
                        Total time: 27062.07s
                               ETA: 1295633.1s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.064s, learning 0.182s)
               Value function loss: 0.4744
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 28.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 11.25s
                        Total time: 27073.32s
                               ETA: 1295525.0s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.692s, learning 0.166s)
               Value function loss: 0.4683
                    Surrogate loss: -0.0221
             Mean action noise std: 0.72
                       Mean reward: 27.67
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 10.86s
                        Total time: 27084.18s
                               ETA: 1295398.6s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.654s, learning 0.163s)
               Value function loss: 0.5786
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 27.99
               Mean episode length: 124.81
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 10.82s
                        Total time: 27094.99s
                               ETA: 1295270.2s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.667s, learning 0.175s)
               Value function loss: 0.4612
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 27.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 10.84s
                        Total time: 27105.83s
                               ETA: 1295143.2s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.814s, learning 0.171s)
               Value function loss: 0.5140
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 28.25
               Mean episode length: 124.75
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 10.99s
                        Total time: 27116.82s
                               ETA: 1295023.2s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.616s, learning 0.179s)
               Value function loss: 0.5568
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 27.56
               Mean episode length: 124.56
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 10.79s
                        Total time: 27127.61s
                               ETA: 1294894.1s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.335s, learning 0.161s)
               Value function loss: 0.5190
                    Surrogate loss: -0.0241
             Mean action noise std: 0.72
                       Mean reward: 28.05
               Mean episode length: 124.72
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 11.50s
                        Total time: 27139.11s
                               ETA: 1294798.7s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.898s, learning 0.159s)
               Value function loss: 0.4504
                    Surrogate loss: -0.0208
             Mean action noise std: 0.72
                       Mean reward: 28.05
               Mean episode length: 123.98
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 11.06s
                        Total time: 27150.17s
                               ETA: 1294682.4s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.850s, learning 0.160s)
               Value function loss: 0.6438
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 27.72
               Mean episode length: 124.57
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 11.01s
                        Total time: 27161.18s
                               ETA: 1294563.9s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.173s)
               Value function loss: 0.5476
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 28.27
               Mean episode length: 124.48
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 11.09s
                        Total time: 27172.26s
                               ETA: 1294449.1s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.834s, learning 0.188s)
               Value function loss: 0.5577
                    Surrogate loss: -0.0247
             Mean action noise std: 0.72
                       Mean reward: 27.95
               Mean episode length: 124.58
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 11.02s
                        Total time: 27183.29s
                               ETA: 1294331.4s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.880s, learning 0.196s)
               Value function loss: 0.6026
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 28.57
               Mean episode length: 124.49
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 11.08s
                        Total time: 27194.36s
                               ETA: 1294216.4s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.760s, learning 0.156s)
               Value function loss: 0.6793
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 28.67
               Mean episode length: 123.79
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 10.92s
                        Total time: 27205.28s
                               ETA: 1294093.9s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.493s, learning 0.174s)
               Value function loss: 0.6391
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 29.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 10.67s
                        Total time: 27215.94s
                               ETA: 1293959.6s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.132s, learning 0.188s)
               Value function loss: 0.5632
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 28.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 11.32s
                        Total time: 27227.26s
                               ETA: 1293856.5s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.436s, learning 0.185s)
               Value function loss: 0.6755
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 29.03
               Mean episode length: 123.95
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 11.62s
                        Total time: 27238.89s
                               ETA: 1293767.8s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.851s, learning 0.177s)
               Value function loss: 0.5466
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 29.06
               Mean episode length: 124.95
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 11.03s
                        Total time: 27249.91s
                               ETA: 1293651.0s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.483s, learning 0.258s)
               Value function loss: 0.5723
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 28.62
               Mean episode length: 123.95
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 10.74s
                        Total time: 27260.65s
                               ETA: 1293520.7s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.713s, learning 0.169s)
               Value function loss: 0.4980
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 29.08
               Mean episode length: 124.84
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 10.88s
                        Total time: 27271.54s
                               ETA: 1293397.2s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.731s, learning 0.170s)
               Value function loss: 0.5051
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 28.41
               Mean episode length: 124.36
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 10.90s
                        Total time: 27282.44s
                               ETA: 1293274.7s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.121s, learning 0.270s)
               Value function loss: 0.5288
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 28.51
               Mean episode length: 124.80
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 11.39s
                        Total time: 27293.83s
                               ETA: 1293175.5s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.209s, learning 0.180s)
               Value function loss: 0.5692
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 28.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 11.39s
                        Total time: 27305.22s
                               ETA: 1293076.3s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.912s, learning 0.218s)
               Value function loss: 0.4747
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 28.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 11.13s
                        Total time: 27316.35s
                               ETA: 1292965.0s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.102s, learning 0.174s)
               Value function loss: 0.4973
                    Surrogate loss: -0.0242
             Mean action noise std: 0.72
                       Mean reward: 28.09
               Mean episode length: 124.58
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 11.28s
                        Total time: 27327.62s
                               ETA: 1292860.6s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.817s, learning 0.189s)
               Value function loss: 0.5537
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 27.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 11.01s
                        Total time: 27338.63s
                               ETA: 1292743.6s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.896s, learning 0.176s)
               Value function loss: 0.5543
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 28.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 11.07s
                        Total time: 27349.70s
                               ETA: 1292629.8s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.729s, learning 0.223s)
               Value function loss: 0.5275
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 27.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 10.95s
                        Total time: 27360.65s
                               ETA: 1292510.4s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.861s, learning 0.205s)
               Value function loss: 0.5364
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 27.43
               Mean episode length: 124.53
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 11.07s
                        Total time: 27371.72s
                               ETA: 1292396.6s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.789s, learning 0.164s)
               Value function loss: 0.5823
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 27.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 10.95s
                        Total time: 27382.67s
                               ETA: 1292277.4s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.828s, learning 0.165s)
               Value function loss: 0.4936
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 27.39
               Mean episode length: 124.98
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 10.99s
                        Total time: 27393.67s
                               ETA: 1292160.3s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.755s, learning 0.164s)
               Value function loss: 0.4444
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 27.27
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 10.92s
                        Total time: 27404.58s
                               ETA: 1292039.7s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.757s, learning 0.167s)
               Value function loss: 0.5181
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 27.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 10.92s
                        Total time: 27415.51s
                               ETA: 1291919.5s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.164s)
               Value function loss: 0.5042
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 27.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 11.20s
                        Total time: 27426.71s
                               ETA: 1291812.4s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.770s, learning 0.175s)
               Value function loss: 0.5242
                    Surrogate loss: -0.0231
             Mean action noise std: 0.72
                       Mean reward: 27.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 10.94s
                        Total time: 27437.65s
                               ETA: 1291693.4s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.548s, learning 0.157s)
               Value function loss: 0.4652
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 27.63
               Mean episode length: 124.84
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 10.70s
                        Total time: 27448.36s
                               ETA: 1291563.2s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.248s, learning 0.198s)
               Value function loss: 0.4993
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 26.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 11.45s
                        Total time: 27459.80s
                               ETA: 1291467.9s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.945s, learning 0.207s)
               Value function loss: 0.4990
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 27.53
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 11.15s
                        Total time: 27470.95s
                               ETA: 1291359.0s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.126s, learning 0.160s)
               Value function loss: 0.4172
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 27.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 11.29s
                        Total time: 27482.24s
                               ETA: 1291256.5s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.888s, learning 0.180s)
               Value function loss: 0.4939
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 26.22
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 11.07s
                        Total time: 27493.31s
                               ETA: 1291143.7s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.139s, learning 0.167s)
               Value function loss: 0.4378
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 26.73
               Mean episode length: 124.11
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 11.31s
                        Total time: 27504.61s
                               ETA: 1291042.3s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.168s, learning 0.163s)
               Value function loss: 0.5051
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 26.00
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 11.33s
                        Total time: 27515.94s
                               ETA: 1290942.1s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.103s, learning 0.195s)
               Value function loss: 0.4527
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 26.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 11.30s
                        Total time: 27527.24s
                               ETA: 1290840.5s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.766s, learning 0.181s)
               Value function loss: 0.5346
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 26.97
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 10.95s
                        Total time: 27538.19s
                               ETA: 1290722.5s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.991s, learning 0.171s)
               Value function loss: 0.5958
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 27.14
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 11.16s
                        Total time: 27549.35s
                               ETA: 1290614.7s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.087s, learning 0.244s)
               Value function loss: 0.4812
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 26.47
               Mean episode length: 124.61
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 11.33s
                        Total time: 27560.68s
                               ETA: 1290514.8s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.705s, learning 0.161s)
               Value function loss: 0.3763
                    Surrogate loss: -0.0246
             Mean action noise std: 0.72
                       Mean reward: 27.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 10.87s
                        Total time: 27571.55s
                               ETA: 1290393.3s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.206s, learning 0.185s)
               Value function loss: 0.3917
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 27.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 11.39s
                        Total time: 27582.94s
                               ETA: 1290296.5s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.740s, learning 0.165s)
               Value function loss: 0.3720
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 27.27
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 10.91s
                        Total time: 27593.85s
                               ETA: 1290177.0s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.159s)
               Value function loss: 0.4500
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 26.83
               Mean episode length: 124.16
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 11.55s
                        Total time: 27605.39s
                               ETA: 1290087.7s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.696s, learning 0.193s)
               Value function loss: 0.4215
                    Surrogate loss: -0.0261
             Mean action noise std: 0.72
                       Mean reward: 27.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 10.89s
                        Total time: 27616.28s
                               ETA: 1289967.6s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.159s)
               Value function loss: 0.3383
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 26.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 11.11s
                        Total time: 27627.39s
                               ETA: 1289857.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.167s)
               Value function loss: 0.4322
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 26.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 11.53s
                        Total time: 27638.92s
                               ETA: 1289767.8s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.963s, learning 0.170s)
               Value function loss: 0.4407
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 27.04
               Mean episode length: 124.25
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 11.13s
                        Total time: 27650.05s
                               ETA: 1289659.5s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.561s, learning 0.163s)
               Value function loss: 0.4630
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 27.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 10.72s
                        Total time: 27660.77s
                               ETA: 1289532.1s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.773s, learning 0.182s)
               Value function loss: 0.4398
                    Surrogate loss: -0.0244
             Mean action noise std: 0.72
                       Mean reward: 27.10
               Mean episode length: 124.81
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 10.95s
                        Total time: 27671.73s
                               ETA: 1289415.7s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.029s, learning 0.227s)
               Value function loss: 0.5026
                    Surrogate loss: -0.0228
             Mean action noise std: 0.72
                       Mean reward: 27.02
               Mean episode length: 124.28
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 11.26s
                        Total time: 27682.98s
                               ETA: 1289313.3s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.559s, learning 0.192s)
               Value function loss: 0.5231
                    Surrogate loss: -0.0230
             Mean action noise std: 0.72
                       Mean reward: 27.97
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 10.75s
                        Total time: 27693.74s
                               ETA: 1289187.5s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.741s, learning 0.220s)
               Value function loss: 0.5586
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 27.23
               Mean episode length: 124.36
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 10.96s
                        Total time: 27704.70s
                               ETA: 1289071.6s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.888s, learning 0.171s)
               Value function loss: 0.5793
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 28.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 11.06s
                        Total time: 27715.76s
                               ETA: 1288960.4s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.619s, learning 0.259s)
               Value function loss: 0.7190
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 28.27
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 10.88s
                        Total time: 27726.63s
                               ETA: 1288840.8s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.163s)
               Value function loss: 0.5462
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 27.06
               Mean episode length: 123.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 11.05s
                        Total time: 27737.68s
                               ETA: 1288729.2s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.165s)
               Value function loss: 0.5667
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 27.69
               Mean episode length: 124.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 10.99s
                        Total time: 27748.67s
                               ETA: 1288615.2s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.735s, learning 0.172s)
               Value function loss: 0.6948
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 28.30
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 10.91s
                        Total time: 27759.58s
                               ETA: 1288497.3s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.936s, learning 0.201s)
               Value function loss: 0.5740
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 27.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 11.14s
                        Total time: 27770.72s
                               ETA: 1288390.2s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.440s, learning 0.189s)
               Value function loss: 0.6208
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 27.87
               Mean episode length: 124.37
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 10.63s
                        Total time: 27781.35s
                               ETA: 1288259.6s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.826s, learning 0.164s)
               Value function loss: 0.5938
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 27.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 10.99s
                        Total time: 27792.34s
                               ETA: 1288145.8s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.595s, learning 0.162s)
               Value function loss: 0.4893
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 28.00
               Mean episode length: 123.21
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 10.76s
                        Total time: 27803.09s
                               ETA: 1288021.4s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.699s, learning 0.158s)
               Value function loss: 0.5366
                    Surrogate loss: -0.0253
             Mean action noise std: 0.72
                       Mean reward: 27.99
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 10.86s
                        Total time: 27813.95s
                               ETA: 1287901.7s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.197s, learning 0.171s)
               Value function loss: 0.5579
                    Surrogate loss: -0.0256
             Mean action noise std: 0.72
                       Mean reward: 28.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 11.37s
                        Total time: 27825.32s
                               ETA: 1287805.7s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.987s, learning 0.159s)
               Value function loss: 0.4991
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 28.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 11.15s
                        Total time: 27836.46s
                               ETA: 1287699.5s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.857s, learning 0.204s)
               Value function loss: 0.5281
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 28.83
               Mean episode length: 124.30
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 11.06s
                        Total time: 27847.52s
                               ETA: 1287589.5s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.943s, learning 0.175s)
               Value function loss: 0.5617
                    Surrogate loss: -0.0248
             Mean action noise std: 0.72
                       Mean reward: 28.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 11.12s
                        Total time: 27858.64s
                               ETA: 1287482.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1566 steps/s (collection: 10.286s, learning 0.174s)
               Value function loss: 0.5445
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 27.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 10.46s
                        Total time: 27869.10s
                               ETA: 1287344.7s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.780s, learning 0.170s)
               Value function loss: 0.5165
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 28.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 10.95s
                        Total time: 27880.05s
                               ETA: 1287229.9s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.999s, learning 0.164s)
               Value function loss: 0.6304
                    Surrogate loss: -0.0243
             Mean action noise std: 0.72
                       Mean reward: 28.33
               Mean episode length: 124.26
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 11.16s
                        Total time: 27891.22s
                               ETA: 1287125.0s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.795s, learning 0.167s)
               Value function loss: 0.6696
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 28.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 10.96s
                        Total time: 27902.18s
                               ETA: 1287011.0s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.914s, learning 0.185s)
               Value function loss: 0.6042
                    Surrogate loss: -0.0235
             Mean action noise std: 0.72
                       Mean reward: 28.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 11.10s
                        Total time: 27913.28s
                               ETA: 1286903.3s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.162s)
               Value function loss: 0.5268
                    Surrogate loss: -0.0214
             Mean action noise std: 0.72
                       Mean reward: 28.58
               Mean episode length: 124.75
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 11.04s
                        Total time: 27924.32s
                               ETA: 1286793.2s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.260s, learning 0.169s)
               Value function loss: 0.5790
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 28.28
               Mean episode length: 124.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 11.43s
                        Total time: 27935.75s
                               ETA: 1286700.9s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.993s, learning 0.180s)
               Value function loss: 0.5438
                    Surrogate loss: -0.0232
             Mean action noise std: 0.72
                       Mean reward: 28.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 11.17s
                        Total time: 27946.92s
                               ETA: 1286596.9s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.642s, learning 0.167s)
               Value function loss: 0.6306
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 28.42
               Mean episode length: 124.10
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 10.81s
                        Total time: 27957.73s
                               ETA: 1286476.3s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.164s)
               Value function loss: 0.5619
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 29.06
               Mean episode length: 124.50
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 11.08s
                        Total time: 27968.81s
                               ETA: 1286368.0s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.970s, learning 0.176s)
               Value function loss: 0.5847
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 28.31
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 11.15s
                        Total time: 27979.95s
                               ETA: 1286263.0s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.266s, learning 0.169s)
               Value function loss: 0.6018
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 28.45
               Mean episode length: 124.86
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 11.43s
                        Total time: 27991.39s
                               ETA: 1286171.5s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.912s, learning 0.181s)
               Value function loss: 0.5885
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 28.67
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 11.09s
                        Total time: 28002.48s
                               ETA: 1286064.2s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.806s, learning 0.191s)
               Value function loss: 0.5929
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 29.71
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 11.00s
                        Total time: 28013.48s
                               ETA: 1285952.6s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.807s, learning 0.177s)
               Value function loss: 0.7451
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 28.84
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 10.98s
                        Total time: 28024.46s
                               ETA: 1285840.6s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.163s)
               Value function loss: 0.6745
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 29.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 11.10s
                        Total time: 28035.56s
                               ETA: 1285733.9s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1104 steps/s (collection: 14.675s, learning 0.162s)
               Value function loss: 0.7271
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 29.57
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 14.84s
                        Total time: 28050.40s
                               ETA: 1285798.6s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.245s, learning 0.169s)
               Value function loss: 0.7390
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 29.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 21.41s
                        Total time: 28071.81s
                               ETA: 1286164.7s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 781 steps/s (collection: 20.790s, learning 0.173s)
               Value function loss: 0.7970
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 29.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 20.96s
                        Total time: 28092.77s
                               ETA: 1286509.7s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 769 steps/s (collection: 21.119s, learning 0.173s)
               Value function loss: 0.8076
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 29.27
               Mean episode length: 124.89
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 21.29s
                        Total time: 28114.07s
                               ETA: 1286869.4s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 742 steps/s (collection: 21.873s, learning 0.197s)
               Value function loss: 0.6197
                    Surrogate loss: -0.0214
             Mean action noise std: 0.72
                       Mean reward: 28.15
               Mean episode length: 124.83
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 22.07s
                        Total time: 28136.14s
                               ETA: 1287264.4s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.085s, learning 0.312s)
               Value function loss: 0.6724
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 28.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 21.40s
                        Total time: 28157.53s
                               ETA: 1287628.1s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.477s, learning 0.192s)
               Value function loss: 0.5438
                    Surrogate loss: -0.0248
             Mean action noise std: 0.72
                       Mean reward: 29.00
               Mean episode length: 123.59
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 21.67s
                        Total time: 28179.20s
                               ETA: 1288004.0s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.231s, learning 0.192s)
               Value function loss: 0.6463
                    Surrogate loss: -0.0212
             Mean action noise std: 0.72
                       Mean reward: 28.83
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 21.42s
                        Total time: 28200.62s
                               ETA: 1288368.3s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 762 steps/s (collection: 21.230s, learning 0.257s)
               Value function loss: 0.6032
                    Surrogate loss: -0.0213
             Mean action noise std: 0.72
                       Mean reward: 28.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 21.49s
                        Total time: 28222.11s
                               ETA: 1288735.1s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.103s, learning 0.255s)
               Value function loss: 0.5401
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 28.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 21.36s
                        Total time: 28243.47s
                               ETA: 1289095.7s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 779 steps/s (collection: 20.816s, learning 0.191s)
               Value function loss: 0.6094
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 28.04
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 21.01s
                        Total time: 28264.48s
                               ETA: 1289439.9s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.257s, learning 0.179s)
               Value function loss: 0.5242
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 28.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 21.44s
                        Total time: 28285.91s
                               ETA: 1289803.3s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 766 steps/s (collection: 21.204s, learning 0.172s)
               Value function loss: 0.5301
                    Surrogate loss: -0.0234
             Mean action noise std: 0.72
                       Mean reward: 28.43
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 21.38s
                        Total time: 28307.29s
                               ETA: 1290163.7s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.514s, learning 0.181s)
               Value function loss: 0.4906
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 27.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 21.70s
                        Total time: 28328.98s
                               ETA: 1290538.2s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.700s, learning 0.206s)
               Value function loss: 0.6519
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 28.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 21.91s
                        Total time: 28350.89s
                               ETA: 1290921.9s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.612s, learning 0.171s)
               Value function loss: 0.5641
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 27.90
               Mean episode length: 124.89
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 21.78s
                        Total time: 28372.67s
                               ETA: 1291299.7s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 785 steps/s (collection: 20.601s, learning 0.254s)
               Value function loss: 0.5667
                    Surrogate loss: -0.0229
             Mean action noise std: 0.72
                       Mean reward: 29.11
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 20.86s
                        Total time: 28393.53s
                               ETA: 1291634.9s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.579s, learning 0.171s)
               Value function loss: 0.6137
                    Surrogate loss: -0.0247
             Mean action noise std: 0.72
                       Mean reward: 29.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 21.75s
                        Total time: 28415.28s
                               ETA: 1292010.4s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 773 steps/s (collection: 20.942s, learning 0.252s)
               Value function loss: 0.6494
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 28.59
               Mean episode length: 124.80
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 21.19s
                        Total time: 28436.47s
                               ETA: 1292360.3s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 777 steps/s (collection: 20.856s, learning 0.213s)
               Value function loss: 0.6155
                    Surrogate loss: -0.0221
             Mean action noise std: 0.72
                       Mean reward: 29.35
               Mean episode length: 124.40
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 21.07s
                        Total time: 28457.54s
                               ETA: 1292704.2s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.732s, learning 0.197s)
               Value function loss: 0.5091
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 28.39
               Mean episode length: 124.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 20.93s
                        Total time: 28478.47s
                               ETA: 1293041.4s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 754 steps/s (collection: 21.557s, learning 0.169s)
               Value function loss: 0.6191
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 28.90
               Mean episode length: 124.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 21.73s
                        Total time: 28500.19s
                               ETA: 1293414.4s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.640s, learning 0.174s)
               Value function loss: 0.5087
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 28.12
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 21.81s
                        Total time: 28522.01s
                               ETA: 1293791.1s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 770 steps/s (collection: 21.060s, learning 0.192s)
               Value function loss: 0.5959
                    Surrogate loss: -0.0242
             Mean action noise std: 0.72
                       Mean reward: 29.01
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 21.25s
                        Total time: 28543.26s
                               ETA: 1294141.9s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.730s, learning 0.185s)
               Value function loss: 0.4730
                    Surrogate loss: -0.0267
             Mean action noise std: 0.72
                       Mean reward: 28.54
               Mean episode length: 124.09
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 20.92s
                        Total time: 28564.18s
                               ETA: 1294477.1s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.440s, learning 0.167s)
               Value function loss: 0.5099
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 28.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 21.61s
                        Total time: 28585.78s
                               ETA: 1294843.4s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.336s, learning 0.195s)
               Value function loss: 0.4804
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 29.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 21.53s
                        Total time: 28607.31s
                               ETA: 1295205.8s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 761 steps/s (collection: 21.363s, learning 0.164s)
               Value function loss: 0.4430
                    Surrogate loss: -0.0263
             Mean action noise std: 0.72
                       Mean reward: 29.02
               Mean episode length: 124.94
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 21.53s
                        Total time: 28628.84s
                               ETA: 1295567.6s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.797s, learning 0.174s)
               Value function loss: 0.4641
                    Surrogate loss: -0.0250
             Mean action noise std: 0.72
                       Mean reward: 28.40
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 21.97s
                        Total time: 28650.81s
                               ETA: 1295949.2s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.446s, learning 0.174s)
               Value function loss: 0.5010
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 28.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 21.62s
                        Total time: 28672.43s
                               ETA: 1296314.6s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.226s, learning 0.196s)
               Value function loss: 0.5136
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 29.46
               Mean episode length: 124.83
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 21.42s
                        Total time: 28693.86s
                               ETA: 1296670.7s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 779 steps/s (collection: 20.860s, learning 0.168s)
               Value function loss: 0.4484
                    Surrogate loss: -0.0282
             Mean action noise std: 0.72
                       Mean reward: 29.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 21.03s
                        Total time: 28714.88s
                               ETA: 1297008.6s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 781 steps/s (collection: 20.774s, learning 0.194s)
               Value function loss: 0.4089
                    Surrogate loss: -0.0279
             Mean action noise std: 0.72
                       Mean reward: 28.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 20.97s
                        Total time: 28735.85s
                               ETA: 1297343.5s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.130s, learning 0.226s)
               Value function loss: 0.4372
                    Surrogate loss: -0.0257
             Mean action noise std: 0.72
                       Mean reward: 28.82
               Mean episode length: 124.45
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 21.36s
                        Total time: 28757.21s
                               ETA: 1297695.5s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 766 steps/s (collection: 21.129s, learning 0.246s)
               Value function loss: 0.5440
                    Surrogate loss: -0.0277
             Mean action noise std: 0.72
                       Mean reward: 28.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 21.37s
                        Total time: 28778.58s
                               ETA: 1298048.0s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 763 steps/s (collection: 21.279s, learning 0.180s)
               Value function loss: 0.4034
                    Surrogate loss: -0.0270
             Mean action noise std: 0.72
                       Mean reward: 28.86
               Mean episode length: 124.51
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 21.46s
                        Total time: 28800.04s
                               ETA: 1298404.1s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 737 steps/s (collection: 21.993s, learning 0.217s)
               Value function loss: 0.3674
                    Surrogate loss: -0.0271
             Mean action noise std: 0.72
                       Mean reward: 28.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 22.21s
                        Total time: 28822.25s
                               ETA: 1298793.5s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.944s, learning 0.168s)
               Value function loss: 0.3548
                    Surrogate loss: -0.0264
             Mean action noise std: 0.72
                       Mean reward: 28.88
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 21.11s
                        Total time: 28843.36s
                               ETA: 1299133.2s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1194 steps/s (collection: 13.525s, learning 0.193s)
               Value function loss: 0.3587
                    Surrogate loss: -0.0288
             Mean action noise std: 0.72
                       Mean reward: 27.31
               Mean episode length: 124.33
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 13.72s
                        Total time: 28857.08s
                               ETA: 1299139.6s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.169s)
               Value function loss: 0.4666
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 27.52
               Mean episode length: 124.11
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 11.51s
                        Total time: 28868.59s
                               ETA: 1299046.8s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.527s, learning 0.168s)
               Value function loss: 0.3857
                    Surrogate loss: -0.0261
             Mean action noise std: 0.72
                       Mean reward: 28.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 11.70s
                        Total time: 28880.29s
                               ETA: 1298962.3s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.362s, learning 0.191s)
               Value function loss: 0.4461
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 28.30
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 11.55s
                        Total time: 28891.84s
                               ETA: 1298871.5s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.050s, learning 0.171s)
               Value function loss: 0.4032
                    Surrogate loss: -0.0270
             Mean action noise std: 0.72
                       Mean reward: 28.38
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 11.22s
                        Total time: 28903.06s
                               ETA: 1298765.8s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.210s, learning 0.191s)
               Value function loss: 0.3522
                    Surrogate loss: -0.0272
             Mean action noise std: 0.72
                       Mean reward: 27.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 11.40s
                        Total time: 28914.46s
                               ETA: 1298668.3s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.808s, learning 0.276s)
               Value function loss: 0.3458
                    Surrogate loss: -0.0286
             Mean action noise std: 0.72
                       Mean reward: 27.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 11.08s
                        Total time: 28925.55s
                               ETA: 1298556.6s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.168s)
               Value function loss: 0.4578
                    Surrogate loss: -0.0288
             Mean action noise std: 0.72
                       Mean reward: 27.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 11.43s
                        Total time: 28936.98s
                               ETA: 1298460.6s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.214s, learning 0.211s)
               Value function loss: 0.4138
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 27.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 11.43s
                        Total time: 28948.40s
                               ETA: 1298364.4s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.214s, learning 0.173s)
               Value function loss: 0.4143
                    Surrogate loss: -0.0272
             Mean action noise std: 0.72
                       Mean reward: 27.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 11.39s
                        Total time: 28959.79s
                               ETA: 1298266.6s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.850s, learning 0.175s)
               Value function loss: 0.3990
                    Surrogate loss: -0.0276
             Mean action noise std: 0.72
                       Mean reward: 27.69
               Mean episode length: 124.61
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.02s
                        Total time: 28970.81s
                               ETA: 1298152.6s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.684s, learning 0.202s)
               Value function loss: 0.4421
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 26.84
               Mean episode length: 124.09
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 10.89s
                        Total time: 28981.70s
                               ETA: 1298032.5s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.931s, learning 0.169s)
               Value function loss: 0.3936
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 26.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 11.10s
                        Total time: 28992.80s
                               ETA: 1297922.1s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.078s, learning 0.174s)
               Value function loss: 0.3455
                    Surrogate loss: -0.0254
             Mean action noise std: 0.72
                       Mean reward: 27.04
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 11.25s
                        Total time: 29004.05s
                               ETA: 1297818.5s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.160s)
               Value function loss: 0.4070
                    Surrogate loss: -0.0291
             Mean action noise std: 0.72
                       Mean reward: 27.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 11.06s
                        Total time: 29015.11s
                               ETA: 1297706.4s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.826s, learning 0.166s)
               Value function loss: 0.3597
                    Surrogate loss: -0.0272
             Mean action noise std: 0.72
                       Mean reward: 26.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 10.99s
                        Total time: 29026.10s
                               ETA: 1297591.4s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.816s, learning 0.304s)
               Value function loss: 0.3677
                    Surrogate loss: -0.0215
             Mean action noise std: 0.72
                       Mean reward: 26.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 11.12s
                        Total time: 29037.22s
                               ETA: 1297482.2s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.872s, learning 0.160s)
               Value function loss: 0.3574
                    Surrogate loss: -0.0266
             Mean action noise std: 0.72
                       Mean reward: 26.74
               Mean episode length: 124.73
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 12.03s
                        Total time: 29049.25s
                               ETA: 1297413.9s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.912s, learning 0.175s)
               Value function loss: 0.3368
                    Surrogate loss: -0.0231
             Mean action noise std: 0.72
                       Mean reward: 27.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 11.09s
                        Total time: 29060.34s
                               ETA: 1297303.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.036s, learning 0.164s)
               Value function loss: 0.3463
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 27.13
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 11.20s
                        Total time: 29071.54s
                               ETA: 1297198.1s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.758s, learning 0.165s)
               Value function loss: 0.3690
                    Surrogate loss: -0.0247
             Mean action noise std: 0.72
                       Mean reward: 26.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 10.92s
                        Total time: 29082.46s
                               ETA: 1297080.4s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.796s, learning 0.163s)
               Value function loss: 0.3271
                    Surrogate loss: -0.0247
             Mean action noise std: 0.72
                       Mean reward: 27.11
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 10.96s
                        Total time: 29093.42s
                               ETA: 1296964.5s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.286s, learning 0.165s)
               Value function loss: 0.3315
                    Surrogate loss: -0.0273
             Mean action noise std: 0.72
                       Mean reward: 27.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 11.45s
                        Total time: 29104.87s
                               ETA: 1296870.6s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.161s)
               Value function loss: 0.4243
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 26.53
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 11.45s
                        Total time: 29116.32s
                               ETA: 1296776.9s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.014s, learning 0.197s)
               Value function loss: 0.4505
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 26.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 11.21s
                        Total time: 29127.53s
                               ETA: 1296672.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.980s, learning 0.159s)
               Value function loss: 0.3755
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 26.85
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 11.14s
                        Total time: 29138.67s
                               ETA: 1296564.9s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.565s, learning 0.162s)
               Value function loss: 0.4613
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 26.84
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 10.73s
                        Total time: 29149.40s
                               ETA: 1296439.1s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.861s, learning 0.161s)
               Value function loss: 0.5077
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 26.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 11.02s
                        Total time: 29160.42s
                               ETA: 1296326.5s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.876s, learning 0.270s)
               Value function loss: 0.4521
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 26.99
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 11.15s
                        Total time: 29171.57s
                               ETA: 1296219.6s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.159s)
               Value function loss: 0.3176
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 26.92
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 11.42s
                        Total time: 29182.99s
                               ETA: 1296124.9s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.103s, learning 0.272s)
               Value function loss: 0.4121
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 26.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 11.37s
                        Total time: 29194.36s
                               ETA: 1296028.3s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.665s, learning 0.169s)
               Value function loss: 0.4086
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 27.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 10.83s
                        Total time: 29205.20s
                               ETA: 1295907.7s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.196s, learning 0.187s)
               Value function loss: 0.4947
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 26.78
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 11.38s
                        Total time: 29216.58s
                               ETA: 1295811.6s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.100s, learning 0.220s)
               Value function loss: 0.3954
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 27.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 11.32s
                        Total time: 29227.90s
                               ETA: 1295712.8s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.864s, learning 0.211s)
               Value function loss: 0.4822
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 27.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 11.08s
                        Total time: 29238.97s
                               ETA: 1295603.2s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.160s)
               Value function loss: 0.5222
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 28.14
               Mean episode length: 124.94
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 11.42s
                        Total time: 29250.39s
                               ETA: 1295509.0s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.891s, learning 0.317s)
               Value function loss: 0.4291
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 27.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 11.21s
                        Total time: 29261.60s
                               ETA: 1295405.5s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.861s, learning 0.172s)
               Value function loss: 0.4350
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 27.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 11.03s
                        Total time: 29272.64s
                               ETA: 1295294.3s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.008s, learning 0.172s)
               Value function loss: 0.5134
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 28.35
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 11.18s
                        Total time: 29283.82s
                               ETA: 1295189.6s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.177s, learning 0.294s)
               Value function loss: 0.5158
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 27.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 11.47s
                        Total time: 29295.29s
                               ETA: 1295098.0s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.850s, learning 0.168s)
               Value function loss: 0.4808
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 27.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 11.02s
                        Total time: 29306.30s
                               ETA: 1294986.4s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.903s, learning 0.173s)
               Value function loss: 0.4770
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 28.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 11.08s
                        Total time: 29317.38s
                               ETA: 1294877.5s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.769s, learning 0.162s)
               Value function loss: 0.5542
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 28.98
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 10.93s
                        Total time: 29328.31s
                               ETA: 1294762.2s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.290s, learning 0.188s)
               Value function loss: 0.6449
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 28.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 11.48s
                        Total time: 29339.79s
                               ETA: 1294671.3s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.890s, learning 0.188s)
               Value function loss: 0.7589
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 29.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.08s
                        Total time: 29350.87s
                               ETA: 1294562.7s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.310s, learning 0.206s)
               Value function loss: 0.3487
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 28.91
               Mean episode length: 124.42
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 11.52s
                        Total time: 29362.39s
                               ETA: 1294473.5s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.250s, learning 0.209s)
               Value function loss: 0.2540
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 29.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 11.46s
                        Total time: 29373.85s
                               ETA: 1294381.9s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.847s, learning 0.161s)
               Value function loss: 0.3166
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 29.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 11.01s
                        Total time: 29384.85s
                               ETA: 1294270.4s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.461s, learning 0.159s)
               Value function loss: 0.3196
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 29.38
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 11.62s
                        Total time: 29396.47s
                               ETA: 1294186.0s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.922s, learning 0.159s)
               Value function loss: 0.2875
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 28.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 11.08s
                        Total time: 29407.56s
                               ETA: 1294078.0s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.178s, learning 0.163s)
               Value function loss: 0.3131
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: 28.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 11.34s
                        Total time: 29418.90s
                               ETA: 1293981.5s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.986s, learning 0.169s)
               Value function loss: 0.3069
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 29.09
               Mean episode length: 124.41
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 11.15s
                        Total time: 29430.05s
                               ETA: 1293876.8s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.580s, learning 0.185s)
               Value function loss: 0.3097
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 29.18
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 10.77s
                        Total time: 29440.82s
                               ETA: 1293755.2s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.396s, learning 0.172s)
               Value function loss: 0.3034
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 29.06
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 11.57s
                        Total time: 29452.38s
                               ETA: 1293668.8s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.534s, learning 0.245s)
               Value function loss: 0.3908
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 29.00
               Mean episode length: 124.39
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 10.78s
                        Total time: 29463.16s
                               ETA: 1293547.9s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.053s, learning 0.184s)
               Value function loss: 0.3438
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 28.99
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 11.24s
                        Total time: 29474.40s
                               ETA: 1293447.3s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.202s, learning 0.170s)
               Value function loss: 0.4003
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 28.87
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 11.37s
                        Total time: 29485.77s
                               ETA: 1293352.6s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.949s, learning 0.178s)
               Value function loss: 0.3998
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 29.93
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 11.13s
                        Total time: 29496.90s
                               ETA: 1293247.2s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.192s, learning 0.199s)
               Value function loss: 0.4395
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 29.82
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 11.39s
                        Total time: 29508.29s
                               ETA: 1293153.5s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.672s, learning 0.171s)
               Value function loss: 0.3933
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: 29.77
               Mean episode length: 123.73
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 10.84s
                        Total time: 29519.13s
                               ETA: 1293035.9s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.039s, learning 0.188s)
               Value function loss: 0.3759
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: 29.65
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 11.23s
                        Total time: 29530.36s
                               ETA: 1292935.1s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.082s, learning 0.165s)
               Value function loss: 0.3757
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 29.08
               Mean episode length: 124.74
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 11.25s
                        Total time: 29541.60s
                               ETA: 1292835.3s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.069s, learning 0.192s)
               Value function loss: 0.4065
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 29.48
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 11.26s
                        Total time: 29552.87s
                               ETA: 1292736.2s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.145s, learning 0.170s)
               Value function loss: 0.4969
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 29.22
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 11.31s
                        Total time: 29564.18s
                               ETA: 1292639.6s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.115s, learning 0.184s)
               Value function loss: 0.4541
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 30.27
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 11.30s
                        Total time: 29575.48s
                               ETA: 1292542.3s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.970s, learning 0.196s)
               Value function loss: 0.4762
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 30.96
               Mean episode length: 124.38
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 11.17s
                        Total time: 29586.65s
                               ETA: 1292439.3s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.884s, learning 0.192s)
               Value function loss: 0.4969
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 30.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 11.08s
                        Total time: 29597.72s
                               ETA: 1292332.5s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.377s, learning 0.177s)
               Value function loss: 0.6495
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 31.04
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 10.55s
                        Total time: 29608.28s
                               ETA: 1292203.0s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.714s, learning 0.181s)
               Value function loss: 0.6748
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 31.11
               Mean episode length: 124.41
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 10.90s
                        Total time: 29619.17s
                               ETA: 1292088.4s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.672s, learning 0.263s)
               Value function loss: 0.7716
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: 32.74
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 10.94s
                        Total time: 29630.11s
                               ETA: 1291975.7s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.111s, learning 0.163s)
               Value function loss: 1.0704
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 31.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 11.27s
                        Total time: 29641.38s
                               ETA: 1291877.8s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.675s, learning 0.178s)
               Value function loss: 0.9700
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 30.58
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 10.85s
                        Total time: 29652.23s
                               ETA: 1291761.7s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.039s, learning 0.179s)
               Value function loss: 0.7790
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 31.85
               Mean episode length: 124.11
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 11.22s
                        Total time: 29663.45s
                               ETA: 1291661.6s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.697s, learning 0.239s)
               Value function loss: 0.8394
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 32.43
               Mean episode length: 124.97
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 10.94s
                        Total time: 29674.39s
                               ETA: 1291549.2s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.969s, learning 0.191s)
               Value function loss: 1.0648
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 32.54
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 11.16s
                        Total time: 29685.55s
                               ETA: 1291446.7s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.356s, learning 0.171s)
               Value function loss: 0.7864
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: 32.22
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 11.53s
                        Total time: 29697.07s
                               ETA: 1291360.3s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.026s, learning 0.168s)
               Value function loss: 0.7080
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 31.93
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 11.19s
                        Total time: 29708.27s
                               ETA: 1291259.5s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.503s, learning 0.187s)
               Value function loss: 0.7041
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 32.61
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 10.69s
                        Total time: 29718.96s
                               ETA: 1291136.8s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.046s, learning 0.186s)
               Value function loss: 0.6332
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 32.41
               Mean episode length: 124.50
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 11.23s
                        Total time: 29730.19s
                               ETA: 1291037.7s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.520s, learning 0.189s)
               Value function loss: 0.7605
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 31.86
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 10.71s
                        Total time: 29740.90s
                               ETA: 1290916.0s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.008s, learning 0.174s)
               Value function loss: 0.7558
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 32.42
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 11.18s
                        Total time: 29752.08s
                               ETA: 1290815.0s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.902s, learning 0.197s)
               Value function loss: 0.6892
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 33.03
               Mean episode length: 124.04
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 11.10s
                        Total time: 29763.18s
                               ETA: 1290710.5s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.036s, learning 0.171s)
               Value function loss: 0.6756
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 31.21
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 11.21s
                        Total time: 29774.39s
                               ETA: 1290610.7s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.599s, learning 0.194s)
               Value function loss: 0.5084
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 32.06
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 10.79s
                        Total time: 29785.18s
                               ETA: 1290493.0s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.775s, learning 0.165s)
               Value function loss: 0.5803
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 33.20
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 10.94s
                        Total time: 29796.12s
                               ETA: 1290381.8s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.932s, learning 0.172s)
               Value function loss: 0.8348
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 33.26
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 11.10s
                        Total time: 29807.22s
                               ETA: 1290277.8s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.016s, learning 0.179s)
               Value function loss: 0.7420
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 33.86
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 11.19s
                        Total time: 29818.42s
                               ETA: 1290177.8s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.749s, learning 0.167s)
               Value function loss: 0.7100
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 33.76
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 10.92s
                        Total time: 29829.33s
                               ETA: 1290065.8s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.216s, learning 0.233s)
               Value function loss: 0.7534
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 33.00
               Mean episode length: 124.98
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.45s
                        Total time: 29840.78s
                               ETA: 1289977.0s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.576s, learning 0.199s)
               Value function loss: 0.8423
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 33.75
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 10.78s
                        Total time: 29851.56s
                               ETA: 1289859.1s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.015s, learning 0.165s)
               Value function loss: 0.9261
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 33.67
               Mean episode length: 124.56
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 11.18s
                        Total time: 29862.74s
                               ETA: 1289758.8s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.080s, learning 0.182s)
               Value function loss: 0.7354
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 35.33
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 11.26s
                        Total time: 29874.00s
                               ETA: 1289662.1s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.594s, learning 0.182s)
               Value function loss: 0.9158
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 35.04
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 10.78s
                        Total time: 29884.77s
                               ETA: 1289544.5s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.548s, learning 0.181s)
               Value function loss: 0.5863
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 33.67
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 10.73s
                        Total time: 29895.50s
                               ETA: 1289425.0s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1548 steps/s (collection: 10.420s, learning 0.161s)
               Value function loss: 0.7537
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 34.25
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 10.58s
                        Total time: 29906.09s
                               ETA: 1289299.2s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.159s)
               Value function loss: 0.5621
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 34.76
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 11.18s
                        Total time: 29917.27s
                               ETA: 1289199.5s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.203s, learning 0.194s)
               Value function loss: 0.5981
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 34.52
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 11.40s
                        Total time: 29928.67s
                               ETA: 1289109.1s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.083s, learning 0.160s)
               Value function loss: 0.6555
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 34.86
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 11.24s
                        Total time: 29939.91s
                               ETA: 1289012.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.048s, learning 0.192s)
               Value function loss: 0.6546
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 34.38
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 11.24s
                        Total time: 29951.15s
                               ETA: 1288914.9s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.778s, learning 0.179s)
               Value function loss: 0.6311
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 34.08
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 10.96s
                        Total time: 29962.11s
                               ETA: 1288805.7s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.097s, learning 0.168s)
               Value function loss: 0.5925
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 34.14
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 11.26s
                        Total time: 29973.37s
                               ETA: 1288709.8s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.125s, learning 0.160s)
               Value function loss: 0.7853
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 34.04
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 11.29s
                        Total time: 29984.66s
                               ETA: 1288615.0s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.098s, learning 0.208s)
               Value function loss: 0.7517
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 34.72
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 11.31s
                        Total time: 29995.96s
                               ETA: 1288521.0s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.753s, learning 0.175s)
               Value function loss: 0.7143
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 35.45
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 10.93s
                        Total time: 30006.89s
                               ETA: 1288411.0s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.168s)
               Value function loss: 0.9348
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 34.83
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 10.94s
                        Total time: 30017.83s
                               ETA: 1288301.6s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.868s, learning 0.179s)
               Value function loss: 1.0646
                    Surrogate loss: -0.0240
             Mean action noise std: 0.71
                       Mean reward: 34.66
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 11.05s
                        Total time: 30028.88s
                               ETA: 1288196.8s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.169s)
               Value function loss: 1.1524
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 34.05
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 10.70s
                        Total time: 30039.58s
                               ETA: 1288077.1s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.964s, learning 0.161s)
               Value function loss: 0.9435
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 34.66
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 11.12s
                        Total time: 30050.70s
                               ETA: 1287975.7s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.138s, learning 0.165s)
               Value function loss: 1.2050
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 33.79
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 11.30s
                        Total time: 30062.01s
                               ETA: 1287882.1s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.853s, learning 0.159s)
               Value function loss: 1.0526
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 33.46
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 11.01s
                        Total time: 30073.02s
                               ETA: 1287776.1s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.720s, learning 0.164s)
               Value function loss: 1.0965
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 33.69
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 10.88s
                        Total time: 30083.90s
                               ETA: 1287664.7s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.772s, learning 0.175s)
               Value function loss: 0.8257
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 33.83
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 10.95s
                        Total time: 30094.85s
                               ETA: 1287556.1s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.233s, learning 0.194s)
               Value function loss: 0.8702
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 33.39
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 11.43s
                        Total time: 30106.27s
                               ETA: 1287468.2s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.329s, learning 0.191s)
               Value function loss: 0.8856
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 32.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 11.52s
                        Total time: 30117.79s
                               ETA: 1287384.2s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.865s, learning 0.164s)
               Value function loss: 0.7414
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 32.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 11.03s
                        Total time: 30128.82s
                               ETA: 1287279.4s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.694s, learning 0.174s)
               Value function loss: 0.8193
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 32.69
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 10.87s
                        Total time: 30139.69s
                               ETA: 1287167.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.696s, learning 0.187s)
               Value function loss: 0.8488
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 32.36
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 10.88s
                        Total time: 30150.57s
                               ETA: 1287056.8s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.849s, learning 0.173s)
               Value function loss: 0.9869
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 32.41
               Mean episode length: 124.35
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 11.02s
                        Total time: 30161.60s
                               ETA: 1286951.9s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.584s, learning 0.194s)
               Value function loss: 0.8423
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 32.10
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 10.78s
                        Total time: 30172.37s
                               ETA: 1286836.6s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.817s, learning 0.218s)
               Value function loss: 0.8835
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 32.56
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 11.03s
                        Total time: 30183.41s
                               ETA: 1286732.4s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.931s, learning 0.164s)
               Value function loss: 0.8332
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 31.96
               Mean episode length: 124.17
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 11.09s
                        Total time: 30194.50s
                               ETA: 1286630.9s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.686s, learning 0.256s)
               Value function loss: 0.9905
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 32.40
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 10.94s
                        Total time: 30205.45s
                               ETA: 1286522.9s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.751s, learning 0.171s)
               Value function loss: 0.7987
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 31.98
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 10.92s
                        Total time: 30216.37s
                               ETA: 1286414.1s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.990s, learning 0.166s)
               Value function loss: 0.6776
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 32.01
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 11.16s
                        Total time: 30227.52s
                               ETA: 1286315.4s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.162s)
               Value function loss: 0.6556
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 31.82
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 10.89s
                        Total time: 30238.42s
                               ETA: 1286205.7s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.555s, learning 0.196s)
               Value function loss: 0.7389
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 31.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 10.75s
                        Total time: 30249.17s
                               ETA: 1286089.9s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.805s, learning 0.168s)
               Value function loss: 0.8393
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 32.83
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 10.97s
                        Total time: 30260.14s
                               ETA: 1285983.7s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.692s, learning 0.166s)
               Value function loss: 0.7729
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 31.85
               Mean episode length: 124.91
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 11.86s
                        Total time: 30272.00s
                               ETA: 1285915.1s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.376s, learning 0.172s)
               Value function loss: 0.9341
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 32.36
               Mean episode length: 124.70
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 11.55s
                        Total time: 30283.55s
                               ETA: 1285833.4s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.164s)
               Value function loss: 0.9407
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 31.91
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 11.06s
                        Total time: 30294.61s
                               ETA: 1285731.1s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.087s, learning 0.170s)
               Value function loss: 0.7586
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 32.16
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 11.26s
                        Total time: 30305.87s
                               ETA: 1285637.2s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.810s, learning 0.161s)
               Value function loss: 0.6974
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 31.56
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 10.97s
                        Total time: 30316.84s
                               ETA: 1285531.3s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.600s, learning 0.178s)
               Value function loss: 1.0164
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 31.79
               Mean episode length: 123.52
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 10.78s
                        Total time: 30327.61s
                               ETA: 1285417.2s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.620s, learning 0.170s)
               Value function loss: 0.8320
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 32.87
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 10.79s
                        Total time: 30338.40s
                               ETA: 1285303.7s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.347s, learning 0.206s)
               Value function loss: 0.8872
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 32.23
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 11.55s
                        Total time: 30349.96s
                               ETA: 1285222.7s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.909s, learning 0.282s)
               Value function loss: 1.0643
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 32.86
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 11.19s
                        Total time: 30361.15s
                               ETA: 1285126.3s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.710s, learning 0.283s)
               Value function loss: 1.2305
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 32.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 10.99s
                        Total time: 30372.14s
                               ETA: 1285021.7s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.057s, learning 0.200s)
               Value function loss: 1.4354
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 31.38
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 11.26s
                        Total time: 30383.40s
                               ETA: 1284928.4s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.179s, learning 0.224s)
               Value function loss: 0.8597
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 32.10
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 11.40s
                        Total time: 30394.80s
                               ETA: 1284841.2s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.514s, learning 0.177s)
               Value function loss: 0.9698
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 33.94
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 10.69s
                        Total time: 30405.49s
                               ETA: 1284724.1s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.132s, learning 0.161s)
               Value function loss: 0.7829
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 31.43
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 11.29s
                        Total time: 30416.79s
                               ETA: 1284632.5s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.926s, learning 0.182s)
               Value function loss: 0.8371
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 32.10
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 11.11s
                        Total time: 30427.89s
                               ETA: 1284533.1s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.963s, learning 0.223s)
               Value function loss: 0.7492
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 33.50
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 11.19s
                        Total time: 30439.08s
                               ETA: 1284437.1s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.802s, learning 0.162s)
               Value function loss: 0.7514
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 32.59
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 10.96s
                        Total time: 30450.04s
                               ETA: 1284331.8s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.913s, learning 0.199s)
               Value function loss: 0.7816
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 31.95
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 11.11s
                        Total time: 30461.16s
                               ETA: 1284232.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.811s, learning 0.208s)
               Value function loss: 0.8215
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 32.57
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 11.02s
                        Total time: 30472.17s
                               ETA: 1284130.0s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.004s, learning 0.178s)
               Value function loss: 0.7143
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 34.11
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 11.18s
                        Total time: 30483.36s
                               ETA: 1284034.2s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.036s, learning 0.180s)
               Value function loss: 0.8780
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 33.25
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 11.22s
                        Total time: 30494.57s
                               ETA: 1283939.8s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.188s, learning 0.194s)
               Value function loss: 0.9690
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 34.86
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 11.38s
                        Total time: 30505.95s
                               ETA: 1283852.5s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.684s, learning 0.176s)
               Value function loss: 0.8695
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 33.15
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 10.86s
                        Total time: 30516.81s
                               ETA: 1283743.3s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.707s, learning 0.180s)
               Value function loss: 0.9324
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 33.80
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 10.89s
                        Total time: 30527.70s
                               ETA: 1283635.3s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.356s, learning 0.187s)
               Value function loss: 1.0642
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 34.51
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.54s
                        Total time: 30539.24s
                               ETA: 1283555.0s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.543s, learning 0.178s)
               Value function loss: 1.0565
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 33.04
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 10.72s
                        Total time: 30549.96s
                               ETA: 1283440.1s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.849s, learning 0.193s)
               Value function loss: 0.9446
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 33.46
               Mean episode length: 123.44
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 11.04s
                        Total time: 30561.01s
                               ETA: 1283338.9s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.914s, learning 0.206s)
               Value function loss: 0.8336
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 35.72
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 11.12s
                        Total time: 30572.13s
                               ETA: 1283241.0s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.944s, learning 0.173s)
               Value function loss: 0.8487
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 33.55
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 11.12s
                        Total time: 30583.24s
                               ETA: 1283143.0s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.660s, learning 0.186s)
               Value function loss: 0.8823
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 35.25
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 10.85s
                        Total time: 30594.09s
                               ETA: 1283033.8s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.410s, learning 0.159s)
               Value function loss: 0.9230
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 35.05
               Mean episode length: 124.23
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 10.57s
                        Total time: 30604.66s
                               ETA: 1282913.0s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.737s, learning 0.199s)
               Value function loss: 0.8143
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 34.99
               Mean episode length: 124.64
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 10.94s
                        Total time: 30615.59s
                               ETA: 1282807.7s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.698s, learning 0.181s)
               Value function loss: 0.9774
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 34.58
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 10.88s
                        Total time: 30626.47s
                               ETA: 1282700.2s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.512s, learning 0.251s)
               Value function loss: 1.1260
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 32.74
               Mean episode length: 124.05
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 10.76s
                        Total time: 30637.23s
                               ETA: 1282587.8s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.631s, learning 0.191s)
               Value function loss: 1.0020
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 35.85
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 10.82s
                        Total time: 30648.06s
                               ETA: 1282478.0s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.553s, learning 0.163s)
               Value function loss: 0.9143
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 34.53
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 10.72s
                        Total time: 30658.77s
                               ETA: 1282363.9s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.637s, learning 0.187s)
               Value function loss: 0.9966
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 33.64
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 10.82s
                        Total time: 30669.60s
                               ETA: 1282254.3s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.876s, learning 0.279s)
               Value function loss: 1.0751
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 33.85
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 11.16s
                        Total time: 30680.75s
                               ETA: 1282158.7s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.653s, learning 0.225s)
               Value function loss: 1.0083
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 32.13
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 10.88s
                        Total time: 30691.63s
                               ETA: 1282051.5s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.167s)
               Value function loss: 1.0127
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 33.89
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 11.27s
                        Total time: 30702.90s
                               ETA: 1281960.9s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.686s, learning 0.185s)
               Value function loss: 1.0947
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 34.72
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 10.87s
                        Total time: 30713.77s
                               ETA: 1281853.7s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.074s, learning 0.167s)
               Value function loss: 1.2069
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 33.62
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 11.24s
                        Total time: 30725.01s
                               ETA: 1281761.9s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.921s, learning 0.161s)
               Value function loss: 1.1042
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 32.53
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 11.08s
                        Total time: 30736.09s
                               ETA: 1281663.6s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.773s, learning 0.206s)
               Value function loss: 1.1698
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 35.05
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 10.98s
                        Total time: 30747.07s
                               ETA: 1281561.1s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.270s, learning 0.188s)
               Value function loss: 1.1206
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 33.96
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 11.46s
                        Total time: 30758.53s
                               ETA: 1281478.6s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.596s, learning 0.218s)
               Value function loss: 1.2820
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 33.79
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 11.81s
                        Total time: 30770.34s
                               ETA: 1281411.0s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.392s, learning 0.166s)
               Value function loss: 1.4826
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 31.76
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 11.56s
                        Total time: 30781.90s
                               ETA: 1281332.8s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.707s, learning 0.171s)
               Value function loss: 1.1786
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 33.33
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 10.88s
                        Total time: 30792.78s
                               ETA: 1281226.3s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.894s, learning 0.159s)
               Value function loss: 1.4486
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 34.84
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 11.05s
                        Total time: 30803.83s
                               ETA: 1281127.3s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.500s, learning 0.160s)
               Value function loss: 1.3767
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 33.50
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 10.66s
                        Total time: 30814.49s
                               ETA: 1281011.9s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.595s, learning 0.183s)
               Value function loss: 0.9844
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 33.49
               Mean episode length: 123.56
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 10.78s
                        Total time: 30825.27s
                               ETA: 1280901.5s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.384s, learning 0.296s)
               Value function loss: 0.9328
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 34.13
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 10.68s
                        Total time: 30835.95s
                               ETA: 1280787.2s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.690s, learning 0.181s)
               Value function loss: 1.1143
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 31.78
               Mean episode length: 124.05
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 10.87s
                        Total time: 30846.82s
                               ETA: 1280680.8s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.810s, learning 0.179s)
               Value function loss: 0.8556
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 32.47
               Mean episode length: 124.91
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 10.99s
                        Total time: 30857.81s
                               ETA: 1280579.5s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.255s, learning 0.190s)
               Value function loss: 0.8491
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 32.36
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 11.44s
                        Total time: 30869.26s
                               ETA: 1280497.1s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1570 steps/s (collection: 10.232s, learning 0.198s)
               Value function loss: 0.9566
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 33.50
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 10.43s
                        Total time: 30879.69s
                               ETA: 1280372.7s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.908s, learning 0.194s)
               Value function loss: 0.9536
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 32.17
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 11.10s
                        Total time: 30890.79s
                               ETA: 1280276.3s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.277s, learning 0.200s)
               Value function loss: 0.8710
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 33.10
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 11.48s
                        Total time: 30902.26s
                               ETA: 1280195.5s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.160s)
               Value function loss: 0.7473
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 32.61
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 11.16s
                        Total time: 30913.43s
                               ETA: 1280101.7s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.186s)
               Value function loss: 0.9093
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 33.08
               Mean episode length: 124.84
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 11.26s
                        Total time: 30924.69s
                               ETA: 1280012.0s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.057s, learning 0.205s)
               Value function loss: 0.8095
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 33.13
               Mean episode length: 124.04
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 11.26s
                        Total time: 30935.95s
                               ETA: 1279922.4s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.548s, learning 0.168s)
               Value function loss: 0.8102
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 33.17
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 11.72s
                        Total time: 30947.66s
                               ETA: 1279851.7s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.810s, learning 0.196s)
               Value function loss: 0.7459
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 32.07
               Mean episode length: 124.06
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 11.01s
                        Total time: 30958.67s
                               ETA: 1279751.7s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.714s, learning 0.191s)
               Value function loss: 0.7671
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 33.02
               Mean episode length: 124.10
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 10.90s
                        Total time: 30969.57s
                               ETA: 1279647.6s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.894s, learning 0.180s)
               Value function loss: 0.7560
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 33.08
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 11.07s
                        Total time: 30980.65s
                               ETA: 1279550.6s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.801s, learning 0.197s)
               Value function loss: 0.7861
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 33.07
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 11.00s
                        Total time: 30991.65s
                               ETA: 1279450.5s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.706s, learning 0.160s)
               Value function loss: 0.6948
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 32.38
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 10.87s
                        Total time: 31002.51s
                               ETA: 1279345.0s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.008s, learning 0.177s)
               Value function loss: 0.6966
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 31.70
               Mean episode length: 124.84
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 11.18s
                        Total time: 31013.70s
                               ETA: 1279252.8s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.859s, learning 0.162s)
               Value function loss: 0.8160
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 30.89
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 11.02s
                        Total time: 31024.72s
                               ETA: 1279153.8s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.885s, learning 0.165s)
               Value function loss: 0.6367
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 29.77
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 11.05s
                        Total time: 31035.77s
                               ETA: 1279056.2s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.834s, learning 0.239s)
               Value function loss: 0.5948
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 30.62
               Mean episode length: 124.24
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 11.07s
                        Total time: 31046.84s
                               ETA: 1278959.5s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.913s, learning 0.173s)
               Value function loss: 0.7232
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 30.30
               Mean episode length: 124.24
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 11.09s
                        Total time: 31057.93s
                               ETA: 1278863.5s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.981s, learning 0.174s)
               Value function loss: 0.8106
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 29.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 11.16s
                        Total time: 31069.08s
                               ETA: 1278770.4s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1440 steps/s (collection: 10.975s, learning 0.399s)
               Value function loss: 0.6532
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 28.87
               Mean episode length: 124.37
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 11.37s
                        Total time: 31080.46s
                               ETA: 1278686.4s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.729s, learning 0.177s)
               Value function loss: 0.4838
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 29.26
               Mean episode length: 124.49
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 10.91s
                        Total time: 31091.36s
                               ETA: 1278583.2s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.818s, learning 0.172s)
               Value function loss: 0.5219
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 27.79
               Mean episode length: 124.23
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 10.99s
                        Total time: 31102.35s
                               ETA: 1278483.5s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.167s)
               Value function loss: 0.5752
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 28.62
               Mean episode length: 124.29
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 10.93s
                        Total time: 31113.28s
                               ETA: 1278381.2s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.974s, learning 0.169s)
               Value function loss: 0.6841
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 28.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 11.14s
                        Total time: 31124.42s
                               ETA: 1278288.0s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.046s, learning 0.168s)
               Value function loss: 0.5433
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 26.82
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 11.21s
                        Total time: 31135.64s
                               ETA: 1278197.7s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.582s, learning 0.166s)
               Value function loss: 0.5655
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 27.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 10.75s
                        Total time: 31146.38s
                               ETA: 1278088.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.002s, learning 0.161s)
               Value function loss: 0.5581
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 28.10
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 11.16s
                        Total time: 31157.55s
                               ETA: 1277996.1s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.133s, learning 0.220s)
               Value function loss: 0.3949
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 26.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 11.35s
                        Total time: 31168.90s
                               ETA: 1277911.7s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.290s, learning 0.207s)
               Value function loss: 0.3900
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 26.11
               Mean episode length: 124.40
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 11.50s
                        Total time: 31180.40s
                               ETA: 1277833.3s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.089s, learning 0.163s)
               Value function loss: 0.6416
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 27.97
               Mean episode length: 124.16
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 11.25s
                        Total time: 31191.65s
                               ETA: 1277745.0s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.645s, learning 0.218s)
               Value function loss: 0.5902
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 27.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 10.86s
                        Total time: 31202.51s
                               ETA: 1277640.7s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.919s, learning 0.164s)
               Value function loss: 0.5455
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 29.01
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 11.08s
                        Total time: 31213.59s
                               ETA: 1277545.5s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.670s, learning 0.194s)
               Value function loss: 0.6480
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 28.16
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 10.86s
                        Total time: 31224.46s
                               ETA: 1277441.5s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.686s, learning 0.214s)
               Value function loss: 0.5952
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 28.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 10.90s
                        Total time: 31235.36s
                               ETA: 1277339.0s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.339s, learning 0.201s)
               Value function loss: 0.6288
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 28.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 10.54s
                        Total time: 31245.90s
                               ETA: 1277221.8s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.171s)
               Value function loss: 0.4899
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 28.60
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 10.70s
                        Total time: 31256.60s
                               ETA: 1277111.3s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.768s, learning 0.201s)
               Value function loss: 0.5297
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 28.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 10.97s
                        Total time: 31267.56s
                               ETA: 1277011.8s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.002s, learning 0.184s)
               Value function loss: 0.4735
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 28.22
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 11.19s
                        Total time: 31278.75s
                               ETA: 1276921.3s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.328s, learning 0.170s)
               Value function loss: 0.5340
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 28.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 11.50s
                        Total time: 31290.25s
                               ETA: 1276843.6s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.626s, learning 0.209s)
               Value function loss: 0.4842
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 27.60
               Mean episode length: 123.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 10.84s
                        Total time: 31301.08s
                               ETA: 1276738.9s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.418s, learning 0.189s)
               Value function loss: 0.4776
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 27.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 10.61s
                        Total time: 31311.69s
                               ETA: 1276625.0s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.892s, learning 0.172s)
               Value function loss: 0.5060
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 27.32
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 11.06s
                        Total time: 31322.75s
                               ETA: 1276529.8s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.668s, learning 0.221s)
               Value function loss: 0.4594
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 27.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 10.89s
                        Total time: 31333.64s
                               ETA: 1276427.5s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.999s, learning 0.161s)
               Value function loss: 0.3909
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 26.96
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 11.16s
                        Total time: 31344.80s
                               ETA: 1276336.4s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.774s, learning 0.173s)
               Value function loss: 0.3624
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 26.75
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 10.95s
                        Total time: 31355.75s
                               ETA: 1276236.6s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.808s, learning 0.160s)
               Value function loss: 0.5054
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 27.27
               Mean episode length: 124.65
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 10.97s
                        Total time: 31366.72s
                               ETA: 1276137.8s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.638s, learning 0.204s)
               Value function loss: 0.4063
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 26.58
               Mean episode length: 124.20
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 10.84s
                        Total time: 31377.56s
                               ETA: 1276033.9s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.729s, learning 0.188s)
               Value function loss: 0.3806
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 10.92s
                        Total time: 31388.48s
                               ETA: 1275933.1s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.794s, learning 0.171s)
               Value function loss: 0.4324
                    Surrogate loss: -0.0081
             Mean action noise std: 0.71
                       Mean reward: 25.51
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 10.97s
                        Total time: 31399.44s
                               ETA: 1275834.4s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.757s, learning 0.175s)
               Value function loss: 0.5303
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 26.30
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 10.93s
                        Total time: 31410.38s
                               ETA: 1275734.4s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.161s)
               Value function loss: 0.4571
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 24.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 10.79s
                        Total time: 31421.17s
                               ETA: 1275628.9s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.107s, learning 0.173s)
               Value function loss: 0.3188
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 25.77
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 11.28s
                        Total time: 31432.45s
                               ETA: 1275543.2s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.068s, learning 0.187s)
               Value function loss: 0.4129
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 25.83
               Mean episode length: 124.35
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 11.26s
                        Total time: 31443.70s
                               ETA: 1275456.5s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.430s, learning 0.197s)
               Value function loss: 0.3952
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 26.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 11.63s
                        Total time: 31455.33s
                               ETA: 1275384.9s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.549s, learning 0.162s)
               Value function loss: 0.4275
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 25.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 10.71s
                        Total time: 31466.04s
                               ETA: 1275276.3s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.926s, learning 0.227s)
               Value function loss: 0.3925
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 26.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 11.15s
                        Total time: 31477.19s
                               ETA: 1275185.7s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.745s, learning 0.216s)
               Value function loss: 0.4923
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 26.06
               Mean episode length: 124.73
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 10.96s
                        Total time: 31488.16s
                               ETA: 1275087.4s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.046s, learning 0.192s)
               Value function loss: 0.4603
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 25.88
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 11.24s
                        Total time: 31499.39s
                               ETA: 1275000.4s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.695s, learning 0.173s)
               Value function loss: 0.4259
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 26.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 10.87s
                        Total time: 31510.26s
                               ETA: 1274898.4s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.787s, learning 0.163s)
               Value function loss: 0.4181
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 26.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 10.95s
                        Total time: 31521.21s
                               ETA: 1274799.9s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.676s, learning 0.208s)
               Value function loss: 0.5256
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 26.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 10.88s
                        Total time: 31532.10s
                               ETA: 1274698.7s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.625s, learning 0.177s)
               Value function loss: 0.6014
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 26.44
               Mean episode length: 124.28
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 10.80s
                        Total time: 31542.90s
                               ETA: 1274594.4s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.338s, learning 0.175s)
               Value function loss: 0.5673
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 27.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 11.51s
                        Total time: 31554.41s
                               ETA: 1274518.8s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.815s, learning 0.253s)
               Value function loss: 0.5888
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 27.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 11.07s
                        Total time: 31565.48s
                               ETA: 1274425.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.548s, learning 0.175s)
               Value function loss: 0.5770
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 27.34
               Mean episode length: 124.17
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 11.72s
                        Total time: 31577.20s
                               ETA: 1274358.2s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.971s, learning 0.173s)
               Value function loss: 0.6029
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 11.14s
                        Total time: 31588.35s
                               ETA: 1274267.9s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.693s, learning 0.172s)
               Value function loss: 0.6251
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 26.81
               Mean episode length: 123.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 10.86s
                        Total time: 31599.21s
                               ETA: 1274166.4s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.957s, learning 0.181s)
               Value function loss: 0.5026
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 27.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 11.14s
                        Total time: 31610.35s
                               ETA: 1274076.0s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.795s, learning 0.202s)
               Value function loss: 0.4362
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 28.06
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 11.00s
                        Total time: 31621.35s
                               ETA: 1273980.0s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.929s, learning 0.174s)
               Value function loss: 0.5040
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 27.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 11.10s
                        Total time: 31632.45s
                               ETA: 1273888.3s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.219s, learning 0.195s)
               Value function loss: 0.5844
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 27.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 11.41s
                        Total time: 31643.87s
                               ETA: 1273809.2s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.855s, learning 0.181s)
               Value function loss: 0.5272
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 27.26
               Mean episode length: 124.32
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 11.04s
                        Total time: 31654.90s
                               ETA: 1273714.9s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.872s, learning 0.167s)
               Value function loss: 0.4912
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 26.98
               Mean episode length: 124.14
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 11.04s
                        Total time: 31665.94s
                               ETA: 1273620.8s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.165s)
               Value function loss: 0.5047
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 27.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 11.50s
                        Total time: 31677.44s
                               ETA: 1273545.3s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.785s, learning 0.167s)
               Value function loss: 0.4183
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 27.88
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 10.95s
                        Total time: 31688.39s
                               ETA: 1273447.9s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.637s, learning 0.163s)
               Value function loss: 0.4569
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 27.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 10.80s
                        Total time: 31699.19s
                               ETA: 1273344.4s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.973s, learning 0.167s)
               Value function loss: 0.5365
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 28.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 11.14s
                        Total time: 31710.33s
                               ETA: 1273254.7s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.691s, learning 0.160s)
               Value function loss: 0.4922
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 27.35
               Mean episode length: 124.27
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 10.85s
                        Total time: 31721.18s
                               ETA: 1273153.3s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.955s, learning 0.199s)
               Value function loss: 0.5149
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 28.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 11.15s
                        Total time: 31732.34s
                               ETA: 1273064.3s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.134s, learning 0.197s)
               Value function loss: 0.5841
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 28.65
               Mean episode length: 124.13
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 11.33s
                        Total time: 31743.67s
                               ETA: 1272982.4s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.614s, learning 0.171s)
               Value function loss: 0.5349
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 28.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 10.78s
                        Total time: 31754.45s
                               ETA: 1272878.7s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.026s, learning 0.168s)
               Value function loss: 0.6172
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 29.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 11.19s
                        Total time: 31765.65s
                               ETA: 1272791.4s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.700s, learning 0.162s)
               Value function loss: 0.5161
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 28.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 10.86s
                        Total time: 31776.51s
                               ETA: 1272691.0s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.789s, learning 0.161s)
               Value function loss: 0.5232
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 29.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 10.95s
                        Total time: 31787.46s
                               ETA: 1272594.1s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.626s, learning 0.165s)
               Value function loss: 0.4557
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 29.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 10.79s
                        Total time: 31798.25s
                               ETA: 1272490.9s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.969s, learning 0.170s)
               Value function loss: 0.5007
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 28.39
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 11.14s
                        Total time: 31809.39s
                               ETA: 1272401.7s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.876s, learning 0.164s)
               Value function loss: 0.4879
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 28.51
               Mean episode length: 123.57
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 11.04s
                        Total time: 31820.43s
                               ETA: 1272308.6s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.994s, learning 0.172s)
               Value function loss: 0.5078
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 28.84
               Mean episode length: 124.16
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 11.17s
                        Total time: 31831.60s
                               ETA: 1272220.6s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.747s, learning 0.164s)
               Value function loss: 0.5072
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 31.04
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 10.91s
                        Total time: 31842.51s
                               ETA: 1272122.5s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.161s, learning 0.188s)
               Value function loss: 0.5025
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 29.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 11.35s
                        Total time: 31853.86s
                               ETA: 1272041.9s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.487s, learning 0.175s)
               Value function loss: 0.4523
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: 29.83
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 10.66s
                        Total time: 31864.52s
                               ETA: 1271934.0s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.162s)
               Value function loss: 0.4926
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 30.45
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 11.47s
                        Total time: 31875.98s
                               ETA: 1271858.2s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.367s, learning 0.231s)
               Value function loss: 0.6243
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 30.26
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 11.60s
                        Total time: 31887.58s
                               ETA: 1271787.8s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.165s)
               Value function loss: 0.4543
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 30.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 11.33s
                        Total time: 31898.91s
                               ETA: 1271706.5s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.537s, learning 0.168s)
               Value function loss: 0.4728
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 30.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 10.71s
                        Total time: 31909.61s
                               ETA: 1271600.6s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.797s, learning 0.163s)
               Value function loss: 0.5113
                    Surrogate loss: -0.0240
             Mean action noise std: 0.71
                       Mean reward: 29.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 10.96s
                        Total time: 31920.57s
                               ETA: 1271505.0s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.715s, learning 0.165s)
               Value function loss: 0.6012
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 29.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 10.88s
                        Total time: 31931.45s
                               ETA: 1271406.2s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.120s, learning 0.232s)
               Value function loss: 0.4939
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: 29.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 11.35s
                        Total time: 31942.80s
                               ETA: 1271326.2s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.907s, learning 0.175s)
               Value function loss: 0.3652
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 29.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 11.08s
                        Total time: 31953.89s
                               ETA: 1271235.6s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.154s, learning 0.324s)
               Value function loss: 0.4466
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 29.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 11.48s
                        Total time: 31965.36s
                               ETA: 1271160.7s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.821s, learning 0.181s)
               Value function loss: 0.3932
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 28.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 11.00s
                        Total time: 31976.37s
                               ETA: 1271067.0s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.922s, learning 0.188s)
               Value function loss: 0.4906
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 29.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 11.11s
                        Total time: 31987.48s
                               ETA: 1270977.7s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.926s, learning 0.218s)
               Value function loss: 0.3663
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 28.15
               Mean episode length: 124.73
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 11.14s
                        Total time: 31998.62s
                               ETA: 1270889.8s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.886s, learning 0.219s)
               Value function loss: 0.4568
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 28.90
               Mean episode length: 124.21
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 11.10s
                        Total time: 32009.72s
                               ETA: 1270800.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.205s, learning 0.177s)
               Value function loss: 0.4358
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 29.00
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 11.38s
                        Total time: 32021.11s
                               ETA: 1270722.0s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.794s, learning 0.163s)
               Value function loss: 0.4099
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 29.88
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 10.96s
                        Total time: 32032.06s
                               ETA: 1270626.9s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.751s, learning 0.165s)
               Value function loss: 0.4706
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 29.74
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 10.92s
                        Total time: 32042.98s
                               ETA: 1270530.2s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.839s, learning 0.169s)
               Value function loss: 0.4867
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 29.74
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 11.01s
                        Total time: 32053.99s
                               ETA: 1270437.2s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.708s, learning 0.197s)
               Value function loss: 0.5378
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 29.54
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 10.90s
                        Total time: 32064.89s
                               ETA: 1270340.2s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.644s, learning 0.161s)
               Value function loss: 0.5101
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 29.84
               Mean episode length: 124.26
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 10.81s
                        Total time: 32075.70s
                               ETA: 1270239.3s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.965s, learning 0.217s)
               Value function loss: 0.5589
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 31.06
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 11.18s
                        Total time: 32086.88s
                               ETA: 1270153.4s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.924s, learning 0.158s)
               Value function loss: 0.5966
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 30.61
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 11.08s
                        Total time: 32097.96s
                               ETA: 1270063.6s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.450s, learning 0.164s)
               Value function loss: 0.5860
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 29.57
               Mean episode length: 124.61
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 11.61s
                        Total time: 32109.57s
                               ETA: 1269994.9s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.904s, learning 0.197s)
               Value function loss: 0.5052
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 30.40
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 11.10s
                        Total time: 32120.68s
                               ETA: 1269906.0s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.172s)
               Value function loss: 0.5184
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 30.19
               Mean episode length: 124.60
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 11.24s
                        Total time: 32131.91s
                               ETA: 1269822.5s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.089s, learning 0.224s)
               Value function loss: 0.4948
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 30.82
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 11.31s
                        Total time: 32143.23s
                               ETA: 1269742.1s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.988s, learning 0.200s)
               Value function loss: 0.5006
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 30.26
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 11.19s
                        Total time: 32154.41s
                               ETA: 1269656.7s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.834s, learning 0.162s)
               Value function loss: 0.4839
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 29.65
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 11.00s
                        Total time: 32165.41s
                               ETA: 1269563.9s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.166s)
               Value function loss: 0.4521
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 30.00
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 10.93s
                        Total time: 32176.34s
                               ETA: 1269468.4s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.935s, learning 0.198s)
               Value function loss: 0.5878
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 31.09
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 11.13s
                        Total time: 32187.47s
                               ETA: 1269381.1s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1454 steps/s (collection: 10.949s, learning 0.316s)
               Value function loss: 0.6270
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 30.54
               Mean episode length: 124.15
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 11.27s
                        Total time: 32198.73s
                               ETA: 1269299.1s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.143s, learning 0.171s)
               Value function loss: 0.5230
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 31.39
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 11.31s
                        Total time: 32210.05s
                               ETA: 1269219.1s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.043s, learning 0.182s)
               Value function loss: 0.5654
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 30.79
               Mean episode length: 124.07
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 11.22s
                        Total time: 32221.27s
                               ETA: 1269135.6s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.966s, learning 0.220s)
               Value function loss: 0.6532
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 31.25
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 11.19s
                        Total time: 32232.46s
                               ETA: 1269050.6s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.072s, learning 0.173s)
               Value function loss: 0.6146
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 30.54
               Mean episode length: 124.25
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 11.24s
                        Total time: 32243.70s
                               ETA: 1268968.0s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.022s, learning 0.164s)
               Value function loss: 0.6020
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 30.88
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 11.19s
                        Total time: 32254.89s
                               ETA: 1268883.1s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.821s, learning 0.167s)
               Value function loss: 0.5866
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 30.78
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 10.99s
                        Total time: 32265.88s
                               ETA: 1268790.6s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.908s, learning 0.185s)
               Value function loss: 0.6899
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 30.93
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 11.09s
                        Total time: 32276.97s
                               ETA: 1268702.2s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.050s, learning 0.162s)
               Value function loss: 0.6277
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 31.07
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 11.21s
                        Total time: 32288.18s
                               ETA: 1268618.6s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.855s, learning 0.166s)
               Value function loss: 0.4549
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 30.28
               Mean episode length: 124.69
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 11.02s
                        Total time: 32299.20s
                               ETA: 1268527.5s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.636s, learning 0.228s)
               Value function loss: 0.5814
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 31.70
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 10.86s
                        Total time: 32310.07s
                               ETA: 1268430.3s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.586s, learning 0.224s)
               Value function loss: 0.5314
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 30.89
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 10.81s
                        Total time: 32320.88s
                               ETA: 1268331.1s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.077s, learning 0.184s)
               Value function loss: 0.5131
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 30.79
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 11.26s
                        Total time: 32332.14s
                               ETA: 1268249.7s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.955s, learning 0.166s)
               Value function loss: 0.4568
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 31.28
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 11.12s
                        Total time: 32343.26s
                               ETA: 1268162.7s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.690s, learning 0.167s)
               Value function loss: 0.4973
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 30.54
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 10.86s
                        Total time: 32354.12s
                               ETA: 1268065.5s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.916s, learning 0.201s)
               Value function loss: 0.5346
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 30.57
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 11.12s
                        Total time: 32365.23s
                               ETA: 1267978.6s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.988s, learning 0.165s)
               Value function loss: 0.5432
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 30.72
               Mean episode length: 124.98
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 11.15s
                        Total time: 32376.39s
                               ETA: 1267893.2s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.538s, learning 0.176s)
               Value function loss: 0.4814
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 30.24
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 10.71s
                        Total time: 32387.10s
                               ETA: 1267790.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.140s, learning 0.163s)
               Value function loss: 0.5670
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 30.73
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 11.30s
                        Total time: 32398.41s
                               ETA: 1267711.1s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.907s, learning 0.184s)
               Value function loss: 0.8178
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 31.87
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 11.09s
                        Total time: 32409.50s
                               ETA: 1267623.4s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.709s, learning 0.164s)
               Value function loss: 0.6432
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 31.59
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 10.87s
                        Total time: 32420.37s
                               ETA: 1267527.3s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.244s, learning 0.271s)
               Value function loss: 0.5334
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 30.24
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 11.52s
                        Total time: 32431.89s
                               ETA: 1267456.3s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.000s, learning 0.175s)
               Value function loss: 0.7389
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 29.88
               Mean episode length: 124.95
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 11.17s
                        Total time: 32443.06s
                               ETA: 1267372.0s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.246s, learning 0.226s)
               Value function loss: 0.8534
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 31.76
               Mean episode length: 124.75
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 11.47s
                        Total time: 32454.53s
                               ETA: 1267299.4s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.146s, learning 0.185s)
               Value function loss: 0.8386
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: 30.74
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 11.33s
                        Total time: 32465.86s
                               ETA: 1267221.4s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.354s, learning 0.239s)
               Value function loss: 0.7373
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 30.47
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 11.59s
                        Total time: 32477.46s
                               ETA: 1267153.6s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.029s, learning 0.173s)
               Value function loss: 0.5803
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 28.87
               Mean episode length: 124.75
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 11.20s
                        Total time: 32488.66s
                               ETA: 1267070.7s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.914s, learning 0.184s)
               Value function loss: 0.5994
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 29.11
               Mean episode length: 124.03
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 11.10s
                        Total time: 32499.76s
                               ETA: 1266983.7s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.535s, learning 0.223s)
               Value function loss: 0.5037
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 30.78
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 10.76s
                        Total time: 32510.51s
                               ETA: 1266883.6s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.132s, learning 0.193s)
               Value function loss: 0.4455
                    Surrogate loss: -0.0258
             Mean action noise std: 0.71
                       Mean reward: 30.59
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 11.33s
                        Total time: 32521.84s
                               ETA: 1266805.6s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.780s, learning 0.223s)
               Value function loss: 0.4871
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 30.86
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 11.00s
                        Total time: 32532.84s
                               ETA: 1266715.1s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.021s, learning 0.172s)
               Value function loss: 0.4792
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 32.19
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 11.19s
                        Total time: 32544.04s
                               ETA: 1266632.1s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.896s, learning 0.213s)
               Value function loss: 0.4112
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 31.78
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 11.11s
                        Total time: 32555.15s
                               ETA: 1266545.9s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.858s, learning 0.180s)
               Value function loss: 0.4118
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 30.57
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 11.04s
                        Total time: 32566.18s
                               ETA: 1266457.0s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.167s, learning 0.178s)
               Value function loss: 0.5662
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 31.21
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 11.35s
                        Total time: 32577.53s
                               ETA: 1266380.0s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.068s, learning 0.304s)
               Value function loss: 0.4653
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 31.07
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 11.37s
                        Total time: 32588.90s
                               ETA: 1266304.2s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.168s)
               Value function loss: 0.4509
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 30.83
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 11.00s
                        Total time: 32599.91s
                               ETA: 1266214.2s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.549s, learning 0.186s)
               Value function loss: 0.4951
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 29.96
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 10.74s
                        Total time: 32610.64s
                               ETA: 1266113.7s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.780s, learning 0.263s)
               Value function loss: 0.4645
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 30.16
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 11.04s
                        Total time: 32621.69s
                               ETA: 1266025.3s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.170s)
               Value function loss: 0.5400
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 30.02
               Mean episode length: 124.19
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 11.47s
                        Total time: 32633.16s
                               ETA: 1265953.6s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.117s, learning 0.196s)
               Value function loss: 0.4624
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 30.14
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 11.31s
                        Total time: 32644.47s
                               ETA: 1265875.8s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.131s, learning 0.164s)
               Value function loss: 0.5345
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 31.51
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 11.30s
                        Total time: 32655.77s
                               ETA: 1265797.3s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.750s, learning 0.168s)
               Value function loss: 0.4299
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 30.26
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 10.92s
                        Total time: 32666.69s
                               ETA: 1265704.2s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.031s, learning 0.172s)
               Value function loss: 0.4624
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 30.13
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 11.20s
                        Total time: 32677.89s
                               ETA: 1265622.3s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.163s)
               Value function loss: 0.4905
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 31.23
               Mean episode length: 124.35
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 11.50s
                        Total time: 32689.39s
                               ETA: 1265551.9s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.861s, learning 0.183s)
               Value function loss: 0.4896
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 31.90
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 11.04s
                        Total time: 32700.43s
                               ETA: 1265463.9s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.827s, learning 0.180s)
               Value function loss: 0.5591
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 32.52
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 11.01s
                        Total time: 32711.44s
                               ETA: 1265374.6s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.824s, learning 0.168s)
               Value function loss: 0.5094
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 31.42
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 10.99s
                        Total time: 32722.43s
                               ETA: 1265284.7s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.165s)
               Value function loss: 0.4986
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 31.44
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 11.10s
                        Total time: 32733.53s
                               ETA: 1265199.1s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.989s, learning 0.166s)
               Value function loss: 0.5122
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 32.23
               Mean episode length: 124.74
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 11.16s
                        Total time: 32744.69s
                               ETA: 1265115.6s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.772s, learning 0.160s)
               Value function loss: 0.7622
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 32.95
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 10.93s
                        Total time: 32755.62s
                               ETA: 1265023.6s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.165s)
               Value function loss: 0.6049
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 33.05
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 11.47s
                        Total time: 32767.09s
                               ETA: 1264952.3s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.827s, learning 0.169s)
               Value function loss: 0.6454
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 33.70
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 11.00s
                        Total time: 32778.08s
                               ETA: 1264862.9s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.857s, learning 0.174s)
               Value function loss: 0.8617
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 33.86
               Mean episode length: 124.60
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 11.03s
                        Total time: 32789.12s
                               ETA: 1264774.9s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.060s, learning 0.192s)
               Value function loss: 1.0582
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 32.29
               Mean episode length: 122.59
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 11.25s
                        Total time: 32800.37s
                               ETA: 1264695.5s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.709s, learning 0.175s)
               Value function loss: 1.0179
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 33.80
               Mean episode length: 124.11
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 10.88s
                        Total time: 32811.25s
                               ETA: 1264602.0s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.851s, learning 0.165s)
               Value function loss: 0.6950
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 32.68
               Mean episode length: 122.63
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 11.02s
                        Total time: 32822.27s
                               ETA: 1264513.6s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.868s, learning 0.261s)
               Value function loss: 1.0138
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 33.80
               Mean episode length: 124.88
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 11.13s
                        Total time: 32833.40s
                               ETA: 1264429.5s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.888s, learning 0.189s)
               Value function loss: 0.8313
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 33.23
               Mean episode length: 122.69
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 11.08s
                        Total time: 32844.47s
                               ETA: 1264343.6s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.384s, learning 0.178s)
               Value function loss: 0.9338
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 32.52
               Mean episode length: 124.15
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 11.56s
                        Total time: 32856.03s
                               ETA: 1264276.4s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.826s, learning 0.227s)
               Value function loss: 0.8672
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 34.23
               Mean episode length: 124.14
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 11.05s
                        Total time: 32867.09s
                               ETA: 1264189.6s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.726s, learning 0.169s)
               Value function loss: 0.6271
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 33.55
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 10.89s
                        Total time: 32877.98s
                               ETA: 1264096.8s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.506s, learning 0.188s)
               Value function loss: 0.5635
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 32.57
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 10.69s
                        Total time: 32888.68s
                               ETA: 1263996.4s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.982s, learning 0.164s)
               Value function loss: 0.5915
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 34.14
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 11.15s
                        Total time: 32899.82s
                               ETA: 1263913.4s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.074s, learning 0.167s)
               Value function loss: 0.6601
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 33.05
               Mean episode length: 123.34
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 11.24s
                        Total time: 32911.06s
                               ETA: 1263834.1s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.173s)
               Value function loss: 0.6617
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 34.24
               Mean episode length: 123.93
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 11.01s
                        Total time: 32922.07s
                               ETA: 1263746.0s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.162s)
               Value function loss: 0.7461
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 32.87
               Mean episode length: 124.08
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 10.87s
                        Total time: 32932.94s
                               ETA: 1263652.5s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.980s, learning 0.173s)
               Value function loss: 0.5940
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 33.27
               Mean episode length: 123.28
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 11.15s
                        Total time: 32944.10s
                               ETA: 1263570.1s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.095s, learning 0.176s)
               Value function loss: 0.5735
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 33.65
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 11.27s
                        Total time: 32955.37s
                               ETA: 1263492.2s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.029s, learning 0.227s)
               Value function loss: 0.6657
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 31.44
               Mean episode length: 121.50
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 11.26s
                        Total time: 32966.62s
                               ETA: 1263413.8s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.738s, learning 0.180s)
               Value function loss: 0.8366
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 34.64
               Mean episode length: 124.32
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 10.92s
                        Total time: 32977.54s
                               ETA: 1263322.4s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.982s, learning 0.171s)
               Value function loss: 0.6968
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 32.33
               Mean episode length: 124.12
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 11.15s
                        Total time: 32988.69s
                               ETA: 1263240.1s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.637s, learning 0.179s)
               Value function loss: 0.6450
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 33.90
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 10.82s
                        Total time: 32999.51s
                               ETA: 1263145.0s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.991s, learning 0.175s)
               Value function loss: 0.6377
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 35.09
               Mean episode length: 124.21
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 11.17s
                        Total time: 33010.68s
                               ETA: 1263063.3s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.643s, learning 0.181s)
               Value function loss: 0.6825
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 33.39
               Mean episode length: 123.84
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 10.82s
                        Total time: 33021.50s
                               ETA: 1262968.6s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.689s, learning 0.161s)
               Value function loss: 0.8098
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 33.62
               Mean episode length: 123.53
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 10.85s
                        Total time: 33032.35s
                               ETA: 1262875.0s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.619s, learning 0.189s)
               Value function loss: 0.6743
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 33.22
               Mean episode length: 122.82
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 10.81s
                        Total time: 33043.16s
                               ETA: 1262779.9s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.195s, learning 0.168s)
               Value function loss: 0.8006
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 37.14
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 11.36s
                        Total time: 33054.52s
                               ETA: 1262706.0s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.166s)
               Value function loss: 0.6496
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 34.07
               Mean episode length: 124.07
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 11.33s
                        Total time: 33065.85s
                               ETA: 1262630.8s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.886s, learning 0.165s)
               Value function loss: 0.5851
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 34.74
               Mean episode length: 124.15
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 11.05s
                        Total time: 33076.90s
                               ETA: 1262545.0s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.652s, learning 0.271s)
               Value function loss: 0.5962
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 33.66
               Mean episode length: 124.25
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 10.92s
                        Total time: 33087.82s
                               ETA: 1262454.5s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.964s, learning 0.164s)
               Value function loss: 0.7814
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 33.36
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 11.13s
                        Total time: 33098.95s
                               ETA: 1262371.9s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.741s, learning 0.172s)
               Value function loss: 0.6897
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 33.92
               Mean episode length: 123.60
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 10.91s
                        Total time: 33109.86s
                               ETA: 1262281.1s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.772s, learning 0.173s)
               Value function loss: 0.6727
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 32.18
               Mean episode length: 122.57
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 10.95s
                        Total time: 33120.81s
                               ETA: 1262191.6s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.597s, learning 0.175s)
               Value function loss: 0.7802
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 34.40
               Mean episode length: 123.87
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 10.77s
                        Total time: 33131.58s
                               ETA: 1262095.6s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.687s, learning 0.195s)
               Value function loss: 0.9004
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 33.53
               Mean episode length: 124.09
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 10.88s
                        Total time: 33142.46s
                               ETA: 1262003.8s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.485s, learning 0.198s)
               Value function loss: 0.7562
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 34.89
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 10.68s
                        Total time: 33153.14s
                               ETA: 1261904.5s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.584s, learning 0.176s)
               Value function loss: 0.6075
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 31.93
               Mean episode length: 123.76
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 10.76s
                        Total time: 33163.90s
                               ETA: 1261808.2s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.757s, learning 0.179s)
               Value function loss: 0.6081
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 32.73
               Mean episode length: 124.12
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 10.94s
                        Total time: 33174.84s
                               ETA: 1261718.7s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.799s, learning 0.164s)
               Value function loss: 0.6217
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 32.01
               Mean episode length: 123.72
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 10.96s
                        Total time: 33185.80s
                               ETA: 1261630.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.750s, learning 0.285s)
               Value function loss: 0.7022
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 33.58
               Mean episode length: 123.40
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 11.04s
                        Total time: 33196.84s
                               ETA: 1261544.7s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.834s, learning 0.187s)
               Value function loss: 0.6793
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 32.06
               Mean episode length: 124.12
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 11.02s
                        Total time: 33207.86s
                               ETA: 1261458.5s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.625s, learning 0.160s)
               Value function loss: 0.6506
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 32.56
               Mean episode length: 123.40
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 10.79s
                        Total time: 33218.65s
                               ETA: 1261363.5s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.761s, learning 0.211s)
               Value function loss: 0.7249
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 33.01
               Mean episode length: 123.40
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 10.97s
                        Total time: 33229.62s
                               ETA: 1261275.6s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.867s, learning 0.191s)
               Value function loss: 0.6845
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 32.14
               Mean episode length: 124.03
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 11.06s
                        Total time: 33240.68s
                               ETA: 1261191.1s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.767s, learning 0.159s)
               Value function loss: 0.6111
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 33.01
               Mean episode length: 123.64
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 10.93s
                        Total time: 33251.60s
                               ETA: 1261101.6s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.548s, learning 0.193s)
               Value function loss: 0.7300
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 31.80
               Mean episode length: 123.35
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 10.74s
                        Total time: 33262.34s
                               ETA: 1261005.2s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.068s, learning 0.173s)
               Value function loss: 0.8809
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 33.37
               Mean episode length: 123.43
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 11.24s
                        Total time: 33273.58s
                               ETA: 1260927.7s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.830s, learning 0.174s)
               Value function loss: 0.7695
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 32.20
               Mean episode length: 122.46
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 11.00s
                        Total time: 33284.59s
                               ETA: 1260841.4s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.814s, learning 0.173s)
               Value function loss: 0.6815
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 31.81
               Mean episode length: 121.06
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 10.99s
                        Total time: 33295.57s
                               ETA: 1260754.5s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.900s, learning 0.262s)
               Value function loss: 0.8704
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 32.86
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 11.16s
                        Total time: 33306.74s
                               ETA: 1260674.2s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.162s)
               Value function loss: 0.9201
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 31.66
               Mean episode length: 123.79
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 11.20s
                        Total time: 33317.93s
                               ETA: 1260595.3s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.695s, learning 0.170s)
               Value function loss: 0.8835
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 32.46
               Mean episode length: 124.59
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 10.86s
                        Total time: 33328.80s
                               ETA: 1260503.9s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.694s, learning 0.212s)
               Value function loss: 0.6368
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 31.74
               Mean episode length: 124.59
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 10.91s
                        Total time: 33339.70s
                               ETA: 1260414.2s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.669s, learning 0.166s)
               Value function loss: 0.7571
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 32.13
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 10.84s
                        Total time: 33350.54s
                               ETA: 1260321.8s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.988s, learning 0.161s)
               Value function loss: 0.6889
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 31.62
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 11.15s
                        Total time: 33361.69s
                               ETA: 1260241.3s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.958s, learning 0.197s)
               Value function loss: 0.8979
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 30.51
               Mean episode length: 122.33
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 11.16s
                        Total time: 33372.84s
                               ETA: 1260161.1s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.297s, learning 0.267s)
               Value function loss: 0.6300
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 31.34
               Mean episode length: 123.70
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 11.56s
                        Total time: 33384.41s
                               ETA: 1260096.4s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.049s, learning 0.162s)
               Value function loss: 0.7082
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 31.22
               Mean episode length: 124.19
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 11.21s
                        Total time: 33395.62s
                               ETA: 1260018.4s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.859s, learning 0.168s)
               Value function loss: 0.7204
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 30.86
               Mean episode length: 123.56
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 11.03s
                        Total time: 33406.64s
                               ETA: 1259933.6s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.519s, learning 0.162s)
               Value function loss: 0.6735
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 30.81
               Mean episode length: 123.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 10.68s
                        Total time: 33417.32s
                               ETA: 1259835.7s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.933s, learning 0.163s)
               Value function loss: 0.7367
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: 31.30
               Mean episode length: 122.53
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 11.10s
                        Total time: 33428.42s
                               ETA: 1259753.6s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.061s, learning 0.189s)
               Value function loss: 0.9273
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 32.44
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 11.25s
                        Total time: 33439.67s
                               ETA: 1259677.3s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.744s, learning 0.160s)
               Value function loss: 0.9790
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 31.50
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 10.90s
                        Total time: 33450.58s
                               ETA: 1259588.1s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.164s)
               Value function loss: 0.7927
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 31.36
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 11.29s
                        Total time: 33461.87s
                               ETA: 1259513.6s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.168s)
               Value function loss: 0.8576
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 31.05
               Mean episode length: 124.94
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 11.08s
                        Total time: 33472.95s
                               ETA: 1259431.1s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.593s, learning 0.228s)
               Value function loss: 0.9522
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 30.26
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 10.82s
                        Total time: 33483.77s
                               ETA: 1259338.9s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.164s)
               Value function loss: 0.9221
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 29.98
               Mean episode length: 124.29
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 11.19s
                        Total time: 33494.96s
                               ETA: 1259260.5s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.036s, learning 0.192s)
               Value function loss: 0.7917
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 30.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 11.23s
                        Total time: 33506.19s
                               ETA: 1259183.7s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.518s, learning 0.167s)
               Value function loss: 0.7795
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 30.98
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 10.69s
                        Total time: 33516.87s
                               ETA: 1259086.6s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.755s, learning 0.155s)
               Value function loss: 0.6777
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 30.57
               Mean episode length: 124.61
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 10.91s
                        Total time: 33527.78s
                               ETA: 1258998.0s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.373s, learning 0.194s)
               Value function loss: 0.7106
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 29.90
               Mean episode length: 124.58
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 11.57s
                        Total time: 33539.35s
                               ETA: 1258934.1s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.887s, learning 0.177s)
               Value function loss: 0.6768
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 30.36
               Mean episode length: 124.86
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 11.06s
                        Total time: 33550.42s
                               ETA: 1258851.4s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.719s, learning 0.160s)
               Value function loss: 0.6165
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 30.48
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 10.88s
                        Total time: 33561.29s
                               ETA: 1258761.8s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.723s, learning 0.158s)
               Value function loss: 0.6840
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 30.49
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 10.88s
                        Total time: 33572.18s
                               ETA: 1258672.3s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.162s)
               Value function loss: 0.7009
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 30.74
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 11.36s
                        Total time: 33583.54s
                               ETA: 1258600.9s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.025s, learning 0.176s)
               Value function loss: 0.5950
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 31.07
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 11.20s
                        Total time: 33594.74s
                               ETA: 1258523.6s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.416s, learning 0.188s)
               Value function loss: 0.6698
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 30.92
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 10.60s
                        Total time: 33605.34s
                               ETA: 1258423.9s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.118s, learning 0.158s)
               Value function loss: 0.9105
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 31.87
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 11.28s
                        Total time: 33616.62s
                               ETA: 1258349.4s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.929s, learning 0.193s)
               Value function loss: 0.8585
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 30.08
               Mean episode length: 123.69
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 11.12s
                        Total time: 33627.74s
                               ETA: 1258269.2s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 921 steps/s (collection: 17.601s, learning 0.172s)
               Value function loss: 0.7705
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 31.05
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 17.77s
                        Total time: 33645.51s
                               ETA: 1258437.9s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.580s, learning 0.169s)
               Value function loss: 0.8188
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 31.79
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 21.75s
                        Total time: 33667.26s
                               ETA: 1258755.0s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.238s, learning 0.164s)
               Value function loss: 0.9692
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 32.09
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 21.40s
                        Total time: 33688.67s
                               ETA: 1259058.9s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.850s, learning 0.183s)
               Value function loss: 0.7727
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 31.36
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 22.03s
                        Total time: 33710.70s
                               ETA: 1259386.2s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.454s, learning 0.203s)
               Value function loss: 0.7945
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 30.81
               Mean episode length: 123.67
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 21.66s
                        Total time: 33732.36s
                               ETA: 1259699.1s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.454s, learning 0.239s)
               Value function loss: 0.7547
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 31.31
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 21.69s
                        Total time: 33754.05s
                               ETA: 1260013.1s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.638s, learning 0.187s)
               Value function loss: 0.7392
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 31.75
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 21.82s
                        Total time: 33775.87s
                               ETA: 1260331.8s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.541s, learning 0.262s)
               Value function loss: 0.7692
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 32.57
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 21.80s
                        Total time: 33797.68s
                               ETA: 1260649.4s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.261s, learning 0.170s)
               Value function loss: 0.6553
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 31.24
               Mean episode length: 124.91
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 21.43s
                        Total time: 33819.11s
                               ETA: 1260952.9s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 742 steps/s (collection: 21.914s, learning 0.161s)
               Value function loss: 0.7126
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 31.32
               Mean episode length: 124.47
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 22.07s
                        Total time: 33841.18s
                               ETA: 1261280.1s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 781 steps/s (collection: 20.774s, learning 0.198s)
               Value function loss: 0.6493
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 31.79
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 20.97s
                        Total time: 33862.15s
                               ETA: 1261566.0s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.077s, learning 0.280s)
               Value function loss: 0.6609
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 31.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 21.36s
                        Total time: 33883.51s
                               ETA: 1261865.9s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 772 steps/s (collection: 21.041s, learning 0.181s)
               Value function loss: 0.5827
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 31.03
               Mean episode length: 123.32
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 21.22s
                        Total time: 33904.73s
                               ETA: 1262160.7s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 771 steps/s (collection: 21.038s, learning 0.188s)
               Value function loss: 0.6758
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 30.78
               Mean episode length: 124.86
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 21.23s
                        Total time: 33925.96s
                               ETA: 1262455.3s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.772s, learning 0.171s)
               Value function loss: 0.8969
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 31.84
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 20.94s
                        Total time: 33946.90s
                               ETA: 1262739.1s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.201s, learning 0.197s)
               Value function loss: 0.7346
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 31.76
               Mean episode length: 124.52
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 21.40s
                        Total time: 33968.30s
                               ETA: 1263039.6s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.866s, learning 0.182s)
               Value function loss: 0.6801
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 32.60
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 22.05s
                        Total time: 33990.35s
                               ETA: 1263364.1s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 771 steps/s (collection: 21.065s, learning 0.167s)
               Value function loss: 0.7903
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 32.50
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 21.23s
                        Total time: 34011.58s
                               ETA: 1263657.9s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.315s, learning 0.259s)
               Value function loss: 0.8636
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 31.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 21.57s
                        Total time: 34033.15s
                               ETA: 1263964.3s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 766 steps/s (collection: 21.226s, learning 0.163s)
               Value function loss: 0.8150
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 32.39
               Mean episode length: 123.30
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 21.39s
                        Total time: 34054.54s
                               ETA: 1264263.5s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.751s, learning 0.198s)
               Value function loss: 0.5823
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 31.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 20.95s
                        Total time: 34075.49s
                               ETA: 1264546.1s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.591s, learning 0.174s)
               Value function loss: 0.6724
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 31.89
               Mean episode length: 124.06
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 21.76s
                        Total time: 34097.25s
                               ETA: 1264858.8s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 769 steps/s (collection: 21.088s, learning 0.193s)
               Value function loss: 0.6182
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 32.65
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 21.28s
                        Total time: 34118.54s
                               ETA: 1265153.2s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 772 steps/s (collection: 21.061s, learning 0.161s)
               Value function loss: 0.7521
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 32.96
               Mean episode length: 124.59
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 21.22s
                        Total time: 34139.76s
                               ETA: 1265445.3s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.769s, learning 0.170s)
               Value function loss: 0.5817
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 32.24
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 20.94s
                        Total time: 34160.70s
                               ETA: 1265726.6s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.466s, learning 0.198s)
               Value function loss: 0.6446
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 31.01
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 21.66s
                        Total time: 34182.36s
                               ETA: 1266034.5s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 772 steps/s (collection: 21.035s, learning 0.179s)
               Value function loss: 0.5469
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 30.94
               Mean episode length: 123.45
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 21.21s
                        Total time: 34203.57s
                               ETA: 1266325.5s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 765 steps/s (collection: 21.236s, learning 0.167s)
               Value function loss: 0.4774
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 30.81
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 21.40s
                        Total time: 34224.98s
                               ETA: 1266623.4s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.906s, learning 0.194s)
               Value function loss: 0.5212
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 31.39
               Mean episode length: 123.72
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 21.10s
                        Total time: 34246.08s
                               ETA: 1266909.7s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.646s, learning 0.168s)
               Value function loss: 0.5812
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 30.82
               Mean episode length: 124.52
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 21.81s
                        Total time: 34267.89s
                               ETA: 1267222.2s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.456s, learning 0.190s)
               Value function loss: 0.6115
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: 29.92
               Mean episode length: 124.33
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 21.65s
                        Total time: 34289.54s
                               ETA: 1267528.3s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.408s, learning 0.167s)
               Value function loss: 0.5655
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 30.59
               Mean episode length: 124.24
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 21.57s
                        Total time: 34311.11s
                               ETA: 1267831.4s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 769 steps/s (collection: 21.132s, learning 0.172s)
               Value function loss: 0.5916
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 31.68
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 21.30s
                        Total time: 34332.42s
                               ETA: 1268124.3s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.255s, learning 0.170s)
               Value function loss: 0.6345
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 30.62
               Mean episode length: 124.69
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 21.43s
                        Total time: 34353.84s
                               ETA: 1268421.5s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 754 steps/s (collection: 21.457s, learning 0.252s)
               Value function loss: 0.6225
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 32.34
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 21.71s
                        Total time: 34375.55s
                               ETA: 1268728.9s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 768 steps/s (collection: 21.148s, learning 0.171s)
               Value function loss: 0.5200
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 31.51
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 21.32s
                        Total time: 34396.87s
                               ETA: 1269021.6s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 767 steps/s (collection: 21.197s, learning 0.163s)
               Value function loss: 0.5539
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 31.52
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 21.36s
                        Total time: 34418.23s
                               ETA: 1269315.7s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.636s, learning 0.198s)
               Value function loss: 0.5358
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 32.19
               Mean episode length: 124.36
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 21.83s
                        Total time: 34440.06s
                               ETA: 1269626.9s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.900s, learning 0.205s)
               Value function loss: 0.5758
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 31.62
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 11.10s
                        Total time: 34451.17s
                               ETA: 1269542.5s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.689s, learning 0.160s)
               Value function loss: 0.5129
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 31.03
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 10.85s
                        Total time: 34462.02s
                               ETA: 1269448.8s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.583s, learning 0.169s)
               Value function loss: 0.5214
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 30.85
               Mean episode length: 124.16
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 10.75s
                        Total time: 34472.77s
                               ETA: 1269351.5s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.015s, learning 0.184s)
               Value function loss: 0.6060
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 31.68
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 11.20s
                        Total time: 34483.97s
                               ETA: 1269270.8s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.834s, learning 0.176s)
               Value function loss: 0.5296
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 31.93
               Mean episode length: 124.39
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 11.01s
                        Total time: 34494.98s
                               ETA: 1269183.2s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.238s, learning 0.168s)
               Value function loss: 0.5430
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 31.43
               Mean episode length: 123.57
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 11.41s
                        Total time: 34506.38s
                               ETA: 1269110.2s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.163s)
               Value function loss: 0.5459
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 31.65
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 10.99s
                        Total time: 34517.38s
                               ETA: 1269022.0s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.880s, learning 0.174s)
               Value function loss: 0.7532
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 31.86
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 11.05s
                        Total time: 34528.43s
                               ETA: 1268936.2s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.731s, learning 0.162s)
               Value function loss: 0.6686
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 30.61
               Mean episode length: 124.13
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 10.89s
                        Total time: 34539.32s
                               ETA: 1268844.4s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.889s, learning 0.202s)
               Value function loss: 0.5815
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 32.08
               Mean episode length: 124.66
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 11.09s
                        Total time: 34550.42s
                               ETA: 1268760.1s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.722s, learning 0.192s)
               Value function loss: 0.6796
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 31.70
               Mean episode length: 124.98
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 10.91s
                        Total time: 34561.33s
                               ETA: 1268669.2s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.684s, learning 0.160s)
               Value function loss: 0.7510
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 32.72
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 10.84s
                        Total time: 34572.17s
                               ETA: 1268575.9s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.714s, learning 0.162s)
               Value function loss: 0.6991
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 30.84
               Mean episode length: 124.33
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 10.88s
                        Total time: 34583.05s
                               ETA: 1268483.8s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.166s)
               Value function loss: 0.4802
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 31.38
               Mean episode length: 123.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 11.49s
                        Total time: 34594.53s
                               ETA: 1268414.1s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.052s, learning 0.178s)
               Value function loss: 0.6560
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 33.10
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 11.23s
                        Total time: 34605.76s
                               ETA: 1268335.1s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.162s)
               Value function loss: 0.6232
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 32.51
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 11.19s
                        Total time: 34616.95s
                               ETA: 1268254.6s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.763s, learning 0.242s)
               Value function loss: 0.7006
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 31.34
               Mean episode length: 123.40
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 11.00s
                        Total time: 34627.96s
                               ETA: 1268167.5s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.809s, learning 0.177s)
               Value function loss: 0.5488
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 32.16
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 10.99s
                        Total time: 34638.94s
                               ETA: 1268079.7s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.743s, learning 0.172s)
               Value function loss: 0.6303
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 32.59
               Mean episode length: 124.08
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 10.91s
                        Total time: 34649.86s
                               ETA: 1267989.4s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.633s, learning 0.179s)
               Value function loss: 0.6232
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 32.40
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 10.81s
                        Total time: 34660.67s
                               ETA: 1267895.4s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.374s, learning 0.189s)
               Value function loss: 0.5514
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 31.62
               Mean episode length: 124.17
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 11.56s
                        Total time: 34672.23s
                               ETA: 1267828.9s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.846s, learning 0.206s)
               Value function loss: 0.6320
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 30.56
               Mean episode length: 124.08
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 11.05s
                        Total time: 34683.28s
                               ETA: 1267743.7s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.789s, learning 0.285s)
               Value function loss: 0.7164
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 31.61
               Mean episode length: 124.41
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 11.07s
                        Total time: 34694.36s
                               ETA: 1267659.4s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.970s, learning 0.158s)
               Value function loss: 0.8573
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 30.02
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 11.13s
                        Total time: 34705.49s
                               ETA: 1267577.2s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.168s)
               Value function loss: 0.7623
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 32.42
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 11.23s
                        Total time: 34716.72s
                               ETA: 1267498.8s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.842s, learning 0.165s)
               Value function loss: 0.7157
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 32.23
               Mean episode length: 124.76
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 11.01s
                        Total time: 34727.73s
                               ETA: 1267412.2s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.926s, learning 0.209s)
               Value function loss: 0.8255
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 32.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 11.13s
                        Total time: 34738.86s
                               ETA: 1267330.4s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.532s, learning 0.178s)
               Value function loss: 0.9780
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 31.68
               Mean episode length: 124.94
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 10.71s
                        Total time: 34749.57s
                               ETA: 1267233.1s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.692s, learning 0.176s)
               Value function loss: 1.0006
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 31.92
               Mean episode length: 123.36
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 10.87s
                        Total time: 34760.44s
                               ETA: 1267141.6s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.942s, learning 0.158s)
               Value function loss: 0.9612
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 32.70
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 11.10s
                        Total time: 34771.54s
                               ETA: 1267058.7s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.934s, learning 0.177s)
               Value function loss: 0.7743
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 32.81
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 11.11s
                        Total time: 34782.65s
                               ETA: 1266976.2s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.983s, learning 0.164s)
               Value function loss: 0.8672
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 33.55
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 11.15s
                        Total time: 34793.80s
                               ETA: 1266895.1s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.808s, learning 0.181s)
               Value function loss: 0.8888
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 33.75
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 10.99s
                        Total time: 34804.79s
                               ETA: 1266808.3s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.808s, learning 0.252s)
               Value function loss: 0.7736
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 33.34
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 11.06s
                        Total time: 34815.85s
                               ETA: 1266724.1s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.999s, learning 0.185s)
               Value function loss: 0.8085
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 32.73
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 11.18s
                        Total time: 34827.03s
                               ETA: 1266644.5s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.689s, learning 0.164s)
               Value function loss: 0.6895
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 32.98
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 10.85s
                        Total time: 34837.88s
                               ETA: 1266552.9s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.649s, learning 0.164s)
               Value function loss: 0.6695
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 34.12
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 10.81s
                        Total time: 34848.70s
                               ETA: 1266459.9s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.844s, learning 0.157s)
               Value function loss: 0.6074
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 32.99
               Mean episode length: 124.34
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 11.00s
                        Total time: 34859.70s
                               ETA: 1266373.8s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.652s, learning 0.161s)
               Value function loss: 0.7833
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 34.03
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 10.81s
                        Total time: 34870.51s
                               ETA: 1266281.0s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.943s, learning 0.158s)
               Value function loss: 0.7518
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 33.34
               Mean episode length: 123.38
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 11.10s
                        Total time: 34881.61s
                               ETA: 1266198.6s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.551s, learning 0.189s)
               Value function loss: 0.6593
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 33.11
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 10.74s
                        Total time: 34892.35s
                               ETA: 1266103.2s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.087s, learning 0.201s)
               Value function loss: 0.7140
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 32.86
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 11.29s
                        Total time: 34903.64s
                               ETA: 1266027.7s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.751s, learning 0.229s)
               Value function loss: 0.8307
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 34.43
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 10.98s
                        Total time: 34914.62s
                               ETA: 1265941.2s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.805s, learning 0.190s)
               Value function loss: 0.8812
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 33.14
               Mean episode length: 124.84
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 10.99s
                        Total time: 34925.62s
                               ETA: 1265855.2s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.624s, learning 0.183s)
               Value function loss: 0.6654
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 33.60
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 10.81s
                        Total time: 34936.42s
                               ETA: 1265762.4s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.158s)
               Value function loss: 0.7514
                    Surrogate loss: -0.0240
             Mean action noise std: 0.71
                       Mean reward: 33.65
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 10.81s
                        Total time: 34947.24s
                               ETA: 1265670.0s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.542s, learning 0.165s)
               Value function loss: 0.6757
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 33.03
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 10.71s
                        Total time: 34957.94s
                               ETA: 1265573.8s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.848s, learning 0.160s)
               Value function loss: 0.7228
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 34.16
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 11.01s
                        Total time: 34968.95s
                               ETA: 1265488.5s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.229s, learning 0.172s)
               Value function loss: 0.6922
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 33.57
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 11.40s
                        Total time: 34980.35s
                               ETA: 1265417.5s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.054s, learning 0.219s)
               Value function loss: 0.7184
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 32.69
               Mean episode length: 123.62
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 11.27s
                        Total time: 34991.63s
                               ETA: 1265341.9s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.915s, learning 0.162s)
               Value function loss: 0.6965
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 34.78
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 11.08s
                        Total time: 35002.70s
                               ETA: 1265259.3s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.826s, learning 0.187s)
               Value function loss: 0.8035
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 33.48
               Mean episode length: 124.37
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 11.01s
                        Total time: 35013.72s
                               ETA: 1265174.4s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.827s, learning 0.173s)
               Value function loss: 0.7002
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 34.49
               Mean episode length: 124.34
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 11.00s
                        Total time: 35024.72s
                               ETA: 1265089.1s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.597s, learning 0.176s)
               Value function loss: 0.7887
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 34.15
               Mean episode length: 124.29
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 10.77s
                        Total time: 35035.49s
                               ETA: 1264995.7s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.160s)
               Value function loss: 1.1527
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 35.80
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 10.82s
                        Total time: 35046.31s
                               ETA: 1264903.9s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.802s, learning 0.192s)
               Value function loss: 0.7767
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 34.83
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 10.99s
                        Total time: 35057.30s
                               ETA: 1264818.5s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.782s, learning 0.159s)
               Value function loss: 0.7211
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 34.77
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 10.94s
                        Total time: 35068.24s
                               ETA: 1264731.3s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.687s, learning 0.188s)
               Value function loss: 0.8239
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 35.24
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 10.88s
                        Total time: 35079.12s
                               ETA: 1264641.8s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.827s, learning 0.168s)
               Value function loss: 0.8792
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 33.03
               Mean episode length: 124.31
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 10.99s
                        Total time: 35090.11s
                               ETA: 1264556.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.166s)
               Value function loss: 0.9353
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 34.38
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 10.88s
                        Total time: 35100.99s
                               ETA: 1264467.3s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.924s, learning 0.217s)
               Value function loss: 0.6721
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 34.94
               Mean episode length: 124.34
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 11.14s
                        Total time: 35112.13s
                               ETA: 1264387.5s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.581s, learning 0.186s)
               Value function loss: 0.7363
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 34.73
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 10.77s
                        Total time: 35122.90s
                               ETA: 1264294.4s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.990s, learning 0.168s)
               Value function loss: 0.7001
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 34.66
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 11.16s
                        Total time: 35134.06s
                               ETA: 1264215.3s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.093s, learning 0.213s)
               Value function loss: 0.8430
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 35.26
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 11.31s
                        Total time: 35145.36s
                               ETA: 1264141.6s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.160s)
               Value function loss: 0.7034
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 34.42
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 11.25s
                        Total time: 35156.61s
                               ETA: 1264065.8s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.582s, learning 0.191s)
               Value function loss: 0.7702
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 34.75
               Mean episode length: 124.86
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 10.77s
                        Total time: 35167.38s
                               ETA: 1263973.1s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.599s, learning 0.198s)
               Value function loss: 0.6801
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 34.21
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 10.80s
                        Total time: 35178.18s
                               ETA: 1263881.3s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.523s, learning 0.230s)
               Value function loss: 0.6926
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 35.05
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 10.75s
                        Total time: 35188.93s
                               ETA: 1263787.9s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.030s, learning 0.171s)
               Value function loss: 0.7209
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 34.68
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 11.20s
                        Total time: 35200.13s
                               ETA: 1263710.7s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.551s, learning 0.168s)
               Value function loss: 0.8582
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 34.82
               Mean episode length: 124.24
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 10.72s
                        Total time: 35210.85s
                               ETA: 1263616.3s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.786s, learning 0.167s)
               Value function loss: 0.9162
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 34.13
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 10.95s
                        Total time: 35221.80s
                               ETA: 1263530.3s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.991s, learning 0.162s)
               Value function loss: 0.7242
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 33.71
               Mean episode length: 124.28
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 11.15s
                        Total time: 35232.96s
                               ETA: 1263451.5s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.507s, learning 0.208s)
               Value function loss: 0.7935
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 34.30
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 11.71s
                        Total time: 35244.67s
                               ETA: 1263393.0s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.355s, learning 0.180s)
               Value function loss: 0.7437
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 33.71
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 11.53s
                        Total time: 35256.21s
                               ETA: 1263328.0s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.025s, learning 0.190s)
               Value function loss: 0.8232
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 33.35
               Mean episode length: 123.51
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 11.21s
                        Total time: 35267.42s
                               ETA: 1263251.5s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.415s, learning 0.162s)
               Value function loss: 0.7181
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 34.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 11.58s
                        Total time: 35279.00s
                               ETA: 1263188.1s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.162s)
               Value function loss: 0.6754
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 33.90
               Mean episode length: 124.51
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 11.33s
                        Total time: 35290.33s
                               ETA: 1263116.1s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.756s, learning 0.174s)
               Value function loss: 0.6403
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 33.84
               Mean episode length: 123.68
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 10.93s
                        Total time: 35301.26s
                               ETA: 1263029.6s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.885s, learning 0.179s)
               Value function loss: 0.6928
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 35.19
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 11.06s
                        Total time: 35312.33s
                               ETA: 1262948.0s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.047s, learning 0.193s)
               Value function loss: 0.7038
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 34.23
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 11.24s
                        Total time: 35323.57s
                               ETA: 1262872.7s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.891s, learning 0.201s)
               Value function loss: 0.6804
                    Surrogate loss: -0.0258
             Mean action noise std: 0.71
                       Mean reward: 33.11
               Mean episode length: 124.59
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 11.09s
                        Total time: 35334.66s
                               ETA: 1262792.2s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.823s, learning 0.193s)
               Value function loss: 0.7254
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 34.55
               Mean episode length: 124.34
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 11.02s
                        Total time: 35345.68s
                               ETA: 1262709.0s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.526s, learning 0.167s)
               Value function loss: 0.4714
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 34.12
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 11.69s
                        Total time: 35357.37s
                               ETA: 1262650.1s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.784s, learning 0.208s)
               Value function loss: 0.5130
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 33.43
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 10.99s
                        Total time: 35368.36s
                               ETA: 1262566.1s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.493s, learning 0.160s)
               Value function loss: 0.6591
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 35.33
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 10.65s
                        Total time: 35379.01s
                               ETA: 1262470.1s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.162s)
               Value function loss: 0.8407
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 34.28
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 10.93s
                        Total time: 35389.94s
                               ETA: 1262383.9s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.294s, learning 0.194s)
               Value function loss: 0.9214
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 34.90
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 11.49s
                        Total time: 35401.43s
                               ETA: 1262317.8s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.009s, learning 0.242s)
               Value function loss: 0.8510
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 34.67
               Mean episode length: 124.99
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 11.25s
                        Total time: 35412.68s
                               ETA: 1262243.3s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.879s, learning 0.179s)
               Value function loss: 0.8167
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 34.53
               Mean episode length: 124.98
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 11.06s
                        Total time: 35423.74s
                               ETA: 1262162.0s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.502s, learning 0.168s)
               Value function loss: 0.9681
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: 34.30
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 10.67s
                        Total time: 35434.41s
                               ETA: 1262066.9s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.922s, learning 0.189s)
               Value function loss: 0.9402
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 34.12
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 11.11s
                        Total time: 35445.52s
                               ETA: 1261987.5s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.033s, learning 0.190s)
               Value function loss: 0.7268
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 33.86
               Mean episode length: 124.38
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 11.22s
                        Total time: 35456.74s
                               ETA: 1261912.2s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.859s, learning 0.190s)
               Value function loss: 0.8881
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 33.88
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 11.05s
                        Total time: 35467.79s
                               ETA: 1261830.7s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.721s, learning 0.285s)
               Value function loss: 0.8991
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 34.43
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 11.01s
                        Total time: 35478.79s
                               ETA: 1261747.8s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.175s, learning 0.164s)
               Value function loss: 0.9018
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 34.16
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 11.34s
                        Total time: 35490.13s
                               ETA: 1261676.8s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.593s, learning 0.165s)
               Value function loss: 0.8024
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 34.74
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 10.76s
                        Total time: 35500.89s
                               ETA: 1261585.2s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.676s, learning 0.254s)
               Value function loss: 0.8465
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 34.58
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 10.93s
                        Total time: 35511.82s
                               ETA: 1261499.7s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.032s, learning 0.197s)
               Value function loss: 0.7169
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 11.23s
                        Total time: 35523.05s
                               ETA: 1261424.9s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.562s, learning 0.167s)
               Value function loss: 0.7400
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 34.52
               Mean episode length: 124.29
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 10.73s
                        Total time: 35533.78s
                               ETA: 1261332.4s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.751s, learning 0.163s)
               Value function loss: 0.6916
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 33.06
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 10.91s
                        Total time: 35544.69s
                               ETA: 1261246.6s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.929s, learning 0.159s)
               Value function loss: 0.7919
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 34.96
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 11.09s
                        Total time: 35555.78s
                               ETA: 1261166.9s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.790s, learning 0.168s)
               Value function loss: 1.0616
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: 34.93
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 10.96s
                        Total time: 35566.74s
                               ETA: 1261082.8s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.610s, learning 0.188s)
               Value function loss: 0.9305
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 34.46
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 10.80s
                        Total time: 35577.54s
                               ETA: 1260992.9s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.891s, learning 0.210s)
               Value function loss: 0.8614
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 36.21
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 11.10s
                        Total time: 35588.64s
                               ETA: 1260913.9s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.816s, learning 0.215s)
               Value function loss: 0.9329
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 35.09
               Mean episode length: 124.92
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 11.03s
                        Total time: 35599.67s
                               ETA: 1260832.5s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.277s, learning 0.169s)
               Value function loss: 0.9952
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 32.90
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 11.45s
                        Total time: 35611.12s
                               ETA: 1260765.8s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.889s, learning 0.197s)
               Value function loss: 0.9962
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: 33.78
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 11.09s
                        Total time: 35622.20s
                               ETA: 1260686.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.168s)
               Value function loss: 0.7728
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 34.76
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 11.43s
                        Total time: 35633.63s
                               ETA: 1260619.2s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.562s, learning 0.165s)
               Value function loss: 0.8137
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 35.10
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 10.73s
                        Total time: 35644.36s
                               ETA: 1260527.1s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.933s, learning 0.168s)
               Value function loss: 0.8513
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 35.71
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 11.10s
                        Total time: 35655.46s
                               ETA: 1260448.4s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.930s, learning 0.184s)
               Value function loss: 0.9112
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 36.12
               Mean episode length: 124.08
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 11.11s
                        Total time: 35666.57s
                               ETA: 1260370.2s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.123s, learning 0.172s)
               Value function loss: 0.8212
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 36.22
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 11.29s
                        Total time: 35677.87s
                               ETA: 1260298.4s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.610s, learning 0.200s)
               Value function loss: 1.0282
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 36.22
               Mean episode length: 124.64
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 10.81s
                        Total time: 35688.68s
                               ETA: 1260209.5s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.978s, learning 0.186s)
               Value function loss: 0.8503
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 36.35
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 11.16s
                        Total time: 35699.84s
                               ETA: 1260133.2s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.329s, learning 0.189s)
               Value function loss: 0.7668
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 35.34
               Mean episode length: 124.06
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 11.52s
                        Total time: 35711.36s
                               ETA: 1260069.4s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.322s, learning 0.198s)
               Value function loss: 0.8500
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 36.25
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 11.52s
                        Total time: 35722.88s
                               ETA: 1260005.8s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.883s, learning 0.169s)
               Value function loss: 1.1560
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 37.76
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 11.05s
                        Total time: 35733.93s
                               ETA: 1259925.6s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.776s, learning 0.191s)
               Value function loss: 1.0467
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 37.42
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 10.97s
                        Total time: 35744.90s
                               ETA: 1259842.6s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.827s, learning 0.198s)
               Value function loss: 0.9095
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 37.12
               Mean episode length: 124.84
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 11.03s
                        Total time: 35755.93s
                               ETA: 1259761.6s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.207s, learning 0.196s)
               Value function loss: 1.0876
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 37.21
               Mean episode length: 124.78
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 11.40s
                        Total time: 35767.33s
                               ETA: 1259693.9s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.796s, learning 0.167s)
               Value function loss: 1.2592
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 35.69
               Mean episode length: 124.57
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 10.96s
                        Total time: 35778.29s
                               ETA: 1259610.9s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.691s, learning 0.168s)
               Value function loss: 1.1786
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 35.67
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 10.86s
                        Total time: 35789.15s
                               ETA: 1259524.2s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.795s, learning 0.202s)
               Value function loss: 0.8530
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 35.22
               Mean episode length: 124.92
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 11.00s
                        Total time: 35800.15s
                               ETA: 1259442.5s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.491s, learning 0.161s)
               Value function loss: 0.8834
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 37.97
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 10.65s
                        Total time: 35810.80s
                               ETA: 1259348.6s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.163s)
               Value function loss: 0.9492
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 36.81
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 11.14s
                        Total time: 35821.94s
                               ETA: 1259272.0s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.730s, learning 0.181s)
               Value function loss: 0.9359
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 36.21
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 10.91s
                        Total time: 35832.85s
                               ETA: 1259187.4s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.365s, learning 0.161s)
               Value function loss: 0.9612
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 37.55
               Mean episode length: 124.27
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 11.53s
                        Total time: 35844.38s
                               ETA: 1259124.4s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.105s, learning 0.189s)
               Value function loss: 1.0231
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 38.78
               Mean episode length: 124.63
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 11.29s
                        Total time: 35855.67s
                               ETA: 1259053.4s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.135s, learning 0.194s)
               Value function loss: 1.1254
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 38.18
               Mean episode length: 124.69
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 11.33s
                        Total time: 35867.00s
                               ETA: 1258983.5s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.825s, learning 0.289s)
               Value function loss: 1.0068
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 38.91
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 11.11s
                        Total time: 35878.11s
                               ETA: 1258906.2s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.175s, learning 0.168s)
               Value function loss: 0.9715
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 38.05
               Mean episode length: 124.82
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 11.34s
                        Total time: 35889.46s
                               ETA: 1258837.0s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.395s, learning 0.280s)
               Value function loss: 0.9038
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 37.86
               Mean episode length: 124.14
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 11.67s
                        Total time: 35901.13s
                               ETA: 1258779.4s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.277s, learning 0.209s)
               Value function loss: 1.2453
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 38.22
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 11.49s
                        Total time: 35912.62s
                               ETA: 1258715.3s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.908s, learning 0.191s)
               Value function loss: 1.2960
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 38.61
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 11.10s
                        Total time: 35923.72s
                               ETA: 1258637.6s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.162s)
               Value function loss: 1.0964
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 37.76
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 11.48s
                        Total time: 35935.20s
                               ETA: 1258573.3s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.821s, learning 0.187s)
               Value function loss: 1.3171
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 37.70
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 11.01s
                        Total time: 35946.20s
                               ETA: 1258492.5s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.805s, learning 0.166s)
               Value function loss: 1.5132
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 40.11
               Mean episode length: 124.87
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 10.97s
                        Total time: 35957.18s
                               ETA: 1258410.5s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.318s, learning 0.171s)
               Value function loss: 1.3932
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 37.57
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 11.49s
                        Total time: 35968.66s
                               ETA: 1258346.7s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.079s, learning 0.161s)
               Value function loss: 0.9792
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 37.04
               Mean episode length: 124.07
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 11.24s
                        Total time: 35979.90s
                               ETA: 1258274.2s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.090s, learning 0.228s)
               Value function loss: 1.1348
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 37.45
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 11.32s
                        Total time: 35991.22s
                               ETA: 1258204.4s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.073s, learning 0.179s)
               Value function loss: 1.2474
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 38.25
               Mean episode length: 123.75
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 11.25s
                        Total time: 36002.47s
                               ETA: 1258132.5s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.999s, learning 0.171s)
               Value function loss: 1.3883
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 38.66
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 11.17s
                        Total time: 36013.64s
                               ETA: 1258057.7s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.740s, learning 0.160s)
               Value function loss: 1.1127
                    Surrogate loss: -0.0081
             Mean action noise std: 0.71
                       Mean reward: 38.99
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 10.90s
                        Total time: 36024.54s
                               ETA: 1257973.4s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.164s)
               Value function loss: 1.3230
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 36.87
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 10.93s
                        Total time: 36035.47s
                               ETA: 1257890.2s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.910s, learning 0.168s)
               Value function loss: 1.2336
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 38.05
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 11.08s
                        Total time: 36046.55s
                               ETA: 1257812.3s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.310s, learning 0.166s)
               Value function loss: 1.1302
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 37.42
               Mean episode length: 124.98
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 11.48s
                        Total time: 36058.02s
                               ETA: 1257748.4s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.124s, learning 0.193s)
               Value function loss: 1.2967
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 37.87
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 11.32s
                        Total time: 36069.34s
                               ETA: 1257678.9s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.151s, learning 0.220s)
               Value function loss: 1.3451
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 37.85
               Mean episode length: 124.97
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 11.37s
                        Total time: 36080.71s
                               ETA: 1257611.4s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.764s, learning 0.185s)
               Value function loss: 1.5432
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 37.81
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 10.95s
                        Total time: 36091.66s
                               ETA: 1257529.2s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.167s)
               Value function loss: 1.1165
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 38.65
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 10.72s
                        Total time: 36102.39s
                               ETA: 1257439.2s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.687s, learning 0.194s)
               Value function loss: 1.1093
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 38.72
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 10.88s
                        Total time: 36113.27s
                               ETA: 1257354.8s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.182s, learning 0.187s)
               Value function loss: 1.3312
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 36.79
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 11.37s
                        Total time: 36124.64s
                               ETA: 1257287.4s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.164s)
               Value function loss: 1.4552
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 37.45
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 11.17s
                        Total time: 36135.81s
                               ETA: 1257213.1s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.848s, learning 0.193s)
               Value function loss: 1.3169
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 38.09
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 11.04s
                        Total time: 36146.85s
                               ETA: 1257134.4s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.642s, learning 0.178s)
               Value function loss: 1.0416
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 38.40
               Mean episode length: 124.03
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 10.82s
                        Total time: 36157.67s
                               ETA: 1257048.0s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.048s, learning 0.164s)
               Value function loss: 1.0759
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 38.31
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 11.21s
                        Total time: 36168.88s
                               ETA: 1256975.3s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.537s, learning 0.161s)
               Value function loss: 1.0798
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 38.24
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 11.70s
                        Total time: 36180.58s
                               ETA: 1256919.6s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.552s, learning 0.195s)
               Value function loss: 1.3176
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 38.69
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 10.75s
                        Total time: 36191.33s
                               ETA: 1256830.8s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.876s, learning 0.170s)
               Value function loss: 1.0458
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 37.49
               Mean episode length: 124.13
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 11.05s
                        Total time: 36202.37s
                               ETA: 1256752.5s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.928s, learning 0.166s)
               Value function loss: 1.1974
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 37.55
               Mean episode length: 124.63
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 11.09s
                        Total time: 36213.47s
                               ETA: 1256675.8s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.168s)
               Value function loss: 1.0726
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 38.06
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 10.70s
                        Total time: 36224.17s
                               ETA: 1256585.6s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.114s, learning 0.175s)
               Value function loss: 0.9968
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 37.64
               Mean episode length: 123.25
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 11.29s
                        Total time: 36235.46s
                               ETA: 1256515.9s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.650s, learning 0.169s)
               Value function loss: 0.9934
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 39.88
               Mean episode length: 124.82
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 10.82s
                        Total time: 36246.28s
                               ETA: 1256429.9s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.868s, learning 0.190s)
               Value function loss: 1.3389
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 38.34
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 11.06s
                        Total time: 36257.33s
                               ETA: 1256352.2s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.588s, learning 0.172s)
               Value function loss: 1.1852
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 39.44
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 10.76s
                        Total time: 36268.09s
                               ETA: 1256264.2s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.107s, learning 0.166s)
               Value function loss: 1.0708
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 39.05
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 11.27s
                        Total time: 36279.37s
                               ETA: 1256194.1s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.864s, learning 0.159s)
               Value function loss: 1.1856
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 38.48
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 11.02s
                        Total time: 36290.39s
                               ETA: 1256115.3s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.280s, learning 0.221s)
               Value function loss: 1.2042
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 38.69
               Mean episode length: 124.48
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 11.50s
                        Total time: 36301.89s
                               ETA: 1256053.2s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.466s, learning 0.167s)
               Value function loss: 1.2114
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 38.76
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 10.63s
                        Total time: 36312.52s
                               ETA: 1255961.0s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.167s)
               Value function loss: 1.0023
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 38.67
               Mean episode length: 124.07
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 10.94s
                        Total time: 36323.47s
                               ETA: 1255879.6s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.929s, learning 0.183s)
               Value function loss: 1.1772
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 39.29
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 11.11s
                        Total time: 36334.58s
                               ETA: 1255804.2s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.488s, learning 0.172s)
               Value function loss: 1.0725
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 38.96
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 11.66s
                        Total time: 36346.24s
                               ETA: 1255747.7s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.623s, learning 0.172s)
               Value function loss: 1.0392
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 39.32
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 10.79s
                        Total time: 36357.03s
                               ETA: 1255661.3s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.416s, learning 0.176s)
               Value function loss: 0.9810
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 38.87
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 10.59s
                        Total time: 36367.62s
                               ETA: 1255568.0s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.969s, learning 0.173s)
               Value function loss: 1.1402
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 39.59
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 11.14s
                        Total time: 36378.77s
                               ETA: 1255493.7s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.403s, learning 0.300s)
               Value function loss: 1.1225
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 38.27
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 11.70s
                        Total time: 36390.47s
                               ETA: 1255438.9s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.163s)
               Value function loss: 1.1206
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 38.60
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 11.37s
                        Total time: 36401.84s
                               ETA: 1255372.7s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.738s, learning 0.210s)
               Value function loss: 1.0734
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 40.64
               Mean episode length: 124.19
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 10.95s
                        Total time: 36412.79s
                               ETA: 1255291.9s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.200s, learning 0.346s)
               Value function loss: 1.1083
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 40.43
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 11.55s
                        Total time: 36424.34s
                               ETA: 1255231.7s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.217s, learning 0.165s)
               Value function loss: 1.3868
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 40.59
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 11.38s
                        Total time: 36435.72s
                               ETA: 1255165.9s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.663s, learning 0.227s)
               Value function loss: 1.2063
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 39.08
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 10.89s
                        Total time: 36446.61s
                               ETA: 1255083.3s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.913s, learning 0.206s)
               Value function loss: 1.0361
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 40.29
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 11.12s
                        Total time: 36457.73s
                               ETA: 1255008.5s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.516s, learning 0.170s)
               Value function loss: 1.2068
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 39.03
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 11.69s
                        Total time: 36469.41s
                               ETA: 1254953.3s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.768s, learning 0.169s)
               Value function loss: 1.4176
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 39.62
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 10.94s
                        Total time: 36480.35s
                               ETA: 1254872.4s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.881s, learning 0.196s)
               Value function loss: 1.2102
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 39.48
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 11.08s
                        Total time: 36491.43s
                               ETA: 1254796.4s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.420s, learning 0.162s)
               Value function loss: 1.0137
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 38.35
               Mean episode length: 124.80
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 11.58s
                        Total time: 36503.01s
                               ETA: 1254737.8s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.842s, learning 0.226s)
               Value function loss: 1.1054
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 39.72
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 11.07s
                        Total time: 36514.08s
                               ETA: 1254661.5s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.761s, learning 0.241s)
               Value function loss: 1.1708
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 39.19
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 11.00s
                        Total time: 36525.08s
                               ETA: 1254583.0s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.034s, learning 0.191s)
               Value function loss: 1.2696
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 39.44
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 11.23s
                        Total time: 36536.31s
                               ETA: 1254512.2s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.764s, learning 0.173s)
               Value function loss: 0.9988
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 38.76
               Mean episode length: 124.42
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 10.94s
                        Total time: 36547.24s
                               ETA: 1254431.6s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.947s, learning 0.241s)
               Value function loss: 1.1555
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 38.67
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 11.19s
                        Total time: 36558.43s
                               ETA: 1254359.6s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.607s, learning 0.181s)
               Value function loss: 0.9940
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 37.82
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 10.79s
                        Total time: 36569.22s
                               ETA: 1254273.9s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.357s, learning 0.216s)
               Value function loss: 0.9778
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 38.38
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 11.57s
                        Total time: 36580.79s
                               ETA: 1254215.2s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.716s, learning 0.175s)
               Value function loss: 0.9687
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 37.83
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 10.89s
                        Total time: 36591.68s
                               ETA: 1254133.2s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.533s, learning 0.177s)
               Value function loss: 1.1710
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 37.96
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 10.71s
                        Total time: 36602.39s
                               ETA: 1254045.0s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.205s, learning 0.184s)
               Value function loss: 1.3492
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 36.88
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 11.39s
                        Total time: 36613.78s
                               ETA: 1253980.1s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.094s, learning 0.237s)
               Value function loss: 1.0876
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 37.30
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 11.33s
                        Total time: 36625.11s
                               ETA: 1253913.3s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.558s, learning 0.162s)
               Value function loss: 1.0673
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 36.37
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 10.72s
                        Total time: 36635.83s
                               ETA: 1253825.6s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.480s, learning 0.239s)
               Value function loss: 1.2144
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 38.73
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 11.72s
                        Total time: 36647.55s
                               ETA: 1253772.1s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.068s, learning 0.169s)
               Value function loss: 1.2264
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 36.83
               Mean episode length: 124.93
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 12.24s
                        Total time: 36659.79s
                               ETA: 1253736.4s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.108s, learning 0.191s)
               Value function loss: 1.0302
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 37.06
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 11.30s
                        Total time: 36671.09s
                               ETA: 1253668.6s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.649s, learning 0.161s)
               Value function loss: 0.9595
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 37.76
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 11.81s
                        Total time: 36682.90s
                               ETA: 1253618.4s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.939s, learning 0.177s)
               Value function loss: 0.9396
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 37.71
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 11.12s
                        Total time: 36694.01s
                               ETA: 1253544.4s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.659s, learning 0.187s)
               Value function loss: 1.0192
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 37.67
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 10.85s
                        Total time: 36704.86s
                               ETA: 1253461.3s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.043s, learning 0.279s)
               Value function loss: 1.0165
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 37.28
               Mean episode length: 124.77
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 11.32s
                        Total time: 36716.18s
                               ETA: 1253394.5s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.125s, learning 0.164s)
               Value function loss: 0.8789
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 37.58
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 11.29s
                        Total time: 36727.47s
                               ETA: 1253326.6s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.600s, learning 0.249s)
               Value function loss: 1.0593
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 37.63
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 10.85s
                        Total time: 36738.32s
                               ETA: 1253243.7s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.955s, learning 0.171s)
               Value function loss: 1.1278
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 37.48
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 11.13s
                        Total time: 36749.45s
                               ETA: 1253170.3s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.930s, learning 0.192s)
               Value function loss: 0.8980
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 37.71
               Mean episode length: 124.25
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 11.12s
                        Total time: 36760.57s
                               ETA: 1253096.8s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.861s, learning 0.205s)
               Value function loss: 0.9770
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 36.97
               Mean episode length: 124.12
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 11.07s
                        Total time: 36771.63s
                               ETA: 1253021.5s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.665s, learning 0.210s)
               Value function loss: 1.2634
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 37.25
               Mean episode length: 124.71
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 10.87s
                        Total time: 36782.51s
                               ETA: 1252939.7s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.859s, learning 0.165s)
               Value function loss: 1.1931
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 37.31
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 11.02s
                        Total time: 36793.53s
                               ETA: 1252863.0s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.389s, learning 0.173s)
               Value function loss: 1.0432
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 36.22
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 11.56s
                        Total time: 36805.09s
                               ETA: 1252804.6s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.936s, learning 0.250s)
               Value function loss: 1.0692
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 37.07
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 11.19s
                        Total time: 36816.28s
                               ETA: 1252733.5s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.932s, learning 0.171s)
               Value function loss: 1.2997
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 35.96
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 11.10s
                        Total time: 36827.38s
                               ETA: 1252659.7s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.163s)
               Value function loss: 1.1935
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 37.25
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 11.15s
                        Total time: 36838.53s
                               ETA: 1252587.5s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.024s, learning 0.236s)
               Value function loss: 0.9353
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 37.30
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 11.26s
                        Total time: 36849.79s
                               ETA: 1252519.0s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.297s, learning 0.159s)
               Value function loss: 1.0169
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 37.54
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 11.46s
                        Total time: 36861.25s
                               ETA: 1252457.3s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.697s, learning 0.191s)
               Value function loss: 0.9421
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 37.18
               Mean episode length: 124.68
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 10.89s
                        Total time: 36872.14s
                               ETA: 1252376.3s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.162s)
               Value function loss: 0.9913
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 37.21
               Mean episode length: 124.60
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 10.82s
                        Total time: 36882.96s
                               ETA: 1252293.0s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.040s, learning 0.197s)
               Value function loss: 0.9199
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 38.82
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 11.24s
                        Total time: 36894.19s
                               ETA: 1252223.9s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.828s, learning 0.168s)
               Value function loss: 0.9569
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 36.91
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 11.00s
                        Total time: 36905.19s
                               ETA: 1252146.8s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.717s, learning 0.159s)
               Value function loss: 0.8604
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 37.52
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 10.88s
                        Total time: 36916.06s
                               ETA: 1252065.5s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.647s, learning 0.221s)
               Value function loss: 0.9881
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 38.88
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 10.87s
                        Total time: 36926.93s
                               ETA: 1251984.1s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.783s, learning 0.182s)
               Value function loss: 0.9217
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 37.38
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 10.97s
                        Total time: 36937.90s
                               ETA: 1251906.0s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.162s)
               Value function loss: 0.9716
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 37.12
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 11.24s
                        Total time: 36949.13s
                               ETA: 1251837.1s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.413s, learning 0.217s)
               Value function loss: 1.1777
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 38.31
               Mean episode length: 124.63
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 11.63s
                        Total time: 36960.76s
                               ETA: 1251781.7s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.952s, learning 0.175s)
               Value function loss: 1.0083
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 38.06
               Mean episode length: 124.53
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 11.13s
                        Total time: 36971.89s
                               ETA: 1251709.2s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.022s, learning 0.176s)
               Value function loss: 1.0001
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 36.24
               Mean episode length: 124.65
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 11.20s
                        Total time: 36983.09s
                               ETA: 1251639.1s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.765s, learning 0.185s)
               Value function loss: 1.1845
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 38.67
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 10.95s
                        Total time: 36994.04s
                               ETA: 1251560.7s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.634s, learning 0.179s)
               Value function loss: 1.3012
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 36.81
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 10.81s
                        Total time: 37004.85s
                               ETA: 1251477.8s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.001s, learning 0.185s)
               Value function loss: 1.1795
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 37.83
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 11.19s
                        Total time: 37016.04s
                               ETA: 1251407.4s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.153s, learning 0.157s)
               Value function loss: 0.9388
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 38.49
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 11.31s
                        Total time: 37027.35s
                               ETA: 1251341.3s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.652s, learning 0.184s)
               Value function loss: 1.0371
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 37.18
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 10.84s
                        Total time: 37038.18s
                               ETA: 1251259.3s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.927s, learning 0.169s)
               Value function loss: 1.1004
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 36.50
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 11.10s
                        Total time: 37049.28s
                               ETA: 1251186.0s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.895s, learning 0.280s)
               Value function loss: 1.2109
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 37.32
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 11.18s
                        Total time: 37060.45s
                               ETA: 1251115.5s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.922s, learning 0.171s)
               Value function loss: 1.0025
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 37.24
               Mean episode length: 124.09
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 11.09s
                        Total time: 37071.55s
                               ETA: 1251042.3s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.170s)
               Value function loss: 1.2383
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 38.73
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 11.12s
                        Total time: 37082.67s
                               ETA: 1250970.1s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.980s, learning 0.173s)
               Value function loss: 1.1832
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 36.48
               Mean episode length: 124.46
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 11.15s
                        Total time: 37093.82s
                               ETA: 1250898.9s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.079s, learning 0.164s)
               Value function loss: 1.0365
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 37.60
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 11.24s
                        Total time: 37105.06s
                               ETA: 1250830.9s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.992s, learning 0.205s)
               Value function loss: 1.1515
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 38.11
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 11.20s
                        Total time: 37116.26s
                               ETA: 1250761.3s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.164s)
               Value function loss: 1.5251
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 38.73
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 11.48s
                        Total time: 37127.74s
                               ETA: 1250701.3s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.165s)
               Value function loss: 1.3874
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 37.88
               Mean episode length: 124.62
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 11.51s
                        Total time: 37139.25s
                               ETA: 1250642.2s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.282s, learning 0.197s)
               Value function loss: 1.2487
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 38.00
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 11.48s
                        Total time: 37150.73s
                               ETA: 1250582.3s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.945s, learning 0.192s)
               Value function loss: 1.3485
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 38.19
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 11.14s
                        Total time: 37161.86s
                               ETA: 1250510.9s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.372s, learning 0.177s)
               Value function loss: 1.4588
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 38.10
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 11.55s
                        Total time: 37173.41s
                               ETA: 1250453.3s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.818s, learning 0.162s)
               Value function loss: 1.4222
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 38.47
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 10.98s
                        Total time: 37184.39s
                               ETA: 1250376.7s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.163s)
               Value function loss: 1.2327
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 39.53
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 11.43s
                        Total time: 37195.82s
                               ETA: 1250315.1s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.752s, learning 0.176s)
               Value function loss: 1.2445
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 38.58
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 10.93s
                        Total time: 37206.75s
                               ETA: 1250236.8s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.820s, learning 0.174s)
               Value function loss: 1.0554
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 39.16
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 10.99s
                        Total time: 37217.74s
                               ETA: 1250160.8s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.174s)
               Value function loss: 1.0544
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 38.04
               Mean episode length: 124.81
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 11.59s
                        Total time: 37229.33s
                               ETA: 1250104.9s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.162s, learning 0.178s)
               Value function loss: 1.1325
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 38.82
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 11.34s
                        Total time: 37240.67s
                               ETA: 1250040.5s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.888s, learning 0.253s)
               Value function loss: 1.0691
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 38.17
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 11.14s
                        Total time: 37251.81s
                               ETA: 1249969.5s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.883s, learning 0.168s)
               Value function loss: 1.1176
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 38.41
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 11.05s
                        Total time: 37262.86s
                               ETA: 1249895.6s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.586s, learning 0.252s)
               Value function loss: 1.0326
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 37.82
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 10.84s
                        Total time: 37273.70s
                               ETA: 1249814.5s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.170s, learning 0.197s)
               Value function loss: 0.9332
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 37.69
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 11.37s
                        Total time: 37285.07s
                               ETA: 1249751.2s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.881s, learning 0.173s)
               Value function loss: 0.9776
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 38.25
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 11.05s
                        Total time: 37296.12s
                               ETA: 1249677.5s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.873s, learning 0.189s)
               Value function loss: 1.4412
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 38.64
               Mean episode length: 124.83
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 11.06s
                        Total time: 37307.19s
                               ETA: 1249604.1s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.897s, learning 0.161s)
               Value function loss: 1.3841
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 37.76
               Mean episode length: 124.65
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 11.06s
                        Total time: 37318.24s
                               ETA: 1249530.6s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.172s, learning 0.169s)
               Value function loss: 1.2158
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 37.70
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 11.34s
                        Total time: 37329.58s
                               ETA: 1249466.6s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.789s, learning 0.178s)
               Value function loss: 1.4207
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 36.63
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 10.97s
                        Total time: 37340.55s
                               ETA: 1249390.1s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1457 steps/s (collection: 10.993s, learning 0.246s)
               Value function loss: 1.5261
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 36.53
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 11.24s
                        Total time: 37351.79s
                               ETA: 1249322.8s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.755s, learning 0.197s)
               Value function loss: 1.5595
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 37.10
               Mean episode length: 124.82
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 10.95s
                        Total time: 37362.74s
                               ETA: 1249245.9s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.910s, learning 0.265s)
               Value function loss: 1.1045
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 37.05
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 11.17s
                        Total time: 37373.92s
                               ETA: 1249176.6s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.304s, learning 0.283s)
               Value function loss: 1.2568
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 36.51
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 11.59s
                        Total time: 37385.50s
                               ETA: 1249121.0s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.712s, learning 0.165s)
               Value function loss: 1.1212
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 36.41
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 10.88s
                        Total time: 37396.38s
                               ETA: 1249041.7s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.839s, learning 0.193s)
               Value function loss: 1.2622
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 36.28
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 11.03s
                        Total time: 37407.41s
                               ETA: 1248967.6s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.865s, learning 0.177s)
               Value function loss: 1.0258
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 36.60
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 11.04s
                        Total time: 37418.45s
                               ETA: 1248894.0s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.697s, learning 0.173s)
               Value function loss: 1.2261
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 36.35
               Mean episode length: 124.60
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 10.87s
                        Total time: 37429.32s
                               ETA: 1248814.6s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.098s, learning 0.165s)
               Value function loss: 1.0958
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 35.26
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 11.26s
                        Total time: 37440.59s
                               ETA: 1248748.4s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.572s, learning 0.163s)
               Value function loss: 0.9485
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 36.40
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 11.74s
                        Total time: 37452.32s
                               ETA: 1248698.0s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.608s, learning 0.211s)
               Value function loss: 1.0664
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 36.26
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 10.82s
                        Total time: 37463.14s
                               ETA: 1248617.0s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.722s, learning 0.209s)
               Value function loss: 1.1864
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 36.52
               Mean episode length: 124.15
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 10.93s
                        Total time: 37474.07s
                               ETA: 1248539.9s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.725s, learning 0.184s)
               Value function loss: 1.3515
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 35.94
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 10.91s
                        Total time: 37484.98s
                               ETA: 1248462.1s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.162s)
               Value function loss: 1.1944
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 36.72
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 10.79s
                        Total time: 37495.78s
                               ETA: 1248380.5s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.914s, learning 0.163s)
               Value function loss: 1.1892
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 36.87
               Mean episode length: 124.87
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 11.08s
                        Total time: 37506.85s
                               ETA: 1248308.3s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.083s, learning 0.173s)
               Value function loss: 1.3511
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 36.61
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 11.26s
                        Total time: 37518.11s
                               ETA: 1248242.1s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.066s, learning 0.180s)
               Value function loss: 1.6262
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 35.80
               Mean episode length: 124.02
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 11.25s
                        Total time: 37529.36s
                               ETA: 1248175.7s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.960s, learning 0.160s)
               Value function loss: 1.3288
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 37.33
               Mean episode length: 124.82
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 11.12s
                        Total time: 37540.47s
                               ETA: 1248105.1s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.644s, learning 0.162s)
               Value function loss: 1.0181
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 36.63
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 10.81s
                        Total time: 37551.28s
                               ETA: 1248024.0s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.669s, learning 0.189s)
               Value function loss: 1.0518
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 36.23
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 10.86s
                        Total time: 37562.14s
                               ETA: 1247944.8s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.896s, learning 0.195s)
               Value function loss: 0.9802
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 35.83
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 11.09s
                        Total time: 37573.23s
                               ETA: 1247873.4s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.153s, learning 0.188s)
               Value function loss: 1.0901
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 37.19
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 11.34s
                        Total time: 37584.57s
                               ETA: 1247810.3s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.871s, learning 0.275s)
               Value function loss: 1.0651
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 35.98
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 11.15s
                        Total time: 37595.72s
                               ETA: 1247740.8s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.170s, learning 0.178s)
               Value function loss: 1.2173
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 37.52
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 11.35s
                        Total time: 37607.06s
                               ETA: 1247677.9s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.187s, learning 0.171s)
               Value function loss: 1.0508
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 36.94
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 11.36s
                        Total time: 37618.42s
                               ETA: 1247615.5s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.082s, learning 0.171s)
               Value function loss: 0.9903
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 37.34
               Mean episode length: 124.04
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 11.25s
                        Total time: 37629.67s
                               ETA: 1247549.7s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.922s, learning 0.163s)
               Value function loss: 0.9544
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 37.89
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 11.09s
                        Total time: 37640.76s
                               ETA: 1247478.3s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.947s, learning 0.168s)
               Value function loss: 1.4202
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 38.63
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 11.12s
                        Total time: 37651.88s
                               ETA: 1247407.9s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.687s, learning 0.163s)
               Value function loss: 1.3708
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 37.17
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 10.85s
                        Total time: 37662.72s
                               ETA: 1247328.8s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.948s, learning 0.176s)
               Value function loss: 1.1719
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 37.61
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 11.12s
                        Total time: 37673.85s
                               ETA: 1247258.8s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.168s)
               Value function loss: 1.2590
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 37.39
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 11.55s
                        Total time: 37685.40s
                               ETA: 1247203.1s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.987s, learning 0.182s)
               Value function loss: 1.5230
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 37.72
               Mean episode length: 124.17
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 11.17s
                        Total time: 37696.57s
                               ETA: 1247134.6s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.726s, learning 0.166s)
               Value function loss: 1.5082
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 38.38
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 10.89s
                        Total time: 37707.46s
                               ETA: 1247057.1s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.658s, learning 0.168s)
               Value function loss: 1.2211
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 38.57
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 10.83s
                        Total time: 37718.29s
                               ETA: 1246977.4s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.900s, learning 0.173s)
               Value function loss: 1.2335
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 37.74
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 11.07s
                        Total time: 37729.36s
                               ETA: 1246905.9s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.020s, learning 0.165s)
               Value function loss: 1.1265
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 37.40
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 11.18s
                        Total time: 37740.55s
                               ETA: 1246838.2s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.223s, learning 0.257s)
               Value function loss: 1.2254
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 37.24
               Mean episode length: 124.10
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 11.48s
                        Total time: 37752.03s
                               ETA: 1246780.3s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.288s, learning 0.171s)
               Value function loss: 1.1656
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 37.59
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 11.46s
                        Total time: 37763.49s
                               ETA: 1246721.7s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.975s, learning 0.194s)
               Value function loss: 1.2555
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 38.20
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 11.17s
                        Total time: 37774.65s
                               ETA: 1246653.5s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.400s, learning 0.201s)
               Value function loss: 1.1020
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 37.77
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 11.60s
                        Total time: 37786.26s
                               ETA: 1246599.7s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.673s, learning 0.176s)
               Value function loss: 1.1557
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 37.75
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 10.85s
                        Total time: 37797.10s
                               ETA: 1246521.0s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.696s, learning 0.194s)
               Value function loss: 0.9188
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 38.09
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 10.89s
                        Total time: 37807.99s
                               ETA: 1246443.8s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.215s, learning 0.194s)
               Value function loss: 0.9543
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 37.65
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 11.41s
                        Total time: 37819.40s
                               ETA: 1246383.7s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.070s, learning 0.299s)
               Value function loss: 1.2416
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 37.35
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 11.37s
                        Total time: 37830.77s
                               ETA: 1246322.4s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.879s, learning 0.174s)
               Value function loss: 1.0665
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 38.59
               Mean episode length: 124.94
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 11.05s
                        Total time: 37841.83s
                               ETA: 1246250.7s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.171s)
               Value function loss: 0.9929
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 38.09
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 10.99s
                        Total time: 37852.81s
                               ETA: 1246176.8s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.706s, learning 0.211s)
               Value function loss: 1.1655
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 38.10
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 10.92s
                        Total time: 37863.73s
                               ETA: 1246100.7s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.936s, learning 0.172s)
               Value function loss: 1.1894
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 37.54
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 11.11s
                        Total time: 37874.84s
                               ETA: 1246030.9s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.171s)
               Value function loss: 1.0936
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 37.09
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 11.24s
                        Total time: 37886.08s
                               ETA: 1245965.3s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.629s, learning 0.168s)
               Value function loss: 0.7960
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 36.69
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 10.80s
                        Total time: 37896.87s
                               ETA: 1245885.4s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.970s, learning 0.205s)
               Value function loss: 0.9351
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 37.21
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 11.18s
                        Total time: 37908.05s
                               ETA: 1245817.9s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.915s, learning 0.170s)
               Value function loss: 0.8682
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 36.52
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 11.08s
                        Total time: 37919.13s
                               ETA: 1245747.5s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.472s, learning 0.167s)
               Value function loss: 0.9768
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 36.87
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 10.64s
                        Total time: 37929.77s
                               ETA: 1245662.5s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.156s, learning 0.171s)
               Value function loss: 0.8128
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 35.92
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 11.33s
                        Total time: 37941.10s
                               ETA: 1245600.1s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.706s, learning 0.195s)
               Value function loss: 0.8848
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 36.61
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 10.90s
                        Total time: 37952.00s
                               ETA: 1245523.8s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.132s, learning 0.180s)
               Value function loss: 0.8670
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 36.35
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 11.31s
                        Total time: 37963.31s
                               ETA: 1245461.0s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.098s, learning 0.178s)
               Value function loss: 0.7570
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 37.27
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 11.28s
                        Total time: 37974.59s
                               ETA: 1245397.1s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.078s, learning 0.164s)
               Value function loss: 0.7724
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 37.15
               Mean episode length: 124.88
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 11.24s
                        Total time: 37985.83s
                               ETA: 1245332.1s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.707s, learning 0.185s)
               Value function loss: 1.0013
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 36.69
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 10.89s
                        Total time: 37996.72s
                               ETA: 1245255.6s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.396s, learning 0.203s)
               Value function loss: 1.0687
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 35.65
               Mean episode length: 124.32
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 11.60s
                        Total time: 38008.32s
                               ETA: 1245202.4s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.445s, learning 0.165s)
               Value function loss: 0.8657
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 36.69
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 10.61s
                        Total time: 38018.93s
                               ETA: 1245116.8s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.586s, learning 0.184s)
               Value function loss: 0.9462
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 35.89
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 10.77s
                        Total time: 38029.70s
                               ETA: 1245036.5s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.258s, learning 0.182s)
               Value function loss: 1.0099
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 36.43
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 11.44s
                        Total time: 38041.14s
                               ETA: 1244978.1s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.756s, learning 0.204s)
               Value function loss: 1.2732
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 34.93
               Mean episode length: 124.32
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 10.96s
                        Total time: 38052.10s
                               ETA: 1244904.2s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.682s, learning 0.171s)
               Value function loss: 1.0253
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 35.69
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 10.85s
                        Total time: 38062.96s
                               ETA: 1244826.7s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.039s, learning 0.298s)
               Value function loss: 1.0867
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 35.32
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 11.34s
                        Total time: 38074.29s
                               ETA: 1244765.1s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.761s, learning 0.178s)
               Value function loss: 1.0021
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 10.94s
                        Total time: 38085.23s
                               ETA: 1244690.5s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.163s)
               Value function loss: 1.0068
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 35.67
               Mean episode length: 124.22
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 10.96s
                        Total time: 38096.19s
                               ETA: 1244616.7s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.952s, learning 0.176s)
               Value function loss: 0.9946
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 35.14
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 11.13s
                        Total time: 38107.32s
                               ETA: 1244548.4s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.347s, learning 0.222s)
               Value function loss: 0.8437
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 35.00
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 11.57s
                        Total time: 38118.89s
                               ETA: 1244494.5s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.243s, learning 0.165s)
               Value function loss: 0.9210
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 34.92
               Mean episode length: 123.86
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 11.41s
                        Total time: 38130.30s
                               ETA: 1244435.4s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.162s)
               Value function loss: 0.8442
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 34.91
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 10.99s
                        Total time: 38141.29s
                               ETA: 1244362.8s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.959s, learning 0.208s)
               Value function loss: 0.8097
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 35.28
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 11.17s
                        Total time: 38152.46s
                               ETA: 1244295.9s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.507s, learning 0.164s)
               Value function loss: 0.8781
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 34.73
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 11.67s
                        Total time: 38164.13s
                               ETA: 1244245.5s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.872s, learning 0.170s)
               Value function loss: 1.0944
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 32.59
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 11.04s
                        Total time: 38175.17s
                               ETA: 1244174.6s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.182s, learning 0.188s)
               Value function loss: 1.1012
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 34.14
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 11.37s
                        Total time: 38186.54s
                               ETA: 1244114.4s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.164s)
               Value function loss: 0.8807
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 34.31
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 11.13s
                        Total time: 38197.67s
                               ETA: 1244046.4s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.972s, learning 0.190s)
               Value function loss: 1.1010
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 34.08
               Mean episode length: 124.76
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 11.16s
                        Total time: 38208.83s
                               ETA: 1243979.5s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.870s, learning 0.232s)
               Value function loss: 1.5702
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 34.32
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 11.10s
                        Total time: 38219.93s
                               ETA: 1243910.7s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.529s, learning 0.191s)
               Value function loss: 1.5225
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 34.40
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 10.72s
                        Total time: 38230.65s
                               ETA: 1243829.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.283s, learning 0.192s)
               Value function loss: 1.2929
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 34.72
               Mean episode length: 124.57
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 11.48s
                        Total time: 38242.13s
                               ETA: 1243773.0s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.866s, learning 0.247s)
               Value function loss: 1.4817
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 34.28
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 11.11s
                        Total time: 38253.24s
                               ETA: 1243704.7s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.081s, learning 0.286s)
               Value function loss: 1.3731
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 33.69
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 11.37s
                        Total time: 38264.61s
                               ETA: 1243644.6s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.769s, learning 0.197s)
               Value function loss: 1.4411
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 34.41
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 10.97s
                        Total time: 38275.57s
                               ETA: 1243571.6s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.148s, learning 0.189s)
               Value function loss: 1.1194
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 34.34
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 11.34s
                        Total time: 38286.91s
                               ETA: 1243510.7s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.008s, learning 0.208s)
               Value function loss: 1.1630
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 34.43
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 11.22s
                        Total time: 38298.13s
                               ETA: 1243445.8s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.927s, learning 0.207s)
               Value function loss: 1.0550
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 34.25
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 11.13s
                        Total time: 38309.26s
                               ETA: 1243378.4s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.160s)
               Value function loss: 1.1205
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 34.18
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 11.17s
                        Total time: 38320.43s
                               ETA: 1243312.2s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.430s, learning 0.206s)
               Value function loss: 0.9866
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 34.23
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 11.64s
                        Total time: 38332.07s
                               ETA: 1243261.1s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.961s, learning 0.253s)
               Value function loss: 1.0610
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 33.81
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 11.21s
                        Total time: 38343.28s
                               ETA: 1243196.4s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.961s, learning 0.191s)
               Value function loss: 1.2965
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 34.04
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 11.15s
                        Total time: 38354.43s
                               ETA: 1243129.6s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.257s, learning 0.178s)
               Value function loss: 1.2387
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 33.03
               Mean episode length: 124.11
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 11.44s
                        Total time: 38365.87s
                               ETA: 1243072.1s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.740s, learning 0.170s)
               Value function loss: 0.8512
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 33.99
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 10.91s
                        Total time: 38376.78s
                               ETA: 1242997.6s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.738s, learning 0.216s)
               Value function loss: 1.0489
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 34.36
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 10.95s
                        Total time: 38387.73s
                               ETA: 1242924.6s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.091s, learning 0.169s)
               Value function loss: 1.2028
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 33.59
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 11.26s
                        Total time: 38398.99s
                               ETA: 1242861.5s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.912s, learning 0.242s)
               Value function loss: 1.0021
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 33.43
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 11.15s
                        Total time: 38410.15s
                               ETA: 1242795.0s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.039s, learning 0.190s)
               Value function loss: 0.7486
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 32.88
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 11.23s
                        Total time: 38421.38s
                               ETA: 1242731.0s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.816s, learning 0.180s)
               Value function loss: 0.7862
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 32.59
               Mean episode length: 123.28
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 11.00s
                        Total time: 38432.37s
                               ETA: 1242659.5s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.957s, learning 0.166s)
               Value function loss: 0.8082
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 32.22
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 11.12s
                        Total time: 38443.50s
                               ETA: 1242592.1s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.887s, learning 0.189s)
               Value function loss: 0.9237
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 33.19
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 11.08s
                        Total time: 38454.57s
                               ETA: 1242523.3s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.702s, learning 0.161s)
               Value function loss: 0.7541
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 32.89
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 10.86s
                        Total time: 38465.43s
                               ETA: 1242447.6s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.819s, learning 0.173s)
               Value function loss: 0.9280
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 32.07
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 10.99s
                        Total time: 38476.43s
                               ETA: 1242376.1s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.088s, learning 0.167s)
               Value function loss: 0.6393
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 32.03
               Mean episode length: 124.22
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 11.25s
                        Total time: 38487.68s
                               ETA: 1242313.2s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.162s)
               Value function loss: 0.6613
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 32.22
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 11.11s
                        Total time: 38498.79s
                               ETA: 1242245.6s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.029s, learning 0.173s)
               Value function loss: 0.7076
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 30.41
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 11.20s
                        Total time: 38509.99s
                               ETA: 1242181.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.166s)
               Value function loss: 0.9869
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 31.32
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 11.51s
                        Total time: 38521.50s
                               ETA: 1242126.3s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.993s, learning 0.166s)
               Value function loss: 0.9566
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 32.34
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 11.16s
                        Total time: 38532.66s
                               ETA: 1242060.4s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.950s, learning 0.159s)
               Value function loss: 0.7687
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 31.37
               Mean episode length: 124.62
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 11.11s
                        Total time: 38543.77s
                               ETA: 1241992.9s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.020s, learning 0.183s)
               Value function loss: 0.8614
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 30.90
               Mean episode length: 124.24
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 11.20s
                        Total time: 38554.97s
                               ETA: 1241928.5s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.232s, learning 0.254s)
               Value function loss: 0.9357
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 31.75
               Mean episode length: 124.09
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 11.49s
                        Total time: 38566.46s
                               ETA: 1241873.2s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.039s, learning 0.233s)
               Value function loss: 1.0639
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 31.84
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 11.27s
                        Total time: 38577.73s
                               ETA: 1241811.1s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.947s, learning 0.184s)
               Value function loss: 0.7792
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 31.72
               Mean episode length: 124.07
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 11.13s
                        Total time: 38588.86s
                               ETA: 1241744.5s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.903s, learning 0.163s)
               Value function loss: 0.8349
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 32.04
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 11.07s
                        Total time: 38599.93s
                               ETA: 1241675.8s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.156s, learning 0.214s)
               Value function loss: 0.8007
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 31.09
               Mean episode length: 124.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 11.37s
                        Total time: 38611.30s
                               ETA: 1241616.9s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.285s, learning 0.167s)
               Value function loss: 0.8410
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 31.20
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 11.45s
                        Total time: 38622.75s
                               ETA: 1241560.7s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.183s)
               Value function loss: 0.8482
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 30.78
               Mean episode length: 124.20
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 11.26s
                        Total time: 38634.00s
                               ETA: 1241498.2s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.940s, learning 0.241s)
               Value function loss: 0.7197
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 30.44
               Mean episode length: 124.94
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 11.18s
                        Total time: 38645.19s
                               ETA: 1241433.4s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.163s)
               Value function loss: 0.8059
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 30.71
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 11.34s
                        Total time: 38656.53s
                               ETA: 1241373.7s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.886s, learning 0.170s)
               Value function loss: 0.8263
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 31.57
               Mean episode length: 124.21
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 11.06s
                        Total time: 38667.58s
                               ETA: 1241305.0s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.132s, learning 0.196s)
               Value function loss: 0.7956
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 31.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 11.33s
                        Total time: 38678.91s
                               ETA: 1241244.9s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.752s, learning 0.197s)
               Value function loss: 0.8122
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 31.69
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 10.95s
                        Total time: 38689.86s
                               ETA: 1241172.8s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.631s, learning 0.178s)
               Value function loss: 1.0988
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 31.66
               Mean episode length: 124.74
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 10.81s
                        Total time: 38700.67s
                               ETA: 1241096.2s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.900s, learning 0.198s)
               Value function loss: 1.0927
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 31.54
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 11.10s
                        Total time: 38711.77s
                               ETA: 1241028.9s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.098s, learning 0.172s)
               Value function loss: 1.0867
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 32.34
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 11.27s
                        Total time: 38723.04s
                               ETA: 1240967.2s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.028s, learning 0.168s)
               Value function loss: 1.2338
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 31.36
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 11.20s
                        Total time: 38734.23s
                               ETA: 1240903.1s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.205s, learning 0.172s)
               Value function loss: 1.3896
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 31.66
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 11.38s
                        Total time: 38745.61s
                               ETA: 1240844.8s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.128s, learning 0.317s)
               Value function loss: 1.4262
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 30.70
               Mean episode length: 124.87
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 11.44s
                        Total time: 38757.06s
                               ETA: 1240788.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.298s, learning 0.180s)
               Value function loss: 0.8480
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 30.86
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 11.48s
                        Total time: 38768.53s
                               ETA: 1240733.8s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.904s, learning 0.161s)
               Value function loss: 1.0631
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 32.08
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.07s
                        Total time: 38779.60s
                               ETA: 1240665.7s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.642s, learning 0.201s)
               Value function loss: 0.8729
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 30.29
               Mean episode length: 123.79
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 10.84s
                        Total time: 38790.44s
                               ETA: 1240590.5s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.611s, learning 0.201s)
               Value function loss: 1.0099
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 31.64
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 10.81s
                        Total time: 38801.25s
                               ETA: 1240514.3s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.174s)
               Value function loss: 0.6724
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 31.48
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 11.21s
                        Total time: 38812.46s
                               ETA: 1240450.9s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.779s, learning 0.165s)
               Value function loss: 0.8094
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 32.65
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 10.94s
                        Total time: 38823.41s
                               ETA: 1240379.0s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.864s, learning 0.170s)
               Value function loss: 0.6887
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 31.71
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 11.03s
                        Total time: 38834.44s
                               ETA: 1240310.1s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.248s, learning 0.239s)
               Value function loss: 0.6623
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 33.04
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 11.49s
                        Total time: 38845.93s
                               ETA: 1240255.7s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.491s, learning 0.163s)
               Value function loss: 0.7243
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 31.74
               Mean episode length: 124.80
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 10.65s
                        Total time: 38856.58s
                               ETA: 1240174.7s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.731s, learning 0.184s)
               Value function loss: 0.7744
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 32.74
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 10.91s
                        Total time: 38867.50s
                               ETA: 1240102.0s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1453 steps/s (collection: 10.899s, learning 0.376s)
               Value function loss: 0.7855
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 32.15
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 11.27s
                        Total time: 38878.77s
                               ETA: 1240040.9s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.258s, learning 0.200s)
               Value function loss: 0.6797
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 31.58
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 11.46s
                        Total time: 38890.23s
                               ETA: 1239985.7s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.818s, learning 0.189s)
               Value function loss: 0.6799
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 30.70
               Mean episode length: 122.95
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 11.01s
                        Total time: 38901.24s
                               ETA: 1239916.1s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.777s, learning 0.160s)
               Value function loss: 0.6898
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 32.36
               Mean episode length: 124.16
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 10.94s
                        Total time: 38912.17s
                               ETA: 1239844.4s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.803s, learning 0.172s)
               Value function loss: 0.7850
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 32.14
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 10.97s
                        Total time: 38923.15s
                               ETA: 1239773.8s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.357s, learning 0.165s)
               Value function loss: 0.6812
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 31.60
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 10.52s
                        Total time: 38933.67s
                               ETA: 1239688.9s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.560s, learning 0.166s)
               Value function loss: 0.5804
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 32.51
               Mean episode length: 124.26
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 10.73s
                        Total time: 38944.39s
                               ETA: 1239610.5s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.737s, learning 0.161s)
               Value function loss: 0.5918
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 32.25
               Mean episode length: 124.12
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 10.90s
                        Total time: 38955.29s
                               ETA: 1239537.7s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.993s, learning 0.165s)
               Value function loss: 0.6012
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 33.56
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 11.16s
                        Total time: 38966.45s
                               ETA: 1239473.2s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.234s, learning 0.212s)
               Value function loss: 0.6736
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 32.91
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 11.45s
                        Total time: 38977.90s
                               ETA: 1239417.8s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.231s, learning 0.161s)
               Value function loss: 0.6031
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 34.27
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 11.39s
                        Total time: 38989.29s
                               ETA: 1239360.8s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.923s, learning 0.162s)
               Value function loss: 0.6768
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 34.04
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 11.09s
                        Total time: 39000.37s
                               ETA: 1239294.0s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.064s, learning 0.185s)
               Value function loss: 0.5760
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 33.69
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 11.25s
                        Total time: 39011.62s
                               ETA: 1239232.6s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.600s, learning 0.165s)
               Value function loss: 0.5935
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 33.47
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 10.77s
                        Total time: 39022.39s
                               ETA: 1239155.7s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.057s, learning 0.186s)
               Value function loss: 0.5731
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 33.35
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 11.24s
                        Total time: 39033.63s
                               ETA: 1239094.1s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.094s, learning 0.199s)
               Value function loss: 0.7620
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 34.09
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 11.29s
                        Total time: 39044.92s
                               ETA: 1239034.1s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.571s, learning 0.173s)
               Value function loss: 0.7145
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 34.76
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 10.74s
                        Total time: 39055.67s
                               ETA: 1238956.7s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.015s, learning 0.181s)
               Value function loss: 0.6463
                    Surrogate loss: -0.0130
             Mean action noise std: 0.70
                       Mean reward: 33.11
               Mean episode length: 124.18
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 11.20s
                        Total time: 39066.86s
                               ETA: 1238893.7s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.025s, learning 0.166s)
               Value function loss: 0.7136
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 34.54
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 11.19s
                        Total time: 39078.06s
                               ETA: 1238830.6s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.970s, learning 0.191s)
               Value function loss: 0.8312
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 33.48
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 11.16s
                        Total time: 39089.22s
                               ETA: 1238766.6s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.661s, learning 0.195s)
               Value function loss: 0.8725
                    Surrogate loss: -0.0186
             Mean action noise std: 0.70
                       Mean reward: 35.27
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 10.86s
                        Total time: 39100.07s
                               ETA: 1238692.9s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.866s, learning 0.174s)
               Value function loss: 0.7276
                    Surrogate loss: -0.0197
             Mean action noise std: 0.70
                       Mean reward: 35.68
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 11.04s
                        Total time: 39111.11s
                               ETA: 1238625.0s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.798s, learning 0.188s)
               Value function loss: 0.7981
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 36.08
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 10.99s
                        Total time: 39122.10s
                               ETA: 1238555.6s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.014s, learning 0.192s)
               Value function loss: 0.8075
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 35.03
               Mean episode length: 123.46
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 11.21s
                        Total time: 39133.31s
                               ETA: 1238493.1s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.094s, learning 0.224s)
               Value function loss: 0.8008
                    Surrogate loss: -0.0150
             Mean action noise std: 0.70
                       Mean reward: 35.20
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 11.32s
                        Total time: 39144.62s
                               ETA: 1238434.2s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.564s, learning 0.195s)
               Value function loss: 0.7855
                    Surrogate loss: -0.0194
             Mean action noise std: 0.70
                       Mean reward: 35.32
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 10.76s
                        Total time: 39155.38s
                               ETA: 1238357.6s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.942s, learning 0.162s)
               Value function loss: 0.8343
                    Surrogate loss: -0.0208
             Mean action noise std: 0.70
                       Mean reward: 35.73
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 11.10s
                        Total time: 39166.49s
                               ETA: 1238292.0s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1561 steps/s (collection: 10.319s, learning 0.172s)
               Value function loss: 0.7962
                    Surrogate loss: -0.0205
             Mean action noise std: 0.70
                       Mean reward: 35.24
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 10.49s
                        Total time: 39176.98s
                               ETA: 1238207.1s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.876s, learning 0.169s)
               Value function loss: 0.8067
                    Surrogate loss: -0.0175
             Mean action noise std: 0.70
                       Mean reward: 35.18
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 11.05s
                        Total time: 39188.02s
                               ETA: 1238139.7s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.826s, learning 0.172s)
               Value function loss: 0.8806
                    Surrogate loss: -0.0125
             Mean action noise std: 0.70
                       Mean reward: 36.09
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 11.00s
                        Total time: 39199.02s
                               ETA: 1238070.9s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.849s, learning 0.214s)
               Value function loss: 0.8892
                    Surrogate loss: -0.0166
             Mean action noise std: 0.70
                       Mean reward: 35.61
               Mean episode length: 124.29
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 11.06s
                        Total time: 39210.08s
                               ETA: 1238004.1s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.745s, learning 0.180s)
               Value function loss: 1.1289
                    Surrogate loss: -0.0124
             Mean action noise std: 0.70
                       Mean reward: 35.54
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 10.93s
                        Total time: 39221.01s
                               ETA: 1237933.1s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.924s, learning 0.216s)
               Value function loss: 0.9306
                    Surrogate loss: -0.0172
             Mean action noise std: 0.70
                       Mean reward: 35.43
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 11.14s
                        Total time: 39232.15s
                               ETA: 1237868.8s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.397s, learning 0.192s)
               Value function loss: 1.0196
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 34.89
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 20.59s
                        Total time: 39252.74s
                               ETA: 1238102.6s

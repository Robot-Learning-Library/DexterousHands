Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-3te429jd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandSwingCup_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/3te429jd
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:3
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=3, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=3, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:3', seed=None, sim_device='cuda:3', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandSwingCup', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_swing_cup', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 300, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.1, 'orientation_scale': 0.1, 'rotRewardScale': 5.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandSwingCup', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandSwingCup_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 6714
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:3
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1121 steps/s (collection: 8.593s, learning 6.012s)
               Value function loss: 12.8465
                    Surrogate loss: 0.0163
             Mean action noise std: 0.80
                  Mean reward/step: -1.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 14.60s
                        Total time: 14.60s
                               ETA: 1460497.0s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1551 steps/s (collection: 10.226s, learning 0.337s)
               Value function loss: 14.6748
                    Surrogate loss: -0.0165
             Mean action noise std: 0.80
                  Mean reward/step: -1.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 10.56s
                        Total time: 25.17s
                               ETA: 1258371.3s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 2301 steps/s (collection: 6.943s, learning 0.177s)
               Value function loss: 13.9062
                    Surrogate loss: 0.0003
             Mean action noise std: 0.80
                  Mean reward/step: -1.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 7.12s
                        Total time: 32.29s
                               ETA: 1076235.3s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 1512 steps/s (collection: 10.644s, learning 0.185s)
               Value function loss: 15.3782
                    Surrogate loss: -0.0107
             Mean action noise std: 0.80
                       Mean reward: -102.89
               Mean episode length: 28.00
                  Mean reward/step: -1.65
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 10.83s
                        Total time: 43.12s
                               ETA: 1077891.0s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 1150 steps/s (collection: 14.032s, learning 0.211s)
               Value function loss: 11.0125
                    Surrogate loss: -0.0221
             Mean action noise std: 0.80
                       Mean reward: -110.73
               Mean episode length: 30.67
                  Mean reward/step: -1.63
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 14.24s
                        Total time: 57.36s
                               ETA: 1147168.1s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 904 steps/s (collection: 17.075s, learning 1.036s)
               Value function loss: 6.6660
                    Surrogate loss: 0.0000
             Mean action noise std: 0.80
                       Mean reward: -112.30
               Mean episode length: 31.20
                  Mean reward/step: -1.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 18.11s
                        Total time: 75.47s
                               ETA: 1257801.4s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 736 steps/s (collection: 22.065s, learning 0.180s)
               Value function loss: 8.5107
                    Surrogate loss: 0.0061
             Mean action noise std: 0.80
                       Mean reward: -113.53
               Mean episode length: 31.94
                  Mean reward/step: -1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 22.25s
                        Total time: 97.72s
                               ETA: 1395873.1s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 689 steps/s (collection: 23.499s, learning 0.271s)
               Value function loss: 5.4291
                    Surrogate loss: -0.0053
             Mean action noise std: 0.80
                       Mean reward: -120.25
               Mean episode length: 34.33
                  Mean reward/step: -1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 23.77s
                        Total time: 121.49s
                               ETA: 1518471.5s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 675 steps/s (collection: 24.055s, learning 0.196s)
               Value function loss: 2.3648
                    Surrogate loss: 0.0099
             Mean action noise std: 0.80
                       Mean reward: -123.60
               Mean episode length: 35.53
                  Mean reward/step: -1.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 24.25s
                        Total time: 145.74s
                               ETA: 1619170.1s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 675 steps/s (collection: 24.046s, learning 0.214s)
               Value function loss: 1.6464
                    Surrogate loss: 0.0021
             Mean action noise std: 0.80
                       Mean reward: -125.62
               Mean episode length: 36.25
                  Mean reward/step: -1.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 24.26s
                        Total time: 170.00s
                               ETA: 1699815.5s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.224s, learning 0.212s)
               Value function loss: 1.2285
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                       Mean reward: -126.96
               Mean episode length: 36.73
                  Mean reward/step: -1.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 24.44s
                        Total time: 194.43s
                               ETA: 1767389.3s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 675 steps/s (collection: 24.072s, learning 0.184s)
               Value function loss: 1.0368
                    Surrogate loss: 0.0087
             Mean action noise std: 0.80
                       Mean reward: -127.92
               Mean episode length: 37.07
                  Mean reward/step: -1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 24.26s
                        Total time: 218.69s
                               ETA: 1822203.2s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.271s, learning 0.191s)
               Value function loss: 0.9176
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: -128.64
               Mean episode length: 37.33
                  Mean reward/step: -1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 24.46s
                        Total time: 243.15s
                               ETA: 1870168.3s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 666 steps/s (collection: 24.357s, learning 0.223s)
               Value function loss: 1.0459
                    Surrogate loss: 0.0175
             Mean action noise std: 0.80
                       Mean reward: -129.20
               Mean episode length: 37.53
                  Mean reward/step: -1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 24.58s
                        Total time: 267.73s
                               ETA: 1912113.6s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 676 steps/s (collection: 24.027s, learning 0.177s)
               Value function loss: 0.8007
                    Surrogate loss: -0.0169
             Mean action noise std: 0.80
                       Mean reward: -129.65
               Mean episode length: 37.69
                  Mean reward/step: -1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 24.20s
                        Total time: 291.93s
                               ETA: 1945955.6s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 667 steps/s (collection: 24.364s, learning 0.183s)
               Value function loss: 0.3898
                    Surrogate loss: -0.0260
             Mean action noise std: 0.80
                       Mean reward: -130.01
               Mean episode length: 37.82
                  Mean reward/step: -1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 24.55s
                        Total time: 316.48s
                               ETA: 1977708.6s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 672 steps/s (collection: 24.163s, learning 0.198s)
               Value function loss: 0.2208
                    Surrogate loss: -0.0262
             Mean action noise std: 0.80
                       Mean reward: -130.32
               Mean episode length: 37.93
                  Mean reward/step: -1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 24.36s
                        Total time: 340.84s
                               ETA: 2004631.9s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.337s, learning 0.172s)
               Value function loss: 0.1783
                    Surrogate loss: -0.0240
             Mean action noise std: 0.80
                       Mean reward: -131.69
               Mean episode length: 38.42
                  Mean reward/step: -1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 24.51s
                        Total time: 365.35s
                               ETA: 2029382.6s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.282s, learning 0.174s)
               Value function loss: 0.1496
                    Surrogate loss: -0.0141
             Mean action noise std: 0.80
                       Mean reward: -133.21
               Mean episode length: 38.99
                  Mean reward/step: -1.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 24.46s
                        Total time: 389.81s
                               ETA: 2051250.3s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.314s, learning 0.193s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 24.51s
                        Total time: 414.31s
                               ETA: 2071181.1s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 692 steps/s (collection: 23.489s, learning 0.171s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0282
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 23.66s
                        Total time: 437.97s
                               ETA: 2085178.0s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 676 steps/s (collection: 24.039s, learning 0.177s)
               Value function loss: 0.0971
                    Surrogate loss: -0.0171
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.40
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 24.22s
                        Total time: 462.19s
                               ETA: 2100427.1s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.230s, learning 0.236s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0194
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 24.47s
                        Total time: 486.66s
                               ETA: 2115431.9s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.092s, learning 0.189s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 24.28s
                        Total time: 510.94s
                               ETA: 2128415.7s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.846s, learning 0.183s)
               Value function loss: 0.1055
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 24.03s
                        Total time: 534.97s
                               ETA: 2139350.9s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.890s, learning 0.180s)
               Value function loss: 0.1851
                    Surrogate loss: -0.0106
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 24.07s
                        Total time: 559.04s
                               ETA: 2149599.8s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.240s, learning 0.193s)
               Value function loss: 0.1193
                    Surrogate loss: 0.0067
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 24.43s
                        Total time: 583.47s
                               ETA: 2160434.6s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.314s, learning 0.183s)
               Value function loss: 0.1934
                    Surrogate loss: 0.0076
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 24.50s
                        Total time: 607.97s
                               ETA: 2170722.7s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.921s, learning 0.179s)
               Value function loss: 0.1393
                    Surrogate loss: -0.0155
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 24.10s
                        Total time: 632.07s
                               ETA: 2178929.4s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 676 steps/s (collection: 23.977s, learning 0.252s)
               Value function loss: 0.1463
                    Surrogate loss: 0.0074
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 24.23s
                        Total time: 656.30s
                               ETA: 2187016.1s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.690s, learning 0.196s)
               Value function loss: 0.2263
                    Surrogate loss: 0.0132
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 23.89s
                        Total time: 680.18s
                               ETA: 2193476.3s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 676 steps/s (collection: 24.032s, learning 0.198s)
               Value function loss: 0.2164
                    Surrogate loss: 0.0159
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 24.23s
                        Total time: 704.41s
                               ETA: 2200606.1s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 684 steps/s (collection: 23.733s, learning 0.188s)
               Value function loss: 0.3092
                    Surrogate loss: -0.0005
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 23.92s
                        Total time: 728.33s
                               ETA: 2206364.5s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.669s, learning 0.237s)
               Value function loss: 0.3100
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 23.91s
                        Total time: 752.24s
                               ETA: 2211738.0s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.583s, learning 0.184s)
               Value function loss: 0.4936
                    Surrogate loss: -0.0127
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 23.77s
                        Total time: 776.01s
                               ETA: 2216408.2s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.747s, learning 0.207s)
               Value function loss: 0.5525
                    Surrogate loss: 0.0152
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 23.95s
                        Total time: 799.96s
                               ETA: 2221334.8s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 675 steps/s (collection: 23.943s, learning 0.313s)
               Value function loss: 0.4383
                    Surrogate loss: -0.0053
             Mean action noise std: 0.80
                       Mean reward: -134.44
               Mean episode length: 39.41
                  Mean reward/step: -1.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 24.26s
                        Total time: 824.22s
                               ETA: 2226811.6s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 892 steps/s (collection: 18.172s, learning 0.186s)
               Value function loss: 195.2360
                    Surrogate loss: 0.0800
             Mean action noise std: 0.80
                       Mean reward: -429.24
               Mean episode length: 299.00
                  Mean reward/step: -1.61
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 18.36s
                        Total time: 842.58s
                               ETA: 2216482.5s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.466s, learning 0.223s)
               Value function loss: 3.0764
                    Surrogate loss: -0.0076
             Mean action noise std: 0.80
                       Mean reward: -429.24
               Mean episode length: 299.00
                  Mean reward/step: -1.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 11.69s
                        Total time: 854.26s
                               ETA: 2189588.6s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.032s, learning 0.167s)
               Value function loss: 1.1818
                    Surrogate loss: -0.0112
             Mean action noise std: 0.80
                       Mean reward: -429.24
               Mean episode length: 299.00
                  Mean reward/step: -1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 12.20s
                        Total time: 866.46s
                               ETA: 2165313.8s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.157s, learning 0.174s)
               Value function loss: 1.2853
                    Surrogate loss: 0.0306
             Mean action noise std: 0.80
                       Mean reward: -429.27
               Mean episode length: 299.02
                  Mean reward/step: -1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 12.33s
                        Total time: 878.79s
                               ETA: 2142542.9s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.907s, learning 0.185s)
               Value function loss: 1.7244
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: -429.07
               Mean episode length: 299.03
                  Mean reward/step: -1.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 12.09s
                        Total time: 890.89s
                               ETA: 2120287.9s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.737s, learning 0.260s)
               Value function loss: 2.8310
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: -429.07
               Mean episode length: 299.03
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 12.00s
                        Total time: 902.88s
                               ETA: 2098846.6s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.955s, learning 0.164s)
               Value function loss: 1.1684
                    Surrogate loss: 0.0247
             Mean action noise std: 0.80
                       Mean reward: -423.51
               Mean episode length: 294.09
                  Mean reward/step: -1.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 12.12s
                        Total time: 915.00s
                               ETA: 2078656.6s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.018s, learning 0.197s)
               Value function loss: 2.3739
                    Surrogate loss: -0.0012
             Mean action noise std: 0.80
                       Mean reward: -414.86
               Mean episode length: 286.82
                  Mean reward/step: -1.27
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 12.22s
                        Total time: 927.22s
                               ETA: 2059576.9s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.238s, learning 0.199s)
               Value function loss: 1.2280
                    Surrogate loss: 0.0063
             Mean action noise std: 0.80
                       Mean reward: -410.03
               Mean episode length: 282.16
                  Mean reward/step: -1.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 12.44s
                        Total time: 939.65s
                               ETA: 2041808.7s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.553s, learning 0.201s)
               Value function loss: 0.5897
                    Surrogate loss: -0.0201
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 11.75s
                        Total time: 951.41s
                               ETA: 2023343.8s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.533s, learning 0.186s)
               Value function loss: 0.1483
                    Surrogate loss: -0.0160
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 11.72s
                        Total time: 963.13s
                               ETA: 2005574.4s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.889s, learning 0.197s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0167
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 12.09s
                        Total time: 975.21s
                               ETA: 1989278.9s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.094s, learning 0.213s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0210
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 12.31s
                        Total time: 987.52s
                               ETA: 1974076.3s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.860s, learning 0.171s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0261
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 12.03s
                        Total time: 999.55s
                               ETA: 1958928.9s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.111s, learning 0.166s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0278
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 12.28s
                        Total time: 1011.83s
                               ETA: 1944834.1s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.972s, learning 0.182s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 12.15s
                        Total time: 1023.98s
                               ETA: 1931039.6s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.614s, learning 0.231s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0111
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 11.85s
                        Total time: 1035.83s
                               ETA: 1917185.2s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.440s, learning 0.302s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0105
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 11.74s
                        Total time: 1047.57s
                               ETA: 1903646.8s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.874s, learning 0.184s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 12.06s
                        Total time: 1059.63s
                               ETA: 1891155.4s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.704s, learning 0.206s)
               Value function loss: 0.0701
                    Surrogate loss: 0.0069
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 11.91s
                        Total time: 1071.54s
                               ETA: 1878841.1s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.145s, learning 0.212s)
               Value function loss: 0.1036
                    Surrogate loss: 0.0049
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 12.36s
                        Total time: 1083.90s
                               ETA: 1867722.2s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.179s, learning 0.181s)
               Value function loss: 0.1612
                    Surrogate loss: -0.0011
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 12.36s
                        Total time: 1096.26s
                               ETA: 1856983.9s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.686s, learning 0.177s)
               Value function loss: 0.2056
                    Surrogate loss: -0.0051
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 11.86s
                        Total time: 1108.12s
                               ETA: 1845775.5s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.736s, learning 0.261s)
               Value function loss: 0.2789
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 12.00s
                        Total time: 1120.12s
                               ETA: 1835154.0s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1337 steps/s (collection: 11.960s, learning 0.286s)
               Value function loss: 0.2757
                    Surrogate loss: -0.0020
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 12.25s
                        Total time: 1132.36s
                               ETA: 1825276.6s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.048s, learning 0.267s)
               Value function loss: 0.1435
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 12.32s
                        Total time: 1144.68s
                               ETA: 1815821.6s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.383s, learning 0.196s)
               Value function loss: 0.1737
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 12.58s
                        Total time: 1157.26s
                               ETA: 1807073.9s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.400s, learning 0.213s)
               Value function loss: 0.1431
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 12.61s
                        Total time: 1169.87s
                               ETA: 1798647.5s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.076s, learning 0.251s)
               Value function loss: 0.2490
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 12.33s
                        Total time: 1182.20s
                               ETA: 1790042.2s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.000s, learning 0.285s)
               Value function loss: 0.3856
                    Surrogate loss: 0.0038
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 12.29s
                        Total time: 1194.48s
                               ETA: 1781632.2s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.144s, learning 0.279s)
               Value function loss: 0.1921
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 12.42s
                        Total time: 1206.90s
                               ETA: 1773670.8s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.386s, learning 0.202s)
               Value function loss: 0.1602
                    Surrogate loss: -0.0095
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 12.59s
                        Total time: 1219.49s
                               ETA: 1766178.6s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.895s, learning 0.281s)
               Value function loss: 0.2070
                    Surrogate loss: 0.0001
             Mean action noise std: 0.80
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 12.18s
                        Total time: 1231.67s
                               ETA: 1758312.3s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.762s, learning 0.172s)
               Value function loss: 0.2250
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 11.93s
                        Total time: 1243.60s
                               ETA: 1750326.6s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1337 steps/s (collection: 11.959s, learning 0.292s)
               Value function loss: 0.2734
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 12.25s
                        Total time: 1255.85s
                               ETA: 1743002.0s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.691s, learning 0.246s)
               Value function loss: 0.3805
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 11.94s
                        Total time: 1267.79s
                               ETA: 1735448.6s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.885s, learning 0.196s)
               Value function loss: 0.5116
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: -407.82
               Mean episode length: 279.94
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 12.08s
                        Total time: 1279.87s
                               ETA: 1728292.6s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.243s, learning 0.212s)
               Value function loss: 179.8083
                    Surrogate loss: 0.0653
             Mean action noise std: 0.79
                       Mean reward: -324.06
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 12.45s
                        Total time: 1292.33s
                               ETA: 1721826.0s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.714s, learning 0.186s)
               Value function loss: 4.1283
                    Surrogate loss: 0.0043
             Mean action noise std: 0.79
                       Mean reward: -324.06
               Mean episode length: 300.00
                  Mean reward/step: -1.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 11.90s
                        Total time: 1304.23s
                               ETA: 1714799.5s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.006s, learning 0.192s)
               Value function loss: 1.1632
                    Surrogate loss: 0.0112
             Mean action noise std: 0.79
                       Mean reward: -324.06
               Mean episode length: 300.00
                  Mean reward/step: -1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 12.20s
                        Total time: 1316.42s
                               ETA: 1708343.2s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.692s, learning 0.271s)
               Value function loss: 1.4604
                    Surrogate loss: 0.0208
             Mean action noise std: 0.79
                       Mean reward: -324.06
               Mean episode length: 300.00
                  Mean reward/step: -1.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 11.96s
                        Total time: 1328.39s
                               ETA: 1701750.6s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.107s, learning 0.281s)
               Value function loss: 1.8617
                    Surrogate loss: 0.0330
             Mean action noise std: 0.79
                       Mean reward: -323.94
               Mean episode length: 300.00
                  Mean reward/step: -1.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 12.39s
                        Total time: 1340.78s
                               ETA: 1695862.2s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.597s, learning 0.175s)
               Value function loss: 1.8885
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: -323.69
               Mean episode length: 300.00
                  Mean reward/step: -1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 11.77s
                        Total time: 1352.55s
                               ETA: 1689350.9s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.940s, learning 0.238s)
               Value function loss: 1.1200
                    Surrogate loss: 0.0858
             Mean action noise std: 0.79
                       Mean reward: -323.69
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 12.18s
                        Total time: 1364.73s
                               ETA: 1683500.5s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.916s, learning 0.176s)
               Value function loss: 1.3439
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: -321.67
               Mean episode length: 300.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 12.09s
                        Total time: 1376.82s
                               ETA: 1677687.6s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.014s, learning 0.191s)
               Value function loss: 0.6619
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: -320.55
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 12.21s
                        Total time: 1389.02s
                               ETA: 1672151.2s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.906s, learning 0.208s)
               Value function loss: 0.3143
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: -320.15
               Mean episode length: 300.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 12.11s
                        Total time: 1401.14s
                               ETA: 1666637.0s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.770s, learning 0.172s)
               Value function loss: 0.2174
                    Surrogate loss: 0.0219
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 11.94s
                        Total time: 1413.08s
                               ETA: 1661051.4s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.127s, learning 0.189s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 12.32s
                        Total time: 1425.40s
                               ETA: 1656029.7s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.849s, learning 0.196s)
               Value function loss: 0.1727
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 12.04s
                        Total time: 1437.44s
                               ETA: 1650811.4s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.125s, learning 0.284s)
               Value function loss: 0.6416
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 12.41s
                        Total time: 1449.85s
                               ETA: 1646124.5s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.952s, learning 0.229s)
               Value function loss: 1.4186
                    Surrogate loss: 0.0071
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 12.18s
                        Total time: 1462.03s
                               ETA: 1641286.4s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.262s, learning 0.286s)
               Value function loss: 2.3169
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 12.55s
                        Total time: 1474.58s
                               ETA: 1636963.3s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.137s, learning 0.330s)
               Value function loss: 2.1787
                    Surrogate loss: 0.0237
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 12.47s
                        Total time: 1487.05s
                               ETA: 1632646.2s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.313s, learning 0.215s)
               Value function loss: 3.2139
                    Surrogate loss: 0.0043
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 12.53s
                        Total time: 1499.57s
                               ETA: 1628488.9s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.188s, learning 0.342s)
               Value function loss: 3.5808
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 12.53s
                        Total time: 1512.11s
                               ETA: 1624423.6s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.041s, learning 0.287s)
               Value function loss: 3.3274
                    Surrogate loss: 0.0086
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 12.33s
                        Total time: 1524.43s
                               ETA: 1620229.2s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.356s, learning 0.291s)
               Value function loss: 6.7108
                    Surrogate loss: 0.0175
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 12.65s
                        Total time: 1537.08s
                               ETA: 1616458.2s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.333s, learning 0.179s)
               Value function loss: 2.0985
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 12.51s
                        Total time: 1549.59s
                               ETA: 1612625.0s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.321s, learning 0.242s)
               Value function loss: 1.4526
                    Surrogate loss: 0.0037
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 12.56s
                        Total time: 1562.16s
                               ETA: 1608923.3s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.290s, learning 0.227s)
               Value function loss: 1.4834
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 12.52s
                        Total time: 1574.67s
                               ETA: 1605250.3s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.258s, learning 0.245s)
               Value function loss: 1.5006
                    Surrogate loss: 0.0110
             Mean action noise std: 0.79
                       Mean reward: -319.90
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 12.50s
                        Total time: 1587.18s
                               ETA: 1601636.9s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.583s, learning 0.176s)
               Value function loss: 1.1544
                    Surrogate loss: -0.0002
             Mean action noise std: 0.79
                       Mean reward: -318.66
               Mean episode length: 298.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 11.76s
                        Total time: 1598.93s
                               ETA: 1597351.3s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.034s, learning 0.207s)
               Value function loss: 1.0380
                    Surrogate loss: 0.0020
             Mean action noise std: 0.79
                       Mean reward: -318.66
               Mean episode length: 298.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 12.24s
                        Total time: 1611.17s
                               ETA: 1593627.4s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.965s, learning 0.199s)
               Value function loss: 0.9959
                    Surrogate loss: 0.0125
             Mean action noise std: 0.79
                       Mean reward: -317.44
               Mean episode length: 298.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 12.16s
                        Total time: 1623.34s
                               ETA: 1589902.0s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.241s, learning 0.302s)
               Value function loss: 0.9354
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: -317.44
               Mean episode length: 298.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 12.54s
                        Total time: 1635.88s
                               ETA: 1586615.7s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.249s, learning 0.193s)
               Value function loss: 0.8680
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: -317.44
               Mean episode length: 298.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 12.44s
                        Total time: 1648.32s
                               ETA: 1583295.2s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.105s, learning 0.264s)
               Value function loss: 0.8699
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: -317.44
               Mean episode length: 298.09
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 12.37s
                        Total time: 1660.69s
                               ETA: 1579968.5s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.285s, learning 0.294s)
               Value function loss: 0.8252
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: -317.44
               Mean episode length: 298.09
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 12.58s
                        Total time: 1673.27s
                               ETA: 1576902.3s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.159s, learning 0.176s)
               Value function loss: 0.6446
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: -316.53
               Mean episode length: 297.59
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 12.34s
                        Total time: 1685.61s
                               ETA: 1573665.6s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.733s, learning 0.170s)
               Value function loss: 0.7245
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: -316.53
               Mean episode length: 297.59
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 11.90s
                        Total time: 1697.51s
                               ETA: 1570088.5s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.701s, learning 0.189s)
               Value function loss: 0.8354
                    Surrogate loss: -0.0023
             Mean action noise std: 0.79
                       Mean reward: -315.59
               Mean episode length: 297.32
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 11.89s
                        Total time: 1709.40s
                               ETA: 1566565.4s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.000s, learning 0.174s)
               Value function loss: 0.7940
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: -314.50
               Mean episode length: 297.11
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 12.17s
                        Total time: 1721.58s
                               ETA: 1563364.2s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.215s, learning 0.336s)
               Value function loss: 0.6009
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: -314.50
               Mean episode length: 297.11
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 12.55s
                        Total time: 1734.13s
                               ETA: 1560559.1s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.187s, learning 0.201s)
               Value function loss: 0.6902
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: -314.50
               Mean episode length: 297.11
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 12.39s
                        Total time: 1746.52s
                               ETA: 1557658.5s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.481s, learning 0.193s)
               Value function loss: 67.7156
                    Surrogate loss: 0.0473
             Mean action noise std: 0.79
                       Mean reward: -119.89
               Mean episode length: 300.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 12.67s
                        Total time: 1759.19s
                               ETA: 1555061.9s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.111s, learning 0.216s)
               Value function loss: 3.4822
                    Surrogate loss: 0.0121
             Mean action noise std: 0.79
                       Mean reward: -119.89
               Mean episode length: 300.00
                  Mean reward/step: -1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 12.33s
                        Total time: 1771.52s
                               ETA: 1552206.5s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.208s, learning 0.169s)
               Value function loss: 3.3440
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: -119.89
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 11.38s
                        Total time: 1782.89s
                               ETA: 1548575.8s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1327 steps/s (collection: 11.958s, learning 0.383s)
               Value function loss: 3.9664
                    Surrogate loss: 0.0030
             Mean action noise std: 0.79
                       Mean reward: -119.59
               Mean episode length: 300.00
                  Mean reward/step: -1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 12.34s
                        Total time: 1795.23s
                               ETA: 1545836.5s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.298s, learning 0.290s)
               Value function loss: 4.1648
                    Surrogate loss: 0.0143
             Mean action noise std: 0.79
                       Mean reward: -119.68
               Mean episode length: 300.00
                  Mean reward/step: -1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 12.59s
                        Total time: 1807.82s
                               ETA: 1543355.2s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.456s, learning 0.308s)
               Value function loss: 3.8639
                    Surrogate loss: 0.0212
             Mean action noise std: 0.79
                       Mean reward: -119.68
               Mean episode length: 300.00
                  Mean reward/step: -1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 12.76s
                        Total time: 1820.59s
                               ETA: 1541064.4s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.358s, learning 0.310s)
               Value function loss: 4.6144
                    Surrogate loss: 0.0075
             Mean action noise std: 0.79
                       Mean reward: -124.73
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 12.67s
                        Total time: 1833.25s
                               ETA: 1538732.4s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.253s, learning 0.291s)
               Value function loss: 1.6999
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: -122.97
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 12.54s
                        Total time: 1845.80s
                               ETA: 1536334.8s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.931s, learning 0.172s)
               Value function loss: 1.3987
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: -122.62
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 12.10s
                        Total time: 1857.90s
                               ETA: 1533613.0s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.821s, learning 0.220s)
               Value function loss: 1.1136
                    Surrogate loss: 0.0351
             Mean action noise std: 0.79
                       Mean reward: -123.05
               Mean episode length: 297.20
                  Mean reward/step: -0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 12.04s
                        Total time: 1869.94s
                               ETA: 1530884.8s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.109s, learning 0.219s)
               Value function loss: 0.4866
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 12.33s
                        Total time: 1882.27s
                               ETA: 1528433.9s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1338 steps/s (collection: 11.980s, learning 0.260s)
               Value function loss: 0.3012
                    Surrogate loss: 0.0217
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 12.24s
                        Total time: 1894.51s
                               ETA: 1525952.1s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.794s, learning 0.268s)
               Value function loss: 0.3368
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 12.06s
                        Total time: 1906.57s
                               ETA: 1523366.7s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.391s, learning 0.208s)
               Value function loss: 0.6439
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 12.60s
                        Total time: 1919.17s
                               ETA: 1521247.7s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.587s, learning 0.251s)
               Value function loss: 1.1046
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 12.84s
                        Total time: 1932.01s
                               ETA: 1519349.8s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.149s, learning 0.192s)
               Value function loss: 1.6112
                    Surrogate loss: 0.0050
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 12.34s
                        Total time: 1944.35s
                               ETA: 1517093.9s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.147s, learning 0.256s)
               Value function loss: 2.4189
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 12.40s
                        Total time: 1956.75s
                               ETA: 1514920.7s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.076s, learning 0.278s)
               Value function loss: 2.0580
                    Surrogate loss: 0.0130
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 12.35s
                        Total time: 1969.11s
                               ETA: 1512743.7s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.317s, learning 0.256s)
               Value function loss: 2.4187
                    Surrogate loss: 0.0070
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 12.57s
                        Total time: 1981.68s
                               ETA: 1510766.0s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.071s, learning 0.336s)
               Value function loss: 5.9089
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 12.41s
                        Total time: 1994.09s
                               ETA: 1508692.7s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1248 steps/s (collection: 12.921s, learning 0.207s)
               Value function loss: 5.6573
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 13.13s
                        Total time: 2007.21s
                               ETA: 1507191.5s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.350s, learning 0.222s)
               Value function loss: 3.4595
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 12.57s
                        Total time: 2019.79s
                               ETA: 1505298.2s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.981s, learning 0.203s)
               Value function loss: 2.6125
                    Surrogate loss: 0.0104
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 12.18s
                        Total time: 2031.97s
                               ETA: 1503146.0s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.493s, learning 0.204s)
               Value function loss: 2.5871
                    Surrogate loss: 0.0207
             Mean action noise std: 0.79
                       Mean reward: -123.45
               Mean episode length: 294.98
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 12.70s
                        Total time: 2044.67s
                               ETA: 1501402.3s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.163s, learning 0.172s)
               Value function loss: 2.4064
                    Surrogate loss: 0.0058
             Mean action noise std: 0.79
                       Mean reward: -122.92
               Mean episode length: 294.98
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 12.34s
                        Total time: 2057.00s
                               ETA: 1499419.8s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.921s, learning 0.181s)
               Value function loss: 2.3268
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: -122.92
               Mean episode length: 294.98
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 12.10s
                        Total time: 2069.10s
                               ETA: 1497297.1s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.221s, learning 0.250s)
               Value function loss: 1.9400
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: -122.75
               Mean episode length: 294.98
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 12.47s
                        Total time: 2081.58s
                               ETA: 1495469.8s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.442s, learning 0.299s)
               Value function loss: 2.2074
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: -122.75
               Mean episode length: 294.98
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 12.74s
                        Total time: 2094.32s
                               ETA: 1493861.6s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.638s, learning 0.204s)
               Value function loss: 2.1243
                    Surrogate loss: 0.0125
             Mean action noise std: 0.79
                       Mean reward: -122.75
               Mean episode length: 294.98
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 12.84s
                        Total time: 2107.16s
                               ETA: 1492346.8s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.162s, learning 0.205s)
               Value function loss: 1.4462
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: -122.75
               Mean episode length: 294.98
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 12.37s
                        Total time: 2119.53s
                               ETA: 1490519.6s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.016s, learning 0.190s)
               Value function loss: 1.1860
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: -124.49
               Mean episode length: 294.41
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 12.21s
                        Total time: 2131.73s
                               ETA: 1488605.4s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.061s, learning 0.189s)
               Value function loss: 0.9834
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: -124.49
               Mean episode length: 294.41
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 12.25s
                        Total time: 2143.98s
                               ETA: 1486747.7s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.979s, learning 0.232s)
               Value function loss: 0.9056
                    Surrogate loss: -0.0186
             Mean action noise std: 0.79
                       Mean reward: -124.49
               Mean episode length: 294.41
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 12.21s
                        Total time: 2156.19s
                               ETA: 1484888.8s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.327s, learning 0.273s)
               Value function loss: 1.0997
                    Surrogate loss: -0.0176
             Mean action noise std: 0.79
                       Mean reward: -124.49
               Mean episode length: 294.41
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 12.60s
                        Total time: 2168.79s
                               ETA: 1483321.0s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.304s, learning 0.307s)
               Value function loss: 1.0937
                    Surrogate loss: -0.0211
             Mean action noise std: 0.79
                       Mean reward: -127.56
               Mean episode length: 294.12
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 12.61s
                        Total time: 2181.40s
                               ETA: 1481782.0s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.163s, learning 0.191s)
               Value function loss: 1.3149
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: -131.09
               Mean episode length: 293.94
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 12.35s
                        Total time: 2193.76s
                               ETA: 1480090.4s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.540s, learning 0.184s)
               Value function loss: 1.5472
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: -132.60
               Mean episode length: 293.81
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 11.72s
                        Total time: 2205.48s
                               ETA: 1477999.2s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.142s, learning 0.249s)
               Value function loss: 73.3893
                    Surrogate loss: 0.0099
             Mean action noise std: 0.79
                       Mean reward: -153.21
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 12.39s
                        Total time: 2217.87s
                               ETA: 1476379.1s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.905s, learning 0.254s)
               Value function loss: 8.1077
                    Surrogate loss: 0.0501
             Mean action noise std: 0.79
                       Mean reward: -153.21
               Mean episode length: 300.00
                  Mean reward/step: -1.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 12.16s
                        Total time: 2230.03s
                               ETA: 1474627.3s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.803s, learning 0.211s)
               Value function loss: 27.5748
                    Surrogate loss: 0.0071
             Mean action noise std: 0.79
                       Mean reward: -149.93
               Mean episode length: 291.51
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 12.01s
                        Total time: 2242.05s
                               ETA: 1472803.0s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.049s, learning 0.190s)
               Value function loss: 3.9184
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: -149.93
               Mean episode length: 291.51
                  Mean reward/step: -1.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 12.24s
                        Total time: 2254.29s
                               ETA: 1471149.4s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.767s, learning 0.199s)
               Value function loss: 1.6640
                    Surrogate loss: 0.0318
             Mean action noise std: 0.79
                       Mean reward: -149.08
               Mean episode length: 291.51
                  Mean reward/step: -1.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 11.97s
                        Total time: 2266.25s
                               ETA: 1469340.2s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.019s, learning 0.173s)
               Value function loss: 1.1406
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: -148.05
               Mean episode length: 291.51
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 12.19s
                        Total time: 2278.44s
                               ETA: 1467699.6s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.942s, learning 0.188s)
               Value function loss: 0.8698
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: -148.05
               Mean episode length: 291.51
                  Mean reward/step: -0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 12.13s
                        Total time: 2290.57s
                               ETA: 1466039.8s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.321s, learning 0.335s)
               Value function loss: 0.7396
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: -145.34
               Mean episode length: 291.51
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 12.66s
                        Total time: 2303.23s
                               ETA: 1464736.3s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.745s, learning 0.310s)
               Value function loss: 0.4689
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: -144.71
               Mean episode length: 291.51
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 12.05s
                        Total time: 2315.28s
                               ETA: 1463068.9s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.122s, learning 0.171s)
               Value function loss: 0.3204
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: -144.38
               Mean episode length: 291.51
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 12.29s
                        Total time: 2327.58s
                               ETA: 1461571.9s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.803s, learning 0.170s)
               Value function loss: 0.3970
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 11.97s
                        Total time: 2339.55s
                               ETA: 1459894.1s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.238s, learning 0.173s)
               Value function loss: 0.5676
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 12.41s
                        Total time: 2351.96s
                               ETA: 1458508.0s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.694s, learning 0.173s)
               Value function loss: 1.0888
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 11.87s
                        Total time: 2363.83s
                               ETA: 1456804.0s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.916s, learning 0.183s)
               Value function loss: 1.8162
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 12.10s
                        Total time: 2375.93s
                               ETA: 1455263.1s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.200s, learning 0.285s)
               Value function loss: 2.0004
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 12.48s
                        Total time: 2388.41s
                               ETA: 1453975.1s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.859s, learning 0.229s)
               Value function loss: 2.2386
                    Surrogate loss: 0.0371
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 12.09s
                        Total time: 2400.50s
                               ETA: 1452462.3s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.960s, learning 0.219s)
               Value function loss: 2.2730
                    Surrogate loss: 0.0247
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 12.18s
                        Total time: 2412.68s
                               ETA: 1451022.7s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.093s, learning 0.180s)
               Value function loss: 2.2383
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 12.27s
                        Total time: 2424.95s
                               ETA: 1449655.9s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.085s, learning 0.169s)
               Value function loss: 1.9884
                    Surrogate loss: 0.0081
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 12.25s
                        Total time: 2437.20s
                               ETA: 1448294.1s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.051s, learning 0.172s)
               Value function loss: 1.8293
                    Surrogate loss: 0.0168
             Mean action noise std: 0.79
                       Mean reward: -141.46
               Mean episode length: 291.51
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 12.22s
                        Total time: 2449.43s
                               ETA: 1446930.0s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.775s, learning 0.183s)
               Value function loss: 1.7614
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: -142.02
               Mean episode length: 290.11
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 11.96s
                        Total time: 2461.38s
                               ETA: 1445426.5s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.759s, learning 0.240s)
               Value function loss: 1.8438
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: -142.02
               Mean episode length: 290.11
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 12.00s
                        Total time: 2473.38s
                               ETA: 1443964.4s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.286s, learning 0.284s)
               Value function loss: 1.4741
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: -141.38
               Mean episode length: 288.84
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 12.57s
                        Total time: 2485.95s
                               ETA: 1442850.3s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.798s, learning 0.181s)
               Value function loss: 1.5054
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: -142.06
               Mean episode length: 287.69
                  Mean reward/step: -0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 11.98s
                        Total time: 2497.93s
                               ETA: 1441408.1s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.814s, learning 0.204s)
               Value function loss: 1.5952
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: -142.06
               Mean episode length: 287.69
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 12.02s
                        Total time: 2509.95s
                               ETA: 1440004.9s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.001s, learning 0.177s)
               Value function loss: 1.8480
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: -141.57
               Mean episode length: 287.69
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 12.18s
                        Total time: 2522.13s
                               ETA: 1438708.5s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.927s, learning 0.176s)
               Value function loss: 2.0973
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: -141.57
               Mean episode length: 287.69
                  Mean reward/step: -0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 12.10s
                        Total time: 2534.23s
                               ETA: 1437384.5s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.319s, learning 0.240s)
               Value function loss: 2.4591
                    Surrogate loss: -0.0019
             Mean action noise std: 0.79
                       Mean reward: -140.92
               Mean episode length: 287.69
                  Mean reward/step: -0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 12.56s
                        Total time: 2546.79s
                               ETA: 1436332.1s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.044s, learning 0.222s)
               Value function loss: 2.7181
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: -140.92
               Mean episode length: 287.69
                  Mean reward/step: -0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 12.27s
                        Total time: 2559.06s
                               ETA: 1435127.4s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.325s, learning 0.311s)
               Value function loss: 3.3780
                    Surrogate loss: 0.0134
             Mean action noise std: 0.79
                       Mean reward: -140.92
               Mean episode length: 287.69
                  Mean reward/step: -0.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 12.64s
                        Total time: 2571.69s
                               ETA: 1434142.3s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.083s, learning 0.287s)
               Value function loss: 3.8802
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: -142.49
               Mean episode length: 287.04
                  Mean reward/step: -0.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 12.37s
                        Total time: 2584.06s
                               ETA: 1433020.7s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.117s, learning 0.175s)
               Value function loss: 3.8249
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: -142.64
               Mean episode length: 287.04
                  Mean reward/step: -0.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 12.29s
                        Total time: 2596.36s
                               ETA: 1431868.4s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.126s, learning 0.275s)
               Value function loss: 3.0939
                    Surrogate loss: 0.0125
             Mean action noise std: 0.79
                       Mean reward: -146.37
               Mean episode length: 286.13
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 12.40s
                        Total time: 2608.76s
                               ETA: 1430788.1s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.386s, learning 0.205s)
               Value function loss: 2.9455
                    Surrogate loss: 0.0043
             Mean action noise std: 0.79
                       Mean reward: -146.37
               Mean episode length: 286.13
                  Mean reward/step: -0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 12.59s
                        Total time: 2621.35s
                               ETA: 1429822.7s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.707s, learning 0.176s)
               Value function loss: 2.4092
                    Surrogate loss: 0.0489
             Mean action noise std: 0.79
                       Mean reward: -147.22
               Mean episode length: 286.13
                  Mean reward/step: -0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 12.88s
                        Total time: 2634.23s
                               ETA: 1429026.8s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.116s, learning 0.374s)
               Value function loss: 2.2358
                    Surrogate loss: 0.0131
             Mean action noise std: 0.79
                       Mean reward: -147.43
               Mean episode length: 285.89
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 12.49s
                        Total time: 2646.72s
                               ETA: 1428026.9s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.697s, learning 0.196s)
               Value function loss: 1.6062
                    Surrogate loss: 0.0057
             Mean action noise std: 0.79
                       Mean reward: -148.10
               Mean episode length: 285.89
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 12.89s
                        Total time: 2659.61s
                               ETA: 1427253.8s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.111s, learning 0.206s)
               Value function loss: 1.4254
                    Surrogate loss: 0.0337
             Mean action noise std: 0.79
                       Mean reward: -148.10
               Mean episode length: 285.89
                  Mean reward/step: -0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 12.32s
                        Total time: 2671.93s
                               ETA: 1426181.4s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.915s, learning 0.177s)
               Value function loss: 64.5978
                    Surrogate loss: 0.0298
             Mean action noise std: 0.79
                       Mean reward: -178.46
               Mean episode length: 300.00
                  Mean reward/step: -1.44
       Mean episode length/episode: 4.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 12.09s
                        Total time: 2684.02s
                               ETA: 1425000.7s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.177s, learning 0.175s)
               Value function loss: 0.2209
                    Surrogate loss: 0.0513
             Mean action noise std: 0.79
                       Mean reward: -178.46
               Mean episode length: 300.00
                  Mean reward/step: -1.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 12.35s
                        Total time: 2696.37s
                               ETA: 1423970.0s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.044s, learning 0.173s)
               Value function loss: 0.5491
                    Surrogate loss: 0.0288
             Mean action noise std: 0.79
                       Mean reward: -176.43
               Mean episode length: 297.17
                  Mean reward/step: -1.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 12.22s
                        Total time: 2708.59s
                               ETA: 1422879.0s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.572s, learning 0.196s)
               Value function loss: 0.3639
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: -177.57
               Mean episode length: 297.17
                  Mean reward/step: -1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 11.77s
                        Total time: 2720.36s
                               ETA: 1421564.7s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.606s, learning 0.213s)
               Value function loss: 0.2980
                    Surrogate loss: 0.0082
             Mean action noise std: 0.79
                       Mean reward: -177.23
               Mean episode length: 297.17
                  Mean reward/step: -0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 11.82s
                        Total time: 2732.18s
                               ETA: 1420290.4s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.194s, learning 0.271s)
               Value function loss: 0.2242
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: -177.23
               Mean episode length: 297.17
                  Mean reward/step: -0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 12.46s
                        Total time: 2744.64s
                               ETA: 1419363.1s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.373s, learning 0.305s)
               Value function loss: 0.2818
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: -175.99
               Mean episode length: 297.17
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 12.68s
                        Total time: 2757.32s
                               ETA: 1418554.8s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.658s, learning 0.188s)
               Value function loss: 0.4964
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: -179.22
               Mean episode length: 294.77
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 12.85s
                        Total time: 2770.16s
                               ETA: 1417840.9s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.293s, learning 0.249s)
               Value function loss: 0.8689
                    Surrogate loss: 0.0036
             Mean action noise std: 0.79
                       Mean reward: -180.19
               Mean episode length: 294.77
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 12.54s
                        Total time: 2782.71s
                               ETA: 1416979.3s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.956s, learning 0.170s)
               Value function loss: 1.6517
                    Surrogate loss: 0.0532
             Mean action noise std: 0.79
                       Mean reward: -180.59
               Mean episode length: 294.77
                  Mean reward/step: -0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 12.13s
                        Total time: 2794.83s
                               ETA: 1415915.9s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.167s)
               Value function loss: 1.9639
                    Surrogate loss: 0.0229
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 11.95s
                        Total time: 2806.78s
                               ETA: 1414772.0s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.968s, learning 0.226s)
               Value function loss: 1.9860
                    Surrogate loss: 0.0260
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 12.19s
                        Total time: 2818.97s
                               ETA: 1413763.9s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.856s, learning 0.270s)
               Value function loss: 1.8458
                    Surrogate loss: 0.0027
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 12.13s
                        Total time: 2831.10s
                               ETA: 1412732.1s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.172s, learning 0.176s)
               Value function loss: 1.9501
                    Surrogate loss: 0.0405
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 12.35s
                        Total time: 2843.45s
                               ETA: 1411820.3s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.505s, learning 0.177s)
               Value function loss: 1.4737
                    Surrogate loss: 0.0069
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 11.68s
                        Total time: 2855.13s
                               ETA: 1410588.6s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.024s, learning 0.259s)
               Value function loss: 1.2764
                    Surrogate loss: 0.0131
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 12.28s
                        Total time: 2867.41s
                               ETA: 1409664.0s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.280s, learning 0.202s)
               Value function loss: 1.1283
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 12.48s
                        Total time: 2879.89s
                               ETA: 1408846.3s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.316s, learning 0.173s)
               Value function loss: 0.9525
                    Surrogate loss: 0.0022
             Mean action noise std: 0.79
                       Mean reward: -182.17
               Mean episode length: 294.77
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 12.49s
                        Total time: 2892.38s
                               ETA: 1408039.3s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.050s, learning 0.175s)
               Value function loss: 0.9906
                    Surrogate loss: 0.0037
             Mean action noise std: 0.79
                       Mean reward: -181.66
               Mean episode length: 293.23
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 12.22s
                        Total time: 2904.61s
                               ETA: 1407112.4s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.629s, learning 0.178s)
               Value function loss: 0.9550
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: -181.66
               Mean episode length: 293.23
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 11.81s
                        Total time: 2916.41s
                               ETA: 1405992.9s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.872s, learning 0.176s)
               Value function loss: 1.0833
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: -181.12
               Mean episode length: 291.86
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 12.05s
                        Total time: 2928.46s
                               ETA: 1404999.7s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.067s, learning 0.270s)
               Value function loss: 1.1512
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: -180.60
               Mean episode length: 290.59
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 12.34s
                        Total time: 2940.80s
                               ETA: 1404153.8s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.074s, learning 0.248s)
               Value function loss: 1.3263
                    Surrogate loss: -0.0250
             Mean action noise std: 0.79
                       Mean reward: -180.60
               Mean episode length: 290.59
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 12.32s
                        Total time: 2953.12s
                               ETA: 1403308.6s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.921s, learning 0.291s)
               Value function loss: 1.5624
                    Surrogate loss: -0.0321
             Mean action noise std: 0.79
                       Mean reward: -180.52
               Mean episode length: 290.59
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 12.21s
                        Total time: 2965.33s
                               ETA: 1402419.1s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.509s, learning 0.300s)
               Value function loss: 1.7455
                    Surrogate loss: -0.0258
             Mean action noise std: 0.79
                       Mean reward: -181.27
               Mean episode length: 290.59
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 12.81s
                        Total time: 2978.14s
                               ETA: 1401819.2s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.339s, learning 0.311s)
               Value function loss: 1.8462
                    Surrogate loss: -0.0248
             Mean action noise std: 0.79
                       Mean reward: -181.46
               Mean episode length: 288.80
                  Mean reward/step: -0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 12.65s
                        Total time: 2990.79s
                               ETA: 1401150.4s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.255s, learning 0.167s)
               Value function loss: 2.1922
                    Surrogate loss: -0.0206
             Mean action noise std: 0.79
                       Mean reward: -180.14
               Mean episode length: 288.80
                  Mean reward/step: -0.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 12.42s
                        Total time: 3003.21s
                               ETA: 1400381.3s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.070s, learning 0.187s)
               Value function loss: 2.7219
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: -180.14
               Mean episode length: 288.80
                  Mean reward/step: -0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 12.26s
                        Total time: 3015.47s
                               ETA: 1399542.8s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.136s, learning 0.190s)
               Value function loss: 2.6842
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: -180.14
               Mean episode length: 288.80
                  Mean reward/step: -0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 12.33s
                        Total time: 3027.80s
                               ETA: 1398743.9s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.502s, learning 0.215s)
               Value function loss: 2.5720
                    Surrogate loss: 0.0535
             Mean action noise std: 0.79
                       Mean reward: -180.23
               Mean episode length: 288.80
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 11.72s
                        Total time: 3039.51s
                               ETA: 1397671.9s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.261s, learning 0.251s)
               Value function loss: 2.4409
                    Surrogate loss: 0.0099
             Mean action noise std: 0.79
                       Mean reward: -179.79
               Mean episode length: 288.80
                  Mean reward/step: -0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 12.51s
                        Total time: 3052.03s
                               ETA: 1396973.5s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.559s, learning 0.262s)
               Value function loss: 2.4052
                    Surrogate loss: 0.0062
             Mean action noise std: 0.79
                       Mean reward: -177.49
               Mean episode length: 288.80
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 12.82s
                        Total time: 3064.85s
                               ETA: 1396421.9s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.231s, learning 0.250s)
               Value function loss: 2.1716
                    Surrogate loss: 0.0094
             Mean action noise std: 0.79
                       Mean reward: -176.43
               Mean episode length: 288.80
                  Mean reward/step: -0.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 12.48s
                        Total time: 3077.33s
                               ETA: 1395721.4s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.279s, learning 0.196s)
               Value function loss: 2.5313
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: -176.43
               Mean episode length: 288.80
                  Mean reward/step: -0.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 12.47s
                        Total time: 3089.80s
                               ETA: 1395024.3s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.103s, learning 0.354s)
               Value function loss: 2.5757
                    Surrogate loss: 0.0357
             Mean action noise std: 0.79
                       Mean reward: -174.04
               Mean episode length: 288.80
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 12.46s
                        Total time: 3102.26s
                               ETA: 1394325.5s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.511s, learning 0.220s)
               Value function loss: 2.0018
                    Surrogate loss: 0.0054
             Mean action noise std: 0.79
                       Mean reward: -173.54
               Mean episode length: 286.98
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 12.73s
                        Total time: 3114.99s
                               ETA: 1393755.0s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.180s, learning 0.213s)
               Value function loss: 1.8776
                    Surrogate loss: 0.0170
             Mean action noise std: 0.79
                       Mean reward: -173.68
               Mean episode length: 286.98
                  Mean reward/step: -0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 12.39s
                        Total time: 3127.38s
                               ETA: 1393039.3s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.422s, learning 0.248s)
               Value function loss: 80.3629
                    Surrogate loss: 0.0053
             Mean action noise std: 0.79
                       Mean reward: -141.65
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 4.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 12.67s
                        Total time: 3140.05s
                               ETA: 1392452.6s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.303s, learning 0.214s)
               Value function loss: 1.1920
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: -141.65
               Mean episode length: 300.00
                  Mean reward/step: -1.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 12.52s
                        Total time: 3152.57s
                               ETA: 1391803.6s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.014s, learning 0.180s)
               Value function loss: 0.5765
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: -141.55
               Mean episode length: 300.00
                  Mean reward/step: -1.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 12.19s
                        Total time: 3164.76s
                               ETA: 1391018.2s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.919s, learning 0.186s)
               Value function loss: 1.2592
                    Surrogate loss: 0.0059
             Mean action noise std: 0.79
                       Mean reward: -141.55
               Mean episode length: 300.00
                  Mean reward/step: -1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 12.10s
                        Total time: 3176.87s
                               ETA: 1390200.5s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.842s, learning 0.173s)
               Value function loss: 0.7473
                    Surrogate loss: 0.0156
             Mean action noise std: 0.79
                       Mean reward: -141.63
               Mean episode length: 300.00
                  Mean reward/step: -0.99
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 12.02s
                        Total time: 3188.88s
                               ETA: 1389350.8s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.064s, learning 0.244s)
               Value function loss: 0.6615
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: -141.68
               Mean episode length: 300.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 12.31s
                        Total time: 3201.19s
                               ETA: 1388635.4s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.799s, learning 0.178s)
               Value function loss: 0.6472
                    Surrogate loss: 0.0035
             Mean action noise std: 0.79
                       Mean reward: -141.68
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 11.98s
                        Total time: 3213.17s
                               ETA: 1387783.2s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.170s)
               Value function loss: 0.9627
                    Surrogate loss: 0.0048
             Mean action noise std: 0.79
                       Mean reward: -142.29
               Mean episode length: 297.57
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 11.52s
                        Total time: 3224.69s
                               ETA: 1386742.1s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.382s, learning 0.204s)
               Value function loss: 0.8895
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: -141.43
               Mean episode length: 295.22
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 12.59s
                        Total time: 3237.28s
                               ETA: 1386165.6s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.864s, learning 0.341s)
               Value function loss: 1.4533
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: -140.37
               Mean episode length: 295.22
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 12.21s
                        Total time: 3249.48s
                               ETA: 1385431.9s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.224s, learning 0.174s)
               Value function loss: 2.4132
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: -140.56
               Mean episode length: 293.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 12.40s
                        Total time: 3261.88s
                               ETA: 1384786.2s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.878s, learning 0.280s)
               Value function loss: 2.2843
                    Surrogate loss: 0.0172
             Mean action noise std: 0.79
                       Mean reward: -140.56
               Mean episode length: 293.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 12.16s
                        Total time: 3274.04s
                               ETA: 1384044.0s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.833s, learning 0.233s)
               Value function loss: 6.1286
                    Surrogate loss: 0.0127
             Mean action noise std: 0.79
                       Mean reward: -140.56
               Mean episode length: 293.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 12.07s
                        Total time: 3286.10s
                               ETA: 1383269.4s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.897s, learning 0.182s)
               Value function loss: 2.1258
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: -140.56
               Mean episode length: 293.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 12.08s
                        Total time: 3298.18s
                               ETA: 1382506.8s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.902s, learning 0.209s)
               Value function loss: 2.9190
                    Surrogate loss: 0.0620
             Mean action noise std: 0.79
                       Mean reward: -140.56
               Mean episode length: 293.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 12.11s
                        Total time: 3310.29s
                               ETA: 1381763.6s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.657s, learning 0.232s)
               Value function loss: 1.6780
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: -141.69
               Mean episode length: 291.21
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 11.89s
                        Total time: 3322.18s
                               ETA: 1380934.4s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.170s, learning 0.237s)
               Value function loss: 1.1769
                    Surrogate loss: 0.0022
             Mean action noise std: 0.79
                       Mean reward: -141.69
               Mean episode length: 291.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 12.41s
                        Total time: 3334.59s
                               ETA: 1380326.4s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.289s, learning 0.201s)
               Value function loss: 0.8420
                    Surrogate loss: -0.0019
             Mean action noise std: 0.79
                       Mean reward: -141.69
               Mean episode length: 291.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 12.49s
                        Total time: 3347.08s
                               ETA: 1379757.5s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1340 steps/s (collection: 11.925s, learning 0.297s)
               Value function loss: 0.8203
                    Surrogate loss: 0.0057
             Mean action noise std: 0.79
                       Mean reward: -142.53
               Mean episode length: 289.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 12.22s
                        Total time: 3359.30s
                               ETA: 1379083.1s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.157s, learning 0.304s)
               Value function loss: 0.7267
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: -142.75
               Mean episode length: 289.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 12.46s
                        Total time: 3371.76s
                               ETA: 1378511.9s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.153s, learning 0.245s)
               Value function loss: 0.8035
                    Surrogate loss: 0.0122
             Mean action noise std: 0.79
                       Mean reward: -143.46
               Mean episode length: 287.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 12.40s
                        Total time: 3384.16s
                               ETA: 1377919.2s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.452s, learning 0.243s)
               Value function loss: 0.7507
                    Surrogate loss: 0.0027
             Mean action noise std: 0.79
                       Mean reward: -143.22
               Mean episode length: 287.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 12.69s
                        Total time: 3396.85s
                               ETA: 1377452.0s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.416s, learning 0.202s)
               Value function loss: 0.7115
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: -143.34
               Mean episode length: 286.11
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 12.62s
                        Total time: 3409.47s
                               ETA: 1376957.6s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.995s, learning 0.181s)
               Value function loss: 0.6966
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: -144.29
               Mean episode length: 284.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 12.18s
                        Total time: 3421.65s
                               ETA: 1376289.2s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.079s, learning 0.177s)
               Value function loss: 0.5665
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: -144.29
               Mean episode length: 284.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 12.26s
                        Total time: 3433.90s
                               ETA: 1375658.1s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.409s, learning 0.320s)
               Value function loss: 0.7436
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: -143.27
               Mean episode length: 284.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 12.73s
                        Total time: 3446.63s
                               ETA: 1375220.5s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.382s, learning 0.296s)
               Value function loss: 0.6974
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: -143.27
               Mean episode length: 284.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 12.68s
                        Total time: 3459.31s
                               ETA: 1374766.3s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.172s, learning 0.174s)
               Value function loss: 0.7922
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: -142.86
               Mean episode length: 284.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 12.35s
                        Total time: 3471.66s
                               ETA: 1374184.0s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1334 steps/s (collection: 11.997s, learning 0.281s)
               Value function loss: 0.9298
                    Surrogate loss: -0.0267
             Mean action noise std: 0.79
                       Mean reward: -142.86
               Mean episode length: 284.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 12.28s
                        Total time: 3483.94s
                               ETA: 1373579.7s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.531s, learning 0.246s)
               Value function loss: 1.1850
                    Surrogate loss: -0.0245
             Mean action noise std: 0.79
                       Mean reward: -142.86
               Mean episode length: 284.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 12.78s
                        Total time: 3496.71s
                               ETA: 1373176.0s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.148s, learning 0.186s)
               Value function loss: 1.5443
                    Surrogate loss: -0.0231
             Mean action noise std: 0.79
                       Mean reward: -141.86
               Mean episode length: 284.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 12.33s
                        Total time: 3509.05s
                               ETA: 1372601.9s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.846s, learning 0.262s)
               Value function loss: 2.2458
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: -142.23
               Mean episode length: 282.15
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 12.11s
                        Total time: 3521.16s
                               ETA: 1371943.9s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.462s, learning 0.343s)
               Value function loss: 2.0557
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: -139.71
               Mean episode length: 282.15
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 12.80s
                        Total time: 3533.96s
                               ETA: 1371561.3s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.498s, learning 0.265s)
               Value function loss: 2.3903
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: -139.71
               Mean episode length: 282.15
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 12.76s
                        Total time: 3546.72s
                               ETA: 1371165.5s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.650s, learning 0.323s)
               Value function loss: 3.6923
                    Surrogate loss: -0.0193
             Mean action noise std: 0.79
                       Mean reward: -138.70
               Mean episode length: 282.15
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 12.97s
                        Total time: 3559.69s
                               ETA: 1370853.5s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.546s, learning 0.292s)
               Value function loss: 4.6453
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: -137.44
               Mean episode length: 282.15
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 12.84s
                        Total time: 3572.53s
                               ETA: 1370491.9s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1236 steps/s (collection: 13.036s, learning 0.219s)
               Value function loss: 5.6160
                    Surrogate loss: -0.0225
             Mean action noise std: 0.79
                       Mean reward: -135.31
               Mean episode length: 282.15
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 13.25s
                        Total time: 3585.79s
                               ETA: 1370292.4s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.531s, learning 0.200s)
               Value function loss: 6.5624
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: -135.31
               Mean episode length: 282.15
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 12.73s
                        Total time: 3598.52s
                               ETA: 1369895.0s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.605s, learning 0.320s)
               Value function loss: 93.5604
                    Surrogate loss: 0.0424
             Mean action noise std: 0.79
                       Mean reward: -79.35
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 4.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 12.93s
                        Total time: 3611.44s
                               ETA: 1369574.2s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.414s, learning 0.363s)
               Value function loss: 0.7222
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: -79.35
               Mean episode length: 300.00
                  Mean reward/step: -1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 12.78s
                        Total time: 3624.22s
                               ETA: 1369199.7s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.294s, learning 0.179s)
               Value function loss: 1.4594
                    Surrogate loss: 0.0141
             Mean action noise std: 0.79
                       Mean reward: -80.29
               Mean episode length: 300.00
                  Mean reward/step: -1.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 12.47s
                        Total time: 3636.69s
                               ETA: 1368713.5s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.104s, learning 0.186s)
               Value function loss: 1.3789
                    Surrogate loss: 0.0079
             Mean action noise std: 0.79
                       Mean reward: -81.99
               Mean episode length: 300.00
                  Mean reward/step: -1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 12.29s
                        Total time: 3648.98s
                               ETA: 1368162.3s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.768s, learning 0.200s)
               Value function loss: 1.1909
                    Surrogate loss: 0.0234
             Mean action noise std: 0.79
                       Mean reward: -80.97
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 11.97s
                        Total time: 3660.95s
                               ETA: 1367494.7s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.796s, learning 0.232s)
               Value function loss: 1.0437
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: -80.97
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 12.03s
                        Total time: 3672.98s
                               ETA: 1366854.8s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.038s, learning 0.274s)
               Value function loss: 1.4183
                    Surrogate loss: 0.0149
             Mean action noise std: 0.79
                       Mean reward: -81.02
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 12.31s
                        Total time: 3685.29s
                               ETA: 1366324.6s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.430s, learning 0.170s)
               Value function loss: 2.7218
                    Surrogate loss: 0.0071
             Mean action noise std: 0.79
                       Mean reward: -80.57
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 12.60s
                        Total time: 3697.89s
                               ETA: 1365904.5s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.401s, learning 0.179s)
               Value function loss: 1.9909
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: -81.41
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 11.58s
                        Total time: 3709.47s
                               ETA: 1365112.2s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.162s, learning 0.292s)
               Value function loss: 2.3491
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: -81.94
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 12.45s
                        Total time: 3721.92s
                               ETA: 1364645.8s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.717s, learning 0.266s)
               Value function loss: 2.6503
                    Surrogate loss: 0.0105
             Mean action noise std: 0.79
                       Mean reward: -80.98
               Mean episode length: 297.82
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 11.98s
                        Total time: 3733.91s
                               ETA: 1364010.7s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.322s, learning 0.271s)
               Value function loss: 2.4811
                    Surrogate loss: 0.0091
             Mean action noise std: 0.79
                       Mean reward: -84.01
               Mean episode length: 289.44
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 12.59s
                        Total time: 3746.50s
                               ETA: 1363602.5s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.043s, learning 0.191s)
               Value function loss: 2.2555
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: -86.84
               Mean episode length: 281.29
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 12.23s
                        Total time: 3758.73s
                               ETA: 1363066.7s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.381s, learning 0.313s)
               Value function loss: 1.7450
                    Surrogate loss: 0.0040
             Mean action noise std: 0.79
                       Mean reward: -88.50
               Mean episode length: 277.40
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 12.69s
                        Total time: 3771.43s
                               ETA: 1362701.4s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.020s, learning 0.215s)
               Value function loss: 2.3282
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: -90.13
               Mean episode length: 273.70
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 12.24s
                        Total time: 3783.66s
                               ETA: 1362173.2s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.729s, learning 0.184s)
               Value function loss: 2.3635
                    Surrogate loss: 0.0026
             Mean action noise std: 0.79
                       Mean reward: -90.81
               Mean episode length: 271.91
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 11.91s
                        Total time: 3795.58s
                               ETA: 1361532.9s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.229s, learning 0.267s)
               Value function loss: 1.7901
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: -90.99
               Mean episode length: 270.20
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 12.50s
                        Total time: 3808.07s
                               ETA: 1361105.3s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.064s, learning 0.180s)
               Value function loss: 1.6045
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: -92.40
               Mean episode length: 268.58
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 12.24s
                        Total time: 3820.31s
                               ETA: 1360591.2s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.027s, learning 0.299s)
               Value function loss: 1.3521
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: -92.16
               Mean episode length: 268.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 12.33s
                        Total time: 3832.64s
                               ETA: 1360109.9s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.528s, learning 0.227s)
               Value function loss: 1.0030
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: -94.91
               Mean episode length: 265.61
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 12.75s
                        Total time: 3845.40s
                               ETA: 1359783.5s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.363s, learning 0.281s)
               Value function loss: 1.0436
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: -96.73
               Mean episode length: 262.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 12.64s
                        Total time: 3858.04s
                               ETA: 1359420.4s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.276s, learning 0.306s)
               Value function loss: 0.8683
                    Surrogate loss: -0.0008
             Mean action noise std: 0.79
                       Mean reward: -96.98
               Mean episode length: 260.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 12.58s
                        Total time: 3870.62s
                               ETA: 1359037.6s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.552s, learning 0.323s)
               Value function loss: 0.7945
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: -96.98
               Mean episode length: 260.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 12.88s
                        Total time: 3883.50s
                               ETA: 1358760.4s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.613s, learning 0.221s)
               Value function loss: 0.7779
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: -95.44
               Mean episode length: 260.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 12.83s
                        Total time: 3896.33s
                               ETA: 1358470.3s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.587s, learning 0.184s)
               Value function loss: 0.7859
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: -95.43
               Mean episode length: 260.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 12.77s
                        Total time: 3909.10s
                               ETA: 1358160.6s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.153s, learning 0.195s)
               Value function loss: 0.8858
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: -95.16
               Mean episode length: 259.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 12.35s
                        Total time: 3921.45s
                               ETA: 1357706.6s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.022s, learning 0.296s)
               Value function loss: 0.9065
                    Surrogate loss: -0.0247
             Mean action noise std: 0.79
                       Mean reward: -95.03
               Mean episode length: 259.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 12.32s
                        Total time: 3933.77s
                               ETA: 1357245.2s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.356s, learning 0.216s)
               Value function loss: 1.0884
                    Surrogate loss: -0.0222
             Mean action noise std: 0.79
                       Mean reward: -95.03
               Mean episode length: 259.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 12.57s
                        Total time: 3946.34s
                               ETA: 1356874.0s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.929s, learning 0.176s)
               Value function loss: 1.3279
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: -95.03
               Mean episode length: 259.17
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 12.11s
                        Total time: 3958.45s
                               ETA: 1356345.6s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.042s, learning 0.187s)
               Value function loss: 1.7215
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: -95.12
               Mean episode length: 259.17
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 12.23s
                        Total time: 3970.67s
                               ETA: 1355863.1s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.132s, learning 0.302s)
               Value function loss: 2.1641
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: -95.88
               Mean episode length: 259.17
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 12.43s
                        Total time: 3983.11s
                               ETA: 1355453.2s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.717s, learning 0.257s)
               Value function loss: 2.8845
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: -96.78
               Mean episode length: 259.17
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 12.97s
                        Total time: 3996.08s
                               ETA: 1355229.3s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.501s, learning 0.261s)
               Value function loss: 2.9092
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: -96.99
               Mean episode length: 256.89
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 12.76s
                        Total time: 4008.85s
                               ETA: 1354935.3s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1231 steps/s (collection: 13.032s, learning 0.276s)
               Value function loss: 4.1957
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: -98.47
               Mean episode length: 256.54
                  Mean reward/step: -0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 13.31s
                        Total time: 4022.15s
                               ETA: 1354827.0s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.453s, learning 0.238s)
               Value function loss: 6.0632
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: -98.64
               Mean episode length: 256.54
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 12.69s
                        Total time: 4034.84s
                               ETA: 1354512.2s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.212s, learning 0.280s)
               Value function loss: 8.5046
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: -99.89
               Mean episode length: 256.54
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 12.49s
                        Total time: 4047.34s
                               ETA: 1354132.7s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.041s, learning 0.182s)
               Value function loss: 5.8206
                    Surrogate loss: -0.0283
             Mean action noise std: 0.79
                       Mean reward: -99.73
               Mean episode length: 256.54
                  Mean reward/step: -0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 12.22s
                        Total time: 4059.56s
                               ETA: 1353666.2s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.368s, learning 0.324s)
               Value function loss: 86.7174
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: -102.45
               Mean episode length: 300.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 4.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 12.69s
                        Total time: 4072.25s
                               ETA: 1353358.4s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.736s, learning 0.352s)
               Value function loss: 2.2012
                    Surrogate loss: 0.0300
             Mean action noise std: 0.79
                       Mean reward: -103.63
               Mean episode length: 298.22
                  Mean reward/step: -1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 13.09s
                        Total time: 4085.34s
                               ETA: 1353184.0s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.799s, learning 0.248s)
               Value function loss: 2.0445
                    Surrogate loss: 0.0027
             Mean action noise std: 0.79
                       Mean reward: -102.50
               Mean episode length: 298.22
                  Mean reward/step: -1.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 12.05s
                        Total time: 4097.39s
                               ETA: 1352666.9s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.824s, learning 0.218s)
               Value function loss: 1.4455
                    Surrogate loss: 0.0000
             Mean action noise std: 0.79
                       Mean reward: -102.50
               Mean episode length: 298.22
                  Mean reward/step: -1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 12.04s
                        Total time: 4109.43s
                               ETA: 1352151.4s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.894s, learning 0.303s)
               Value function loss: 0.9496
                    Surrogate loss: 0.0272
             Mean action noise std: 0.79
                       Mean reward: -101.11
               Mean episode length: 298.22
                  Mean reward/step: -1.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 12.20s
                        Total time: 4121.63s
                               ETA: 1351689.9s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.907s, learning 0.278s)
               Value function loss: 1.1178
                    Surrogate loss: 0.0326
             Mean action noise std: 0.79
                       Mean reward: -100.86
               Mean episode length: 298.22
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 12.18s
                        Total time: 4133.81s
                               ETA: 1351227.3s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.586s, learning 0.355s)
               Value function loss: 1.1007
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: -100.86
               Mean episode length: 298.22
                  Mean reward/step: -0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 12.94s
                        Total time: 4146.75s
                               ETA: 1351014.2s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.202s, learning 0.169s)
               Value function loss: 1.1794
                    Surrogate loss: 0.0078
             Mean action noise std: 0.79
                       Mean reward: -103.05
               Mean episode length: 298.22
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 12.37s
                        Total time: 4159.12s
                               ETA: 1350617.5s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.739s, learning 0.173s)
               Value function loss: 1.1021
                    Surrogate loss: 0.0042
             Mean action noise std: 0.79
                       Mean reward: -106.21
               Mean episode length: 294.14
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 11.91s
                        Total time: 4171.03s
                               ETA: 1350074.5s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.034s, learning 0.187s)
               Value function loss: 0.9499
                    Surrogate loss: 0.0355
             Mean action noise std: 0.79
                       Mean reward: -106.67
               Mean episode length: 294.14
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 12.22s
                        Total time: 4183.26s
                               ETA: 1349634.9s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.902s, learning 0.250s)
               Value function loss: 1.4097
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: -101.85
               Mean episode length: 291.95
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 12.15s
                        Total time: 4195.41s
                               ETA: 1349175.8s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.975s, learning 0.216s)
               Value function loss: 1.8720
                    Surrogate loss: 0.0170
             Mean action noise std: 0.79
                       Mean reward: -102.47
               Mean episode length: 291.95
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 12.19s
                        Total time: 4207.60s
                               ETA: 1348731.8s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.954s, learning 0.225s)
               Value function loss: 1.7631
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: -103.58
               Mean episode length: 289.89
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 12.18s
                        Total time: 4219.78s
                               ETA: 1348287.1s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.866s, learning 0.213s)
               Value function loss: 3.1046
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: -103.77
               Mean episode length: 289.89
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 12.08s
                        Total time: 4231.86s
                               ETA: 1347812.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.240s, learning 0.298s)
               Value function loss: 1.5258
                    Surrogate loss: 0.0240
             Mean action noise std: 0.79
                       Mean reward: -104.11
               Mean episode length: 289.89
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 12.54s
                        Total time: 4244.40s
                               ETA: 1347487.5s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.138s, learning 0.188s)
               Value function loss: 1.1404
                    Surrogate loss: 0.0147
             Mean action noise std: 0.79
                       Mean reward: -103.44
               Mean episode length: 289.89
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 12.33s
                        Total time: 4256.72s
                               ETA: 1347097.1s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.053s, learning 0.203s)
               Value function loss: 0.8726
                    Surrogate loss: 0.0034
             Mean action noise std: 0.79
                       Mean reward: -103.09
               Mean episode length: 286.39
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 12.26s
                        Total time: 4268.98s
                               ETA: 1346686.9s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.491s, learning 0.313s)
               Value function loss: 0.7047
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: -103.09
               Mean episode length: 286.39
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 12.80s
                        Total time: 4281.78s
                               ETA: 1346451.5s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.534s, learning 0.171s)
               Value function loss: 0.4992
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: -104.88
               Mean episode length: 284.81
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 12.71s
                        Total time: 4294.49s
                               ETA: 1346186.7s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.697s, learning 0.220s)
               Value function loss: 0.4524
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: -105.03
               Mean episode length: 284.81
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 11.92s
                        Total time: 4306.40s
                               ETA: 1345677.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.240s, learning 0.201s)
               Value function loss: 0.5283
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: -105.01
               Mean episode length: 284.81
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 12.44s
                        Total time: 4318.85s
                               ETA: 1345333.7s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.297s, learning 0.239s)
               Value function loss: 0.5667
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: -106.44
               Mean episode length: 284.81
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 12.54s
                        Total time: 4331.38s
                               ETA: 1345021.9s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.405s, learning 0.269s)
               Value function loss: 0.4931
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: -107.67
               Mean episode length: 284.81
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 12.67s
                        Total time: 4344.06s
                               ETA: 1344754.9s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.723s, learning 0.206s)
               Value function loss: 0.5583
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: -110.72
               Mean episode length: 283.63
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 11.93s
                        Total time: 4355.98s
                               ETA: 1344259.3s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.949s, learning 0.179s)
               Value function loss: 0.4690
                    Surrogate loss: -0.0190
             Mean action noise std: 0.79
                       Mean reward: -110.72
               Mean episode length: 283.63
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 12.13s
                        Total time: 4368.11s
                               ETA: 1343827.9s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.886s, learning 0.276s)
               Value function loss: 0.6744
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: -111.11
               Mean episode length: 283.63
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 12.16s
                        Total time: 4380.27s
                               ETA: 1343409.7s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.340s, learning 0.189s)
               Value function loss: 0.5348
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: -111.11
               Mean episode length: 283.63
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 12.53s
                        Total time: 4392.80s
                               ETA: 1343106.2s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.137s, learning 0.191s)
               Value function loss: 0.7586
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: -110.76
               Mean episode length: 283.63
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 12.33s
                        Total time: 4405.13s
                               ETA: 1342743.1s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.624s, learning 0.230s)
               Value function loss: 0.9405
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: -110.76
               Mean episode length: 283.63
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 12.85s
                        Total time: 4417.98s
                               ETA: 1342542.0s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.932s, learning 0.278s)
               Value function loss: 1.4752
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: -110.76
               Mean episode length: 283.63
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 12.21s
                        Total time: 4430.19s
                               ETA: 1342146.7s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.052s, learning 0.180s)
               Value function loss: 1.4174
                    Surrogate loss: 0.0023
             Mean action noise std: 0.78
                       Mean reward: -110.76
               Mean episode length: 283.63
                  Mean reward/step: -0.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 12.23s
                        Total time: 4442.43s
                               ETA: 1341760.6s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.860s, learning 0.193s)
               Value function loss: 1.9865
                    Surrogate loss: 0.0131
             Mean action noise std: 0.78
                       Mean reward: -112.28
               Mean episode length: 283.63
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 12.05s
                        Total time: 4454.48s
                               ETA: 1341322.8s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.016s, learning 0.184s)
               Value function loss: 2.2353
                    Surrogate loss: 0.0097
             Mean action noise std: 0.78
                       Mean reward: -113.89
               Mean episode length: 283.63
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 12.20s
                        Total time: 4466.68s
                               ETA: 1340931.7s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.694s, learning 0.178s)
               Value function loss: 2.2753
                    Surrogate loss: 0.0139
             Mean action noise std: 0.78
                       Mean reward: -114.38
               Mean episode length: 283.63
                  Mean reward/step: -0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 11.87s
                        Total time: 4478.55s
                               ETA: 1340444.7s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.864s, learning 0.202s)
               Value function loss: 2.5878
                    Surrogate loss: 0.0067
             Mean action noise std: 0.78
                       Mean reward: -117.19
               Mean episode length: 282.40
                  Mean reward/step: -0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 12.07s
                        Total time: 4490.62s
                               ETA: 1340018.7s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.099s, learning 0.191s)
               Value function loss: 2.6759
                    Surrogate loss: 0.0261
             Mean action noise std: 0.78
                       Mean reward: -117.22
               Mean episode length: 282.40
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 12.29s
                        Total time: 4502.91s
                               ETA: 1339661.7s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.291s, learning 0.228s)
               Value function loss: 3.8096
                    Surrogate loss: 0.0210
             Mean action noise std: 0.78
                       Mean reward: -115.17
               Mean episode length: 282.40
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 12.52s
                        Total time: 4515.43s
                               ETA: 1339374.6s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.729s, learning 0.169s)
               Value function loss: 4.2346
                    Surrogate loss: 0.0003
             Mean action noise std: 0.78
                       Mean reward: -116.61
               Mean episode length: 280.78
                  Mean reward/step: -0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 11.90s
                        Total time: 4527.32s
                               ETA: 1338905.5s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.264s, learning 0.180s)
               Value function loss: 80.2695
                    Surrogate loss: 0.0260
             Mean action noise std: 0.78
                       Mean reward: -129.69
               Mean episode length: 300.00
                  Mean reward/step: -1.35
       Mean episode length/episode: 4.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 12.44s
                        Total time: 4539.77s
                               ETA: 1338600.0s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.176s, learning 0.179s)
               Value function loss: 6.7863
                    Surrogate loss: 0.2299
             Mean action noise std: 0.78
                       Mean reward: -129.43
               Mean episode length: 300.00
                  Mean reward/step: -1.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 12.36s
                        Total time: 4552.12s
                               ETA: 1338270.3s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.296s, learning 0.248s)
               Value function loss: 0.9328
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: -129.64
               Mean episode length: 300.00
                  Mean reward/step: -1.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 12.54s
                        Total time: 4564.67s
                               ETA: 1337997.9s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.932s, learning 0.284s)
               Value function loss: 0.8208
                    Surrogate loss: 0.0074
             Mean action noise std: 0.78
                       Mean reward: -129.34
               Mean episode length: 300.00
                  Mean reward/step: -1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 12.22s
                        Total time: 4576.88s
                               ETA: 1337631.0s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.127s, learning 0.211s)
               Value function loss: 2.4986
                    Surrogate loss: 0.0094
             Mean action noise std: 0.78
                       Mean reward: -130.71
               Mean episode length: 298.61
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 12.34s
                        Total time: 4589.22s
                               ETA: 1337301.6s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.950s, learning 0.177s)
               Value function loss: 1.9903
                    Surrogate loss: 0.0083
             Mean action noise std: 0.78
                       Mean reward: -130.71
               Mean episode length: 298.61
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 12.13s
                        Total time: 4601.35s
                               ETA: 1336913.0s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.903s, learning 0.171s)
               Value function loss: 2.8370
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: -134.97
               Mean episode length: 295.45
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 12.07s
                        Total time: 4613.42s
                               ETA: 1336511.1s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.935s, learning 0.180s)
               Value function loss: 1.9365
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: -132.21
               Mean episode length: 293.86
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 12.11s
                        Total time: 4625.54s
                               ETA: 1336123.1s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.169s, learning 0.178s)
               Value function loss: 1.8112
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: -129.60
               Mean episode length: 291.48
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 12.35s
                        Total time: 4637.88s
                               ETA: 1335804.3s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.066s, learning 0.169s)
               Value function loss: 1.7014
                    Surrogate loss: 0.0079
             Mean action noise std: 0.78
                       Mean reward: -128.96
               Mean episode length: 291.48
                  Mean reward/step: -0.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 12.23s
                        Total time: 4650.12s
                               ETA: 1335454.9s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.226s, learning 0.259s)
               Value function loss: 1.6948
                    Surrogate loss: 0.0287
             Mean action noise std: 0.78
                       Mean reward: -130.14
               Mean episode length: 291.48
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 12.49s
                        Total time: 4662.60s
                               ETA: 1335179.4s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.351s, learning 0.282s)
               Value function loss: 2.5765
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: -132.18
               Mean episode length: 290.42
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 12.63s
                        Total time: 4675.24s
                               ETA: 1334947.5s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.916s, learning 0.185s)
               Value function loss: 1.5337
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: -136.90
               Mean episode length: 289.69
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 12.10s
                        Total time: 4687.34s
                               ETA: 1334565.4s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.944s, learning 0.178s)
               Value function loss: 1.1387
                    Surrogate loss: 0.0095
             Mean action noise std: 0.78
                       Mean reward: -135.37
               Mean episode length: 289.69
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 12.12s
                        Total time: 4699.46s
                               ETA: 1334191.1s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.396s, learning 0.216s)
               Value function loss: 0.7631
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: -134.34
               Mean episode length: 289.69
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 12.61s
                        Total time: 4712.07s
                               ETA: 1333957.6s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.806s, learning 0.273s)
               Value function loss: 0.4449
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: -133.63
               Mean episode length: 289.69
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 12.08s
                        Total time: 4724.15s
                               ETA: 1333575.1s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.407s, learning 0.258s)
               Value function loss: 0.7390
                    Surrogate loss: -0.0002
             Mean action noise std: 0.78
                       Mean reward: -133.68
               Mean episode length: 287.99
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 12.66s
                        Total time: 4736.81s
                               ETA: 1333359.6s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.250s, learning 0.191s)
               Value function loss: 0.4124
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: -133.88
               Mean episode length: 287.99
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 12.44s
                        Total time: 4749.26s
                               ETA: 1333082.6s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.874s, learning 0.165s)
               Value function loss: 0.4292
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: -133.21
               Mean episode length: 287.99
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 12.04s
                        Total time: 4761.29s
                               ETA: 1332694.2s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.941s, learning 0.202s)
               Value function loss: 0.5221
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: -134.20
               Mean episode length: 286.50
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 12.14s
                        Total time: 4773.44s
                               ETA: 1332337.2s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.063s, learning 0.197s)
               Value function loss: 0.4226
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: -134.07
               Mean episode length: 286.50
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 12.26s
                        Total time: 4785.70s
                               ETA: 1332014.7s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.364s, learning 0.275s)
               Value function loss: 1.2080
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: -144.93
               Mean episode length: 282.53
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 12.64s
                        Total time: 4798.34s
                               ETA: 1331799.2s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.349s, learning 0.213s)
               Value function loss: 0.4803
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: -146.50
               Mean episode length: 281.27
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 12.56s
                        Total time: 4810.90s
                               ETA: 1331563.5s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.533s, learning 0.296s)
               Value function loss: 0.5885
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: -148.77
               Mean episode length: 280.13
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 12.83s
                        Total time: 4823.73s
                               ETA: 1331402.5s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.533s, learning 0.302s)
               Value function loss: 0.5242
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: -148.49
               Mean episode length: 280.13
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 12.84s
                        Total time: 4836.56s
                               ETA: 1331244.1s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.639s, learning 0.188s)
               Value function loss: 0.9701
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: -150.67
               Mean episode length: 279.17
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 12.83s
                        Total time: 4849.39s
                               ETA: 1331084.2s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.039s, learning 0.189s)
               Value function loss: 0.7605
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: -150.67
               Mean episode length: 279.17
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 12.23s
                        Total time: 4861.62s
                               ETA: 1330761.3s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.395s, learning 0.236s)
               Value function loss: 1.0059
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: -150.67
               Mean episode length: 279.17
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 12.63s
                        Total time: 4874.25s
                               ETA: 1330550.1s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.147s, learning 0.176s)
               Value function loss: 1.3042
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: -150.67
               Mean episode length: 279.17
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 12.32s
                        Total time: 4886.57s
                               ETA: 1330256.0s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.822s, learning 0.273s)
               Value function loss: 1.5312
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: -150.67
               Mean episode length: 279.17
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 12.10s
                        Total time: 4898.67s
                               ETA: 1329901.6s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.522s, learning 0.192s)
               Value function loss: 2.3567
                    Surrogate loss: 0.0239
             Mean action noise std: 0.78
                       Mean reward: -150.41
               Mean episode length: 279.17
                  Mean reward/step: -0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 11.71s
                        Total time: 4910.38s
                               ETA: 1329445.9s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.923s, learning 0.176s)
               Value function loss: 2.4994
                    Surrogate loss: 0.0211
             Mean action noise std: 0.78
                       Mean reward: -153.47
               Mean episode length: 279.17
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 12.10s
                        Total time: 4922.48s
                               ETA: 1329096.7s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.061s, learning 0.202s)
               Value function loss: 2.3456
                    Surrogate loss: 0.0080
             Mean action noise std: 0.78
                       Mean reward: -152.95
               Mean episode length: 279.17
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 12.26s
                        Total time: 4934.74s
                               ETA: 1328793.2s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.014s, learning 0.180s)
               Value function loss: 2.8570
                    Surrogate loss: 0.0076
             Mean action noise std: 0.78
                       Mean reward: -152.49
               Mean episode length: 279.17
                  Mean reward/step: -0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 12.19s
                        Total time: 4946.94s
                               ETA: 1328472.9s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.227s, learning 0.167s)
               Value function loss: 2.3186
                    Surrogate loss: 0.0355
             Mean action noise std: 0.78
                       Mean reward: -151.40
               Mean episode length: 279.17
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 12.39s
                        Total time: 4959.33s
                               ETA: 1328207.7s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.260s, learning 0.193s)
               Value function loss: 3.7656
                    Surrogate loss: 0.0189
             Mean action noise std: 0.78
                       Mean reward: -149.96
               Mean episode length: 277.01
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 12.45s
                        Total time: 4971.79s
                               ETA: 1327959.9s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.059s, learning 0.177s)
               Value function loss: 2.6687
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: -149.68
               Mean episode length: 277.01
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 12.24s
                        Total time: 4984.02s
                               ETA: 1327655.3s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.382s, learning 0.174s)
               Value function loss: 84.3973
                    Surrogate loss: 0.0137
             Mean action noise std: 0.78
                       Mean reward: -114.25
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 4.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 12.56s
                        Total time: 4996.58s
                               ETA: 1327437.3s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.595s, learning 0.174s)
               Value function loss: 4.6568
                    Surrogate loss: 0.0135
             Mean action noise std: 0.78
                       Mean reward: -114.14
               Mean episode length: 300.00
                  Mean reward/step: -1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 11.77s
                        Total time: 5008.35s
                               ETA: 1327011.9s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.644s, learning 0.311s)
               Value function loss: 1.4780
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: -114.86
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 11.96s
                        Total time: 5020.30s
                               ETA: 1326638.0s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.024s, learning 0.299s)
               Value function loss: 1.6896
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: -114.86
               Mean episode length: 300.00
                  Mean reward/step: -1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 12.32s
                        Total time: 5032.63s
                               ETA: 1326363.0s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.019s, learning 0.183s)
               Value function loss: 0.8770
                    Surrogate loss: 0.0206
             Mean action noise std: 0.78
                       Mean reward: -114.23
               Mean episode length: 300.00
                  Mean reward/step: -1.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 12.20s
                        Total time: 5044.83s
                               ETA: 1326057.3s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.821s, learning 0.180s)
               Value function loss: 0.9579
                    Surrogate loss: 0.0009
             Mean action noise std: 0.78
                       Mean reward: -114.43
               Mean episode length: 294.97
                  Mean reward/step: -0.99
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 12.00s
                        Total time: 5056.83s
                               ETA: 1325700.6s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.756s, learning 0.197s)
               Value function loss: 1.4631
                    Surrogate loss: 0.0081
             Mean action noise std: 0.78
                       Mean reward: -114.43
               Mean episode length: 294.97
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 11.95s
                        Total time: 5068.78s
                               ETA: 1325333.1s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.015s, learning 0.205s)
               Value function loss: 1.6327
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: -117.59
               Mean episode length: 292.54
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 12.22s
                        Total time: 5081.00s
                               ETA: 1325037.2s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.988s, learning 0.190s)
               Value function loss: 1.6417
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: -121.40
               Mean episode length: 292.54
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 12.18s
                        Total time: 5093.18s
                               ETA: 1324731.7s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.458s, learning 0.278s)
               Value function loss: 1.4388
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: -120.96
               Mean episode length: 290.26
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 12.74s
                        Total time: 5105.91s
                               ETA: 1324572.5s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.421s, learning 0.228s)
               Value function loss: 1.0520
                    Surrogate loss: 0.0064
             Mean action noise std: 0.78
                       Mean reward: -123.07
               Mean episode length: 288.02
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 12.65s
                        Total time: 5118.56s
                               ETA: 1324391.6s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.567s, learning 0.272s)
               Value function loss: 1.0316
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: -124.16
               Mean episode length: 288.02
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 12.84s
                        Total time: 5131.40s
                               ETA: 1324260.6s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.014s, learning 0.199s)
               Value function loss: 1.5145
                    Surrogate loss: 0.0073
             Mean action noise std: 0.78
                       Mean reward: -130.99
               Mean episode length: 288.02
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 12.21s
                        Total time: 5143.61s
                               ETA: 1323969.1s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.567s, learning 0.339s)
               Value function loss: 1.7897
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: -132.91
               Mean episode length: 286.06
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 12.91s
                        Total time: 5156.52s
                               ETA: 1323857.0s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.323s, learning 0.313s)
               Value function loss: 1.6648
                    Surrogate loss: 0.0141
             Mean action noise std: 0.78
                       Mean reward: -134.35
               Mean episode length: 286.06
                  Mean reward/step: -0.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 12.64s
                        Total time: 5169.16s
                               ETA: 1323676.0s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.636s, learning 0.226s)
               Value function loss: 2.1431
                    Surrogate loss: 0.0170
             Mean action noise std: 0.78
                       Mean reward: -137.70
               Mean episode length: 281.34
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 12.86s
                        Total time: 5182.02s
                               ETA: 1323553.9s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.189s, learning 0.182s)
               Value function loss: 1.7820
                    Surrogate loss: 0.0130
             Mean action noise std: 0.78
                       Mean reward: -139.40
               Mean episode length: 274.36
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 12.37s
                        Total time: 5194.39s
                               ETA: 1323307.1s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.482s, learning 0.302s)
               Value function loss: 1.3231
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: -140.20
               Mean episode length: 274.36
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 12.78s
                        Total time: 5207.17s
                               ETA: 1323166.5s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.965s, learning 0.185s)
               Value function loss: 1.3080
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: -146.05
               Mean episode length: 271.18
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 12.15s
                        Total time: 5219.32s
                               ETA: 1322865.9s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.621s, learning 0.213s)
               Value function loss: 1.2125
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: -148.92
               Mean episode length: 269.69
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 11.83s
                        Total time: 5231.16s
                               ETA: 1322486.6s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.272s, learning 0.264s)
               Value function loss: 1.0007
                    Surrogate loss: 0.0016
             Mean action noise std: 0.78
                       Mean reward: -149.59
               Mean episode length: 269.69
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 12.54s
                        Total time: 5243.69s
                               ETA: 1322286.4s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.447s, learning 0.222s)
               Value function loss: 1.0749
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: -153.10
               Mean episode length: 267.85
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 12.67s
                        Total time: 5256.36s
                               ETA: 1322120.6s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.256s, learning 0.181s)
               Value function loss: 1.0513
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: -153.72
               Mean episode length: 263.25
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 12.44s
                        Total time: 5268.80s
                               ETA: 1321897.5s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.245s, learning 0.272s)
               Value function loss: 0.7582
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: -156.95
               Mean episode length: 263.25
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 12.52s
                        Total time: 5281.31s
                               ETA: 1321695.5s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.127s, learning 0.173s)
               Value function loss: 0.8124
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: -158.21
               Mean episode length: 261.09
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 12.30s
                        Total time: 5293.62s
                               ETA: 1321440.3s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.055s, learning 0.206s)
               Value function loss: 0.9750
                    Surrogate loss: -0.0199
             Mean action noise std: 0.78
                       Mean reward: -158.99
               Mean episode length: 258.69
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 12.26s
                        Total time: 5305.88s
                               ETA: 1321176.4s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.971s, learning 0.186s)
               Value function loss: 1.0440
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: -160.08
               Mean episode length: 256.78
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 12.16s
                        Total time: 5318.03s
                               ETA: 1320888.2s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.642s, learning 0.174s)
               Value function loss: 1.1745
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: -160.08
               Mean episode length: 256.78
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 11.82s
                        Total time: 5329.85s
                               ETA: 1320516.6s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.256s, learning 0.180s)
               Value function loss: 1.4223
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: -160.08
               Mean episode length: 256.78
                  Mean reward/step: -0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 12.44s
                        Total time: 5342.28s
                               ETA: 1320299.9s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.000s, learning 0.203s)
               Value function loss: 1.6683
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: -160.08
               Mean episode length: 256.78
                  Mean reward/step: -0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 12.20s
                        Total time: 5354.49s
                               ETA: 1320027.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.050s, learning 0.243s)
               Value function loss: 1.7236
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: -160.08
               Mean episode length: 256.78
                  Mean reward/step: -0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 12.29s
                        Total time: 5366.78s
                               ETA: 1319777.6s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.300s, learning 0.279s)
               Value function loss: 2.2374
                    Surrogate loss: 0.0559
             Mean action noise std: 0.78
                       Mean reward: -161.65
               Mean episode length: 256.78
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 12.58s
                        Total time: 5379.36s
                               ETA: 1319599.3s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.109s, learning 0.172s)
               Value function loss: 1.9764
                    Surrogate loss: 0.0055
             Mean action noise std: 0.78
                       Mean reward: -163.15
               Mean episode length: 256.78
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 12.28s
                        Total time: 5391.64s
                               ETA: 1319349.0s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.199s, learning 0.209s)
               Value function loss: 1.9910
                    Surrogate loss: 0.0205
             Mean action noise std: 0.78
                       Mean reward: -165.14
               Mean episode length: 255.09
                  Mean reward/step: -0.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 12.41s
                        Total time: 5404.05s
                               ETA: 1319130.8s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.064s, learning 0.199s)
               Value function loss: 1.9732
                    Surrogate loss: 0.0104
             Mean action noise std: 0.78
                       Mean reward: -168.11
               Mean episode length: 255.09
                  Mean reward/step: -0.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 12.26s
                        Total time: 5416.31s
                               ETA: 1318878.2s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.983s, learning 0.197s)
               Value function loss: 2.0493
                    Surrogate loss: 0.0031
             Mean action noise std: 0.78
                       Mean reward: -169.53
               Mean episode length: 255.09
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 12.18s
                        Total time: 5428.49s
                               ETA: 1318607.0s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.160s, learning 0.220s)
               Value function loss: 1.5530
                    Surrogate loss: 0.0250
             Mean action noise std: 0.78
                       Mean reward: -169.42
               Mean episode length: 255.09
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 12.38s
                        Total time: 5440.87s
                               ETA: 1318385.2s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.412s, learning 0.267s)
               Value function loss: 1.3894
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: -170.50
               Mean episode length: 255.09
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 12.68s
                        Total time: 5453.55s
                               ETA: 1318236.8s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.305s, learning 0.214s)
               Value function loss: 90.8243
                    Surrogate loss: 0.0199
             Mean action noise std: 0.78
                       Mean reward: -169.81
               Mean episode length: 300.00
                  Mean reward/step: -1.37
       Mean episode length/episode: 4.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 12.52s
                        Total time: 5466.07s
                               ETA: 1318050.5s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.883s, learning 0.195s)
               Value function loss: 1.0775
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: -169.64
               Mean episode length: 300.00
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 12.08s
                        Total time: 5478.15s
                               ETA: 1317758.7s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.485s, learning 0.168s)
               Value function loss: 1.0173
                    Surrogate loss: 0.0097
             Mean action noise std: 0.78
                       Mean reward: -169.60
               Mean episode length: 297.17
                  Mean reward/step: -1.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 11.65s
                        Total time: 5489.80s
                               ETA: 1317366.4s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.609s, learning 0.180s)
               Value function loss: 0.6901
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: -169.85
               Mean episode length: 297.17
                  Mean reward/step: -1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 11.79s
                        Total time: 5501.59s
                               ETA: 1317008.6s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.410s, learning 0.298s)
               Value function loss: 1.0863
                    Surrogate loss: 0.0139
             Mean action noise std: 0.78
                       Mean reward: -168.11
               Mean episode length: 294.76
                  Mean reward/step: -0.97
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 12.71s
                        Total time: 5514.30s
                               ETA: 1316871.9s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.397s, learning 0.330s)
               Value function loss: 0.7199
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: -168.11
               Mean episode length: 294.76
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 12.73s
                        Total time: 5527.02s
                               ETA: 1316740.4s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.918s, learning 0.171s)
               Value function loss: 1.0246
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: -167.43
               Mean episode length: 289.91
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 12.09s
                        Total time: 5539.11s
                               ETA: 1316457.8s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.812s, learning 0.195s)
               Value function loss: 1.3810
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: -162.73
               Mean episode length: 280.24
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 12.01s
                        Total time: 5551.12s
                               ETA: 1316157.3s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.026s, learning 0.256s)
               Value function loss: 1.0681
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: -160.33
               Mean episode length: 275.49
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 12.28s
                        Total time: 5563.40s
                               ETA: 1315923.0s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.123s, learning 0.332s)
               Value function loss: 0.9304
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: -159.72
               Mean episode length: 275.49
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 12.46s
                        Total time: 5575.86s
                               ETA: 1315730.5s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.181s, learning 0.303s)
               Value function loss: 1.0913
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: -156.21
               Mean episode length: 273.17
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 12.48s
                        Total time: 5588.34s
                               ETA: 1315545.8s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.261s, learning 0.164s)
               Value function loss: 1.3041
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: -157.59
               Mean episode length: 270.98
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 12.42s
                        Total time: 5600.77s
                               ETA: 1315347.9s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.736s, learning 0.171s)
               Value function loss: 1.8132
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: -156.73
               Mean episode length: 270.98
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 11.91s
                        Total time: 5612.67s
                               ETA: 1315029.6s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.936s, learning 0.246s)
               Value function loss: 1.9192
                    Surrogate loss: 0.0215
             Mean action noise std: 0.78
                       Mean reward: -156.22
               Mean episode length: 270.98
                  Mean reward/step: -0.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 12.18s
                        Total time: 5624.86s
                               ETA: 1314777.0s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.183s, learning 0.349s)
               Value function loss: 2.3757
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: -157.85
               Mean episode length: 264.86
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 12.53s
                        Total time: 5637.39s
                               ETA: 1314607.1s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.074s, learning 0.315s)
               Value function loss: 2.4952
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: -157.61
               Mean episode length: 255.44
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 12.39s
                        Total time: 5649.78s
                               ETA: 1314404.8s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.169s, learning 0.269s)
               Value function loss: 2.1929
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: -154.61
               Mean episode length: 247.13
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 12.44s
                        Total time: 5662.21s
                               ETA: 1314214.6s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.127s, learning 0.182s)
               Value function loss: 1.7884
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: -155.05
               Mean episode length: 245.49
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 12.31s
                        Total time: 5674.52s
                               ETA: 1313995.2s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.781s, learning 0.196s)
               Value function loss: 1.3440
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: -157.23
               Mean episode length: 240.82
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 11.98s
                        Total time: 5686.50s
                               ETA: 1313700.2s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.165s, learning 0.181s)
               Value function loss: 1.1087
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: -156.70
               Mean episode length: 237.88
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 12.35s
                        Total time: 5698.85s
                               ETA: 1313491.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.657s, learning 0.224s)
               Value function loss: 1.3898
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: -156.66
               Mean episode length: 237.88
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 11.88s
                        Total time: 5710.73s
                               ETA: 1313177.1s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1328 steps/s (collection: 11.890s, learning 0.441s)
               Value function loss: 1.8702
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: -153.37
               Mean episode length: 237.88
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 12.33s
                        Total time: 5723.06s
                               ETA: 1312967.3s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.264s, learning 0.172s)
               Value function loss: 1.9022
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: -152.99
               Mean episode length: 237.88
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 12.44s
                        Total time: 5735.49s
                               ETA: 1312782.2s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.158s, learning 0.216s)
               Value function loss: 2.5725
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: -150.98
               Mean episode length: 238.65
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 12.37s
                        Total time: 5747.87s
                               ETA: 1312583.8s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.308s, learning 0.191s)
               Value function loss: 5.8674
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: -150.98
               Mean episode length: 238.65
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 12.50s
                        Total time: 5760.37s
                               ETA: 1312414.6s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.714s, learning 0.206s)
               Value function loss: 30.9151
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: -158.03
               Mean episode length: 240.04
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 11.92s
                        Total time: 5772.29s
                               ETA: 1312114.6s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.767s, learning 0.189s)
               Value function loss: 1.6970
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: -158.02
               Mean episode length: 237.61
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 11.96s
                        Total time: 5784.24s
                               ETA: 1311824.0s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.683s, learning 0.177s)
               Value function loss: 2.3857
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: -158.02
               Mean episode length: 237.61
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 11.86s
                        Total time: 5796.10s
                               ETA: 1311513.3s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.079s, learning 0.183s)
               Value function loss: 9.5680
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: -161.39
               Mean episode length: 236.83
                  Mean reward/step: -0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 12.26s
                        Total time: 5808.37s
                               ETA: 1311294.5s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.406s, learning 0.277s)
               Value function loss: 1.8459
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: -161.39
               Mean episode length: 236.83
                  Mean reward/step: -0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 11.68s
                        Total time: 5820.05s
                               ETA: 1310946.3s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.135s, learning 0.321s)
               Value function loss: 1.8785
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: -162.19
               Mean episode length: 239.20
                  Mean reward/step: -0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 12.46s
                        Total time: 5832.51s
                               ETA: 1310773.3s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.061s, learning 0.187s)
               Value function loss: 2.9215
                    Surrogate loss: 0.0184
             Mean action noise std: 0.78
                       Mean reward: -164.14
               Mean episode length: 241.68
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 12.25s
                        Total time: 5844.75s
                               ETA: 1310554.2s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.162s, learning 0.170s)
               Value function loss: 2.6827
                    Surrogate loss: 0.0063
             Mean action noise std: 0.78
                       Mean reward: -164.94
               Mean episode length: 244.13
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 12.33s
                        Total time: 5857.09s
                               ETA: 1310355.1s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.054s, learning 0.319s)
               Value function loss: 2.5187
                    Surrogate loss: 0.0045
             Mean action noise std: 0.78
                       Mean reward: -166.40
               Mean episode length: 244.15
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 12.37s
                        Total time: 5869.46s
                               ETA: 1310165.8s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.722s, learning 0.294s)
               Value function loss: 2.8558
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: -167.97
               Mean episode length: 246.56
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 12.02s
                        Total time: 5881.48s
                               ETA: 1309897.9s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.102s, learning 0.263s)
               Value function loss: 2.5819
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: -169.47
               Mean episode length: 251.32
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 12.37s
                        Total time: 5893.84s
                               ETA: 1309708.6s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.344s, learning 0.280s)
               Value function loss: 2.6597
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: -168.42
               Mean episode length: 251.32
                  Mean reward/step: -0.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 12.62s
                        Total time: 5906.46s
                               ETA: 1309577.5s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.549s, learning 0.285s)
               Value function loss: 82.6919
                    Surrogate loss: 0.0564
             Mean action noise std: 0.78
                       Mean reward: -153.78
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 12.83s
                        Total time: 5919.30s
                               ETA: 1309493.6s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.565s, learning 0.308s)
               Value function loss: 0.5855
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: -153.78
               Mean episode length: 300.00
                  Mean reward/step: -1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 12.87s
                        Total time: 5932.17s
                               ETA: 1309418.2s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.141s, learning 0.229s)
               Value function loss: 1.8059
                    Surrogate loss: 0.0265
             Mean action noise std: 0.78
                       Mean reward: -153.17
               Mean episode length: 300.00
                  Mean reward/step: -1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 12.37s
                        Total time: 5944.54s
                               ETA: 1309232.6s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.565s, learning 0.201s)
               Value function loss: 0.7853
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: -153.17
               Mean episode length: 300.00
                  Mean reward/step: -1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 11.77s
                        Total time: 5956.31s
                               ETA: 1308914.8s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.107s, learning 0.344s)
               Value function loss: 0.7805
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: -153.10
               Mean episode length: 300.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 12.45s
                        Total time: 5968.76s
                               ETA: 1308748.8s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.293s, learning 0.321s)
               Value function loss: 0.8424
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: -153.24
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 12.61s
                        Total time: 5981.37s
                               ETA: 1308618.9s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.293s, learning 0.225s)
               Value function loss: 0.9962
                    Surrogate loss: 0.0097
             Mean action noise std: 0.78
                       Mean reward: -153.24
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 12.52s
                        Total time: 5993.89s
                               ETA: 1308468.7s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.930s, learning 0.223s)
               Value function loss: 1.5281
                    Surrogate loss: 0.0107
             Mean action noise std: 0.78
                       Mean reward: -154.33
               Mean episode length: 300.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 12.15s
                        Total time: 6006.04s
                               ETA: 1308239.8s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.672s, learning 0.268s)
               Value function loss: 1.7721
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: -153.06
               Mean episode length: 297.61
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 12.94s
                        Total time: 6018.98s
                               ETA: 1308182.6s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.989s, learning 0.173s)
               Value function loss: 1.4705
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: -152.47
               Mean episode length: 297.61
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 12.16s
                        Total time: 6031.15s
                               ETA: 1307957.0s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.831s, learning 0.173s)
               Value function loss: 1.3649
                    Surrogate loss: 0.0204
             Mean action noise std: 0.78
                       Mean reward: -153.15
               Mean episode length: 297.61
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 12.00s
                        Total time: 6043.15s
                               ETA: 1307697.9s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.814s, learning 0.183s)
               Value function loss: 1.3465
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: -151.96
               Mean episode length: 295.43
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 12.00s
                        Total time: 6055.14s
                               ETA: 1307438.5s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.053s, learning 0.288s)
               Value function loss: 1.0826
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: -150.73
               Mean episode length: 295.43
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 12.34s
                        Total time: 6067.49s
                               ETA: 1307254.3s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.069s, learning 0.258s)
               Value function loss: 1.1218
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: -152.66
               Mean episode length: 287.49
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 12.33s
                        Total time: 6079.81s
                               ETA: 1307068.0s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.939s, learning 0.193s)
               Value function loss: 0.9416
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: -152.04
               Mean episode length: 287.49
                  Mean reward/step: -0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 12.13s
                        Total time: 6091.95s
                               ETA: 1306840.5s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.740s, learning 0.172s)
               Value function loss: 1.0739
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: -148.22
               Mean episode length: 287.49
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 11.91s
                        Total time: 6103.86s
                               ETA: 1306566.9s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.329s, learning 0.200s)
               Value function loss: 0.8398
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: -141.46
               Mean episode length: 285.07
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 12.53s
                        Total time: 6116.39s
                               ETA: 1306426.1s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.052s, learning 0.275s)
               Value function loss: 1.3619
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: -139.45
               Mean episode length: 281.75
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 12.33s
                        Total time: 6128.71s
                               ETA: 1306243.0s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.334s, learning 0.235s)
               Value function loss: 0.7577
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: -138.79
               Mean episode length: 280.16
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 12.57s
                        Total time: 6141.28s
                               ETA: 1306111.9s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.859s, learning 0.225s)
               Value function loss: 0.5758
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: -139.85
               Mean episode length: 280.16
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 12.08s
                        Total time: 6153.37s
                               ETA: 1305878.6s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.711s, learning 0.179s)
               Value function loss: 0.6043
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: -142.49
               Mean episode length: 280.16
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 11.89s
                        Total time: 6165.26s
                               ETA: 1305605.0s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1338 steps/s (collection: 11.917s, learning 0.322s)
               Value function loss: 0.9234
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: -139.77
               Mean episode length: 280.16
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 12.24s
                        Total time: 6177.50s
                               ETA: 1305406.3s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.301s, learning 0.293s)
               Value function loss: 1.1210
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: -141.79
               Mean episode length: 280.16
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 12.59s
                        Total time: 6190.09s
                               ETA: 1305283.2s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.166s, learning 0.173s)
               Value function loss: 1.6299
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: -142.69
               Mean episode length: 278.38
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 12.34s
                        Total time: 6202.43s
                               ETA: 1305106.9s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.115s, learning 0.202s)
               Value function loss: 1.6595
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: -142.38
               Mean episode length: 278.38
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 12.32s
                        Total time: 6214.75s
                               ETA: 1304926.8s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.935s, learning 0.176s)
               Value function loss: 1.6844
                    Surrogate loss: 0.0344
             Mean action noise std: 0.78
                       Mean reward: -144.19
               Mean episode length: 276.24
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 12.11s
                        Total time: 6226.86s
                               ETA: 1304704.1s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.939s, learning 0.274s)
               Value function loss: 1.3035
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: -143.42
               Mean episode length: 276.24
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 12.21s
                        Total time: 6239.07s
                               ETA: 1304503.6s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.406s, learning 0.305s)
               Value function loss: 1.5822
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: -143.42
               Mean episode length: 276.24
                  Mean reward/step: -0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 12.71s
                        Total time: 6251.78s
                               ETA: 1304408.0s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.372s, learning 0.281s)
               Value function loss: 1.3851
                    Surrogate loss: 0.0035
             Mean action noise std: 0.78
                       Mean reward: -142.70
               Mean episode length: 276.24
                  Mean reward/step: -0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 12.65s
                        Total time: 6264.44s
                               ETA: 1304300.4s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.195s, learning 0.178s)
               Value function loss: 1.0739
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: -142.70
               Mean episode length: 276.24
                  Mean reward/step: -0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 12.37s
                        Total time: 6276.81s
                               ETA: 1304135.1s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.068s, learning 0.241s)
               Value function loss: 1.0761
                    Surrogate loss: 0.0092
             Mean action noise std: 0.78
                       Mean reward: -142.70
               Mean episode length: 276.24
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 12.31s
                        Total time: 6289.12s
                               ETA: 1303957.2s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.147s, learning 0.165s)
               Value function loss: 1.1920
                    Surrogate loss: 0.0055
             Mean action noise std: 0.78
                       Mean reward: -140.95
               Mean episode length: 276.24
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 12.31s
                        Total time: 6301.43s
                               ETA: 1303780.5s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.937s, learning 0.207s)
               Value function loss: 1.2923
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: -143.38
               Mean episode length: 278.18
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 12.14s
                        Total time: 6313.58s
                               ETA: 1303569.9s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.986s, learning 0.171s)
               Value function loss: 2.2923
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: -142.83
               Mean episode length: 278.18
                  Mean reward/step: -0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 12.16s
                        Total time: 6325.73s
                               ETA: 1303362.9s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.743s, learning 0.309s)
               Value function loss: 1.8668
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: -142.60
               Mean episode length: 278.18
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 12.05s
                        Total time: 6337.79s
                               ETA: 1303135.0s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.435s, learning 0.287s)
               Value function loss: 1.6415
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: -142.48
               Mean episode length: 278.18
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 12.72s
                        Total time: 6350.51s
                               ETA: 1303045.5s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.955s, learning 0.204s)
               Value function loss: 2.7318
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: -141.98
               Mean episode length: 278.18
                  Mean reward/step: -0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 12.16s
                        Total time: 6362.67s
                               ETA: 1302841.0s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.177s)
               Value function loss: 1.9769
                    Surrogate loss: 0.0738
             Mean action noise std: 0.78
                       Mean reward: -140.66
               Mean episode length: 278.18
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 12.22s
                        Total time: 6374.88s
                               ETA: 1302649.3s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.323s, learning 0.179s)
               Value function loss: 94.0309
                    Surrogate loss: 0.0598
             Mean action noise std: 0.78
                       Mean reward: -129.82
               Mean episode length: 300.00
                  Mean reward/step: -1.25
       Mean episode length/episode: 4.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 12.50s
                        Total time: 6387.39s
                               ETA: 1302516.4s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.118s, learning 0.212s)
               Value function loss: 1.9886
                    Surrogate loss: 0.0438
             Mean action noise std: 0.78
                       Mean reward: -133.31
               Mean episode length: 299.09
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 12.33s
                        Total time: 6399.72s
                               ETA: 1302349.0s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.873s, learning 0.211s)
               Value function loss: 1.0432
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: -134.47
               Mean episode length: 296.50
                  Mean reward/step: -1.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 12.08s
                        Total time: 6411.80s
                               ETA: 1302132.2s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1268 steps/s (collection: 12.586s, learning 0.332s)
               Value function loss: 0.6282
                    Surrogate loss: -0.0035
             Mean action noise std: 0.78
                       Mean reward: -134.73
               Mean episode length: 296.50
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 12.92s
                        Total time: 6424.72s
                               ETA: 1302085.3s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.135s, learning 0.256s)
               Value function loss: 0.6174
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: -134.40
               Mean episode length: 296.50
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 12.39s
                        Total time: 6437.11s
                               ETA: 1301931.9s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.219s, learning 0.179s)
               Value function loss: 0.5558
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: -134.40
               Mean episode length: 296.50
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 12.40s
                        Total time: 6449.51s
                               ETA: 1301780.5s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.601s, learning 0.182s)
               Value function loss: 0.5457
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: -133.70
               Mean episode length: 296.50
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 11.78s
                        Total time: 6461.29s
                               ETA: 1301505.7s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.868s, learning 0.192s)
               Value function loss: 0.8836
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: -135.09
               Mean episode length: 296.50
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 12.06s
                        Total time: 6473.35s
                               ETA: 1301287.7s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.877s, learning 0.202s)
               Value function loss: 1.3706
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: -132.22
               Mean episode length: 294.15
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 12.08s
                        Total time: 6485.43s
                               ETA: 1301074.3s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.718s, learning 0.200s)
               Value function loss: 1.8366
                    Surrogate loss: 0.0231
             Mean action noise std: 0.78
                       Mean reward: -133.86
               Mean episode length: 294.15
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 11.92s
                        Total time: 6497.35s
                               ETA: 1300829.4s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.036s, learning 0.173s)
               Value function loss: 1.5829
                    Surrogate loss: 0.0091
             Mean action noise std: 0.78
                       Mean reward: -132.99
               Mean episode length: 294.15
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 12.21s
                        Total time: 6509.56s
                               ETA: 1300643.7s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.306s, learning 0.176s)
               Value function loss: 1.5303
                    Surrogate loss: 0.0095
             Mean action noise std: 0.78
                       Mean reward: -137.16
               Mean episode length: 294.14
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 12.48s
                        Total time: 6522.04s
                               ETA: 1300513.0s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.431s, learning 0.224s)
               Value function loss: 1.7788
                    Surrogate loss: 0.0015
             Mean action noise std: 0.78
                       Mean reward: -139.24
               Mean episode length: 294.14
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 12.66s
                        Total time: 6534.70s
                               ETA: 1300417.4s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.490s, learning 0.171s)
               Value function loss: 1.9364
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: -138.24
               Mean episode length: 294.14
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 12.66s
                        Total time: 6547.36s
                               ETA: 1300323.2s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.097s, learning 0.179s)
               Value function loss: 1.7283
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: -138.40
               Mean episode length: 294.14
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 12.28s
                        Total time: 6559.63s
                               ETA: 1300153.0s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.697s, learning 0.202s)
               Value function loss: 2.4953
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: -141.30
               Mean episode length: 294.14
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 11.90s
                        Total time: 6571.53s
                               ETA: 1299908.8s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.357s, learning 0.179s)
               Value function loss: 2.2271
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: -143.84
               Mean episode length: 290.74
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 12.54s
                        Total time: 6584.07s
                               ETA: 1299791.4s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.904s, learning 0.181s)
               Value function loss: 1.3737
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: -140.84
               Mean episode length: 284.19
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 12.08s
                        Total time: 6596.15s
                               ETA: 1299585.4s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.091s, learning 0.195s)
               Value function loss: 1.0546
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: -138.33
               Mean episode length: 282.62
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 12.29s
                        Total time: 6608.44s
                               ETA: 1299419.8s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.208s, learning 0.206s)
               Value function loss: 1.2017
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: -137.81
               Mean episode length: 275.23
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 12.41s
                        Total time: 6620.85s
                               ETA: 1299280.0s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.899s, learning 0.191s)
               Value function loss: 0.8489
                    Surrogate loss: 0.0044
             Mean action noise std: 0.78
                       Mean reward: -136.65
               Mean episode length: 277.82
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 12.09s
                        Total time: 6632.94s
                               ETA: 1299077.1s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.861s, learning 0.220s)
               Value function loss: 1.3717
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: -141.45
               Mean episode length: 277.82
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 12.08s
                        Total time: 6645.02s
                               ETA: 1298873.2s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.685s, learning 0.174s)
               Value function loss: 1.5155
                    Surrogate loss: 0.0122
             Mean action noise std: 0.78
                       Mean reward: -142.52
               Mean episode length: 276.58
                  Mean reward/step: -0.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 11.86s
                        Total time: 6656.88s
                               ETA: 1298626.8s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.298s, learning 0.220s)
               Value function loss: 1.9461
                    Surrogate loss: 0.0016
             Mean action noise std: 0.78
                       Mean reward: -145.35
               Mean episode length: 276.58
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 12.52s
                        Total time: 6669.40s
                               ETA: 1298509.8s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.140s, learning 0.179s)
               Value function loss: 1.6173
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: -146.59
               Mean episode length: 275.49
                  Mean reward/step: -0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 12.32s
                        Total time: 6681.72s
                               ETA: 1298354.4s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.377s, learning 0.203s)
               Value function loss: 2.0183
                    Surrogate loss: 0.0194
             Mean action noise std: 0.78
                       Mean reward: -147.76
               Mean episode length: 275.49
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 12.58s
                        Total time: 6694.30s
                               ETA: 1298250.2s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.207s, learning 0.181s)
               Value function loss: 1.4494
                    Surrogate loss: 0.0213
             Mean action noise std: 0.78
                       Mean reward: -148.26
               Mean episode length: 275.49
                  Mean reward/step: -0.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 12.39s
                        Total time: 6706.69s
                               ETA: 1298109.1s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.243s, learning 0.183s)
               Value function loss: 1.2128
                    Surrogate loss: -0.0019
             Mean action noise std: 0.78
                       Mean reward: -148.26
               Mean episode length: 275.49
                  Mean reward/step: -0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 12.43s
                        Total time: 6719.11s
                               ETA: 1297975.9s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.202s, learning 0.194s)
               Value function loss: 1.2241
                    Surrogate loss: 0.0050
             Mean action noise std: 0.78
                       Mean reward: -148.97
               Mean episode length: 273.10
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 12.40s
                        Total time: 6731.51s
                               ETA: 1297837.5s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.997s, learning 0.182s)
               Value function loss: 1.1817
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: -148.97
               Mean episode length: 273.10
                  Mean reward/step: -0.40
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 12.18s
                        Total time: 6743.69s
                               ETA: 1297657.6s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.131s, learning 0.180s)
               Value function loss: 1.2976
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: -148.24
               Mean episode length: 275.45
                  Mean reward/step: -0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 12.31s
                        Total time: 6756.00s
                               ETA: 1297503.8s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.181s, learning 0.388s)
               Value function loss: 1.5102
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: -146.42
               Mean episode length: 275.45
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 12.57s
                        Total time: 6768.57s
                               ETA: 1297400.0s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.954s, learning 0.202s)
               Value function loss: 1.6123
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: -148.08
               Mean episode length: 275.45
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 12.16s
                        Total time: 6780.72s
                               ETA: 1297217.6s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.235s, learning 0.176s)
               Value function loss: 1.7582
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: -145.35
               Mean episode length: 275.46
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 12.41s
                        Total time: 6793.13s
                               ETA: 1297084.6s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.828s, learning 0.199s)
               Value function loss: 1.7500
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: -145.79
               Mean episode length: 275.46
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 12.03s
                        Total time: 6805.16s
                               ETA: 1296878.6s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.676s, learning 0.205s)
               Value function loss: 1.6972
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: -147.76
               Mean episode length: 275.46
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 11.88s
                        Total time: 6817.04s
                               ETA: 1296645.8s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.262s, learning 0.185s)
               Value function loss: 1.4372
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: -147.90
               Mean episode length: 275.46
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 12.45s
                        Total time: 6829.49s
                               ETA: 1296521.3s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.179s, learning 0.207s)
               Value function loss: 90.5402
                    Surrogate loss: 0.0331
             Mean action noise std: 0.78
                       Mean reward: -141.10
               Mean episode length: 300.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 4.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 12.39s
                        Total time: 6841.88s
                               ETA: 1296385.5s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.424s, learning 0.284s)
               Value function loss: 0.4239
                    Surrogate loss: 0.0130
             Mean action noise std: 0.78
                       Mean reward: -141.09
               Mean episode length: 300.00
                  Mean reward/step: -1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 12.71s
                        Total time: 6854.58s
                               ETA: 1296311.1s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.945s, learning 0.176s)
               Value function loss: 4.0808
                    Surrogate loss: 0.0262
             Mean action noise std: 0.78
                       Mean reward: -141.92
               Mean episode length: 300.00
                  Mean reward/step: -1.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 12.12s
                        Total time: 6866.70s
                               ETA: 1296126.2s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.131s, learning 0.203s)
               Value function loss: 0.6135
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: -141.92
               Mean episode length: 300.00
                  Mean reward/step: -1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 12.33s
                        Total time: 6879.04s
                               ETA: 1295982.1s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.122s, learning 0.206s)
               Value function loss: 0.4852
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: -142.90
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 12.33s
                        Total time: 6891.37s
                               ETA: 1295837.2s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.271s, learning 0.178s)
               Value function loss: 0.7344
                    Surrogate loss: 0.0156
             Mean action noise std: 0.78
                       Mean reward: -141.54
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 12.45s
                        Total time: 6903.81s
                               ETA: 1295715.8s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.926s, learning 0.200s)
               Value function loss: 0.6027
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: -141.54
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 12.13s
                        Total time: 6915.94s
                               ETA: 1295534.1s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.754s, learning 0.197s)
               Value function loss: 1.2146
                    Surrogate loss: 0.0148
             Mean action noise std: 0.78
                       Mean reward: -137.89
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 11.95s
                        Total time: 6927.89s
                               ETA: 1295320.5s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.067s, learning 0.180s)
               Value function loss: 1.5248
                    Surrogate loss: 0.0043
             Mean action noise std: 0.78
                       Mean reward: -133.40
               Mean episode length: 300.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 12.25s
                        Total time: 6940.14s
                               ETA: 1295162.7s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.241s, learning 0.192s)
               Value function loss: 1.4583
                    Surrogate loss: 0.0156
             Mean action noise std: 0.78
                       Mean reward: -134.35
               Mean episode length: 300.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 12.43s
                        Total time: 6952.57s
                               ETA: 1295040.2s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.439s, learning 0.217s)
               Value function loss: 1.1635
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: -133.39
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 12.66s
                        Total time: 6965.23s
                               ETA: 1294959.7s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.158s, learning 0.217s)
               Value function loss: 1.2273
                    Surrogate loss: -0.0002
             Mean action noise std: 0.78
                       Mean reward: -131.24
               Mean episode length: 300.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 12.38s
                        Total time: 6977.60s
                               ETA: 1294827.3s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.083s, learning 0.201s)
               Value function loss: 0.5887
                    Surrogate loss: 0.0096
             Mean action noise std: 0.78
                       Mean reward: -129.65
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 12.28s
                        Total time: 6989.89s
                               ETA: 1294678.2s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.097s, learning 0.216s)
               Value function loss: 0.7157
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: -127.25
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 12.31s
                        Total time: 7002.20s
                               ETA: 1294535.1s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.997s, learning 0.200s)
               Value function loss: 0.2798
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: -127.25
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 12.20s
                        Total time: 7014.40s
                               ETA: 1294371.1s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.302s, learning 0.193s)
               Value function loss: 0.7954
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: -123.07
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 12.49s
                        Total time: 7026.89s
                               ETA: 1294262.4s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.110s, learning 0.176s)
               Value function loss: 0.8683
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: -118.75
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 12.29s
                        Total time: 7039.18s
                               ETA: 1294115.9s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.263s, learning 0.180s)
               Value function loss: 0.9012
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: -108.94
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 12.44s
                        Total time: 7051.62s
                               ETA: 1293998.5s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.415s, learning 0.173s)
               Value function loss: 0.7103
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: -104.47
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 12.59s
                        Total time: 7064.21s
                               ETA: 1293908.2s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.899s, learning 0.195s)
               Value function loss: 0.6697
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: -100.03
               Mean episode length: 297.03
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 12.09s
                        Total time: 7076.30s
                               ETA: 1293727.7s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.274s, learning 0.178s)
               Value function loss: 0.4851
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: -97.20
               Mean episode length: 295.57
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 12.45s
                        Total time: 7088.75s
                               ETA: 1293613.2s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.703s, learning 0.212s)
               Value function loss: 0.4701
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: -94.90
               Mean episode length: 294.19
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 11.91s
                        Total time: 7100.67s
                               ETA: 1293401.3s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.906s, learning 0.195s)
               Value function loss: 0.4375
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: -94.43
               Mean episode length: 294.19
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 12.10s
                        Total time: 7112.77s
                               ETA: 1293223.8s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.233s, learning 0.182s)
               Value function loss: 0.3743
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: -93.82
               Mean episode length: 294.19
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 12.42s
                        Total time: 7125.19s
                               ETA: 1293104.2s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.956s, learning 0.194s)
               Value function loss: 0.3895
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: -93.16
               Mean episode length: 294.19
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 12.15s
                        Total time: 7137.34s
                               ETA: 1292936.9s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.045s, learning 0.168s)
               Value function loss: 0.4472
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: -92.94
               Mean episode length: 294.19
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 12.21s
                        Total time: 7149.55s
                               ETA: 1292781.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.216s, learning 0.190s)
               Value function loss: 0.4132
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: -93.98
               Mean episode length: 293.25
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 12.41s
                        Total time: 7161.96s
                               ETA: 1292661.5s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.006s, learning 0.223s)
               Value function loss: 0.3162
                    Surrogate loss: -0.0254
             Mean action noise std: 0.78
                       Mean reward: -93.98
               Mean episode length: 293.25
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 12.23s
                        Total time: 7174.18s
                               ETA: 1292510.0s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.383s, learning 0.179s)
               Value function loss: 0.3336
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: -91.97
               Mean episode length: 293.25
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 12.56s
                        Total time: 7186.75s
                               ETA: 1292418.9s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.974s, learning 0.173s)
               Value function loss: 0.3243
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: -92.31
               Mean episode length: 290.86
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 12.15s
                        Total time: 7198.89s
                               ETA: 1292253.5s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.037s, learning 0.179s)
               Value function loss: 0.2752
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: -92.31
               Mean episode length: 290.86
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 12.22s
                        Total time: 7211.11s
                               ETA: 1292101.0s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.678s, learning 0.185s)
               Value function loss: 0.6493
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: -97.19
               Mean episode length: 290.34
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 11.86s
                        Total time: 7222.97s
                               ETA: 1291885.8s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.894s, learning 0.177s)
               Value function loss: 0.4690
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: -97.10
               Mean episode length: 290.34
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 12.07s
                        Total time: 7235.04s
                               ETA: 1291708.7s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.983s, learning 0.187s)
               Value function loss: 0.4665
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: -99.38
               Mean episode length: 289.99
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 12.17s
                        Total time: 7247.21s
                               ETA: 1291549.7s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.329s, learning 0.196s)
               Value function loss: 0.5790
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: -100.89
               Mean episode length: 288.18
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 12.52s
                        Total time: 7259.74s
                               ETA: 1291454.2s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.886s, learning 0.200s)
               Value function loss: 0.5453
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: -103.74
               Mean episode length: 288.18
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 12.09s
                        Total time: 7271.82s
                               ETA: 1291281.1s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.843s, learning 0.185s)
               Value function loss: 0.4455
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: -103.62
               Mean episode length: 288.18
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 12.03s
                        Total time: 7283.85s
                               ETA: 1291098.4s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.679s, learning 0.193s)
               Value function loss: 0.5056
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: -103.62
               Mean episode length: 288.18
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 11.87s
                        Total time: 7295.72s
                               ETA: 1290888.8s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.165s, learning 0.257s)
               Value function loss: 97.7559
                    Surrogate loss: 0.0347
             Mean action noise std: 0.78
                       Mean reward: -97.05
               Mean episode length: 300.00
                  Mean reward/step: -1.08
       Mean episode length/episode: 4.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 12.42s
                        Total time: 7308.15s
                               ETA: 1290776.8s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.756s, learning 0.175s)
               Value function loss: 0.9864
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: -97.12
               Mean episode length: 300.00
                  Mean reward/step: -1.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 11.93s
                        Total time: 7320.08s
                               ETA: 1290578.8s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.917s, learning 0.180s)
               Value function loss: 1.4274
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: -97.10
               Mean episode length: 300.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 12.10s
                        Total time: 7332.17s
                               ETA: 1290410.7s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.314s, learning 0.198s)
               Value function loss: 1.1458
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: -99.81
               Mean episode length: 299.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 12.51s
                        Total time: 7344.69s
                               ETA: 1290316.0s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.929s, learning 0.256s)
               Value function loss: 0.4882
                    Surrogate loss: 0.0063
             Mean action noise std: 0.78
                       Mean reward: -99.09
               Mean episode length: 299.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 12.19s
                        Total time: 7356.87s
                               ETA: 1290164.2s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.149s, learning 0.180s)
               Value function loss: 0.3366
                    Surrogate loss: 0.0119
             Mean action noise std: 0.78
                       Mean reward: -99.09
               Mean episode length: 299.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 12.33s
                        Total time: 7369.20s
                               ETA: 1290038.1s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.063s, learning 0.193s)
               Value function loss: 0.5765
                    Surrogate loss: 0.0171
             Mean action noise std: 0.78
                       Mean reward: -99.27
               Mean episode length: 299.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 12.26s
                        Total time: 7381.46s
                               ETA: 1289899.7s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.225s, learning 0.258s)
               Value function loss: 0.9854
                    Surrogate loss: 0.0026
             Mean action noise std: 0.78
                       Mean reward: -100.30
               Mean episode length: 299.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 12.48s
                        Total time: 7393.94s
                               ETA: 1289801.3s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.122s, learning 0.186s)
               Value function loss: 1.4495
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: -102.01
               Mean episode length: 299.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 12.31s
                        Total time: 7406.25s
                               ETA: 1289672.8s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.496s, learning 0.279s)
               Value function loss: 1.7971
                    Surrogate loss: 0.0219
             Mean action noise std: 0.78
                       Mean reward: -99.75
               Mean episode length: 299.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 12.77s
                        Total time: 7419.02s
                               ETA: 1289625.7s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.836s, learning 0.184s)
               Value function loss: 2.4640
                    Surrogate loss: 0.0147
             Mean action noise std: 0.78
                       Mean reward: -101.17
               Mean episode length: 299.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 12.02s
                        Total time: 7431.04s
                               ETA: 1289448.0s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.866s, learning 0.209s)
               Value function loss: 1.7072
                    Surrogate loss: 0.0019
             Mean action noise std: 0.78
                       Mean reward: -100.71
               Mean episode length: 299.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 12.08s
                        Total time: 7443.12s
                               ETA: 1289280.2s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.144s, learning 0.176s)
               Value function loss: 0.7617
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: -100.16
               Mean episode length: 299.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 12.32s
                        Total time: 7455.44s
                               ETA: 1289155.4s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.027s, learning 0.182s)
               Value function loss: 0.5284
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: -100.49
               Mean episode length: 299.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 12.21s
                        Total time: 7467.65s
                               ETA: 1289011.8s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.820s, learning 0.204s)
               Value function loss: 0.3553
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: -100.36
               Mean episode length: 299.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 12.02s
                        Total time: 7479.67s
                               ETA: 1288836.7s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.041s, learning 0.259s)
               Value function loss: 0.5727
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: -96.90
               Mean episode length: 299.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 12.30s
                        Total time: 7491.97s
                               ETA: 1288709.7s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.885s, learning 0.222s)
               Value function loss: 0.5835
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: -97.29
               Mean episode length: 299.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 12.11s
                        Total time: 7504.08s
                               ETA: 1288549.9s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.272s, learning 0.295s)
               Value function loss: 0.7292
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: -98.91
               Mean episode length: 299.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 12.57s
                        Total time: 7516.64s
                               ETA: 1288469.4s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.000s, learning 0.308s)
               Value function loss: 0.4563
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: -99.00
               Mean episode length: 299.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 12.31s
                        Total time: 7528.95s
                               ETA: 1288344.9s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.047s, learning 0.244s)
               Value function loss: 0.6223
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: -98.88
               Mean episode length: 299.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 12.29s
                        Total time: 7541.24s
                               ETA: 1288217.9s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.443s, learning 0.186s)
               Value function loss: 0.2543
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: -98.37
               Mean episode length: 299.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 12.63s
                        Total time: 7553.87s
                               ETA: 1288149.0s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.702s, learning 0.192s)
               Value function loss: 0.3924
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: -95.90
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 11.89s
                        Total time: 7565.77s
                               ETA: 1287955.1s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.026s, learning 0.251s)
               Value function loss: 0.1518
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: -95.87
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 12.28s
                        Total time: 7578.04s
                               ETA: 1287826.9s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.910s, learning 0.170s)
               Value function loss: 0.3559
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: -95.41
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 12.08s
                        Total time: 7590.12s
                               ETA: 1287665.7s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.125s, learning 0.198s)
               Value function loss: 0.2238
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: -93.57
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 12.32s
                        Total time: 7602.45s
                               ETA: 1287546.1s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.833s, learning 0.179s)
               Value function loss: 0.4491
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: -93.68
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 12.01s
                        Total time: 7614.46s
                               ETA: 1287374.4s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.049s, learning 0.205s)
               Value function loss: 0.3176
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: -93.69
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 12.25s
                        Total time: 7626.71s
                               ETA: 1287243.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.158s, learning 0.313s)
               Value function loss: 0.2358
                    Surrogate loss: -0.0230
             Mean action noise std: 0.77
                       Mean reward: -93.69
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 12.47s
                        Total time: 7639.18s
                               ETA: 1287150.5s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.543s, learning 0.333s)
               Value function loss: 0.3610
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: -92.61
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 12.88s
                        Total time: 7652.06s
                               ETA: 1287125.5s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.647s, learning 0.232s)
               Value function loss: 0.2384
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: -92.61
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 11.88s
                        Total time: 7663.94s
                               ETA: 1286933.2s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.552s, learning 0.190s)
               Value function loss: 0.2574
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: -92.47
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 11.74s
                        Total time: 7675.68s
                               ETA: 1286718.5s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.763s, learning 0.191s)
               Value function loss: 0.3909
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: -91.24
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 11.95s
                        Total time: 7687.63s
                               ETA: 1286539.8s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.057s, learning 0.191s)
               Value function loss: 0.2956
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: -92.53
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 12.25s
                        Total time: 7699.88s
                               ETA: 1286410.9s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.268s, learning 0.314s)
               Value function loss: 0.3947
                    Surrogate loss: 0.0010
             Mean action noise std: 0.77
                       Mean reward: -92.68
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 12.58s
                        Total time: 7712.46s
                               ETA: 1286338.1s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.160s, learning 0.211s)
               Value function loss: 0.2401
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: -91.54
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 12.37s
                        Total time: 7724.83s
                               ETA: 1286230.3s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.031s, learning 0.168s)
               Value function loss: 0.3249
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: -91.34
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 12.20s
                        Total time: 7737.03s
                               ETA: 1286094.2s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.709s, learning 0.173s)
               Value function loss: 0.2068
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: -90.99
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 11.88s
                        Total time: 7748.91s
                               ETA: 1285905.9s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.119s, learning 0.264s)
               Value function loss: 95.0173
                    Surrogate loss: 0.0126
             Mean action noise std: 0.77
                       Mean reward: -83.12
               Mean episode length: 300.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 4.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 12.38s
                        Total time: 7761.30s
                               ETA: 1285801.3s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.859s, learning 0.209s)
               Value function loss: 0.7347
                    Surrogate loss: 0.0380
             Mean action noise std: 0.77
                       Mean reward: -83.11
               Mean episode length: 300.00
                  Mean reward/step: -1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 12.07s
                        Total time: 7773.37s
                               ETA: 1285644.8s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.951s, learning 0.189s)
               Value function loss: 1.6807
                    Surrogate loss: 0.0020
             Mean action noise std: 0.77
                       Mean reward: -83.46
               Mean episode length: 300.00
                  Mean reward/step: -1.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 12.14s
                        Total time: 7785.50s
                               ETA: 1285500.7s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.557s, learning 0.221s)
               Value function loss: 0.7658
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: -83.36
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.78s
                        Total time: 7797.28s
                               ETA: 1285297.4s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.184s, learning 0.294s)
               Value function loss: 0.5296
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: -83.16
               Mean episode length: 300.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 12.48s
                        Total time: 7809.76s
                               ETA: 1285210.0s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.162s, learning 0.413s)
               Value function loss: 0.8066
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: -82.17
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 12.58s
                        Total time: 7822.34s
                               ETA: 1285138.9s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.404s, learning 0.226s)
               Value function loss: 1.7459
                    Surrogate loss: 0.0126
             Mean action noise std: 0.77
                       Mean reward: -82.17
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 12.63s
                        Total time: 7834.97s
                               ETA: 1285076.8s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.719s, learning 0.173s)
               Value function loss: 1.4252
                    Surrogate loss: 0.0149
             Mean action noise std: 0.77
                       Mean reward: -84.48
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 11.89s
                        Total time: 7846.86s
                               ETA: 1284894.1s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.854s, learning 0.210s)
               Value function loss: 1.6052
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: -85.64
               Mean episode length: 300.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 12.06s
                        Total time: 7858.92s
                               ETA: 1284740.0s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.685s, learning 0.200s)
               Value function loss: 1.4250
                    Surrogate loss: 0.0301
             Mean action noise std: 0.77
                       Mean reward: -86.30
               Mean episode length: 297.72
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 11.88s
                        Total time: 7870.81s
                               ETA: 1284557.1s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.989s, learning 0.181s)
               Value function loss: 1.1692
                    Surrogate loss: 0.0052
             Mean action noise std: 0.77
                       Mean reward: -85.94
               Mean episode length: 297.72
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 12.17s
                        Total time: 7882.98s
                               ETA: 1284421.3s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.733s, learning 0.172s)
               Value function loss: 1.6232
                    Surrogate loss: 0.0020
             Mean action noise std: 0.77
                       Mean reward: -85.27
               Mean episode length: 297.72
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 11.90s
                        Total time: 7894.88s
                               ETA: 1284242.8s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.122s, learning 0.192s)
               Value function loss: 0.5367
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: -85.00
               Mean episode length: 297.72
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 12.31s
                        Total time: 7907.20s
                               ETA: 1284131.2s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.197s, learning 0.199s)
               Value function loss: 0.4413
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: -84.34
               Mean episode length: 297.72
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 12.40s
                        Total time: 7919.59s
                               ETA: 1284033.3s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.149s, learning 0.176s)
               Value function loss: 0.1690
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: -84.27
               Mean episode length: 297.72
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 12.32s
                        Total time: 7931.92s
                               ETA: 1283924.1s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.836s, learning 0.202s)
               Value function loss: 0.4653
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: -83.83
               Mean episode length: 297.72
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 12.04s
                        Total time: 7943.95s
                               ETA: 1283768.8s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.944s, learning 0.266s)
               Value function loss: 0.5858
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: -81.55
               Mean episode length: 297.72
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 12.21s
                        Total time: 7956.16s
                               ETA: 1283641.7s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.487s, learning 0.170s)
               Value function loss: 0.5819
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: -80.96
               Mean episode length: 296.07
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 12.66s
                        Total time: 7968.82s
                               ETA: 1283587.2s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.953s, learning 0.187s)
               Value function loss: 0.5553
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: -80.45
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 12.14s
                        Total time: 7980.96s
                               ETA: 1283449.5s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.028s, learning 0.205s)
               Value function loss: 0.3912
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: -79.03
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 12.23s
                        Total time: 7993.19s
                               ETA: 1283327.3s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.278s, learning 0.320s)
               Value function loss: 0.3533
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: -78.36
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 12.60s
                        Total time: 8005.79s
                               ETA: 1283263.9s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.202s, learning 0.249s)
               Value function loss: 0.2186
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -77.95
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 12.45s
                        Total time: 8018.24s
                               ETA: 1283177.1s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.291s, learning 0.340s)
               Value function loss: 0.3061
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: -76.68
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 12.63s
                        Total time: 8030.87s
                               ETA: 1283119.3s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.343s, learning 0.328s)
               Value function loss: 0.1619
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: -76.74
               Mean episode length: 294.45
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 12.67s
                        Total time: 8043.54s
                               ETA: 1283068.0s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.891s, learning 0.273s)
               Value function loss: 0.2533
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: -74.51
               Mean episode length: 294.45
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 12.16s
                        Total time: 8055.71s
                               ETA: 1282936.2s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.059s, learning 0.311s)
               Value function loss: 0.2612
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: -72.01
               Mean episode length: 294.45
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 12.37s
                        Total time: 8068.08s
                               ETA: 1282837.5s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.229s, learning 0.270s)
               Value function loss: 0.2551
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: -71.83
               Mean episode length: 294.45
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 12.50s
                        Total time: 8080.58s
                               ETA: 1282759.4s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.556s, learning 0.346s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: -71.83
               Mean episode length: 294.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 11.90s
                        Total time: 8092.48s
                               ETA: 1282587.0s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.168s, learning 0.270s)
               Value function loss: 0.1352
                    Surrogate loss: 0.0003
             Mean action noise std: 0.77
                       Mean reward: -71.66
               Mean episode length: 294.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 12.44s
                        Total time: 8104.92s
                               ETA: 1282499.9s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.347s, learning 0.194s)
               Value function loss: 0.1667
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: -70.95
               Mean episode length: 296.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 12.54s
                        Total time: 8117.46s
                               ETA: 1282429.4s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.726s, learning 0.202s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: -70.95
               Mean episode length: 296.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 11.93s
                        Total time: 8129.39s
                               ETA: 1282262.4s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.967s, learning 0.250s)
               Value function loss: 0.2711
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: -71.00
               Mean episode length: 296.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 12.22s
                        Total time: 8141.60s
                               ETA: 1282141.3s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.192s, learning 0.194s)
               Value function loss: 0.2764
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -70.69
               Mean episode length: 296.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 12.39s
                        Total time: 8153.99s
                               ETA: 1282047.1s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.780s, learning 0.249s)
               Value function loss: 0.3267
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: -72.31
               Mean episode length: 296.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 12.03s
                        Total time: 8166.02s
                               ETA: 1281897.3s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.093s, learning 0.186s)
               Value function loss: 0.4846
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: -72.83
               Mean episode length: 296.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 12.28s
                        Total time: 8178.30s
                               ETA: 1281787.0s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.157s, learning 0.270s)
               Value function loss: 0.2602
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: -73.27
               Mean episode length: 296.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 12.43s
                        Total time: 8190.73s
                               ETA: 1281700.1s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.361s, learning 0.189s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: -73.40
               Mean episode length: 296.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 12.55s
                        Total time: 8203.28s
                               ETA: 1281632.7s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.894s, learning 0.192s)
               Value function loss: 0.1632
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: -73.33
               Mean episode length: 296.32
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 12.09s
                        Total time: 8215.36s
                               ETA: 1281493.2s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.759s, learning 0.193s)
               Value function loss: 97.2945
                    Surrogate loss: 0.0357
             Mean action noise std: 0.77
                       Mean reward: -69.07
               Mean episode length: 300.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 4.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 11.95s
                        Total time: 8227.31s
                               ETA: 1281333.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.176s, learning 0.221s)
               Value function loss: 0.9936
                    Surrogate loss: 0.0179
             Mean action noise std: 0.77
                       Mean reward: -68.85
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 12.40s
                        Total time: 8239.71s
                               ETA: 1281242.8s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.182s)
               Value function loss: 1.9994
                    Surrogate loss: 0.0197
             Mean action noise std: 0.77
                       Mean reward: -68.95
               Mean episode length: 300.00
                  Mean reward/step: -1.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 12.11s
                        Total time: 8251.82s
                               ETA: 1281107.4s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.927s, learning 0.192s)
               Value function loss: 0.2266
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: -68.49
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 12.12s
                        Total time: 8263.94s
                               ETA: 1280974.4s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.806s, learning 0.213s)
               Value function loss: 0.6847
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: -68.43
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 12.02s
                        Total time: 8275.95s
                               ETA: 1280826.4s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.572s, learning 0.191s)
               Value function loss: 0.6609
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: -68.43
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 11.76s
                        Total time: 8287.72s
                               ETA: 1280639.2s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.130s, learning 0.198s)
               Value function loss: 1.7694
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: -68.49
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 12.33s
                        Total time: 8300.05s
                               ETA: 1280539.7s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.566s, learning 0.196s)
               Value function loss: 1.6745
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: -68.77
               Mean episode length: 300.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 11.76s
                        Total time: 8311.81s
                               ETA: 1280353.5s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.821s, learning 0.281s)
               Value function loss: 2.0557
                    Surrogate loss: 0.0039
             Mean action noise std: 0.77
                       Mean reward: -68.59
               Mean episode length: 300.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 12.10s
                        Total time: 8323.91s
                               ETA: 1280220.0s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.578s, learning 0.200s)
               Value function loss: 1.5234
                    Surrogate loss: 0.0227
             Mean action noise std: 0.77
                       Mean reward: -68.45
               Mean episode length: 300.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 12.78s
                        Total time: 8336.69s
                               ETA: 1280190.7s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.371s, learning 0.185s)
               Value function loss: 1.9169
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: -67.91
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 12.56s
                        Total time: 8349.24s
                               ETA: 1280127.4s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.267s, learning 0.197s)
               Value function loss: 2.1752
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: -67.71
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 12.46s
                        Total time: 8361.71s
                               ETA: 1280050.0s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.618s, learning 0.181s)
               Value function loss: 1.6007
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: -67.78
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 11.80s
                        Total time: 8373.51s
                               ETA: 1279871.2s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.430s, learning 0.295s)
               Value function loss: 1.1552
                    Surrogate loss: 0.0015
             Mean action noise std: 0.77
                       Mean reward: -67.69
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 12.73s
                        Total time: 8386.23s
                               ETA: 1279834.4s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.292s, learning 0.369s)
               Value function loss: 1.1641
                    Surrogate loss: 0.0094
             Mean action noise std: 0.77
                       Mean reward: -68.28
               Mean episode length: 298.16
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 12.66s
                        Total time: 8398.89s
                               ETA: 1279787.8s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.209s, learning 0.332s)
               Value function loss: 1.0265
                    Surrogate loss: 0.0000
             Mean action noise std: 0.77
                       Mean reward: -66.70
               Mean episode length: 298.16
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 12.54s
                        Total time: 8411.43s
                               ETA: 1279723.2s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1240 steps/s (collection: 12.834s, learning 0.379s)
               Value function loss: 1.1447
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: -68.03
               Mean episode length: 298.16
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 13.21s
                        Total time: 8424.65s
                               ETA: 1279760.6s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.985s, learning 0.204s)
               Value function loss: 0.7340
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: -68.69
               Mean episode length: 296.52
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 12.19s
                        Total time: 8436.84s
                               ETA: 1279642.7s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.724s, learning 0.199s)
               Value function loss: 0.4993
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: -68.15
               Mean episode length: 296.52
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 11.92s
                        Total time: 8448.76s
                               ETA: 1279484.8s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.926s, learning 0.284s)
               Value function loss: 0.7412
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: -70.73
               Mean episode length: 296.52
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 12.21s
                        Total time: 8460.97s
                               ETA: 1279370.7s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1334 steps/s (collection: 11.995s, learning 0.287s)
               Value function loss: 0.3001
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: -70.42
               Mean episode length: 296.52
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 12.28s
                        Total time: 8473.25s
                               ETA: 1279267.8s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.207s, learning 0.205s)
               Value function loss: 0.4127
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: -70.06
               Mean episode length: 296.52
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 12.41s
                        Total time: 8485.66s
                               ETA: 1279184.7s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.923s, learning 0.172s)
               Value function loss: 0.2015
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: -70.11
               Mean episode length: 296.52
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 12.09s
                        Total time: 8497.76s
                               ETA: 1279054.1s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.818s, learning 0.209s)
               Value function loss: 0.3180
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: -69.14
               Mean episode length: 296.52
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 12.03s
                        Total time: 8509.79s
                               ETA: 1278913.8s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.279s, learning 0.202s)
               Value function loss: 0.1745
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: -69.09
               Mean episode length: 296.52
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 12.48s
                        Total time: 8522.27s
                               ETA: 1278841.9s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.073s, learning 0.185s)
               Value function loss: 0.3465
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: -68.41
               Mean episode length: 296.52
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 12.26s
                        Total time: 8534.52s
                               ETA: 1278736.8s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.589s, learning 0.174s)
               Value function loss: 0.2224
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: -68.31
               Mean episode length: 296.52
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 11.76s
                        Total time: 8546.29s
                               ETA: 1278557.9s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.160s, learning 0.334s)
               Value function loss: 0.1165
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: -68.31
               Mean episode length: 296.52
                  Mean reward/step: -0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 12.49s
                        Total time: 8558.78s
                               ETA: 1278488.7s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.495s, learning 0.208s)
               Value function loss: 0.2430
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: -68.25
               Mean episode length: 296.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 12.70s
                        Total time: 8571.48s
                               ETA: 1278450.8s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.175s, learning 0.217s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: -68.25
               Mean episode length: 296.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 12.39s
                        Total time: 8583.87s
                               ETA: 1278366.7s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.706s, learning 0.195s)
               Value function loss: 0.1734
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: -68.24
               Mean episode length: 296.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 11.90s
                        Total time: 8595.78s
                               ETA: 1278209.9s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.672s, learning 0.192s)
               Value function loss: 0.3159
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: -68.30
               Mean episode length: 296.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 11.86s
                        Total time: 8607.64s
                               ETA: 1278048.0s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.668s, learning 0.221s)
               Value function loss: 0.2990
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: -69.77
               Mean episode length: 296.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 11.89s
                        Total time: 8619.53s
                               ETA: 1277890.2s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.876s, learning 0.199s)
               Value function loss: 0.3797
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: -69.83
               Mean episode length: 296.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 12.08s
                        Total time: 8631.60s
                               ETA: 1277760.5s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.648s, learning 0.192s)
               Value function loss: 0.2316
                    Surrogate loss: 0.0032
             Mean action noise std: 0.77
                       Mean reward: -69.70
               Mean episode length: 296.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 11.84s
                        Total time: 8643.44s
                               ETA: 1277596.3s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.491s, learning 0.299s)
               Value function loss: 0.2830
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: -71.63
               Mean episode length: 298.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 12.79s
                        Total time: 8656.23s
                               ETA: 1277572.8s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.912s, learning 0.199s)
               Value function loss: 0.1641
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: -72.15
               Mean episode length: 298.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 12.11s
                        Total time: 8668.35s
                               ETA: 1277449.2s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.974s, learning 0.190s)
               Value function loss: 99.3877
                    Surrogate loss: 0.0166
             Mean action noise std: 0.77
                       Mean reward: -69.47
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 4.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 12.16s
                        Total time: 8680.51s
                               ETA: 1277333.7s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.720s, learning 0.182s)
               Value function loss: 0.4731
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: -69.54
               Mean episode length: 300.00
                  Mean reward/step: -1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 11.90s
                        Total time: 8692.41s
                               ETA: 1277180.1s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.095s, learning 0.305s)
               Value function loss: 1.2817
                    Surrogate loss: 0.0130
             Mean action noise std: 0.77
                       Mean reward: -69.41
               Mean episode length: 300.00
                  Mean reward/step: -1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 12.40s
                        Total time: 8704.81s
                               ETA: 1277099.9s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.122s, learning 0.239s)
               Value function loss: 0.7486
                    Surrogate loss: 0.0030
             Mean action noise std: 0.77
                       Mean reward: -69.53
               Mean episode length: 300.00
                  Mean reward/step: -0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 12.36s
                        Total time: 8717.17s
                               ETA: 1277014.2s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.759s, learning 0.188s)
               Value function loss: 0.4206
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: -69.12
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 11.95s
                        Total time: 8729.12s
                               ETA: 1276868.2s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.913s, learning 0.203s)
               Value function loss: 0.5809
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: -69.34
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 12.12s
                        Total time: 8741.23s
                               ETA: 1276747.2s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1335 steps/s (collection: 11.953s, learning 0.318s)
               Value function loss: 1.2340
                    Surrogate loss: 0.0222
             Mean action noise std: 0.77
                       Mean reward: -69.34
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 12.27s
                        Total time: 8753.50s
                               ETA: 1276649.2s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.518s, learning 0.281s)
               Value function loss: 1.2834
                    Surrogate loss: -0.0028
             Mean action noise std: 0.77
                       Mean reward: -72.45
               Mean episode length: 295.35
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 12.80s
                        Total time: 8766.30s
                               ETA: 1276628.3s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.464s, learning 0.353s)
               Value function loss: 1.8468
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: -76.88
               Mean episode length: 292.97
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 12.82s
                        Total time: 8779.12s
                               ETA: 1276610.0s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.108s, learning 0.195s)
               Value function loss: 1.6999
                    Surrogate loss: 0.0059
             Mean action noise std: 0.77
                       Mean reward: -76.80
               Mean episode length: 292.97
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 12.30s
                        Total time: 8791.42s
                               ETA: 1276517.2s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1253 steps/s (collection: 12.704s, learning 0.365s)
               Value function loss: 0.7550
                    Surrogate loss: 0.0353
             Mean action noise std: 0.77
                       Mean reward: -77.38
               Mean episode length: 292.97
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 13.07s
                        Total time: 8804.49s
                               ETA: 1276535.6s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.550s, learning 0.353s)
               Value function loss: 0.6348
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: -81.74
               Mean episode length: 288.72
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 12.90s
                        Total time: 8817.40s
                               ETA: 1276530.1s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.882s, learning 0.186s)
               Value function loss: 0.4661
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: -82.48
               Mean episode length: 288.72
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 12.07s
                        Total time: 8829.46s
                               ETA: 1276403.8s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.751s, learning 0.172s)
               Value function loss: 0.7859
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: -85.42
               Mean episode length: 286.74
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 11.92s
                        Total time: 8841.39s
                               ETA: 1276256.8s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.993s, learning 0.180s)
               Value function loss: 0.4052
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: -87.56
               Mean episode length: 284.80
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 12.17s
                        Total time: 8853.56s
                               ETA: 1276146.1s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.750s, learning 0.180s)
               Value function loss: 0.6999
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: -87.92
               Mean episode length: 284.80
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 11.93s
                        Total time: 8865.49s
                               ETA: 1276000.8s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.981s, learning 0.201s)
               Value function loss: 0.6758
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: -88.51
               Mean episode length: 284.80
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 12.18s
                        Total time: 8877.67s
                               ETA: 1275892.2s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.860s, learning 0.260s)
               Value function loss: 0.7441
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -89.21
               Mean episode length: 284.80
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 12.12s
                        Total time: 8889.79s
                               ETA: 1275774.9s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.045s, learning 0.275s)
               Value function loss: 0.7401
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: -92.28
               Mean episode length: 281.65
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 12.32s
                        Total time: 8902.11s
                               ETA: 1275686.5s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.362s, learning 0.251s)
               Value function loss: 0.5432
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: -92.02
               Mean episode length: 281.65
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 12.61s
                        Total time: 8914.72s
                               ETA: 1275640.5s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.307s, learning 0.278s)
               Value function loss: 0.4053
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: -92.88
               Mean episode length: 281.65
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 12.58s
                        Total time: 8927.31s
                               ETA: 1275590.4s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.022s, learning 0.224s)
               Value function loss: 0.3230
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: -90.66
               Mean episode length: 286.30
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 12.25s
                        Total time: 8939.56s
                               ETA: 1275492.1s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1340 steps/s (collection: 11.992s, learning 0.234s)
               Value function loss: 0.4153
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: -87.77
               Mean episode length: 286.30
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 12.23s
                        Total time: 8951.78s
                               ETA: 1275391.2s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.054s, learning 0.206s)
               Value function loss: 0.2939
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: -86.48
               Mean episode length: 286.21
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 12.26s
                        Total time: 8964.04s
                               ETA: 1275295.4s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.449s, learning 0.180s)
               Value function loss: 0.3280
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: -86.00
               Mean episode length: 286.21
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 12.63s
                        Total time: 8976.67s
                               ETA: 1275252.2s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.374s, learning 0.211s)
               Value function loss: 0.3724
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: -85.50
               Mean episode length: 286.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 12.58s
                        Total time: 8989.25s
                               ETA: 1275202.8s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.891s, learning 0.283s)
               Value function loss: 0.4008
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: -83.68
               Mean episode length: 286.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 12.17s
                        Total time: 9001.43s
                               ETA: 1275095.3s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.898s, learning 0.179s)
               Value function loss: 0.2621
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: -83.68
               Mean episode length: 286.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 12.08s
                        Total time: 9013.50s
                               ETA: 1274974.3s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.011s, learning 0.191s)
               Value function loss: 0.2902
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -81.84
               Mean episode length: 288.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 12.20s
                        Total time: 9025.71s
                               ETA: 1274871.3s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.371s, learning 0.196s)
               Value function loss: 0.3135
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: -81.58
               Mean episode length: 288.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 12.57s
                        Total time: 9038.27s
                               ETA: 1274820.2s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.219s, learning 0.293s)
               Value function loss: 0.2972
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: -81.58
               Mean episode length: 288.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 12.51s
                        Total time: 9050.79s
                               ETA: 1274761.4s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.269s, learning 0.292s)
               Value function loss: 0.3852
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: -79.07
               Mean episode length: 290.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 11.56s
                        Total time: 9062.35s
                               ETA: 1274568.8s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.208s, learning 0.188s)
               Value function loss: 0.4129
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: -79.26
               Mean episode length: 292.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 12.40s
                        Total time: 9074.74s
                               ETA: 1274494.2s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.846s, learning 0.238s)
               Value function loss: 0.4521
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: -75.92
               Mean episode length: 294.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 12.08s
                        Total time: 9086.82s
                               ETA: 1274375.9s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.018s, learning 0.180s)
               Value function loss: 0.5471
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: -75.03
               Mean episode length: 294.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 12.20s
                        Total time: 9099.02s
                               ETA: 1274274.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.769s, learning 0.243s)
               Value function loss: 0.3452
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: -74.70
               Mean episode length: 294.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 12.01s
                        Total time: 9111.04s
                               ETA: 1274146.2s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.375s, learning 0.178s)
               Value function loss: 0.3682
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: -74.41
               Mean episode length: 294.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 12.55s
                        Total time: 9123.59s
                               ETA: 1274094.4s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.144s, learning 0.205s)
               Value function loss: 0.3546
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: -74.07
               Mean episode length: 294.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 12.35s
                        Total time: 9135.94s
                               ETA: 1274014.1s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.014s, learning 0.204s)
               Value function loss: 76.9121
                    Surrogate loss: 0.0400
             Mean action noise std: 0.77
                       Mean reward: -56.36
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 12.22s
                        Total time: 9148.16s
                               ETA: 1273915.9s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.115s, learning 0.174s)
               Value function loss: 4.0431
                    Surrogate loss: 0.0095
             Mean action noise std: 0.77
                       Mean reward: -56.33
               Mean episode length: 300.00
                  Mean reward/step: -1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 12.29s
                        Total time: 9160.44s
                               ETA: 1273827.8s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.850s, learning 0.252s)
               Value function loss: 7.0436
                    Surrogate loss: 0.0589
             Mean action noise std: 0.77
                       Mean reward: -57.75
               Mean episode length: 300.00
                  Mean reward/step: -1.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 12.10s
                        Total time: 9172.55s
                               ETA: 1273714.0s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.131s, learning 0.231s)
               Value function loss: 0.2562
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: -57.67
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 12.36s
                        Total time: 9184.91s
                               ETA: 1273636.5s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.087s, learning 0.178s)
               Value function loss: 0.5659
                    Surrogate loss: 0.0061
             Mean action noise std: 0.77
                       Mean reward: -57.30
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 12.27s
                        Total time: 9197.17s
                               ETA: 1273545.7s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.762s, learning 0.282s)
               Value function loss: 0.4173
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: -57.30
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 12.04s
                        Total time: 9209.22s
                               ETA: 1273424.6s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.918s, learning 0.194s)
               Value function loss: 1.3564
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: -57.71
               Mean episode length: 300.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 12.11s
                        Total time: 9221.33s
                               ETA: 1273313.2s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.071s, learning 0.228s)
               Value function loss: 2.6391
                    Surrogate loss: 0.0222
             Mean action noise std: 0.77
                       Mean reward: -60.98
               Mean episode length: 292.74
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 12.30s
                        Total time: 9233.63s
                               ETA: 1273227.8s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.423s, learning 0.186s)
               Value function loss: 1.8817
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: -60.88
               Mean episode length: 292.74
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 12.61s
                        Total time: 9246.24s
                               ETA: 1273185.3s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.195s)
               Value function loss: 0.4408
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: -61.67
               Mean episode length: 290.48
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 11.95s
                        Total time: 9258.19s
                               ETA: 1273051.8s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.504s, learning 0.376s)
               Value function loss: 0.3519
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: -61.67
               Mean episode length: 290.48
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 11.88s
                        Total time: 9270.06s
                               ETA: 1272909.4s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.755s, learning 0.303s)
               Value function loss: 0.3675
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: -63.35
               Mean episode length: 288.41
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 12.06s
                        Total time: 9282.12s
                               ETA: 1272791.8s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.166s, learning 0.304s)
               Value function loss: 0.2288
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: -63.46
               Mean episode length: 288.41
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 12.47s
                        Total time: 9294.59s
                               ETA: 1272731.0s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.118s, learning 0.320s)
               Value function loss: 0.4289
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: -62.83
               Mean episode length: 288.41
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 12.44s
                        Total time: 9307.03s
                               ETA: 1272665.9s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.056s, learning 0.319s)
               Value function loss: 0.4325
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: -63.20
               Mean episode length: 286.04
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 12.37s
                        Total time: 9319.41s
                               ETA: 1272592.4s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.137s, learning 0.296s)
               Value function loss: 0.6044
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: -63.32
               Mean episode length: 283.63
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 12.43s
                        Total time: 9331.84s
                               ETA: 1272526.8s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.639s, learning 0.298s)
               Value function loss: 0.5360
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: -65.00
               Mean episode length: 283.63
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 12.94s
                        Total time: 9344.78s
                               ETA: 1272530.2s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.756s, learning 0.182s)
               Value function loss: 0.8158
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: -64.33
               Mean episode length: 283.63
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 11.94s
                        Total time: 9356.71s
                               ETA: 1272397.7s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.827s, learning 0.197s)
               Value function loss: 0.6017
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: -62.42
               Mean episode length: 283.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 12.02s
                        Total time: 9368.74s
                               ETA: 1272277.1s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.679s, learning 0.188s)
               Value function loss: 0.7212
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: -60.65
               Mean episode length: 286.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 11.87s
                        Total time: 9380.60s
                               ETA: 1272135.6s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.795s, learning 0.223s)
               Value function loss: 0.3522
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: -60.33
               Mean episode length: 288.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 12.02s
                        Total time: 9392.62s
                               ETA: 1272014.8s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.939s, learning 0.223s)
               Value function loss: 0.4590
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: -61.55
               Mean episode length: 290.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 12.16s
                        Total time: 9404.78s
                               ETA: 1271913.8s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.774s, learning 0.212s)
               Value function loss: 0.2785
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: -61.67
               Mean episode length: 290.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 11.99s
                        Total time: 9416.77s
                               ETA: 1271789.2s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.037s, learning 0.201s)
               Value function loss: 0.4274
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: -61.67
               Mean episode length: 290.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 12.24s
                        Total time: 9429.01s
                               ETA: 1271699.1s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.594s, learning 0.198s)
               Value function loss: 0.2379
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: -60.81
               Mean episode length: 293.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 11.79s
                        Total time: 9440.80s
                               ETA: 1271549.1s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.186s, learning 0.228s)
               Value function loss: 0.4776
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: -60.25
               Mean episode length: 293.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 12.41s
                        Total time: 9453.22s
                               ETA: 1271483.1s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.196s, learning 0.254s)
               Value function loss: 0.3042
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: -60.12
               Mean episode length: 293.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 12.45s
                        Total time: 9465.67s
                               ETA: 1271422.0s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.801s, learning 0.254s)
               Value function loss: 0.2345
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: -60.12
               Mean episode length: 293.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 12.06s
                        Total time: 9477.72s
                               ETA: 1271308.1s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.395s, learning 0.203s)
               Value function loss: 0.3715
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: -61.27
               Mean episode length: 290.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 11.60s
                        Total time: 9489.32s
                               ETA: 1271133.2s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.175s, learning 0.195s)
               Value function loss: 0.2259
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: -61.27
               Mean episode length: 290.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 12.37s
                        Total time: 9501.69s
                               ETA: 1271062.0s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.081s, learning 0.278s)
               Value function loss: 0.2673
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: -59.02
               Mean episode length: 292.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 12.36s
                        Total time: 9514.05s
                               ETA: 1270989.6s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.157s, learning 0.163s)
               Value function loss: 0.3784
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: -59.05
               Mean episode length: 292.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 12.32s
                        Total time: 9526.37s
                               ETA: 1270912.1s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.881s, learning 0.208s)
               Value function loss: 0.3865
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: -58.81
               Mean episode length: 292.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 12.09s
                        Total time: 9538.46s
                               ETA: 1270803.9s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.636s, learning 0.321s)
               Value function loss: 0.4408
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: -58.12
               Mean episode length: 295.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 11.96s
                        Total time: 9550.41s
                               ETA: 1270678.5s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.047s, learning 0.262s)
               Value function loss: 0.3155
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: -59.56
               Mean episode length: 293.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 12.31s
                        Total time: 9562.72s
                               ETA: 1270600.2s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.436s, learning 0.300s)
               Value function loss: 0.3913
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: -59.10
               Mean episode length: 293.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 12.74s
                        Total time: 9575.46s
                               ETA: 1270578.7s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.174s, learning 0.330s)
               Value function loss: 0.2501
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: -59.06
               Mean episode length: 293.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 12.50s
                        Total time: 9587.96s
                               ETA: 1270526.4s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.375s, learning 0.293s)
               Value function loss: 84.1120
                    Surrogate loss: 0.0429
             Mean action noise std: 0.77
                       Mean reward: -46.31
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 12.67s
                        Total time: 9600.63s
                               ETA: 1270496.0s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.521s, learning 0.350s)
               Value function loss: 0.1899
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: -45.97
               Mean episode length: 300.00
                  Mean reward/step: -1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 12.87s
                        Total time: 9613.50s
                               ETA: 1270492.6s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.049s, learning 0.309s)
               Value function loss: 3.7586
                    Surrogate loss: 0.0383
             Mean action noise std: 0.77
                       Mean reward: -46.75
               Mean episode length: 300.00
                  Mean reward/step: -1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 12.36s
                        Total time: 9625.86s
                               ETA: 1270421.2s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.216s, learning 0.230s)
               Value function loss: 0.5606
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: -46.43
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 12.45s
                        Total time: 9638.30s
                               ETA: 1270361.8s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.147s, learning 0.195s)
               Value function loss: 0.3573
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: -46.39
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 12.34s
                        Total time: 9650.65s
                               ETA: 1270288.7s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.006s, learning 0.175s)
               Value function loss: 0.8207
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: -46.26
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 12.18s
                        Total time: 9662.83s
                               ETA: 1270194.6s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.097s, learning 0.328s)
               Value function loss: 2.0364
                    Surrogate loss: 0.0280
             Mean action noise std: 0.77
                       Mean reward: -46.26
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 12.43s
                        Total time: 9675.25s
                               ETA: 1270132.8s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.806s, learning 0.186s)
               Value function loss: 1.3721
                    Surrogate loss: 0.0113
             Mean action noise std: 0.77
                       Mean reward: -47.73
               Mean episode length: 297.55
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 11.99s
                        Total time: 9687.24s
                               ETA: 1270014.3s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.643s, learning 0.191s)
               Value function loss: 1.8666
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: -48.21
               Mean episode length: 297.55
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 11.83s
                        Total time: 9699.08s
                               ETA: 1269875.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.903s, learning 0.292s)
               Value function loss: 1.5315
                    Surrogate loss: 0.0008
             Mean action noise std: 0.77
                       Mean reward: -48.11
               Mean episode length: 297.55
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 12.20s
                        Total time: 9711.27s
                               ETA: 1269784.2s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.115s, learning 0.205s)
               Value function loss: 0.6181
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: -48.40
               Mean episode length: 297.55
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 12.32s
                        Total time: 9723.59s
                               ETA: 1269709.4s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.587s, learning 0.224s)
               Value function loss: 0.5667
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: -48.64
               Mean episode length: 297.55
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 11.81s
                        Total time: 9735.40s
                               ETA: 1269568.4s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.480s, learning 0.304s)
               Value function loss: 0.3124
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: -48.58
               Mean episode length: 297.55
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 12.78s
                        Total time: 9748.19s
                               ETA: 1269554.4s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.990s, learning 0.201s)
               Value function loss: 0.5050
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: -48.81
               Mean episode length: 295.19
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 12.19s
                        Total time: 9760.38s
                               ETA: 1269463.3s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.079s, learning 0.190s)
               Value function loss: 0.2973
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: -49.67
               Mean episode length: 292.72
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 12.27s
                        Total time: 9772.65s
                               ETA: 1269382.6s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.879s, learning 0.181s)
               Value function loss: 0.6502
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: -50.49
               Mean episode length: 290.26
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 12.06s
                        Total time: 9784.71s
                               ETA: 1269274.9s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.463s, learning 0.193s)
               Value function loss: 0.7085
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -49.53
               Mean episode length: 290.26
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 12.66s
                        Total time: 9797.36s
                               ETA: 1269244.6s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.079s, learning 0.192s)
               Value function loss: 0.7822
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: -49.01
               Mean episode length: 290.26
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 12.27s
                        Total time: 9809.63s
                               ETA: 1269164.6s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.889s, learning 0.213s)
               Value function loss: 0.8446
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -50.32
               Mean episode length: 287.82
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 12.10s
                        Total time: 9821.74s
                               ETA: 1269062.9s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.209s, learning 0.205s)
               Value function loss: 0.7417
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: -48.97
               Mean episode length: 290.27
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 12.41s
                        Total time: 9834.15s
                               ETA: 1269001.8s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.704s, learning 0.177s)
               Value function loss: 0.4814
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: -49.68
               Mean episode length: 290.27
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 11.88s
                        Total time: 9846.03s
                               ETA: 1268872.2s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.540s, learning 0.225s)
               Value function loss: 0.3976
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: -49.80
               Mean episode length: 287.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 12.77s
                        Total time: 9858.80s
                               ETA: 1268856.6s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.218s, learning 0.188s)
               Value function loss: 0.3977
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: -49.58
               Mean episode length: 287.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 12.41s
                        Total time: 9871.20s
                               ETA: 1268794.8s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.759s, learning 0.229s)
               Value function loss: 0.3120
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: -49.58
               Mean episode length: 287.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 11.99s
                        Total time: 9883.19s
                               ETA: 1268679.4s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.081s, learning 0.191s)
               Value function loss: 0.3070
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: -50.91
               Mean episode length: 287.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 12.27s
                        Total time: 9895.46s
                               ETA: 1268600.8s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.970s, learning 0.208s)
               Value function loss: 0.4065
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: -51.77
               Mean episode length: 285.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 12.18s
                        Total time: 9907.64s
                               ETA: 1268510.4s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.338s, learning 0.184s)
               Value function loss: 0.4028
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: -52.36
               Mean episode length: 282.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 12.52s
                        Total time: 9920.16s
                               ETA: 1268464.1s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.945s, learning 0.211s)
               Value function loss: 0.2084
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: -52.36
               Mean episode length: 282.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 12.16s
                        Total time: 9932.32s
                               ETA: 1268371.2s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.941s, learning 0.180s)
               Value function loss: 0.2339
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: -52.37
               Mean episode length: 282.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 12.12s
                        Total time: 9944.44s
                               ETA: 1268274.1s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.897s, learning 0.183s)
               Value function loss: 0.3651
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: -52.26
               Mean episode length: 282.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 12.08s
                        Total time: 9956.52s
                               ETA: 1268171.9s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.036s, learning 0.218s)
               Value function loss: 0.2177
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: -52.26
               Mean episode length: 282.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 12.25s
                        Total time: 9968.77s
                               ETA: 1268092.0s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.193s, learning 0.329s)
               Value function loss: 0.3796
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: -51.67
               Mean episode length: 285.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 12.52s
                        Total time: 9981.30s
                               ETA: 1268046.4s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.388s, learning 0.309s)
               Value function loss: 0.4069
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: -51.17
               Mean episode length: 287.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 12.70s
                        Total time: 9993.99s
                               ETA: 1268023.0s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.463s, learning 0.227s)
               Value function loss: 0.5345
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: -50.83
               Mean episode length: 290.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 12.69s
                        Total time: 10006.68s
                               ETA: 1267998.8s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.189s, learning 0.171s)
               Value function loss: 0.6013
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: -51.27
               Mean episode length: 290.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 12.36s
                        Total time: 10019.04s
                               ETA: 1267932.9s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.695s, learning 0.213s)
               Value function loss: 0.4521
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: -53.23
               Mean episode length: 290.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 11.91s
                        Total time: 10030.95s
                               ETA: 1267810.1s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.018s, learning 0.194s)
               Value function loss: 0.3248
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: -53.32
               Mean episode length: 290.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 12.21s
                        Total time: 10043.16s
                               ETA: 1267725.8s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.068s, learning 0.175s)
               Value function loss: 0.3123
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: -53.38
               Mean episode length: 290.28
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 12.24s
                        Total time: 10055.41s
                               ETA: 1267645.7s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.974s, learning 0.191s)
               Value function loss: 89.3098
                    Surrogate loss: 0.0321
             Mean action noise std: 0.77
                       Mean reward: -55.48
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 12.16s
                        Total time: 10067.57s
                               ETA: 1267555.9s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.889s, learning 0.189s)
               Value function loss: 3.2375
                    Surrogate loss: 0.0037
             Mean action noise std: 0.77
                       Mean reward: -55.60
               Mean episode length: 300.00
                  Mean reward/step: -1.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 12.08s
                        Total time: 10079.65s
                               ETA: 1267455.3s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.334s, learning 0.232s)
               Value function loss: 1.5483
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: -56.03
               Mean episode length: 300.00
                  Mean reward/step: -1.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 12.57s
                        Total time: 10092.22s
                               ETA: 1267416.3s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.024s, learning 0.205s)
               Value function loss: 0.4644
                    Surrogate loss: 0.0110
             Mean action noise std: 0.77
                       Mean reward: -55.90
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 12.23s
                        Total time: 10104.45s
                               ETA: 1267335.1s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.379s, learning 0.186s)
               Value function loss: 0.5218
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: -56.96
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 12.57s
                        Total time: 10117.01s
                               ETA: 1267296.2s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.040s, learning 0.195s)
               Value function loss: 0.3809
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: -57.64
               Mean episode length: 297.68
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 12.23s
                        Total time: 10129.25s
                               ETA: 1267216.0s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.435s, learning 0.280s)
               Value function loss: 1.1713
                    Surrogate loss: 0.0037
             Mean action noise std: 0.77
                       Mean reward: -58.14
               Mean episode length: 297.68
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 12.71s
                        Total time: 10141.96s
                               ETA: 1267195.9s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.231s, learning 0.202s)
               Value function loss: 1.9839
                    Surrogate loss: 0.0007
             Mean action noise std: 0.77
                       Mean reward: -62.96
               Mean episode length: 288.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 12.43s
                        Total time: 10154.39s
                               ETA: 1267140.7s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.665s, learning 0.211s)
               Value function loss: 1.5066
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: -68.62
               Mean episode length: 278.65
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 11.88s
                        Total time: 10166.27s
                               ETA: 1267016.2s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.878s, learning 0.210s)
               Value function loss: 1.1560
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: -73.73
               Mean episode length: 274.16
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 12.09s
                        Total time: 10178.36s
                               ETA: 1266918.3s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.050s, learning 0.319s)
               Value function loss: 1.2619
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: -73.41
               Mean episode length: 274.16
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 12.37s
                        Total time: 10190.73s
                               ETA: 1266855.6s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.418s, learning 0.351s)
               Value function loss: 1.4632
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: -74.30
               Mean episode length: 272.03
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 12.77s
                        Total time: 10203.50s
                               ETA: 1266842.6s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.145s, learning 0.343s)
               Value function loss: 2.3298
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: -73.99
               Mean episode length: 272.03
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 12.49s
                        Total time: 10215.98s
                               ETA: 1266794.8s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.951s, learning 0.173s)
               Value function loss: 3.9557
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: -74.13
               Mean episode length: 272.03
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 12.12s
                        Total time: 10228.11s
                               ETA: 1266702.1s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.965s, learning 0.176s)
               Value function loss: 7.0928
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: -75.28
               Mean episode length: 267.16
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 12.14s
                        Total time: 10240.25s
                               ETA: 1266611.5s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.676s, learning 0.177s)
               Value function loss: 9.1112
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: -75.60
               Mean episode length: 264.77
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 11.85s
                        Total time: 10252.10s
                               ETA: 1266485.7s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.096s, learning 0.371s)
               Value function loss: 23.7154
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: -76.97
               Mean episode length: 263.06
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 12.47s
                        Total time: 10264.57s
                               ETA: 1266436.0s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.658s, learning 0.200s)
               Value function loss: 4.3607
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: -74.27
               Mean episode length: 262.89
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 11.86s
                        Total time: 10276.43s
                               ETA: 1266311.1s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.115s, learning 0.278s)
               Value function loss: 2.5641
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: -72.11
               Mean episode length: 265.33
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 12.39s
                        Total time: 10288.82s
                               ETA: 1266252.5s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.339s, learning 0.197s)
               Value function loss: 2.3404
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: -71.19
               Mean episode length: 272.57
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 12.54s
                        Total time: 10301.36s
                               ETA: 1266211.6s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.822s, learning 0.212s)
               Value function loss: 1.7018
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: -72.49
               Mean episode length: 273.51
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 12.03s
                        Total time: 10313.39s
                               ETA: 1266109.1s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1337 steps/s (collection: 11.964s, learning 0.285s)
               Value function loss: 1.7142
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: -66.20
               Mean episode length: 275.75
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 12.25s
                        Total time: 10325.64s
                               ETA: 1266033.2s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.211s, learning 0.317s)
               Value function loss: 1.2329
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: -66.31
               Mean episode length: 275.75
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 12.53s
                        Total time: 10338.17s
                               ETA: 1265991.7s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.240s, learning 0.257s)
               Value function loss: 1.3531
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: -68.09
               Mean episode length: 273.20
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 12.50s
                        Total time: 10350.67s
                               ETA: 1265946.4s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.545s, learning 0.227s)
               Value function loss: 1.2781
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: -68.35
               Mean episode length: 273.20
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 12.77s
                        Total time: 10363.44s
                               ETA: 1265934.8s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.911s, learning 0.211s)
               Value function loss: 1.5453
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: -70.39
               Mean episode length: 270.76
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 12.12s
                        Total time: 10375.56s
                               ETA: 1265843.8s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.697s, learning 0.237s)
               Value function loss: 1.0555
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -69.54
               Mean episode length: 270.76
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 11.93s
                        Total time: 10387.49s
                               ETA: 1265730.2s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.194s, learning 0.303s)
               Value function loss: 1.0935
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: -69.54
               Mean episode length: 270.76
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 12.50s
                        Total time: 10399.99s
                               ETA: 1265685.4s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.206s, learning 0.294s)
               Value function loss: 1.6663
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: -71.61
               Mean episode length: 274.13
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 12.50s
                        Total time: 10412.49s
                               ETA: 1265640.9s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.181s, learning 0.172s)
               Value function loss: 2.4246
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: -71.61
               Mean episode length: 274.13
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 12.35s
                        Total time: 10424.85s
                               ETA: 1265578.8s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.772s, learning 0.202s)
               Value function loss: 2.2628
                    Surrogate loss: 0.1532
             Mean action noise std: 0.76
                       Mean reward: -73.88
               Mean episode length: 274.13
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 11.97s
                        Total time: 10436.82s
                               ETA: 1265470.7s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.076s, learning 0.197s)
               Value function loss: 1.2439
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: -80.05
               Mean episode length: 270.88
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 12.27s
                        Total time: 10449.09s
                               ETA: 1265399.0s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.099s, learning 0.187s)
               Value function loss: 1.0020
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: -85.05
               Mean episode length: 272.84
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 12.29s
                        Total time: 10461.38s
                               ETA: 1265329.0s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.095s, learning 0.224s)
               Value function loss: 1.2167
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: -87.00
               Mean episode length: 272.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 12.32s
                        Total time: 10473.70s
                               ETA: 1265263.3s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.096s, learning 0.204s)
               Value function loss: 1.1201
                    Surrogate loss: 0.0127
             Mean action noise std: 0.76
                       Mean reward: -87.14
               Mean episode length: 274.55
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 12.30s
                        Total time: 10486.00s
                               ETA: 1265195.4s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.372s, learning 0.270s)
               Value function loss: 0.9340
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: -86.23
               Mean episode length: 274.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 12.64s
                        Total time: 10498.64s
                               ETA: 1265168.8s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.104s, learning 0.215s)
               Value function loss: 0.8225
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: -86.18
               Mean episode length: 274.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 12.32s
                        Total time: 10510.96s
                               ETA: 1265103.3s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.972s, learning 0.179s)
               Value function loss: 114.1050
                    Surrogate loss: 0.0345
             Mean action noise std: 0.76
                       Mean reward: -78.07
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 4.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 12.15s
                        Total time: 10523.11s
                               ETA: 1265017.9s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.602s, learning 0.190s)
               Value function loss: 0.8391
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: -78.73
               Mean episode length: 300.00
                  Mean reward/step: -1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 11.79s
                        Total time: 10534.90s
                               ETA: 1264889.5s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.427s, learning 0.223s)
               Value function loss: 6.9966
                    Surrogate loss: 0.0453
             Mean action noise std: 0.76
                       Mean reward: -76.57
               Mean episode length: 300.00
                  Mean reward/step: -1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 12.65s
                        Total time: 10547.55s
                               ETA: 1264864.3s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.959s, learning 0.208s)
               Value function loss: 0.7272
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: -76.73
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 12.17s
                        Total time: 10559.72s
                               ETA: 1264781.3s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.664s, learning 0.278s)
               Value function loss: 0.3698
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: -77.70
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 11.94s
                        Total time: 10571.66s
                               ETA: 1264671.5s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.054s, learning 0.235s)
               Value function loss: 0.6204
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: -79.69
               Mean episode length: 297.70
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 12.29s
                        Total time: 10583.95s
                               ETA: 1264603.3s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.192s, learning 0.303s)
               Value function loss: 0.4908
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -79.69
               Mean episode length: 297.70
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 12.50s
                        Total time: 10596.44s
                               ETA: 1264560.0s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.036s, learning 0.292s)
               Value function loss: 2.1822
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: -85.83
               Mean episode length: 290.33
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 12.33s
                        Total time: 10608.77s
                               ETA: 1264496.8s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.402s, learning 0.180s)
               Value function loss: 2.9264
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: -93.28
               Mean episode length: 283.16
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 11.58s
                        Total time: 10620.35s
                               ETA: 1264344.9s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.959s, learning 0.182s)
               Value function loss: 1.5588
                    Surrogate loss: 0.0085
             Mean action noise std: 0.76
                       Mean reward: -97.81
               Mean episode length: 278.51
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 12.14s
                        Total time: 10632.50s
                               ETA: 1264259.8s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.197s, learning 0.304s)
               Value function loss: 1.3149
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: -99.54
               Mean episode length: 278.51
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 12.50s
                        Total time: 10645.00s
                               ETA: 1264217.6s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.219s, learning 0.205s)
               Value function loss: 0.9862
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: -100.07
               Mean episode length: 278.51
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 12.42s
                        Total time: 10657.42s
                               ETA: 1264166.3s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.078s, learning 0.183s)
               Value function loss: 1.0634
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: -101.53
               Mean episode length: 278.51
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 12.26s
                        Total time: 10669.68s
                               ETA: 1264095.9s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.266s, learning 0.327s)
               Value function loss: 1.1174
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: -102.67
               Mean episode length: 278.51
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 12.59s
                        Total time: 10682.27s
                               ETA: 1264064.9s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.098s, learning 0.188s)
               Value function loss: 1.0209
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: -102.64
               Mean episode length: 278.51
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 12.29s
                        Total time: 10694.56s
                               ETA: 1263997.5s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.815s, learning 0.209s)
               Value function loss: 1.7353
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: -108.18
               Mean episode length: 276.70
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 12.02s
                        Total time: 10706.58s
                               ETA: 1263899.4s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.056s, learning 0.211s)
               Value function loss: 1.6778
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: -101.59
               Mean episode length: 276.70
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 12.27s
                        Total time: 10718.85s
                               ETA: 1263830.2s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.634s, learning 0.170s)
               Value function loss: 1.7064
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: -100.56
               Mean episode length: 276.70
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 11.80s
                        Total time: 10730.65s
                               ETA: 1263706.5s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.985s, learning 0.177s)
               Value function loss: 1.5400
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: -99.49
               Mean episode length: 283.94
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 12.16s
                        Total time: 10742.82s
                               ETA: 1263625.2s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.680s, learning 0.177s)
               Value function loss: 1.2566
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: -98.27
               Mean episode length: 283.92
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 11.86s
                        Total time: 10754.67s
                               ETA: 1263508.4s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.000s, learning 0.339s)
               Value function loss: 0.9876
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: -100.39
               Mean episode length: 288.74
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 12.34s
                        Total time: 10767.01s
                               ETA: 1263448.3s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.353s, learning 0.176s)
               Value function loss: 1.0459
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: -98.96
               Mean episode length: 292.08
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 12.53s
                        Total time: 10779.54s
                               ETA: 1263410.6s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.741s, learning 0.179s)
               Value function loss: 0.8606
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: -98.88
               Mean episode length: 291.97
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 11.92s
                        Total time: 10791.46s
                               ETA: 1263301.6s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.236s, learning 0.351s)
               Value function loss: 0.6928
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: -99.59
               Mean episode length: 291.97
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 12.59s
                        Total time: 10804.05s
                               ETA: 1263270.8s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.174s, learning 0.184s)
               Value function loss: 0.9423
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: -97.36
               Mean episode length: 289.59
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 12.36s
                        Total time: 10816.41s
                               ETA: 1263213.4s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.034s, learning 0.176s)
               Value function loss: 0.9198
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: -100.47
               Mean episode length: 289.59
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 12.21s
                        Total time: 10828.62s
                               ETA: 1263138.8s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.619s, learning 0.377s)
               Value function loss: 0.7394
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: -101.10
               Mean episode length: 288.63
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 12.00s
                        Total time: 10840.61s
                               ETA: 1263039.4s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.023s, learning 0.188s)
               Value function loss: 0.4388
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: -102.22
               Mean episode length: 281.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 12.21s
                        Total time: 10852.82s
                               ETA: 1262965.3s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.601s, learning 0.200s)
               Value function loss: 0.7072
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: -101.67
               Mean episode length: 276.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 11.80s
                        Total time: 10864.62s
                               ETA: 1262843.7s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.811s, learning 0.214s)
               Value function loss: 0.6063
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: -99.23
               Mean episode length: 276.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 12.02s
                        Total time: 10876.65s
                               ETA: 1262748.3s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.511s, learning 0.225s)
               Value function loss: 0.4615
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: -99.23
               Mean episode length: 276.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 12.74s
                        Total time: 10889.38s
                               ETA: 1262735.5s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.627s, learning 0.196s)
               Value function loss: 0.7504
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: -101.36
               Mean episode length: 275.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 11.82s
                        Total time: 10901.21s
                               ETA: 1262617.0s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.010s, learning 0.294s)
               Value function loss: 0.7047
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: -101.21
               Mean episode length: 275.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 12.30s
                        Total time: 10913.51s
                               ETA: 1262554.4s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.930s, learning 0.196s)
               Value function loss: 0.8005
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: -105.23
               Mean episode length: 275.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 12.13s
                        Total time: 10925.64s
                               ETA: 1262471.3s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.972s, learning 0.214s)
               Value function loss: 0.7477
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: -103.83
               Mean episode length: 275.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 12.19s
                        Total time: 10937.82s
                               ETA: 1262395.2s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.218s, learning 0.249s)
               Value function loss: 0.8045
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: -103.36
               Mean episode length: 273.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 12.47s
                        Total time: 10950.29s
                               ETA: 1262351.7s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.064s, learning 0.198s)
               Value function loss: 0.6204
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: -105.45
               Mean episode length: 273.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 12.26s
                        Total time: 10962.55s
                               ETA: 1262284.7s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.946s, learning 0.195s)
               Value function loss: 1.0558
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: -105.58
               Mean episode length: 273.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 12.14s
                        Total time: 10974.69s
                               ETA: 1262204.0s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.203s, learning 0.240s)
               Value function loss: 93.7661
                    Surrogate loss: 0.0619
             Mean action noise std: 0.76
                       Mean reward: -65.33
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 4.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 12.44s
                        Total time: 10987.13s
                               ETA: 1262158.1s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.505s, learning 0.273s)
               Value function loss: 5.8701
                    Surrogate loss: 0.0427
             Mean action noise std: 0.76
                       Mean reward: -65.27
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 12.78s
                        Total time: 10999.91s
                               ETA: 1262150.8s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.845s, learning 0.292s)
               Value function loss: 11.2395
                    Surrogate loss: 0.0031
             Mean action noise std: 0.76
                       Mean reward: -64.57
               Mean episode length: 300.00
                  Mean reward/step: -1.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 12.14s
                        Total time: 11012.05s
                               ETA: 1262070.0s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.691s, learning 0.201s)
               Value function loss: 1.1859
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: -68.87
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 11.89s
                        Total time: 11023.94s
                               ETA: 1261961.3s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.906s, learning 0.228s)
               Value function loss: 0.5747
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: -66.34
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 12.13s
                        Total time: 11036.08s
                               ETA: 1261880.5s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.937s, learning 0.205s)
               Value function loss: 0.2931
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: -66.20
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 12.14s
                        Total time: 11048.22s
                               ETA: 1261800.8s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.538s, learning 0.226s)
               Value function loss: 0.4418
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: -67.51
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 11.76s
                        Total time: 11059.98s
                               ETA: 1261678.0s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.279s, learning 0.234s)
               Value function loss: 1.7282
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: -71.22
               Mean episode length: 295.15
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 12.51s
                        Total time: 11072.50s
                               ETA: 1261640.9s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.937s, learning 0.181s)
               Value function loss: 2.9657
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: -76.97
               Mean episode length: 292.77
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 12.12s
                        Total time: 11084.61s
                               ETA: 1261558.8s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1340 steps/s (collection: 11.988s, learning 0.231s)
               Value function loss: 2.6470
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: -85.31
               Mean episode length: 290.18
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 12.22s
                        Total time: 11096.83s
                               ETA: 1261488.4s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.007s, learning 0.171s)
               Value function loss: 2.3368
                    Surrogate loss: 0.0026
             Mean action noise std: 0.76
                       Mean reward: -94.36
               Mean episode length: 290.18
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 12.18s
                        Total time: 11109.01s
                               ETA: 1261413.5s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.810s, learning 0.193s)
               Value function loss: 3.1358
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: -95.76
               Mean episode length: 290.18
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 12.00s
                        Total time: 11121.01s
                               ETA: 1261318.9s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.255s, learning 0.312s)
               Value function loss: 5.3921
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: -94.92
               Mean episode length: 290.18
                  Mean reward/step: -0.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 12.57s
                        Total time: 11133.58s
                               ETA: 1261288.4s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.667s, learning 0.213s)
               Value function loss: 1.8086
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: -97.68
               Mean episode length: 290.18
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 11.88s
                        Total time: 11145.46s
                               ETA: 1261180.2s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.130s, learning 0.278s)
               Value function loss: 1.6885
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: -100.78
               Mean episode length: 290.18
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 12.41s
                        Total time: 11157.87s
                               ETA: 1261131.8s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.443s, learning 0.191s)
               Value function loss: 1.9627
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: -103.68
               Mean episode length: 287.76
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 12.63s
                        Total time: 11170.50s
                               ETA: 1261109.0s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.315s, learning 0.239s)
               Value function loss: 1.5926
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: -108.60
               Mean episode length: 287.76
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 12.55s
                        Total time: 11183.06s
                               ETA: 1261077.3s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.158s, learning 0.192s)
               Value function loss: 1.7563
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: -111.41
               Mean episode length: 287.76
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 12.35s
                        Total time: 11195.41s
                               ETA: 1261022.5s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.209s, learning 0.211s)
               Value function loss: 1.3329
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: -117.30
               Mean episode length: 290.21
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 12.42s
                        Total time: 11207.82s
                               ETA: 1260975.7s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.798s, learning 0.280s)
               Value function loss: 1.3161
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: -119.80
               Mean episode length: 290.21
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 12.08s
                        Total time: 11219.90s
                               ETA: 1260890.7s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.509s, learning 0.188s)
               Value function loss: 0.9450
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: -121.03
               Mean episode length: 292.61
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 12.70s
                        Total time: 11232.60s
                               ETA: 1260875.3s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.076s, learning 0.179s)
               Value function loss: 1.2919
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: -119.82
               Mean episode length: 294.99
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 12.25s
                        Total time: 11244.86s
                               ETA: 1260810.3s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.663s, learning 0.191s)
               Value function loss: 0.7832
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: -120.52
               Mean episode length: 294.99
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 11.85s
                        Total time: 11256.71s
                               ETA: 1260700.6s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.714s, learning 0.176s)
               Value function loss: 0.9179
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -115.62
               Mean episode length: 296.97
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 11.89s
                        Total time: 11268.60s
                               ETA: 1260595.1s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.031s, learning 0.211s)
               Value function loss: 0.6889
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: -113.65
               Mean episode length: 296.97
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 12.24s
                        Total time: 11280.84s
                               ETA: 1260529.2s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.730s, learning 0.192s)
               Value function loss: 1.0727
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: -110.40
               Mean episode length: 296.97
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 11.92s
                        Total time: 11292.76s
                               ETA: 1260427.6s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.133s, learning 0.292s)
               Value function loss: 0.7278
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: -110.87
               Mean episode length: 296.97
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 12.42s
                        Total time: 11305.19s
                               ETA: 1260382.3s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.596s, learning 0.328s)
               Value function loss: 0.7404
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: -107.78
               Mean episode length: 296.97
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 12.92s
                        Total time: 11318.11s
                               ETA: 1260392.7s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1233 steps/s (collection: 12.890s, learning 0.390s)
               Value function loss: 0.9287
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: -107.46
               Mean episode length: 296.97
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 13.28s
                        Total time: 11331.39s
                               ETA: 1260442.6s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.669s, learning 0.279s)
               Value function loss: 0.7680
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: -109.89
               Mean episode length: 296.30
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 12.95s
                        Total time: 11344.34s
                               ETA: 1260455.5s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.056s, learning 0.222s)
               Value function loss: 0.7903
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: -112.58
               Mean episode length: 296.30
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 12.28s
                        Total time: 11356.62s
                               ETA: 1260393.9s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.312s, learning 0.175s)
               Value function loss: 0.9239
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: -117.47
               Mean episode length: 297.33
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 12.49s
                        Total time: 11369.11s
                               ETA: 1260355.7s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.165s, learning 0.231s)
               Value function loss: 0.7642
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: -121.68
               Mean episode length: 296.92
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 12.40s
                        Total time: 11381.50s
                               ETA: 1260307.4s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.103s, learning 0.220s)
               Value function loss: 0.6347
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: -120.39
               Mean episode length: 296.92
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 12.32s
                        Total time: 11393.82s
                               ETA: 1260251.1s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.192s, learning 0.218s)
               Value function loss: 0.5930
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: -118.74
               Mean episode length: 296.92
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 12.41s
                        Total time: 11406.23s
                               ETA: 1260204.5s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.564s, learning 0.274s)
               Value function loss: 0.7680
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: -115.89
               Mean episode length: 294.59
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 12.84s
                        Total time: 11419.07s
                               ETA: 1260205.2s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.427s, learning 0.234s)
               Value function loss: 0.6536
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: -116.05
               Mean episode length: 294.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 12.66s
                        Total time: 11431.73s
                               ETA: 1260186.4s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.424s, learning 0.309s)
               Value function loss: 100.9156
                    Surrogate loss: 0.0355
             Mean action noise std: 0.76
                       Mean reward: -93.02
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 4.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 12.73s
                        Total time: 11444.47s
                               ETA: 1260175.6s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.062s, learning 0.252s)
               Value function loss: 0.5829
                    Surrogate loss: 0.0231
             Mean action noise std: 0.76
                       Mean reward: -94.93
               Mean episode length: 300.00
                  Mean reward/step: -1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 12.31s
                        Total time: 11456.78s
                               ETA: 1260118.5s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.125s, learning 0.166s)
               Value function loss: 2.4019
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: -94.47
               Mean episode length: 300.00
                  Mean reward/step: -1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 12.29s
                        Total time: 11469.07s
                               ETA: 1260059.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.065s, learning 0.196s)
               Value function loss: 0.4994
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: -95.06
               Mean episode length: 300.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 12.26s
                        Total time: 11481.33s
                               ETA: 1259996.6s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.997s, learning 0.182s)
               Value function loss: 0.2930
                    Surrogate loss: 0.0037
             Mean action noise std: 0.76
                       Mean reward: -95.06
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 12.18s
                        Total time: 11493.51s
                               ETA: 1259925.2s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.046s, learning 0.281s)
               Value function loss: 0.4044
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: -96.28
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 12.33s
                        Total time: 11505.84s
                               ETA: 1259870.1s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.266s, learning 0.219s)
               Value function loss: 0.2060
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: -96.28
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 12.49s
                        Total time: 11518.32s
                               ETA: 1259832.4s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.540s, learning 0.163s)
               Value function loss: 0.9788
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: -99.97
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 11.70s
                        Total time: 11530.03s
                               ETA: 1259709.3s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.414s, learning 0.306s)
               Value function loss: 1.4310
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -112.76
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 12.72s
                        Total time: 11542.75s
                               ETA: 1259697.5s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.242s, learning 0.209s)
               Value function loss: 1.6165
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: -114.05
               Mean episode length: 300.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 12.45s
                        Total time: 11555.20s
                               ETA: 1259656.2s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.961s, learning 0.198s)
               Value function loss: 1.6369
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: -116.50
               Mean episode length: 300.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 12.16s
                        Total time: 11567.36s
                               ETA: 1259583.3s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.963s, learning 0.228s)
               Value function loss: 2.4516
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: -116.85
               Mean episode length: 300.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 12.19s
                        Total time: 11579.55s
                               ETA: 1259514.0s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.367s, learning 0.304s)
               Value function loss: 2.9862
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: -120.79
               Mean episode length: 300.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 12.67s
                        Total time: 11592.22s
                               ETA: 1259496.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.531s, learning 0.270s)
               Value function loss: 1.3177
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -122.81
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 12.80s
                        Total time: 11605.02s
                               ETA: 1259494.0s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.411s, learning 0.202s)
               Value function loss: 0.8549
                    Surrogate loss: -0.0000
             Mean action noise std: 0.76
                       Mean reward: -123.26
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 12.61s
                        Total time: 11617.63s
                               ETA: 1259470.7s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.808s, learning 0.219s)
               Value function loss: 1.1594
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: -121.79
               Mean episode length: 297.65
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 12.03s
                        Total time: 11629.66s
                               ETA: 1259383.9s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.038s, learning 0.187s)
               Value function loss: 1.2040
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: -128.86
               Mean episode length: 297.65
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 12.22s
                        Total time: 11641.88s
                               ETA: 1259318.8s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.959s, learning 0.175s)
               Value function loss: 0.9840
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: -120.45
               Mean episode length: 297.65
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 12.13s
                        Total time: 11654.02s
                               ETA: 1259243.8s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.990s, learning 0.173s)
               Value function loss: 0.8849
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: -120.66
               Mean episode length: 295.16
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 12.16s
                        Total time: 11666.18s
                               ETA: 1259172.1s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.397s, learning 0.267s)
               Value function loss: 0.5234
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: -120.14
               Mean episode length: 295.16
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 12.66s
                        Total time: 11678.84s
                               ETA: 1259154.7s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.016s, learning 0.308s)
               Value function loss: 0.5301
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: -111.19
               Mean episode length: 295.16
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 12.32s
                        Total time: 11691.17s
                               ETA: 1259100.6s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.286s, learning 0.217s)
               Value function loss: 0.5925
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: -105.31
               Mean episode length: 293.81
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 12.50s
                        Total time: 11703.67s
                               ETA: 1259065.8s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.948s, learning 0.196s)
               Value function loss: 0.5206
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: -106.87
               Mean episode length: 293.81
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 12.14s
                        Total time: 11715.81s
                               ETA: 1258992.5s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.052s, learning 0.192s)
               Value function loss: 0.3660
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: -104.64
               Mean episode length: 293.81
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 12.24s
                        Total time: 11728.06s
                               ETA: 1258930.2s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.348s, learning 0.189s)
               Value function loss: 0.5263
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: -103.61
               Mean episode length: 293.81
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 12.54s
                        Total time: 11740.60s
                               ETA: 1258899.3s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.825s, learning 0.208s)
               Value function loss: 0.5316
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: -101.41
               Mean episode length: 293.81
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 12.03s
                        Total time: 11752.63s
                               ETA: 1258814.6s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.830s, learning 0.278s)
               Value function loss: 0.5370
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: -97.69
               Mean episode length: 291.89
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 12.11s
                        Total time: 11764.74s
                               ETA: 1258737.9s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.127s, learning 0.177s)
               Value function loss: 0.2975
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: -97.50
               Mean episode length: 291.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 12.30s
                        Total time: 11777.04s
                               ETA: 1258682.3s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.042s, learning 0.204s)
               Value function loss: 0.4344
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: -96.90
               Mean episode length: 291.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 12.25s
                        Total time: 11789.29s
                               ETA: 1258620.6s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.103s, learning 0.262s)
               Value function loss: 0.5358
                    Surrogate loss: -0.0018
             Mean action noise std: 0.76
                       Mean reward: -100.42
               Mean episode length: 291.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 12.37s
                        Total time: 11801.65s
                               ETA: 1258571.8s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.516s, learning 0.304s)
               Value function loss: 0.2315
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: -100.42
               Mean episode length: 291.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 11.82s
                        Total time: 11813.47s
                               ETA: 1258464.9s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.212s, learning 0.167s)
               Value function loss: 0.4994
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: -99.36
               Mean episode length: 291.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 12.38s
                        Total time: 11825.85s
                               ETA: 1258417.8s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.066s, learning 0.183s)
               Value function loss: 0.3968
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: -94.39
               Mean episode length: 291.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 12.25s
                        Total time: 11838.10s
                               ETA: 1258356.8s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1335 steps/s (collection: 11.961s, learning 0.307s)
               Value function loss: 0.5322
                    Surrogate loss: 0.0000
             Mean action noise std: 0.76
                       Mean reward: -94.36
               Mean episode length: 291.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 12.27s
                        Total time: 11850.37s
                               ETA: 1258298.1s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.892s, learning 0.182s)
               Value function loss: 0.4958
                    Surrogate loss: -0.0012
             Mean action noise std: 0.76
                       Mean reward: -94.29
               Mean episode length: 291.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 12.07s
                        Total time: 11862.44s
                               ETA: 1258218.9s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.072s, learning 0.237s)
               Value function loss: 0.5209
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: -95.97
               Mean episode length: 288.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 12.31s
                        Total time: 11874.75s
                               ETA: 1258164.6s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.914s, learning 0.177s)
               Value function loss: 0.5580
                    Surrogate loss: -0.0005
             Mean action noise std: 0.76
                       Mean reward: -95.45
               Mean episode length: 291.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 12.09s
                        Total time: 11886.84s
                               ETA: 1258087.5s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.859s, learning 0.172s)
               Value function loss: 0.3944
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: -95.09
               Mean episode length: 291.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 12.03s
                        Total time: 11898.87s
                               ETA: 1258004.2s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.569s, learning 0.240s)
               Value function loss: 87.9354
                    Surrogate loss: 0.0796
             Mean action noise std: 0.76
                       Mean reward: -82.23
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 4.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 12.81s
                        Total time: 11911.68s
                               ETA: 1258003.0s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.356s, learning 0.252s)
               Value function loss: 0.4895
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: -82.34
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 12.61s
                        Total time: 11924.29s
                               ETA: 1257980.7s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.640s, learning 0.204s)
               Value function loss: 0.3870
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: -85.28
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 11.84s
                        Total time: 11936.13s
                               ETA: 1257878.0s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.196s)
               Value function loss: 0.2963
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: -85.28
               Mean episode length: 297.29
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 11.95s
                        Total time: 11948.08s
                               ETA: 1257786.3s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.103s, learning 0.261s)
               Value function loss: 0.6252
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: -87.14
               Mean episode length: 289.29
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 12.36s
                        Total time: 11960.44s
                               ETA: 1257738.5s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.002s, learning 0.285s)
               Value function loss: 0.3245
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: -87.66
               Mean episode length: 286.91
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 12.29s
                        Total time: 11972.73s
                               ETA: 1257682.7s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.511s, learning 0.174s)
               Value function loss: 0.8562
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: -88.71
               Mean episode length: 281.89
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 12.69s
                        Total time: 11985.42s
                               ETA: 1257668.8s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.127s, learning 0.183s)
               Value function loss: 2.8911
                    Surrogate loss: 0.0033
             Mean action noise std: 0.76
                       Mean reward: -90.90
               Mean episode length: 269.81
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 12.31s
                        Total time: 11997.73s
                               ETA: 1257615.7s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.212s, learning 0.304s)
               Value function loss: 1.5792
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: -98.61
               Mean episode length: 258.21
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 12.52s
                        Total time: 12010.24s
                               ETA: 1257584.2s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.104s, learning 0.193s)
               Value function loss: 0.9496
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: -99.65
               Mean episode length: 251.39
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 12.30s
                        Total time: 12022.54s
                               ETA: 1257529.8s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.905s, learning 0.196s)
               Value function loss: 0.6579
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: -105.53
               Mean episode length: 236.15
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 12.10s
                        Total time: 12034.64s
                               ETA: 1257455.0s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.905s, learning 0.275s)
               Value function loss: 0.4876
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: -108.47
               Mean episode length: 229.79
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 12.18s
                        Total time: 12046.82s
                               ETA: 1257388.5s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.390s, learning 0.232s)
               Value function loss: 0.3831
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: -109.22
               Mean episode length: 227.73
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 12.62s
                        Total time: 12059.44s
                               ETA: 1257368.3s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.029s, learning 0.185s)
               Value function loss: 0.6247
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: -110.08
               Mean episode length: 227.73
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 12.21s
                        Total time: 12071.66s
                               ETA: 1257305.5s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.140s, learning 0.174s)
               Value function loss: 0.7384
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: -107.33
               Mean episode length: 228.78
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 12.31s
                        Total time: 12083.97s
                               ETA: 1257253.3s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.745s, learning 0.201s)
               Value function loss: 1.0208
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: -103.16
               Mean episode length: 241.50
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 11.95s
                        Total time: 12095.92s
                               ETA: 1257163.0s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.414s, learning 0.266s)
               Value function loss: 0.8558
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: -103.92
               Mean episode length: 241.50
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 12.68s
                        Total time: 12108.60s
                               ETA: 1257149.1s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.690s, learning 0.187s)
               Value function loss: 1.0047
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: -101.27
               Mean episode length: 253.58
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 11.88s
                        Total time: 12120.47s
                               ETA: 1257051.8s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.307s, learning 0.314s)
               Value function loss: 0.6014
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: -97.92
               Mean episode length: 253.58
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 12.62s
                        Total time: 12133.10s
                               ETA: 1257031.8s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.268s, learning 0.208s)
               Value function loss: 0.6539
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: -95.61
               Mean episode length: 261.37
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 12.48s
                        Total time: 12145.57s
                               ETA: 1256996.8s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.265s, learning 0.288s)
               Value function loss: 0.3929
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: -91.79
               Mean episode length: 268.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 12.55s
                        Total time: 12158.12s
                               ETA: 1256969.8s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.075s, learning 0.183s)
               Value function loss: 0.6416
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: -85.96
               Mean episode length: 277.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 12.26s
                        Total time: 12170.38s
                               ETA: 1256912.2s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.173s, learning 0.190s)
               Value function loss: 0.2232
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: -86.06
               Mean episode length: 277.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 12.36s
                        Total time: 12182.74s
                               ETA: 1256865.8s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.266s, learning 0.186s)
               Value function loss: 0.5289
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: -80.76
               Mean episode length: 287.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 12.45s
                        Total time: 12195.20s
                               ETA: 1256828.4s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.851s, learning 0.199s)
               Value function loss: 0.4014
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: -79.92
               Mean episode length: 285.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 12.05s
                        Total time: 12207.25s
                               ETA: 1256749.9s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.056s, learning 0.275s)
               Value function loss: 0.6898
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: -76.83
               Mean episode length: 289.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 12.33s
                        Total time: 12219.58s
                               ETA: 1256700.4s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.598s, learning 0.310s)
               Value function loss: 0.3803
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: -74.13
               Mean episode length: 293.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 12.91s
                        Total time: 12232.49s
                               ETA: 1256710.2s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.966s, learning 0.201s)
               Value function loss: 0.4145
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: -76.00
               Mean episode length: 292.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 12.17s
                        Total time: 12244.65s
                               ETA: 1256644.0s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.625s, learning 0.177s)
               Value function loss: 0.6869
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: -76.52
               Mean episode length: 292.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 11.80s
                        Total time: 12256.45s
                               ETA: 1256540.3s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.500s, learning 0.267s)
               Value function loss: 0.3954
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: -75.27
               Mean episode length: 292.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 12.77s
                        Total time: 12269.22s
                               ETA: 1256535.6s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.781s, learning 0.220s)
               Value function loss: 0.3152
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: -75.13
               Mean episode length: 292.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 12.00s
                        Total time: 12281.22s
                               ETA: 1256452.6s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.245s, learning 0.295s)
               Value function loss: 0.6289
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: -77.30
               Mean episode length: 292.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 12.54s
                        Total time: 12293.76s
                               ETA: 1256424.9s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.011s, learning 0.215s)
               Value function loss: 0.6182
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: -74.67
               Mean episode length: 292.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 12.23s
                        Total time: 12305.99s
                               ETA: 1256365.1s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.016s, learning 0.180s)
               Value function loss: 0.5848
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: -74.19
               Mean episode length: 292.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 12.20s
                        Total time: 12318.18s
                               ETA: 1256302.4s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.138s, learning 0.202s)
               Value function loss: 0.5396
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: -74.17
               Mean episode length: 292.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 12.34s
                        Total time: 12330.52s
                               ETA: 1256254.5s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.098s, learning 0.169s)
               Value function loss: 0.6249
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: -74.02
               Mean episode length: 292.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 12.27s
                        Total time: 12342.79s
                               ETA: 1256199.2s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.073s, learning 0.179s)
               Value function loss: 0.5933
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: -74.20
               Mean episode length: 292.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 12.25s
                        Total time: 12355.04s
                               ETA: 1256142.5s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.161s, learning 0.211s)
               Value function loss: 94.7705
                    Surrogate loss: 0.0208
             Mean action noise std: 0.76
                       Mean reward: -57.20
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 4.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 12.37s
                        Total time: 12367.41s
                               ETA: 1256098.0s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.282s, learning 0.306s)
               Value function loss: 0.3124
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: -57.53
               Mean episode length: 300.00
                  Mean reward/step: -1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 12.59s
                        Total time: 12380.00s
                               ETA: 1256075.4s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.507s, learning 0.191s)
               Value function loss: 0.3236
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: -57.63
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 12.70s
                        Total time: 12392.70s
                               ETA: 1256064.1s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.884s, learning 0.189s)
               Value function loss: 0.6336
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: -58.36
               Mean episode length: 297.56
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 12.07s
                        Total time: 12404.77s
                               ETA: 1255989.5s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.067s, learning 0.383s)
               Value function loss: 0.4696
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: -57.98
               Mean episode length: 297.56
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 12.45s
                        Total time: 12417.22s
                               ETA: 1255953.2s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.140s, learning 0.175s)
               Value function loss: 0.7546
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: -59.88
               Mean episode length: 297.56
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 12.31s
                        Total time: 12429.54s
                               ETA: 1255903.2s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.729s, learning 0.186s)
               Value function loss: 0.7927
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: -58.10
               Mean episode length: 297.56
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 11.92s
                        Total time: 12441.45s
                               ETA: 1255813.0s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.432s, learning 0.202s)
               Value function loss: 1.7615
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: -58.99
               Mean episode length: 297.56
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 12.63s
                        Total time: 12454.09s
                               ETA: 1255795.5s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.141s, learning 0.223s)
               Value function loss: 2.6927
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: -62.31
               Mean episode length: 292.67
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 12.36s
                        Total time: 12466.45s
                               ETA: 1255750.7s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.255s, learning 0.183s)
               Value function loss: 2.0147
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: -64.81
               Mean episode length: 292.67
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 12.44s
                        Total time: 12478.89s
                               ETA: 1255713.4s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.744s, learning 0.189s)
               Value function loss: 0.9584
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: -65.51
               Mean episode length: 292.67
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 11.93s
                        Total time: 12490.82s
                               ETA: 1255625.5s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.087s, learning 0.184s)
               Value function loss: 1.1744
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: -65.76
               Mean episode length: 292.67
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 12.27s
                        Total time: 12503.09s
                               ETA: 1255571.6s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.176s, learning 0.299s)
               Value function loss: 1.0267
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: -66.01
               Mean episode length: 290.60
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 12.47s
                        Total time: 12515.57s
                               ETA: 1255538.3s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.243s, learning 0.306s)
               Value function loss: 0.8623
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: -65.35
               Mean episode length: 288.06
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 12.55s
                        Total time: 12528.12s
                               ETA: 1255512.5s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.092s, learning 0.222s)
               Value function loss: 0.7613
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: -64.74
               Mean episode length: 290.50
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 12.31s
                        Total time: 12540.43s
                               ETA: 1255463.1s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.117s, learning 0.219s)
               Value function loss: 1.4369
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: -66.76
               Mean episode length: 290.50
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 12.34s
                        Total time: 12552.77s
                               ETA: 1255416.0s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.107s, learning 0.211s)
               Value function loss: 1.2409
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: -64.91
               Mean episode length: 288.07
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 12.32s
                        Total time: 12565.08s
                               ETA: 1255367.3s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.584s, learning 0.312s)
               Value function loss: 1.6012
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: -63.76
               Mean episode length: 292.96
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 12.90s
                        Total time: 12577.98s
                               ETA: 1255376.2s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.134s, learning 0.175s)
               Value function loss: 2.0834
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: -68.58
               Mean episode length: 292.96
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 12.31s
                        Total time: 12590.29s
                               ETA: 1255326.6s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.033s, learning 0.190s)
               Value function loss: 1.5307
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: -67.48
               Mean episode length: 285.60
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 12.22s
                        Total time: 12602.51s
                               ETA: 1255268.5s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.088s, learning 0.189s)
               Value function loss: 1.7876
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: -68.57
               Mean episode length: 285.60
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 12.28s
                        Total time: 12614.79s
                               ETA: 1255215.9s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.247s, learning 0.334s)
               Value function loss: 1.9176
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: -72.07
               Mean episode length: 283.59
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 12.58s
                        Total time: 12627.37s
                               ETA: 1255193.5s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.137s, learning 0.297s)
               Value function loss: 0.6753
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: -70.18
               Mean episode length: 283.59
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 12.43s
                        Total time: 12639.80s
                               ETA: 1255156.6s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.081s, learning 0.172s)
               Value function loss: 0.5434
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: -70.94
               Mean episode length: 281.14
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 12.25s
                        Total time: 12652.06s
                               ETA: 1255101.7s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.161s, learning 0.259s)
               Value function loss: 0.7585
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: -75.78
               Mean episode length: 282.12
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 12.42s
                        Total time: 12664.48s
                               ETA: 1255063.6s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.281s, learning 0.234s)
               Value function loss: 0.6972
                    Surrogate loss: 0.0003
             Mean action noise std: 0.76
                       Mean reward: -76.62
               Mean episode length: 284.66
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 12.51s
                        Total time: 12676.99s
                               ETA: 1255034.8s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.827s, learning 0.297s)
               Value function loss: 0.7560
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: -77.63
               Mean episode length: 284.66
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 12.12s
                        Total time: 12689.12s
                               ETA: 1254967.4s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.918s, learning 0.199s)
               Value function loss: 0.3986
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: -79.29
               Mean episode length: 282.28
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 12.12s
                        Total time: 12701.23s
                               ETA: 1254899.5s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.203s, learning 0.261s)
               Value function loss: 0.4671
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: -77.58
               Mean episode length: 282.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 12.46s
                        Total time: 12713.70s
                               ETA: 1254865.8s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.140s, learning 0.216s)
               Value function loss: 0.5899
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: -77.16
               Mean episode length: 284.71
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 12.36s
                        Total time: 12726.05s
                               ETA: 1254821.6s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.690s, learning 0.199s)
               Value function loss: 0.3079
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: -77.16
               Mean episode length: 284.71
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 11.89s
                        Total time: 12737.94s
                               ETA: 1254731.5s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.396s, learning 0.294s)
               Value function loss: 0.5451
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: -75.75
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 12.69s
                        Total time: 12750.63s
                               ETA: 1254720.4s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.083s, learning 0.178s)
               Value function loss: 0.6411
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: -78.98
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 12.26s
                        Total time: 12762.89s
                               ETA: 1254667.1s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.666s, learning 0.226s)
               Value function loss: 0.6576
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: -82.54
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.89s
                        Total time: 12774.78s
                               ETA: 1254577.6s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.028s, learning 0.176s)
               Value function loss: 0.4855
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: -82.93
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 12.20s
                        Total time: 12786.99s
                               ETA: 1254518.9s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.533s, learning 0.204s)
               Value function loss: 0.5768
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: -78.59
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 11.74s
                        Total time: 12798.73s
                               ETA: 1254414.5s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.940s, learning 0.280s)
               Value function loss: 0.5536
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: -77.52
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 12.22s
                        Total time: 12810.95s
                               ETA: 1254357.6s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.125s, learning 0.201s)
               Value function loss: 0.5868
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: -77.70
               Mean episode length: 284.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 12.33s
                        Total time: 12823.27s
                               ETA: 1254311.1s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.754s, learning 0.276s)
               Value function loss: 83.7037
                    Surrogate loss: 0.0625
             Mean action noise std: 0.76
                       Mean reward: -64.28
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 4.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 12.03s
                        Total time: 12835.30s
                               ETA: 1254235.7s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.460s, learning 0.230s)
               Value function loss: 0.3961
                    Surrogate loss: 0.0067
             Mean action noise std: 0.76
                       Mean reward: -64.91
               Mean episode length: 297.65
                  Mean reward/step: -1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 12.69s
                        Total time: 12847.99s
                               ETA: 1254224.9s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.130s, learning 0.174s)
               Value function loss: 0.3798
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: -64.32
               Mean episode length: 297.65
                  Mean reward/step: -0.99
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 12.30s
                        Total time: 12860.30s
                               ETA: 1254176.5s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.964s, learning 0.239s)
               Value function loss: 0.2941
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: -65.00
               Mean episode length: 297.65
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 12.20s
                        Total time: 12872.50s
                               ETA: 1254118.4s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.748s, learning 0.213s)
               Value function loss: 0.6608
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: -68.15
               Mean episode length: 295.97
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 11.96s
                        Total time: 12884.46s
                               ETA: 1254036.7s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.848s, learning 0.200s)
               Value function loss: 0.3078
                    Surrogate loss: 0.0007
             Mean action noise std: 0.76
                       Mean reward: -69.06
               Mean episode length: 293.35
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 12.05s
                        Total time: 12896.51s
                               ETA: 1253963.6s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.643s, learning 0.194s)
               Value function loss: 0.6355
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: -75.77
               Mean episode length: 290.87
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 11.84s
                        Total time: 12908.34s
                               ETA: 1253870.2s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.870s, learning 0.195s)
               Value function loss: 2.1698
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: -77.38
               Mean episode length: 281.16
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 12.07s
                        Total time: 12920.41s
                               ETA: 1253799.1s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.127s, learning 0.198s)
               Value function loss: 2.4550
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: -78.18
               Mean episode length: 281.16
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 12.33s
                        Total time: 12932.73s
                               ETA: 1253753.3s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.276s, learning 0.205s)
               Value function loss: 2.4450
                    Surrogate loss: 0.0258
             Mean action noise std: 0.76
                       Mean reward: -82.59
               Mean episode length: 281.16
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 12.48s
                        Total time: 12945.22s
                               ETA: 1253722.6s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.785s, learning 0.236s)
               Value function loss: 1.3206
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: -81.78
               Mean episode length: 281.16
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 12.02s
                        Total time: 12957.24s
                               ETA: 1253647.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.099s, learning 0.177s)
               Value function loss: 0.7845
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: -86.11
               Mean episode length: 278.98
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 12.28s
                        Total time: 12969.51s
                               ETA: 1253597.1s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.242s, learning 0.175s)
               Value function loss: 0.5924
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: -86.63
               Mean episode length: 278.98
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 12.42s
                        Total time: 12981.93s
                               ETA: 1253560.5s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.268s, learning 0.189s)
               Value function loss: 0.6678
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: -88.28
               Mean episode length: 276.65
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 12.46s
                        Total time: 12994.39s
                               ETA: 1253527.7s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.928s, learning 0.188s)
               Value function loss: 0.8503
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: -85.37
               Mean episode length: 278.33
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 12.12s
                        Total time: 13006.50s
                               ETA: 1253462.2s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.228s, learning 0.176s)
               Value function loss: 1.1190
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: -78.58
               Mean episode length: 285.89
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 12.40s
                        Total time: 13018.91s
                               ETA: 1253424.4s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.901s, learning 0.174s)
               Value function loss: 0.9182
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: -80.48
               Mean episode length: 283.50
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 12.07s
                        Total time: 13030.98s
                               ETA: 1253355.0s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.087s, learning 0.179s)
               Value function loss: 1.2172
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: -80.87
               Mean episode length: 288.31
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 12.27s
                        Total time: 13043.25s
                               ETA: 1253304.1s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.831s, learning 0.191s)
               Value function loss: 0.9181
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: -82.86
               Mean episode length: 288.31
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 12.02s
                        Total time: 13055.27s
                               ETA: 1253229.8s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.210s, learning 0.184s)
               Value function loss: 0.8662
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: -85.86
               Mean episode length: 288.31
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 12.39s
                        Total time: 13067.66s
                               ETA: 1253191.3s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.005s, learning 0.224s)
               Value function loss: 0.4795
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: -84.87
               Mean episode length: 288.31
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 12.23s
                        Total time: 13079.89s
                               ETA: 1253137.1s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.144s, learning 0.175s)
               Value function loss: 0.8438
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: -89.72
               Mean episode length: 287.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 12.32s
                        Total time: 13092.21s
                               ETA: 1253091.7s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.593s, learning 0.178s)
               Value function loss: 0.4583
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: -89.98
               Mean episode length: 287.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 11.77s
                        Total time: 13103.98s
                               ETA: 1252993.8s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.833s, learning 0.220s)
               Value function loss: 0.6539
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: -89.68
               Mean episode length: 289.06
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 12.05s
                        Total time: 13116.03s
                               ETA: 1252923.0s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.981s, learning 0.193s)
               Value function loss: 0.6146
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: -88.42
               Mean episode length: 287.95
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 12.17s
                        Total time: 13128.21s
                               ETA: 1252863.9s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.024s, learning 0.204s)
               Value function loss: 0.7545
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: -93.30
               Mean episode length: 285.50
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 12.23s
                        Total time: 13140.43s
                               ETA: 1252810.0s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.323s, learning 0.321s)
               Value function loss: 0.6378
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: -95.71
               Mean episode length: 283.08
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 12.64s
                        Total time: 13153.08s
                               ETA: 1252795.9s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.887s, learning 0.207s)
               Value function loss: 0.5498
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: -98.98
               Mean episode length: 281.02
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 12.09s
                        Total time: 13165.17s
                               ETA: 1252729.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.988s, learning 0.189s)
               Value function loss: 0.8059
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: -100.87
               Mean episode length: 281.02
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 12.18s
                        Total time: 13177.35s
                               ETA: 1252671.1s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.893s, learning 0.187s)
               Value function loss: 0.6219
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: -101.32
               Mean episode length: 283.43
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 12.08s
                        Total time: 13189.43s
                               ETA: 1252603.5s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.437s, learning 0.179s)
               Value function loss: 0.7165
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: -101.87
               Mean episode length: 283.42
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 12.62s
                        Total time: 13202.05s
                               ETA: 1252586.9s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.278s, learning 0.199s)
               Value function loss: 1.1634
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: -105.33
               Mean episode length: 283.42
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 12.48s
                        Total time: 13214.52s
                               ETA: 1252557.1s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.049s, learning 0.201s)
               Value function loss: 1.3144
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: -111.10
               Mean episode length: 283.42
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 12.25s
                        Total time: 13226.77s
                               ETA: 1252505.9s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.025s, learning 0.220s)
               Value function loss: 1.5252
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -113.14
               Mean episode length: 283.42
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 12.24s
                        Total time: 13239.02s
                               ETA: 1252454.2s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.730s, learning 0.209s)
               Value function loss: 1.6325
                    Surrogate loss: 0.0000
             Mean action noise std: 0.76
                       Mean reward: -114.08
               Mean episode length: 283.42
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 11.94s
                        Total time: 13250.96s
                               ETA: 1252373.8s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.932s, learning 0.181s)
               Value function loss: 1.5191
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: -112.71
               Mean episode length: 283.42
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 12.11s
                        Total time: 13263.07s
                               ETA: 1252309.8s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.820s, learning 0.170s)
               Value function loss: 1.7133
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: -112.31
               Mean episode length: 283.42
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 11.99s
                        Total time: 13275.06s
                               ETA: 1252234.4s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.307s, learning 0.191s)
               Value function loss: 86.8633
                    Surrogate loss: 0.0262
             Mean action noise std: 0.76
                       Mean reward: -89.65
               Mean episode length: 300.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 4.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 12.50s
                        Total time: 13287.56s
                               ETA: 1252206.9s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.718s, learning 0.165s)
               Value function loss: 2.0788
                    Surrogate loss: 0.0042
             Mean action noise std: 0.76
                       Mean reward: -89.65
               Mean episode length: 300.00
                  Mean reward/step: -1.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 11.88s
                        Total time: 13299.44s
                               ETA: 1252121.6s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.018s, learning 0.175s)
               Value function loss: 0.6867
                    Surrogate loss: 0.0008
             Mean action noise std: 0.76
                       Mean reward: -87.49
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 12.19s
                        Total time: 13311.64s
                               ETA: 1252065.6s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.871s, learning 0.200s)
               Value function loss: 0.6257
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: -88.14
               Mean episode length: 297.65
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 12.07s
                        Total time: 13323.71s
                               ETA: 1251998.2s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.777s, learning 0.208s)
               Value function loss: 0.4335
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: -87.37
               Mean episode length: 294.93
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 11.98s
                        Total time: 13335.69s
                               ETA: 1251922.8s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.988s, learning 0.191s)
               Value function loss: 0.6289
                    Surrogate loss: 0.0090
             Mean action noise std: 0.76
                       Mean reward: -83.94
               Mean episode length: 294.93
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 12.18s
                        Total time: 13347.87s
                               ETA: 1251865.8s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.778s, learning 0.186s)
               Value function loss: 0.4256
                    Surrogate loss: -0.0005
             Mean action noise std: 0.76
                       Mean reward: -84.18
               Mean episode length: 294.93
                  Mean reward/step: -0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 11.96s
                        Total time: 13359.84s
                               ETA: 1251788.7s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.147s, learning 0.199s)
               Value function loss: 2.0782
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: -90.11
               Mean episode length: 294.93
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 12.35s
                        Total time: 13372.18s
                               ETA: 1251747.5s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.289s, learning 0.219s)
               Value function loss: 2.6780
                    Surrogate loss: 0.0028
             Mean action noise std: 0.76
                       Mean reward: -98.92
               Mean episode length: 294.93
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 12.51s
                        Total time: 13384.69s
                               ETA: 1251721.5s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.345s, learning 0.175s)
               Value function loss: 4.2601
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: -98.90
               Mean episode length: 294.93
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 12.52s
                        Total time: 13397.21s
                               ETA: 1251696.6s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.945s, learning 0.182s)
               Value function loss: 2.2198
                    Surrogate loss: 0.0002
             Mean action noise std: 0.76
                       Mean reward: -100.22
               Mean episode length: 294.93
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 12.13s
                        Total time: 13409.34s
                               ETA: 1251635.1s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.724s, learning 0.188s)
               Value function loss: 0.9109
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: -99.16
               Mean episode length: 294.93
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 11.91s
                        Total time: 13421.25s
                               ETA: 1251553.5s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.104s, learning 0.176s)
               Value function loss: 0.7505
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: -96.35
               Mean episode length: 294.93
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 12.28s
                        Total time: 13433.53s
                               ETA: 1251506.4s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.371s, learning 0.222s)
               Value function loss: 0.7693
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: -97.68
               Mean episode length: 297.28
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 12.59s
                        Total time: 13446.12s
                               ETA: 1251488.6s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.054s, learning 0.275s)
               Value function loss: 0.7324
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: -100.40
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 12.33s
                        Total time: 13458.45s
                               ETA: 1251446.2s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.129s, learning 0.280s)
               Value function loss: 1.2736
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: -100.75
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 12.41s
                        Total time: 13470.86s
                               ETA: 1251411.3s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.071s, learning 0.201s)
               Value function loss: 1.1473
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: -101.12
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 12.27s
                        Total time: 13483.13s
                               ETA: 1251363.7s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.956s, learning 0.196s)
               Value function loss: 1.4408
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: -98.69
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 12.15s
                        Total time: 13495.28s
                               ETA: 1251304.9s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.986s, learning 0.194s)
               Value function loss: 0.9205
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: -101.62
               Mean episode length: 295.69
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 12.18s
                        Total time: 13507.46s
                               ETA: 1251249.0s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.873s, learning 0.356s)
               Value function loss: 0.7561
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: -106.43
               Mean episode length: 293.40
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 12.23s
                        Total time: 13519.69s
                               ETA: 1251197.7s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.021s, learning 0.259s)
               Value function loss: 0.5121
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: -106.07
               Mean episode length: 293.40
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 12.28s
                        Total time: 13531.97s
                               ETA: 1251151.1s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.303s, learning 0.185s)
               Value function loss: 0.7308
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: -102.59
               Mean episode length: 293.40
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 12.49s
                        Total time: 13544.46s
                               ETA: 1251123.8s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.096s, learning 0.173s)
               Value function loss: 0.6127
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -102.74
               Mean episode length: 293.40
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 12.27s
                        Total time: 13556.73s
                               ETA: 1251076.3s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.161s, learning 0.210s)
               Value function loss: 0.4570
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: -102.13
               Mean episode length: 293.40
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 12.37s
                        Total time: 13569.10s
                               ETA: 1251038.3s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.315s, learning 0.265s)
               Value function loss: 0.7456
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: -103.98
               Mean episode length: 293.40
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 12.58s
                        Total time: 13581.68s
                               ETA: 1251019.6s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.925s, learning 0.183s)
               Value function loss: 0.9656
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: -104.25
               Mean episode length: 292.35
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 12.11s
                        Total time: 13593.79s
                               ETA: 1250957.5s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.369s, learning 0.210s)
               Value function loss: 0.6395
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: -106.47
               Mean episode length: 291.38
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 12.58s
                        Total time: 13606.37s
                               ETA: 1250938.7s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.005s, learning 0.185s)
               Value function loss: 0.3846
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: -106.14
               Mean episode length: 291.38
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 12.19s
                        Total time: 13618.56s
                               ETA: 1250884.3s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.080s, learning 0.237s)
               Value function loss: 0.4566
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: -101.96
               Mean episode length: 291.38
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 12.32s
                        Total time: 13630.88s
                               ETA: 1250841.6s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.950s, learning 0.179s)
               Value function loss: 0.5402
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: -100.73
               Mean episode length: 291.38
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 12.13s
                        Total time: 13643.01s
                               ETA: 1250781.7s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.779s, learning 0.195s)
               Value function loss: 0.2266
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: -100.77
               Mean episode length: 291.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 11.97s
                        Total time: 13654.98s
                               ETA: 1250707.6s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.783s, learning 0.276s)
               Value function loss: 0.5272
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: -99.14
               Mean episode length: 291.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 12.06s
                        Total time: 13667.04s
                               ETA: 1250641.5s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.330s, learning 0.174s)
               Value function loss: 0.5763
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: -96.25
               Mean episode length: 290.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 12.50s
                        Total time: 13679.54s
                               ETA: 1250616.2s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.123s, learning 0.211s)
               Value function loss: 0.5508
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: -93.01
               Mean episode length: 292.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 12.33s
                        Total time: 13691.88s
                               ETA: 1250575.3s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.372s, learning 0.178s)
               Value function loss: 0.3759
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -90.77
               Mean episode length: 292.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 12.55s
                        Total time: 13704.43s
                               ETA: 1250554.2s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.177s, learning 0.197s)
               Value function loss: 0.4627
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: -88.03
               Mean episode length: 294.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 12.37s
                        Total time: 13716.80s
                               ETA: 1250517.1s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.351s, learning 0.214s)
               Value function loss: 0.7025
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: -80.19
               Mean episode length: 292.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 12.56s
                        Total time: 13729.37s
                               ETA: 1250497.4s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.004s, learning 0.192s)
               Value function loss: 0.3779
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: -85.72
               Mean episode length: 292.30
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 12.20s
                        Total time: 13741.56s
                               ETA: 1250444.2s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.198s, learning 0.220s)
               Value function loss: 86.3456
                    Surrogate loss: 0.0630
             Mean action noise std: 0.75
                       Mean reward: -60.46
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 4.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 12.42s
                        Total time: 13753.98s
                               ETA: 1250411.2s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.109s, learning 0.265s)
               Value function loss: 4.6854
                    Surrogate loss: 0.0014
             Mean action noise std: 0.75
                       Mean reward: -60.27
               Mean episode length: 300.00
                  Mean reward/step: -1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 12.37s
                        Total time: 13766.35s
                               ETA: 1250374.3s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.085s, learning 0.298s)
               Value function loss: 1.6815
                    Surrogate loss: 0.0414
             Mean action noise std: 0.75
                       Mean reward: -59.91
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 12.38s
                        Total time: 13778.74s
                               ETA: 1250338.2s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.176s, learning 0.204s)
               Value function loss: 0.4943
                    Surrogate loss: 0.0040
             Mean action noise std: 0.75
                       Mean reward: -60.29
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 12.38s
                        Total time: 13791.12s
                               ETA: 1250301.9s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.684s, learning 0.199s)
               Value function loss: 0.6122
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: -60.94
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 11.88s
                        Total time: 13803.00s
                               ETA: 1250220.7s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.104s, learning 0.183s)
               Value function loss: 0.4576
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: -60.63
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 12.29s
                        Total time: 13815.29s
                               ETA: 1250176.1s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.559s, learning 0.188s)
               Value function loss: 0.9866
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: -62.07
               Mean episode length: 300.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 11.75s
                        Total time: 13827.03s
                               ETA: 1250082.7s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.687s, learning 0.194s)
               Value function loss: 2.6332
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: -65.78
               Mean episode length: 285.52
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 11.88s
                        Total time: 13838.91s
                               ETA: 1250001.5s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.998s, learning 0.174s)
               Value function loss: 2.1067
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: -65.72
               Mean episode length: 285.52
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 12.17s
                        Total time: 13851.09s
                               ETA: 1249946.8s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.646s, learning 0.273s)
               Value function loss: 1.1536
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: -67.34
               Mean episode length: 285.52
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 11.92s
                        Total time: 13863.01s
                               ETA: 1249869.4s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.229s, learning 0.233s)
               Value function loss: 0.7928
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: -69.57
               Mean episode length: 283.31
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 12.46s
                        Total time: 13875.47s
                               ETA: 1249840.9s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.048s, learning 0.284s)
               Value function loss: 0.9192
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: -68.12
               Mean episode length: 281.23
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 12.33s
                        Total time: 13887.80s
                               ETA: 1249800.9s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.825s, learning 0.335s)
               Value function loss: 0.5922
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: -67.96
               Mean episode length: 281.23
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 12.16s
                        Total time: 13899.96s
                               ETA: 1249745.3s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.933s, learning 0.181s)
               Value function loss: 0.9696
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -67.48
               Mean episode length: 281.23
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 12.11s
                        Total time: 13912.07s
                               ETA: 1249685.8s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.926s, learning 0.170s)
               Value function loss: 0.8314
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: -67.07
               Mean episode length: 281.23
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 12.10s
                        Total time: 13924.17s
                               ETA: 1249624.8s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.243s, learning 0.210s)
               Value function loss: 1.0557
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: -67.24
               Mean episode length: 283.68
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 12.45s
                        Total time: 13936.62s
                               ETA: 1249595.8s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.257s, learning 0.253s)
               Value function loss: 0.8704
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: -68.54
               Mean episode length: 283.68
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 12.51s
                        Total time: 13949.13s
                               ETA: 1249572.0s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.876s, learning 0.193s)
               Value function loss: 1.3122
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: -67.09
               Mean episode length: 287.74
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 12.07s
                        Total time: 13961.20s
                               ETA: 1249508.7s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.974s, learning 0.188s)
               Value function loss: 1.0465
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: -67.96
               Mean episode length: 292.53
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 12.16s
                        Total time: 13973.37s
                               ETA: 1249453.8s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.972s, learning 0.235s)
               Value function loss: 0.7384
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -68.40
               Mean episode length: 292.53
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 12.21s
                        Total time: 13985.57s
                               ETA: 1249403.0s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.386s, learning 0.266s)
               Value function loss: 0.4947
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: -68.28
               Mean episode length: 292.53
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 12.65s
                        Total time: 13998.22s
                               ETA: 1249392.0s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.911s, learning 0.184s)
               Value function loss: 0.9319
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: -62.51
               Mean episode length: 294.74
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 12.09s
                        Total time: 14010.32s
                               ETA: 1249331.3s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.724s, learning 0.197s)
               Value function loss: 0.2983
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: -62.62
               Mean episode length: 294.74
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 11.92s
                        Total time: 14022.24s
                               ETA: 1249255.2s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.059s, learning 0.210s)
               Value function loss: 0.6313
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: -65.07
               Mean episode length: 294.74
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 12.27s
                        Total time: 14034.51s
                               ETA: 1249210.2s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.101s, learning 0.175s)
               Value function loss: 0.6397
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: -62.28
               Mean episode length: 296.82
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 12.28s
                        Total time: 14046.78s
                               ETA: 1249165.9s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1337 steps/s (collection: 11.967s, learning 0.284s)
               Value function loss: 0.9056
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: -63.40
               Mean episode length: 296.82
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 12.25s
                        Total time: 14059.04s
                               ETA: 1249119.5s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.296s, learning 0.210s)
               Value function loss: 0.5208
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: -65.17
               Mean episode length: 295.94
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 12.51s
                        Total time: 14071.54s
                               ETA: 1249095.7s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.071s, learning 0.218s)
               Value function loss: 0.3692
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: -65.03
               Mean episode length: 295.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 12.29s
                        Total time: 14083.83s
                               ETA: 1249052.7s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.368s, learning 0.214s)
               Value function loss: 0.6586
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: -65.91
               Mean episode length: 295.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 12.58s
                        Total time: 14096.41s
                               ETA: 1249035.7s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.813s, learning 0.184s)
               Value function loss: 0.3482
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: -66.03
               Mean episode length: 295.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 12.00s
                        Total time: 14108.41s
                               ETA: 1248966.8s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.886s, learning 0.205s)
               Value function loss: 0.3162
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: -65.60
               Mean episode length: 295.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 12.09s
                        Total time: 14120.50s
                               ETA: 1248906.5s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.097s, learning 0.179s)
               Value function loss: 0.5946
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: -66.90
               Mean episode length: 295.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 12.28s
                        Total time: 14132.78s
                               ETA: 1248862.6s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.853s, learning 0.182s)
               Value function loss: 0.5238
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: -64.94
               Mean episode length: 299.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 12.04s
                        Total time: 14144.81s
                               ETA: 1248797.4s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.071s, learning 0.216s)
               Value function loss: 0.4971
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: -64.39
               Mean episode length: 299.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 12.29s
                        Total time: 14157.10s
                               ETA: 1248754.6s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.683s, learning 0.265s)
               Value function loss: 0.4763
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: -64.47
               Mean episode length: 298.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 11.95s
                        Total time: 14169.05s
                               ETA: 1248681.9s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.534s, learning 0.181s)
               Value function loss: 1.0308
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: -66.26
               Mean episode length: 298.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 12.71s
                        Total time: 14181.76s
                               ETA: 1248676.9s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.609s, learning 0.222s)
               Value function loss: 0.3991
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: -69.24
               Mean episode length: 298.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 11.83s
                        Total time: 14193.59s
                               ETA: 1248594.2s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.720s, learning 0.199s)
               Value function loss: 93.7261
                    Surrogate loss: 0.0449
             Mean action noise std: 0.75
                       Mean reward: -64.41
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 4.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 15.92s
                        Total time: 14209.51s
                               ETA: 1248870.9s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.630s, learning 0.258s)
               Value function loss: 0.5745
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: -64.41
               Mean episode length: 300.00
                  Mean reward/step: -1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 23.89s
                        Total time: 14233.40s
                               ETA: 1249846.8s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.117s, learning 0.214s)
               Value function loss: 0.3056
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: -64.23
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 23.33s
                        Total time: 14256.73s
                               ETA: 1250772.0s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.565s, learning 0.222s)
               Value function loss: 0.5401
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: -63.80
               Mean episode length: 300.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 23.79s
                        Total time: 14280.52s
                               ETA: 1251735.6s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.286s, learning 0.193s)
               Value function loss: 0.9274
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: -63.85
               Mean episode length: 294.63
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 23.48s
                        Total time: 14304.00s
                               ETA: 1252670.3s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.574s, learning 0.202s)
               Value function loss: 1.5567
                    Surrogate loss: 0.0112
             Mean action noise std: 0.75
                       Mean reward: -65.22
               Mean episode length: 284.07
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 23.78s
                        Total time: 14327.77s
                               ETA: 1253629.4s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.807s, learning 0.342s)
               Value function loss: 0.9109
                    Surrogate loss: 0.0102
             Mean action noise std: 0.75
                       Mean reward: -66.67
               Mean episode length: 278.98
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 24.15s
                        Total time: 14351.92s
                               ETA: 1254619.5s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.724s, learning 0.317s)
               Value function loss: 2.1057
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: -73.39
               Mean episode length: 256.95
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 24.04s
                        Total time: 14375.96s
                               ETA: 1255598.2s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.574s, learning 0.185s)
               Value function loss: 2.0386
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -86.58
               Mean episode length: 223.65
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 23.76s
                        Total time: 14399.72s
                               ETA: 1256550.6s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.846s, learning 0.184s)
               Value function loss: 1.0559
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: -89.90
               Mean episode length: 219.43
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 24.03s
                        Total time: 14423.75s
                               ETA: 1257524.8s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.353s, learning 0.308s)
               Value function loss: 0.7542
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -93.97
               Mean episode length: 216.14
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 23.66s
                        Total time: 14447.41s
                               ETA: 1258465.2s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.539s, learning 0.230s)
               Value function loss: 1.3659
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: -91.49
               Mean episode length: 219.87
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 23.77s
                        Total time: 14471.18s
                               ETA: 1259413.3s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.523s, learning 0.180s)
               Value function loss: 1.1102
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -93.68
               Mean episode length: 223.80
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 23.70s
                        Total time: 14494.89s
                               ETA: 1260353.9s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.964s, learning 0.189s)
               Value function loss: 0.9255
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: -95.13
               Mean episode length: 224.70
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 24.15s
                        Total time: 14519.04s
                               ETA: 1261332.0s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.615s, learning 0.260s)
               Value function loss: 1.1621
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: -96.93
               Mean episode length: 222.79
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 23.88s
                        Total time: 14542.91s
                               ETA: 1262284.1s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.310s, learning 0.292s)
               Value function loss: 1.5146
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: -98.79
               Mean episode length: 223.93
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 23.60s
                        Total time: 14566.52s
                               ETA: 1263210.8s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.683s, learning 0.303s)
               Value function loss: 1.5296
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: -92.19
               Mean episode length: 238.80
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 23.99s
                        Total time: 14590.50s
                               ETA: 1264169.1s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.322s, learning 0.183s)
               Value function loss: 1.8985
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -93.14
               Mean episode length: 245.58
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 23.50s
                        Total time: 14614.01s
                               ETA: 1265084.1s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.073s, learning 0.196s)
               Value function loss: 1.9907
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: -91.62
               Mean episode length: 260.98
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 23.27s
                        Total time: 14637.28s
                               ETA: 1265977.1s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.422s, learning 0.198s)
               Value function loss: 1.5270
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: -89.50
               Mean episode length: 266.80
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 23.62s
                        Total time: 14660.90s
                               ETA: 1266898.7s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.724s, learning 0.221s)
               Value function loss: 1.7440
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: -91.95
               Mean episode length: 268.96
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 23.95s
                        Total time: 14684.84s
                               ETA: 1267846.8s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.321s, learning 0.353s)
               Value function loss: 1.3932
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: -90.16
               Mean episode length: 270.46
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 23.67s
                        Total time: 14708.51s
                               ETA: 1268769.8s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.870s, learning 0.199s)
               Value function loss: 1.1223
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: -85.49
               Mean episode length: 276.60
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 24.07s
                        Total time: 14732.58s
                               ETA: 1269725.2s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.248s, learning 0.182s)
               Value function loss: 0.9851
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: -85.36
               Mean episode length: 276.19
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 23.43s
                        Total time: 14756.01s
                               ETA: 1270623.9s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.753s, learning 0.200s)
               Value function loss: 1.0711
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -80.55
               Mean episode length: 280.05
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 23.95s
                        Total time: 14779.97s
                               ETA: 1271566.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 695 steps/s (collection: 23.358s, learning 0.208s)
               Value function loss: 1.0575
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: -76.01
               Mean episode length: 283.70
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 23.57s
                        Total time: 14803.53s
                               ETA: 1272473.1s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.422s, learning 0.174s)
               Value function loss: 0.8965
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: -73.71
               Mean episode length: 285.44
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 23.60s
                        Total time: 14827.13s
                               ETA: 1273381.2s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.227s, learning 0.229s)
               Value function loss: 0.7632
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: -73.63
               Mean episode length: 285.44
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 23.46s
                        Total time: 14850.58s
                               ETA: 1274275.6s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 709 steps/s (collection: 22.911s, learning 0.180s)
               Value function loss: 0.7842
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: -72.86
               Mean episode length: 285.44
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 23.09s
                        Total time: 14873.68s
                               ETA: 1275137.1s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 702 steps/s (collection: 22.951s, learning 0.386s)
               Value function loss: 0.8753
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: -69.25
               Mean episode length: 289.59
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 23.34s
                        Total time: 14897.01s
                               ETA: 1276018.2s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.396s, learning 0.201s)
               Value function loss: 0.5936
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: -69.34
               Mean episode length: 289.59
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 23.60s
                        Total time: 14920.61s
                               ETA: 1276919.9s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.512s, learning 0.181s)
               Value function loss: 0.8134
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: -70.60
               Mean episode length: 289.59
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 23.69s
                        Total time: 14944.30s
                               ETA: 1277828.3s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.245s, learning 0.205s)
               Value function loss: 0.8697
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: -65.58
               Mean episode length: 291.17
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 23.45s
                        Total time: 14967.75s
                               ETA: 1278714.3s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.183s, learning 0.215s)
               Value function loss: 0.9008
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: -65.85
               Mean episode length: 291.17
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 23.40s
                        Total time: 14991.15s
                               ETA: 1279594.3s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 695 steps/s (collection: 23.315s, learning 0.259s)
               Value function loss: 0.9965
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: -68.21
               Mean episode length: 292.38
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 23.57s
                        Total time: 15014.72s
                               ETA: 1280487.8s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.566s, learning 0.297s)
               Value function loss: 1.0928
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: -68.20
               Mean episode length: 293.60
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 23.86s
                        Total time: 15038.59s
                               ETA: 1281404.2s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.104s, learning 0.189s)
               Value function loss: 1.2854
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: -65.08
               Mean episode length: 294.98
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 23.29s
                        Total time: 15061.88s
                               ETA: 1282270.6s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.656s, learning 0.172s)
               Value function loss: 1.3574
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: -67.43
               Mean episode length: 294.98
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 23.83s
                        Total time: 15085.71s
                               ETA: 1283180.9s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.452s, learning 0.172s)
               Value function loss: 71.2827
                    Surrogate loss: 0.0412
             Mean action noise std: 0.75
                       Mean reward: -67.81
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 4.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 16.62s
                        Total time: 15102.33s
                               ETA: 1283477.4s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.780s, learning 0.190s)
               Value function loss: 0.7615
                    Surrogate loss: 0.0265
             Mean action noise std: 0.75
                       Mean reward: -67.77
               Mean episode length: 300.00
                  Mean reward/step: -1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 11.97s
                        Total time: 15114.30s
                               ETA: 1283378.2s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.629s, learning 0.210s)
               Value function loss: 0.4111
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: -69.00
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 11.84s
                        Total time: 15126.14s
                               ETA: 1283268.0s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.826s, learning 0.292s)
               Value function loss: 0.3697
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: -71.35
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 12.12s
                        Total time: 15138.26s
                               ETA: 1283181.6s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.772s, learning 0.314s)
               Value function loss: 0.7481
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: -70.30
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 12.09s
                        Total time: 15150.34s
                               ETA: 1283092.7s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.482s, learning 0.338s)
               Value function loss: 0.9637
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: -70.86
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 12.82s
                        Total time: 15163.16s
                               ETA: 1283065.9s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1263 steps/s (collection: 12.690s, learning 0.282s)
               Value function loss: 1.2863
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: -69.11
               Mean episode length: 300.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 12.97s
                        Total time: 15176.14s
                               ETA: 1283052.1s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.388s, learning 0.285s)
               Value function loss: 2.2533
                    Surrogate loss: 0.0049
             Mean action noise std: 0.75
                       Mean reward: -74.90
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 12.67s
                        Total time: 15188.81s
                               ETA: 1283013.0s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.205s, learning 0.298s)
               Value function loss: 1.2973
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -77.48
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 12.50s
                        Total time: 15201.31s
                               ETA: 1282959.5s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.268s, learning 0.167s)
               Value function loss: 0.8791
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -81.03
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 12.44s
                        Total time: 15213.75s
                               ETA: 1282900.5s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.172s, learning 0.179s)
               Value function loss: 1.2972
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: -84.97
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 12.35s
                        Total time: 15226.10s
                               ETA: 1282834.4s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.775s, learning 0.170s)
               Value function loss: 1.1475
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: -84.83
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 11.95s
                        Total time: 15238.04s
                               ETA: 1282734.3s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.222s, learning 0.172s)
               Value function loss: 1.4893
                    Surrogate loss: 0.0060
             Mean action noise std: 0.75
                       Mean reward: -83.60
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 12.39s
                        Total time: 15250.44s
                               ETA: 1282672.1s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.833s, learning 0.200s)
               Value function loss: 2.4479
                    Surrogate loss: 0.0041
             Mean action noise std: 0.75
                       Mean reward: -80.33
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 12.03s
                        Total time: 15262.47s
                               ETA: 1282579.6s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.666s, learning 0.210s)
               Value function loss: 3.8094
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: -77.25
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 11.88s
                        Total time: 15274.35s
                               ETA: 1282474.1s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.042s, learning 0.224s)
               Value function loss: 4.2139
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: -75.73
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 12.27s
                        Total time: 15286.61s
                               ETA: 1282401.4s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.325s, learning 0.177s)
               Value function loss: 4.2959
                    Surrogate loss: 0.0049
             Mean action noise std: 0.75
                       Mean reward: -72.63
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 12.50s
                        Total time: 15299.11s
                               ETA: 1282348.6s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.074s, learning 0.178s)
               Value function loss: 12.3510
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: -68.91
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 12.25s
                        Total time: 15311.37s
                               ETA: 1282275.0s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.438s, learning 0.192s)
               Value function loss: 11.3912
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: -62.43
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 12.63s
                        Total time: 15324.00s
                               ETA: 1282233.0s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.685s, learning 0.193s)
               Value function loss: 10.9280
                    Surrogate loss: 0.0413
             Mean action noise std: 0.75
                       Mean reward: -63.06
               Mean episode length: 298.54
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 12.88s
                        Total time: 15336.87s
                               ETA: 1282211.9s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.249s, learning 0.315s)
               Value function loss: 203.3149
                    Surrogate loss: 0.0001
             Mean action noise std: 0.75
                       Mean reward: -90.63
               Mean episode length: 297.17
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 12.56s
                        Total time: 15349.44s
                               ETA: 1282164.6s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.578s, learning 0.213s)
               Value function loss: 1.1426
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: -92.49
               Mean episode length: 297.17
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 12.79s
                        Total time: 15362.23s
                               ETA: 1282136.3s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.385s, learning 0.275s)
               Value function loss: 0.5416
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: -92.78
               Mean episode length: 295.95
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 12.66s
                        Total time: 15374.89s
                               ETA: 1282097.0s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.369s, learning 0.246s)
               Value function loss: 0.7041
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: -91.99
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 12.61s
                        Total time: 15387.50s
                               ETA: 1282054.1s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.129s, learning 0.284s)
               Value function loss: 0.6613
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: -92.43
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 12.41s
                        Total time: 15399.92s
                               ETA: 1281994.3s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.048s, learning 0.262s)
               Value function loss: 0.8521
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: -93.36
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 12.31s
                        Total time: 15412.23s
                               ETA: 1281926.2s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.623s, learning 0.299s)
               Value function loss: 0.5902
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: -93.76
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 12.92s
                        Total time: 15425.15s
                               ETA: 1281908.9s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.323s, learning 0.363s)
               Value function loss: 0.5286
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: -95.48
               Mean episode length: 295.95
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 12.69s
                        Total time: 15437.83s
                               ETA: 1281872.1s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.157s, learning 0.192s)
               Value function loss: 0.7324
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: -94.90
               Mean episode length: 295.95
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 12.35s
                        Total time: 15450.18s
                               ETA: 1281807.4s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.114s, learning 0.182s)
               Value function loss: 0.4309
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: -95.12
               Mean episode length: 295.95
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 12.30s
                        Total time: 15462.48s
                               ETA: 1281738.3s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.077s, learning 0.197s)
               Value function loss: 0.4748
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: -95.33
               Mean episode length: 295.95
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 12.27s
                        Total time: 15474.75s
                               ETA: 1281667.6s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.661s, learning 0.180s)
               Value function loss: 0.7202
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: -95.04
               Mean episode length: 295.95
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 12.84s
                        Total time: 15487.59s
                               ETA: 1281643.8s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.876s, learning 0.179s)
               Value function loss: 0.6687
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: -94.23
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 12.06s
                        Total time: 15499.65s
                               ETA: 1281555.1s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.191s, learning 0.291s)
               Value function loss: 0.7053
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: -93.84
               Mean episode length: 295.95
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 12.48s
                        Total time: 15512.13s
                               ETA: 1281501.7s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.137s, learning 0.255s)
               Value function loss: 0.9391
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: -94.23
               Mean episode length: 295.95
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 12.39s
                        Total time: 15524.52s
                               ETA: 1281441.0s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.359s, learning 0.289s)
               Value function loss: 1.1261
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: -92.87
               Mean episode length: 297.41
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 12.65s
                        Total time: 15537.17s
                               ETA: 1281401.5s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.263s, learning 0.283s)
               Value function loss: 1.1369
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: -66.36
               Mean episode length: 298.78
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 12.55s
                        Total time: 15549.72s
                               ETA: 1281353.7s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.649s, learning 0.196s)
               Value function loss: 72.2202
                    Surrogate loss: 0.0136
             Mean action noise std: 0.75
                       Mean reward: -57.29
               Mean episode length: 300.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 4.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 12.85s
                        Total time: 15562.56s
                               ETA: 1281330.6s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.253s, learning 0.178s)
               Value function loss: 1.1278
                    Surrogate loss: 0.0212
             Mean action noise std: 0.75
                       Mean reward: -57.29
               Mean episode length: 300.00
                  Mean reward/step: -1.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 12.43s
                        Total time: 15574.99s
                               ETA: 1281273.3s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.525s, learning 0.201s)
               Value function loss: 0.1569
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: -58.57
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 12.73s
                        Total time: 15587.72s
                               ETA: 1281240.5s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.651s, learning 0.178s)
               Value function loss: 0.3550
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: -59.41
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 11.83s
                        Total time: 15599.55s
                               ETA: 1281134.0s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.803s, learning 0.198s)
               Value function loss: 0.4255
                    Surrogate loss: 0.0040
             Mean action noise std: 0.75
                       Mean reward: -59.78
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 12.00s
                        Total time: 15611.55s
                               ETA: 1281041.7s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.165s, learning 0.277s)
               Value function loss: 0.8317
                    Surrogate loss: 0.0040
             Mean action noise std: 0.75
                       Mean reward: -59.83
               Mean episode length: 300.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 12.44s
                        Total time: 15623.99s
                               ETA: 1280985.7s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.259s, learning 0.231s)
               Value function loss: 1.4487
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: -59.83
               Mean episode length: 300.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 12.49s
                        Total time: 15636.48s
                               ETA: 1280933.8s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.859s, learning 0.205s)
               Value function loss: 1.3844
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: -60.77
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 12.06s
                        Total time: 15648.54s
                               ETA: 1280847.0s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.914s, learning 0.194s)
               Value function loss: 1.6256
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: -59.47
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 12.11s
                        Total time: 15660.65s
                               ETA: 1280764.0s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.192s, learning 0.179s)
               Value function loss: 0.8664
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -56.56
               Mean episode length: 300.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 12.37s
                        Total time: 15673.02s
                               ETA: 1280702.6s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.971s, learning 0.197s)
               Value function loss: 0.6396
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: -53.50
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 12.17s
                        Total time: 15685.19s
                               ETA: 1280624.6s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.848s, learning 0.191s)
               Value function loss: 0.7939
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: -52.10
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 12.04s
                        Total time: 15697.23s
                               ETA: 1280536.3s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.719s, learning 0.174s)
               Value function loss: 0.6931
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: -52.49
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 11.89s
                        Total time: 15709.12s
                               ETA: 1280436.1s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.036s, learning 0.195s)
               Value function loss: 0.6045
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: -52.17
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 12.23s
                        Total time: 15721.35s
                               ETA: 1280363.7s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.001s, learning 0.184s)
               Value function loss: 0.5965
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: -51.90
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 12.18s
                        Total time: 15733.54s
                               ETA: 1280287.6s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.760s, learning 0.173s)
               Value function loss: 0.9042
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: -52.99
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 11.93s
                        Total time: 15745.47s
                               ETA: 1280191.1s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.919s, learning 0.167s)
               Value function loss: 0.6409
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: -51.98
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 12.09s
                        Total time: 15757.56s
                               ETA: 1280107.3s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.204s, learning 0.177s)
               Value function loss: 0.6643
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: -53.80
               Mean episode length: 298.33
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 12.38s
                        Total time: 15769.94s
                               ETA: 1280047.4s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.448s, learning 0.261s)
               Value function loss: 0.9341
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: -54.88
               Mean episode length: 298.33
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 12.71s
                        Total time: 15782.65s
                               ETA: 1280014.2s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.079s, learning 0.167s)
               Value function loss: 0.6809
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: -57.84
               Mean episode length: 298.33
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 12.25s
                        Total time: 15794.90s
                               ETA: 1279943.7s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.792s, learning 0.188s)
               Value function loss: 0.4336
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: -58.32
               Mean episode length: 298.33
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 11.98s
                        Total time: 15806.87s
                               ETA: 1279851.6s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.810s, learning 0.187s)
               Value function loss: 0.6672
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: -60.94
               Mean episode length: 296.98
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 12.00s
                        Total time: 15818.87s
                               ETA: 1279761.0s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.303s, learning 0.281s)
               Value function loss: 0.4035
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: -62.64
               Mean episode length: 295.72
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 12.58s
                        Total time: 15831.46s
                               ETA: 1279717.9s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.504s, learning 0.292s)
               Value function loss: 0.4187
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -64.90
               Mean episode length: 294.52
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 12.80s
                        Total time: 15844.25s
                               ETA: 1279692.1s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.715s, learning 0.174s)
               Value function loss: 0.4432
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: -64.86
               Mean episode length: 294.52
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 12.89s
                        Total time: 15857.14s
                               ETA: 1279673.8s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.002s, learning 0.273s)
               Value function loss: 0.5084
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: -64.27
               Mean episode length: 294.52
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 12.27s
                        Total time: 15869.41s
                               ETA: 1279605.9s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.362s, learning 0.277s)
               Value function loss: 0.4057
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: -65.78
               Mean episode length: 294.52
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 12.64s
                        Total time: 15882.05s
                               ETA: 1279567.6s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.573s, learning 0.323s)
               Value function loss: 0.3414
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: -64.62
               Mean episode length: 296.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 12.90s
                        Total time: 15894.95s
                               ETA: 1279549.9s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.049s, learning 0.251s)
               Value function loss: 0.3868
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: -64.63
               Mean episode length: 296.19
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 12.30s
                        Total time: 15907.25s
                               ETA: 1279484.4s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.825s, learning 0.209s)
               Value function loss: 0.4517
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: -65.33
               Mean episode length: 296.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 12.03s
                        Total time: 15919.28s
                               ETA: 1279397.5s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.822s, learning 0.172s)
               Value function loss: 0.2286
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: -65.36
               Mean episode length: 296.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 11.99s
                        Total time: 15931.28s
                               ETA: 1279307.6s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.785s, learning 0.206s)
               Value function loss: 0.4970
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -66.85
               Mean episode length: 296.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 12.99s
                        Total time: 15944.27s
                               ETA: 1279297.7s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.122s, learning 0.207s)
               Value function loss: 0.4934
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: -66.84
               Mean episode length: 296.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 12.33s
                        Total time: 15956.60s
                               ETA: 1279234.7s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.912s, learning 0.316s)
               Value function loss: 0.5723
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: -67.18
               Mean episode length: 296.19
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 12.23s
                        Total time: 15968.83s
                               ETA: 1279163.8s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.116s, learning 0.175s)
               Value function loss: 0.6352
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: -67.64
               Mean episode length: 296.19
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 12.29s
                        Total time: 15981.12s
                               ETA: 1279098.0s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.629s, learning 0.179s)
               Value function loss: 0.7990
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: -67.87
               Mean episode length: 296.19
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 11.81s
                        Total time: 15992.92s
                               ETA: 1278993.7s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.169s, learning 0.256s)
               Value function loss: 0.9224
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: -67.41
               Mean episode length: 297.54
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 12.43s
                        Total time: 16005.35s
                               ETA: 1278938.8s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.074s, learning 0.247s)
               Value function loss: 1.0681
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: -67.79
               Mean episode length: 297.54
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 12.32s
                        Total time: 16017.67s
                               ETA: 1278875.7s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.598s, learning 0.292s)
               Value function loss: 70.4217
                    Surrogate loss: 0.0711
             Mean action noise std: 0.75
                       Mean reward: -61.10
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 4.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 12.89s
                        Total time: 16030.56s
                               ETA: 1278858.0s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1329 steps/s (collection: 11.837s, learning 0.488s)
               Value function loss: 0.1829
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: -61.11
               Mean episode length: 300.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 12.32s
                        Total time: 16042.89s
                               ETA: 1278795.3s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.879s, learning 0.190s)
               Value function loss: 0.2946
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: -61.63
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 12.07s
                        Total time: 16054.95s
                               ETA: 1278712.4s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.189s, learning 0.193s)
               Value function loss: 0.2828
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: -62.12
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 12.38s
                        Total time: 16067.34s
                               ETA: 1278654.4s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.688s, learning 0.230s)
               Value function loss: 0.6881
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: -63.24
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 12.92s
                        Total time: 16080.25s
                               ETA: 1278639.1s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.481s, learning 0.187s)
               Value function loss: 0.7915
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: -62.62
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 12.67s
                        Total time: 16092.92s
                               ETA: 1278604.0s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.605s, learning 0.175s)
               Value function loss: 1.3079
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: -64.10
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 12.78s
                        Total time: 16105.70s
                               ETA: 1278577.9s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.445s, learning 0.194s)
               Value function loss: 1.9065
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: -64.98
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 12.64s
                        Total time: 16118.34s
                               ETA: 1278540.6s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.879s, learning 0.194s)
               Value function loss: 1.2317
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: -65.27
               Mean episode length: 300.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 12.07s
                        Total time: 16130.42s
                               ETA: 1278458.4s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.498s, learning 0.199s)
               Value function loss: 0.7957
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: -63.75
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 11.70s
                        Total time: 16142.11s
                               ETA: 1278346.6s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.260s, learning 0.207s)
               Value function loss: 0.7464
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: -61.51
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 12.47s
                        Total time: 16154.58s
                               ETA: 1278295.8s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.056s, learning 0.313s)
               Value function loss: 0.8409
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: -60.49
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 12.37s
                        Total time: 16166.95s
                               ETA: 1278237.4s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.387s, learning 0.289s)
               Value function loss: 0.8286
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: -60.24
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 12.68s
                        Total time: 16179.63s
                               ETA: 1278203.4s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.816s, learning 0.164s)
               Value function loss: 1.2595
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: -57.36
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 11.98s
                        Total time: 16191.61s
                               ETA: 1278114.4s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.687s, learning 0.198s)
               Value function loss: 1.1711
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: -59.56
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 11.89s
                        Total time: 16203.49s
                               ETA: 1278018.1s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.760s, learning 0.205s)
               Value function loss: 1.0959
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: -59.39
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 11.96s
                        Total time: 16215.46s
                               ETA: 1277928.1s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.345s, learning 0.349s)
               Value function loss: 1.2964
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: -59.66
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 12.69s
                        Total time: 16228.15s
                               ETA: 1277895.7s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.659s, learning 0.289s)
               Value function loss: 1.3702
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: -56.38
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 12.95s
                        Total time: 16241.10s
                               ETA: 1277883.3s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.019s, learning 0.285s)
               Value function loss: 1.0502
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: -57.38
               Mean episode length: 298.49
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 12.30s
                        Total time: 16253.40s
                               ETA: 1277820.2s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.069s, learning 0.196s)
               Value function loss: 0.9080
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: -57.60
               Mean episode length: 298.49
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 12.27s
                        Total time: 16265.67s
                               ETA: 1277754.2s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.299s, learning 0.165s)
               Value function loss: 0.7200
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: -58.32
               Mean episode length: 298.49
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 12.46s
                        Total time: 16278.13s
                               ETA: 1277703.9s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.179s, learning 0.189s)
               Value function loss: 0.9736
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -58.75
               Mean episode length: 297.20
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 12.37s
                        Total time: 16290.50s
                               ETA: 1277646.1s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.904s, learning 0.339s)
               Value function loss: 0.6973
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -59.68
               Mean episode length: 295.99
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 12.24s
                        Total time: 16302.74s
                               ETA: 1277578.6s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.967s, learning 0.241s)
               Value function loss: 0.7534
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: -58.58
               Mean episode length: 294.88
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 12.21s
                        Total time: 16314.95s
                               ETA: 1277508.5s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.264s, learning 0.215s)
               Value function loss: 0.7180
                    Surrogate loss: 0.0071
             Mean action noise std: 0.75
                       Mean reward: -58.19
               Mean episode length: 294.88
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 12.48s
                        Total time: 16327.43s
                               ETA: 1277459.7s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.049s, learning 0.250s)
               Value function loss: 0.7336
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: -57.46
               Mean episode length: 294.88
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 12.30s
                        Total time: 16339.73s
                               ETA: 1277396.8s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.247s, learning 0.171s)
               Value function loss: 0.5532
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: -57.90
               Mean episode length: 294.88
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 12.42s
                        Total time: 16352.15s
                               ETA: 1277343.3s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.568s, learning 0.202s)
               Value function loss: 0.4161
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: -57.78
               Mean episode length: 294.88
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 11.77s
                        Total time: 16363.92s
                               ETA: 1277239.3s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.144s, learning 0.202s)
               Value function loss: 0.6284
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: -58.07
               Mean episode length: 294.88
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 12.35s
                        Total time: 16376.26s
                               ETA: 1277180.3s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.389s, learning 0.342s)
               Value function loss: 0.3649
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: -58.11
               Mean episode length: 294.88
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 12.73s
                        Total time: 16388.99s
                               ETA: 1277151.4s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.772s, learning 0.365s)
               Value function loss: 0.5688
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: -62.64
               Mean episode length: 294.88
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 13.14s
                        Total time: 16402.13s
                               ETA: 1277154.2s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.544s, learning 0.307s)
               Value function loss: 0.6544
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -60.72
               Mean episode length: 296.39
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 12.85s
                        Total time: 16414.98s
                               ETA: 1277134.7s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.527s, learning 0.279s)
               Value function loss: 0.5177
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: -60.61
               Mean episode length: 296.39
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 12.81s
                        Total time: 16427.79s
                               ETA: 1277111.8s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.183s, learning 0.216s)
               Value function loss: 0.4841
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: -60.70
               Mean episode length: 296.39
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 12.40s
                        Total time: 16440.19s
                               ETA: 1277057.2s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.747s, learning 0.219s)
               Value function loss: 0.5625
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: -60.63
               Mean episode length: 296.39
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 11.97s
                        Total time: 16452.15s
                               ETA: 1276969.0s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.934s, learning 0.198s)
               Value function loss: 0.7979
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: -61.14
               Mean episode length: 296.39
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 12.13s
                        Total time: 16464.28s
                               ETA: 1276893.8s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.048s, learning 0.191s)
               Value function loss: 0.7520
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: -61.31
               Mean episode length: 296.39
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 12.24s
                        Total time: 16476.52s
                               ETA: 1276827.1s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.284s, learning 0.212s)
               Value function loss: 71.9089
                    Surrogate loss: 0.0405
             Mean action noise std: 0.75
                       Mean reward: -70.20
               Mean episode length: 300.00
                  Mean reward/step: -0.28
       Mean episode length/episode: 4.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 12.50s
                        Total time: 16489.02s
                               ETA: 1276780.4s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.948s, learning 0.268s)
               Value function loss: 0.1364
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: -70.20
               Mean episode length: 300.00
                  Mean reward/step: -1.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 12.22s
                        Total time: 16501.24s
                               ETA: 1276712.0s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.325s, learning 0.257s)
               Value function loss: 0.2073
                    Surrogate loss: 0.0024
             Mean action noise std: 0.75
                       Mean reward: -70.68
               Mean episode length: 300.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 12.58s
                        Total time: 16513.82s
                               ETA: 1276672.1s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.033s, learning 0.185s)
               Value function loss: 0.4903
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: -70.87
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 12.22s
                        Total time: 16526.04s
                               ETA: 1276603.9s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.983s, learning 0.193s)
               Value function loss: 0.5849
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: -69.73
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 12.18s
                        Total time: 16538.21s
                               ETA: 1276532.7s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.275s, learning 0.211s)
               Value function loss: 0.7747
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: -70.12
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 12.49s
                        Total time: 16550.70s
                               ETA: 1276485.5s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.046s, learning 0.233s)
               Value function loss: 0.9249
                    Surrogate loss: 0.0362
             Mean action noise std: 0.75
                       Mean reward: -70.13
               Mean episode length: 300.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 12.28s
                        Total time: 16562.98s
                               ETA: 1276422.3s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.888s, learning 0.294s)
               Value function loss: 0.9595
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: -71.90
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 12.18s
                        Total time: 16575.16s
                               ETA: 1276351.8s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.701s, learning 0.181s)
               Value function loss: 1.5987
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: -69.25
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 11.88s
                        Total time: 16587.04s
                               ETA: 1276258.3s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.959s, learning 0.181s)
               Value function loss: 0.7626
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: -67.04
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 12.14s
                        Total time: 16599.18s
                               ETA: 1276184.7s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.159s, learning 0.248s)
               Value function loss: 0.6605
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: -65.95
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 12.41s
                        Total time: 16611.59s
                               ETA: 1276131.8s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.538s, learning 0.288s)
               Value function loss: 0.8362
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: -63.11
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 12.83s
                        Total time: 16624.41s
                               ETA: 1276111.1s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.637s, learning 0.233s)
               Value function loss: 1.0280
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: -62.85
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 12.87s
                        Total time: 16637.28s
                               ETA: 1276093.7s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.973s, learning 0.167s)
               Value function loss: 1.0047
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: -62.69
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 12.14s
                        Total time: 16649.42s
                               ETA: 1276020.5s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.376s, learning 0.230s)
               Value function loss: 0.9198
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: -63.35
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 12.61s
                        Total time: 16662.03s
                               ETA: 1275983.0s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.970s, learning 0.192s)
               Value function loss: 1.1400
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -62.78
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 12.16s
                        Total time: 16674.19s
                               ETA: 1275911.6s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.773s, learning 0.243s)
               Value function loss: 0.9655
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: -64.07
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 12.02s
                        Total time: 16686.21s
                               ETA: 1275829.1s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.153s, learning 0.180s)
               Value function loss: 1.1251
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: -62.21
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 12.33s
                        Total time: 16698.54s
                               ETA: 1275771.0s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.448s, learning 0.177s)
               Value function loss: 1.1882
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: -62.91
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 12.62s
                        Total time: 16711.16s
                               ETA: 1275735.2s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.820s, learning 0.188s)
               Value function loss: 1.2116
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: -69.00
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 12.01s
                        Total time: 16723.17s
                               ETA: 1275652.4s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.980s, learning 0.170s)
               Value function loss: 0.8381
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: -70.35
               Mean episode length: 298.54
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 12.15s
                        Total time: 16735.32s
                               ETA: 1275580.5s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.969s, learning 0.188s)
               Value function loss: 0.8535
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: -68.33
               Mean episode length: 298.54
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 12.16s
                        Total time: 16747.48s
                               ETA: 1275509.2s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.789s, learning 0.215s)
               Value function loss: 0.8593
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: -68.39
               Mean episode length: 298.54
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 12.00s
                        Total time: 16759.48s
                               ETA: 1275426.4s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.055s, learning 0.177s)
               Value function loss: 0.9023
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: -69.21
               Mean episode length: 298.54
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 12.23s
                        Total time: 16771.71s
                               ETA: 1275361.0s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.620s, learning 0.235s)
               Value function loss: 0.9720
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: -69.40
               Mean episode length: 298.54
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 12.85s
                        Total time: 16784.57s
                               ETA: 1275343.0s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.875s, learning 0.175s)
               Value function loss: 0.8702
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: -67.73
               Mean episode length: 296.16
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 12.05s
                        Total time: 16796.62s
                               ETA: 1275263.9s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.262s, learning 0.182s)
               Value function loss: 0.5838
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: -67.61
               Mean episode length: 296.16
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 12.44s
                        Total time: 16809.06s
                               ETA: 1275214.9s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.012s, learning 0.168s)
               Value function loss: 0.5064
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -67.63
               Mean episode length: 296.16
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 12.18s
                        Total time: 16821.24s
                               ETA: 1275145.8s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.992s, learning 0.193s)
               Value function loss: 0.5160
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: -67.73
               Mean episode length: 296.16
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 12.18s
                        Total time: 16833.43s
                               ETA: 1275077.2s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.957s, learning 0.257s)
               Value function loss: 0.5251
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: -69.14
               Mean episode length: 296.16
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 12.21s
                        Total time: 16845.64s
                               ETA: 1275011.0s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.086s, learning 0.166s)
               Value function loss: 0.2246
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: -67.90
               Mean episode length: 296.16
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 12.25s
                        Total time: 16857.89s
                               ETA: 1274947.7s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.018s, learning 0.172s)
               Value function loss: 0.3947
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: -64.28
               Mean episode length: 296.16
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 12.19s
                        Total time: 16870.08s
                               ETA: 1274879.8s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.812s, learning 0.182s)
               Value function loss: 0.3499
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: -62.27
               Mean episode length: 296.16
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.99s
                        Total time: 16882.08s
                               ETA: 1274797.2s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.792s, learning 0.342s)
               Value function loss: 0.3953
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: -62.54
               Mean episode length: 296.16
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 12.13s
                        Total time: 16894.21s
                               ETA: 1274725.2s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.611s, learning 0.313s)
               Value function loss: 0.4490
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -61.19
               Mean episode length: 297.62
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 12.92s
                        Total time: 16907.14s
                               ETA: 1274712.9s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.840s, learning 0.221s)
               Value function loss: 0.4439
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: -61.81
               Mean episode length: 297.62
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 13.06s
                        Total time: 16920.20s
                               ETA: 1274710.9s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.223s, learning 0.234s)
               Value function loss: 0.4108
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: -63.23
               Mean episode length: 297.62
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 12.46s
                        Total time: 16932.66s
                               ETA: 1274663.4s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.360s, learning 0.216s)
               Value function loss: 0.4780
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: -62.75
               Mean episode length: 297.62
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 12.58s
                        Total time: 16945.23s
                               ETA: 1274625.0s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.669s, learning 0.324s)
               Value function loss: 63.9470
                    Surrogate loss: 0.0344
             Mean action noise std: 0.75
                       Mean reward: -68.06
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 4.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 12.99s
                        Total time: 16958.22s
                               ETA: 1274617.9s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.194s, learning 0.288s)
               Value function loss: 0.1745
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: -67.92
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 12.48s
                        Total time: 16970.71s
                               ETA: 1274572.4s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.073s, learning 0.167s)
               Value function loss: 0.2671
                    Surrogate loss: 0.0068
             Mean action noise std: 0.75
                       Mean reward: -68.16
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 12.24s
                        Total time: 16982.95s
                               ETA: 1274508.8s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.869s, learning 0.397s)
               Value function loss: 0.2856
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -70.01
               Mean episode length: 297.71
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 12.27s
                        Total time: 16995.21s
                               ETA: 1274447.2s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.993s, learning 0.164s)
               Value function loss: 0.8164
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: -68.12
               Mean episode length: 297.71
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 12.16s
                        Total time: 17007.37s
                               ETA: 1274377.6s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.595s, learning 0.221s)
               Value function loss: 1.1479
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: -66.00
               Mean episode length: 297.71
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.82s
                        Total time: 17019.19s
                               ETA: 1274282.5s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.094s, learning 0.194s)
               Value function loss: 0.3833
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: -67.77
               Mean episode length: 295.17
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 12.29s
                        Total time: 17031.47s
                               ETA: 1274222.9s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.097s, learning 0.257s)
               Value function loss: 1.4889
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: -62.74
               Mean episode length: 295.17
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 12.35s
                        Total time: 17043.83s
                               ETA: 1274168.2s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.114s, learning 0.217s)
               Value function loss: 1.6369
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: -65.68
               Mean episode length: 295.06
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 12.33s
                        Total time: 17056.16s
                               ETA: 1274111.9s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.068s, learning 0.166s)
               Value function loss: 0.9221
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: -65.31
               Mean episode length: 297.35
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 12.23s
                        Total time: 17068.39s
                               ETA: 1274048.4s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.628s, learning 0.173s)
               Value function loss: 0.9971
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: -62.39
               Mean episode length: 297.12
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.80s
                        Total time: 17080.19s
                               ETA: 1273952.7s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.260s, learning 0.172s)
               Value function loss: 1.0949
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: -63.03
               Mean episode length: 297.12
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 12.43s
                        Total time: 17092.63s
                               ETA: 1273904.2s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.554s, learning 0.286s)
               Value function loss: 1.8940
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: -65.03
               Mean episode length: 297.12
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 12.84s
                        Total time: 17105.47s
                               ETA: 1273886.0s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.048s, learning 0.204s)
               Value function loss: 1.9423
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: -66.21
               Mean episode length: 295.17
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 12.25s
                        Total time: 17117.72s
                               ETA: 1273824.2s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.721s, learning 0.174s)
               Value function loss: 2.2023
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: -66.96
               Mean episode length: 290.25
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 12.90s
                        Total time: 17130.61s
                               ETA: 1273810.3s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.314s, learning 0.236s)
               Value function loss: 4.6563
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: -64.11
               Mean episode length: 290.36
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 12.55s
                        Total time: 17143.16s
                               ETA: 1273770.7s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.732s, learning 0.187s)
               Value function loss: 17.0188
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: -62.67
               Mean episode length: 290.36
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 11.92s
                        Total time: 17155.08s
                               ETA: 1273684.3s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.682s, learning 0.186s)
               Value function loss: 1.7750
                    Surrogate loss: 0.0519
             Mean action noise std: 0.75
                       Mean reward: -61.76
               Mean episode length: 290.36
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 11.87s
                        Total time: 17166.95s
                               ETA: 1273594.2s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.333s, learning 0.196s)
               Value function loss: 1.3329
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: -63.92
               Mean episode length: 291.55
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 12.53s
                        Total time: 17179.48s
                               ETA: 1273553.3s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.934s, learning 0.173s)
               Value function loss: 1.0898
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: -64.22
               Mean episode length: 291.55
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 12.11s
                        Total time: 17191.59s
                               ETA: 1273481.1s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.125s, learning 0.287s)
               Value function loss: 0.7228
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: -64.00
               Mean episode length: 291.55
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 12.41s
                        Total time: 17204.00s
                               ETA: 1273431.5s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.401s, learning 0.213s)
               Value function loss: 1.1985
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: -61.11
               Mean episode length: 293.50
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 12.61s
                        Total time: 17216.61s
                               ETA: 1273397.0s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.389s, learning 0.196s)
               Value function loss: 0.7104
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: -59.99
               Mean episode length: 295.99
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 12.59s
                        Total time: 17229.20s
                               ETA: 1273360.4s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.025s, learning 0.185s)
               Value function loss: 0.7499
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: -58.63
               Mean episode length: 298.42
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 12.21s
                        Total time: 17241.41s
                               ETA: 1273296.2s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.091s, learning 0.251s)
               Value function loss: 0.7618
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: -58.98
               Mean episode length: 295.85
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 12.34s
                        Total time: 17253.75s
                               ETA: 1273241.7s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.324s, learning 0.187s)
               Value function loss: 0.8728
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: -59.37
               Mean episode length: 293.32
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 12.51s
                        Total time: 17266.26s
                               ETA: 1273199.7s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.336s, learning 0.205s)
               Value function loss: 0.5124
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: -58.92
               Mean episode length: 293.32
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 12.54s
                        Total time: 17278.80s
                               ETA: 1273160.1s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.367s, learning 0.214s)
               Value function loss: 0.5127
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: -58.57
               Mean episode length: 293.32
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 12.58s
                        Total time: 17291.38s
                               ETA: 1273123.3s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.459s, learning 0.212s)
               Value function loss: 0.7139
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: -56.06
               Mean episode length: 294.90
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 12.67s
                        Total time: 17304.06s
                               ETA: 1273093.3s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.491s, learning 0.285s)
               Value function loss: 0.3646
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: -56.07
               Mean episode length: 294.90
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 12.78s
                        Total time: 17316.83s
                               ETA: 1273071.0s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.307s, learning 0.267s)
               Value function loss: 0.3989
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: -56.81
               Mean episode length: 292.65
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 12.57s
                        Total time: 17329.41s
                               ETA: 1273033.9s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.818s, learning 0.202s)
               Value function loss: 0.5156
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: -55.62
               Mean episode length: 292.65
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 13.02s
                        Total time: 17342.43s
                               ETA: 1273029.6s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.876s, learning 0.176s)
               Value function loss: 0.4593
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: -55.49
               Mean episode length: 292.65
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 12.05s
                        Total time: 17354.48s
                               ETA: 1272954.2s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.069s, learning 0.174s)
               Value function loss: 0.4106
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: -56.23
               Mean episode length: 290.26
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 12.24s
                        Total time: 17366.72s
                               ETA: 1272892.9s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.916s, learning 0.186s)
               Value function loss: 0.5912
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: -56.08
               Mean episode length: 290.26
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 12.10s
                        Total time: 17378.82s
                               ETA: 1272821.4s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.976s, learning 0.170s)
               Value function loss: 0.7029
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -55.52
               Mean episode length: 290.26
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 12.15s
                        Total time: 17390.97s
                               ETA: 1272753.1s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.172s, learning 0.213s)
               Value function loss: 0.5643
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: -57.42
               Mean episode length: 290.26
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 12.38s
                        Total time: 17403.35s
                               ETA: 1272702.4s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.456s, learning 0.297s)
               Value function loss: 72.0647
                    Surrogate loss: 0.0087
             Mean action noise std: 0.75
                       Mean reward: -59.58
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 4.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 12.75s
                        Total time: 17416.11s
                               ETA: 1272678.7s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.604s, learning 0.197s)
               Value function loss: 0.3749
                    Surrogate loss: 0.0763
             Mean action noise std: 0.75
                       Mean reward: -59.58
               Mean episode length: 300.00
                  Mean reward/step: -1.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 12.80s
                        Total time: 17428.91s
                               ETA: 1272658.5s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.991s, learning 0.175s)
               Value function loss: 0.2188
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: -60.23
               Mean episode length: 300.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 12.17s
                        Total time: 17441.07s
                               ETA: 1272591.9s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.633s, learning 0.281s)
               Value function loss: 0.4053
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: -62.34
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 11.91s
                        Total time: 17452.99s
                               ETA: 1272507.2s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.537s, learning 0.207s)
               Value function loss: 0.4638
                    Surrogate loss: 0.0021
             Mean action noise std: 0.75
                       Mean reward: -62.87
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 12.74s
                        Total time: 17465.73s
                               ETA: 1272483.0s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.120s, learning 0.281s)
               Value function loss: 0.7681
                    Surrogate loss: -0.0003
             Mean action noise std: 0.75
                       Mean reward: -65.54
               Mean episode length: 300.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 12.40s
                        Total time: 17478.13s
                               ETA: 1272433.8s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.072s, learning 0.273s)
               Value function loss: 1.5563
                    Surrogate loss: 0.0046
             Mean action noise std: 0.75
                       Mean reward: -65.16
               Mean episode length: 300.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 12.34s
                        Total time: 17490.48s
                               ETA: 1272380.6s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.447s, learning 0.293s)
               Value function loss: 1.1345
                    Surrogate loss: 0.0030
             Mean action noise std: 0.75
                       Mean reward: -75.29
               Mean episode length: 295.11
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 12.74s
                        Total time: 17503.22s
                               ETA: 1272356.2s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.189s, learning 0.171s)
               Value function loss: 1.7263
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: -86.02
               Mean episode length: 295.11
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 12.36s
                        Total time: 17515.58s
                               ETA: 1272304.2s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.782s, learning 0.191s)
               Value function loss: 0.9309
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: -83.31
               Mean episode length: 295.11
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 11.97s
                        Total time: 17527.55s
                               ETA: 1272224.2s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.925s, learning 0.216s)
               Value function loss: 0.9315
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: -81.94
               Mean episode length: 295.11
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 12.14s
                        Total time: 17539.69s
                               ETA: 1272156.4s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.125s, learning 0.199s)
               Value function loss: 1.3173
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: -73.76
               Mean episode length: 297.56
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 12.32s
                        Total time: 17552.02s
                               ETA: 1272102.0s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.627s, learning 0.186s)
               Value function loss: 1.3342
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: -78.18
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 11.81s
                        Total time: 17563.83s
                               ETA: 1272010.6s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.219s, learning 0.230s)
               Value function loss: 1.2356
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: -77.49
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 12.45s
                        Total time: 17576.28s
                               ETA: 1271965.4s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.415s, learning 0.219s)
               Value function loss: 1.3064
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: -74.78
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 12.63s
                        Total time: 17588.91s
                               ETA: 1271933.6s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.670s, learning 0.261s)
               Value function loss: 1.2228
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: -71.71
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 12.93s
                        Total time: 17601.84s
                               ETA: 1271923.3s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.444s, learning 0.317s)
               Value function loss: 1.0759
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: -79.72
               Mean episode length: 298.23
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 12.76s
                        Total time: 17614.60s
                               ETA: 1271900.7s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.301s, learning 0.308s)
               Value function loss: 0.9278
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: -78.91
               Mean episode length: 298.23
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 12.61s
                        Total time: 17627.21s
                               ETA: 1271867.2s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.896s, learning 0.198s)
               Value function loss: 1.0925
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: -80.04
               Mean episode length: 298.23
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 12.09s
                        Total time: 17639.31s
                               ETA: 1271796.5s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.463s, learning 0.271s)
               Value function loss: 0.8873
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: -85.00
               Mean episode length: 298.23
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 12.73s
                        Total time: 17652.04s
                               ETA: 1271772.0s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.408s, learning 0.273s)
               Value function loss: 0.6904
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: -85.05
               Mean episode length: 298.23
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 12.68s
                        Total time: 17664.72s
                               ETA: 1271743.8s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.072s, learning 0.180s)
               Value function loss: 0.8328
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: -77.51
               Mean episode length: 298.23
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 12.25s
                        Total time: 17676.97s
                               ETA: 1271684.7s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.930s, learning 0.206s)
               Value function loss: 0.7620
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: -76.71
               Mean episode length: 295.49
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 12.14s
                        Total time: 17689.11s
                               ETA: 1271617.4s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.184s, learning 0.240s)
               Value function loss: 0.5777
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -76.94
               Mean episode length: 295.49
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 12.42s
                        Total time: 17701.53s
                               ETA: 1271570.8s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.356s, learning 0.389s)
               Value function loss: 0.6829
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: -78.60
               Mean episode length: 295.49
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 12.75s
                        Total time: 17714.28s
                               ETA: 1271547.3s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.207s, learning 0.291s)
               Value function loss: 0.7361
                    Surrogate loss: -0.0000
             Mean action noise std: 0.75
                       Mean reward: -72.93
               Mean episode length: 294.85
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 12.50s
                        Total time: 17726.77s
                               ETA: 1271506.1s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.600s, learning 0.189s)
               Value function loss: 0.4992
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: -72.36
               Mean episode length: 294.85
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 12.79s
                        Total time: 17739.56s
                               ETA: 1271485.8s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.432s, learning 0.188s)
               Value function loss: 0.4030
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: -70.60
               Mean episode length: 292.32
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 12.62s
                        Total time: 17752.18s
                               ETA: 1271453.4s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.528s, learning 0.285s)
               Value function loss: 0.3801
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: -70.12
               Mean episode length: 292.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 12.81s
                        Total time: 17765.00s
                               ETA: 1271434.9s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.233s, learning 0.330s)
               Value function loss: 0.4770
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: -70.14
               Mean episode length: 290.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 12.56s
                        Total time: 17777.56s
                               ETA: 1271398.5s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.168s, learning 0.166s)
               Value function loss: 0.3475
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: -70.12
               Mean episode length: 290.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 12.33s
                        Total time: 17789.89s
                               ETA: 1271345.8s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.327s, learning 0.311s)
               Value function loss: 0.6281
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: -70.01
               Mean episode length: 290.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 12.64s
                        Total time: 17802.53s
                               ETA: 1271314.8s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.716s, learning 0.280s)
               Value function loss: 0.4657
                    Surrogate loss: 0.0056
             Mean action noise std: 0.75
                       Mean reward: -64.97
               Mean episode length: 290.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 13.00s
                        Total time: 17815.53s
                               ETA: 1271309.4s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.261s, learning 0.208s)
               Value function loss: 0.4884
                    Surrogate loss: 0.0057
             Mean action noise std: 0.75
                       Mean reward: -65.04
               Mean episode length: 290.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 12.47s
                        Total time: 17828.00s
                               ETA: 1271266.4s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.773s, learning 0.195s)
               Value function loss: 0.4888
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: -64.53
               Mean episode length: 290.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 11.97s
                        Total time: 17839.96s
                               ETA: 1271187.7s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.572s, learning 0.195s)
               Value function loss: 0.5443
                    Surrogate loss: 0.0049
             Mean action noise std: 0.75
                       Mean reward: -63.84
               Mean episode length: 290.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 12.77s
                        Total time: 17852.73s
                               ETA: 1271166.1s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.254s, learning 0.278s)
               Value function loss: 0.5844
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: -62.28
               Mean episode length: 290.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 12.53s
                        Total time: 17865.26s
                               ETA: 1271127.7s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.693s, learning 0.234s)
               Value function loss: 0.7073
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: -64.39
               Mean episode length: 290.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 12.93s
                        Total time: 17878.19s
                               ETA: 1271117.5s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1230 steps/s (collection: 12.946s, learning 0.365s)
               Value function loss: 68.7356
                    Surrogate loss: 0.0871
             Mean action noise std: 0.75
                       Mean reward: -46.24
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 4.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 13.31s
                        Total time: 17891.50s
                               ETA: 1271134.5s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.701s, learning 0.275s)
               Value function loss: 0.2535
                    Surrogate loss: 0.0082
             Mean action noise std: 0.75
                       Mean reward: -47.38
               Mean episode length: 297.51
                  Mean reward/step: -1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 12.98s
                        Total time: 17904.48s
                               ETA: 1271127.7s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.205s, learning 0.186s)
               Value function loss: 0.3696
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: -48.26
               Mean episode length: 297.51
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 12.39s
                        Total time: 17916.87s
                               ETA: 1271079.5s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.319s, learning 0.248s)
               Value function loss: 0.3977
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: -47.52
               Mean episode length: 297.51
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 12.57s
                        Total time: 17929.44s
                               ETA: 1271043.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.955s, learning 0.184s)
               Value function loss: 0.8044
                    Surrogate loss: 0.0032
             Mean action noise std: 0.75
                       Mean reward: -53.11
               Mean episode length: 294.88
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 12.14s
                        Total time: 17941.58s
                               ETA: 1270977.7s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.945s, learning 0.222s)
               Value function loss: 0.8805
                    Surrogate loss: 0.0042
             Mean action noise std: 0.75
                       Mean reward: -56.81
               Mean episode length: 294.88
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 12.17s
                        Total time: 17953.74s
                               ETA: 1270913.7s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.750s, learning 0.202s)
               Value function loss: 1.0683
                    Surrogate loss: 0.0087
             Mean action noise std: 0.75
                       Mean reward: -60.40
               Mean episode length: 294.88
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 11.95s
                        Total time: 17965.70s
                               ETA: 1270834.6s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.231s, learning 0.173s)
               Value function loss: 1.8529
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: -65.84
               Mean episode length: 294.88
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 12.40s
                        Total time: 17978.10s
                               ETA: 1270787.5s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.387s, learning 0.198s)
               Value function loss: 1.2789
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: -66.45
               Mean episode length: 297.37
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 12.59s
                        Total time: 17990.69s
                               ETA: 1270753.2s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.193s, learning 0.319s)
               Value function loss: 0.9447
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: -63.35
               Mean episode length: 297.37
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 12.51s
                        Total time: 18003.20s
                               ETA: 1270713.9s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.291s, learning 0.220s)
               Value function loss: 0.7956
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: -54.70
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 12.51s
                        Total time: 18015.71s
                               ETA: 1270674.4s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.575s, learning 0.175s)
               Value function loss: 0.6808
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: -50.02
               Mean episode length: 297.38
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 12.75s
                        Total time: 18028.46s
                               ETA: 1270651.9s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1331 steps/s (collection: 11.963s, learning 0.344s)
               Value function loss: 0.6259
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: -56.07
               Mean episode length: 297.38
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 12.31s
                        Total time: 18040.77s
                               ETA: 1270598.2s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.027s, learning 0.307s)
               Value function loss: 0.6963
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: -58.63
               Mean episode length: 290.51
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 12.33s
                        Total time: 18053.10s
                               ETA: 1270546.5s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.459s, learning 0.307s)
               Value function loss: 0.7699
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: -59.01
               Mean episode length: 288.67
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 12.77s
                        Total time: 18065.87s
                               ETA: 1270525.2s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.490s, learning 0.272s)
               Value function loss: 0.8816
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: -57.42
               Mean episode length: 285.90
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 12.76s
                        Total time: 18078.63s
                               ETA: 1270503.6s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.188s, learning 0.170s)
               Value function loss: 0.5746
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: -58.70
               Mean episode length: 284.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 12.36s
                        Total time: 18090.99s
                               ETA: 1270453.7s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.838s, learning 0.182s)
               Value function loss: 0.8331
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: -57.34
               Mean episode length: 284.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 12.02s
                        Total time: 18103.01s
                               ETA: 1270380.1s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.071s, learning 0.263s)
               Value function loss: 0.9497
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: -62.39
               Mean episode length: 284.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 12.33s
                        Total time: 18115.34s
                               ETA: 1270328.5s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.317s, learning 0.422s)
               Value function loss: 0.7479
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: -61.24
               Mean episode length: 286.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 12.74s
                        Total time: 18128.08s
                               ETA: 1270305.4s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.107s, learning 0.177s)
               Value function loss: 0.6301
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: -62.34
               Mean episode length: 286.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 12.28s
                        Total time: 18140.36s
                               ETA: 1270250.5s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.455s, learning 0.181s)
               Value function loss: 0.9058
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -54.94
               Mean episode length: 286.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 11.64s
                        Total time: 18152.00s
                               ETA: 1270150.3s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.091s, learning 0.216s)
               Value function loss: 0.7302
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: -52.24
               Mean episode length: 293.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 12.31s
                        Total time: 18164.30s
                               ETA: 1270097.1s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.424s, learning 0.294s)
               Value function loss: 0.8124
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: -52.86
               Mean episode length: 295.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 12.72s
                        Total time: 18177.02s
                               ETA: 1270072.7s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.177s, learning 0.409s)
               Value function loss: 1.1353
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: -51.10
               Mean episode length: 295.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 12.59s
                        Total time: 18189.61s
                               ETA: 1270039.1s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.488s, learning 0.327s)
               Value function loss: 0.9388
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: -47.93
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 12.82s
                        Total time: 18202.42s
                               ETA: 1270021.6s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.995s, learning 0.218s)
               Value function loss: 0.7521
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: -46.76
               Mean episode length: 300.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 12.21s
                        Total time: 18214.64s
                               ETA: 1269962.0s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.353s, learning 0.201s)
               Value function loss: 0.6561
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: -47.18
               Mean episode length: 300.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 12.55s
                        Total time: 18227.19s
                               ETA: 1269926.4s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.005s, learning 0.254s)
               Value function loss: 0.9042
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: -47.02
               Mean episode length: 295.30
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 12.26s
                        Total time: 18239.45s
                               ETA: 1269870.2s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.182s, learning 0.242s)
               Value function loss: 0.6535
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: -44.37
               Mean episode length: 295.30
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 12.42s
                        Total time: 18251.87s
                               ETA: 1269825.5s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.600s, learning 0.174s)
               Value function loss: 0.7629
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: -44.30
               Mean episode length: 295.30
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 11.77s
                        Total time: 18263.65s
                               ETA: 1269735.7s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.719s, learning 0.182s)
               Value function loss: 0.8906
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -42.08
               Mean episode length: 295.30
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 11.90s
                        Total time: 18275.55s
                               ETA: 1269654.8s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.301s, learning 0.213s)
               Value function loss: 0.8444
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: -45.07
               Mean episode length: 293.68
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 12.51s
                        Total time: 18288.06s
                               ETA: 1269616.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.221s, learning 0.179s)
               Value function loss: 1.1198
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: -45.25
               Mean episode length: 293.68
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 12.40s
                        Total time: 18300.46s
                               ETA: 1269570.5s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.555s, learning 0.295s)
               Value function loss: 1.2539
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: -44.14
               Mean episode length: 291.24
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 12.85s
                        Total time: 18313.31s
                               ETA: 1269555.6s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.367s, learning 0.244s)
               Value function loss: 1.3270
                    Surrogate loss: 0.0092
             Mean action noise std: 0.74
                       Mean reward: -43.53
               Mean episode length: 291.24
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 12.61s
                        Total time: 18325.92s
                               ETA: 1269524.2s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.697s, learning 0.240s)
               Value function loss: 0.7627
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -45.22
               Mean episode length: 288.86
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 12.94s
                        Total time: 18338.86s
                               ETA: 1269515.4s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.558s, learning 0.194s)
               Value function loss: 73.4453
                    Surrogate loss: 0.0238
             Mean action noise std: 0.74
                       Mean reward: -15.52
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 4.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 12.75s
                        Total time: 18351.61s
                               ETA: 1269493.8s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.941s, learning 0.256s)
               Value function loss: 0.4097
                    Surrogate loss: 0.0149
             Mean action noise std: 0.74
                       Mean reward: -16.02
               Mean episode length: 300.00
                  Mean reward/step: -1.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 12.20s
                        Total time: 18363.81s
                               ETA: 1269433.8s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.156s, learning 0.255s)
               Value function loss: 0.3601
                    Surrogate loss: 0.0000
             Mean action noise std: 0.74
                       Mean reward: -17.45
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 12.41s
                        Total time: 18376.22s
                               ETA: 1269388.7s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.437s, learning 0.255s)
               Value function loss: 0.5089
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: -17.49
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 12.69s
                        Total time: 18388.91s
                               ETA: 1269363.0s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.455s, learning 0.178s)
               Value function loss: 0.6368
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: -18.83
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 12.63s
                        Total time: 18401.55s
                               ETA: 1269333.3s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.162s, learning 0.178s)
               Value function loss: 1.0275
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: -19.14
               Mean episode length: 300.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 12.34s
                        Total time: 18413.89s
                               ETA: 1269283.4s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.812s, learning 0.235s)
               Value function loss: 0.8560
                    Surrogate loss: 0.0080
             Mean action noise std: 0.74
                       Mean reward: -19.69
               Mean episode length: 300.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 12.05s
                        Total time: 18425.93s
                               ETA: 1269213.4s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.515s, learning 0.292s)
               Value function loss: 2.6521
                    Surrogate loss: 0.0137
             Mean action noise std: 0.74
                       Mean reward: -19.65
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 12.81s
                        Total time: 18438.74s
                               ETA: 1269195.8s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.046s, learning 0.181s)
               Value function loss: 3.0352
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: -37.22
               Mean episode length: 299.95
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 12.23s
                        Total time: 18450.97s
                               ETA: 1269138.2s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.509s, learning 0.265s)
               Value function loss: 1.5361
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -33.95
               Mean episode length: 299.95
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 11.77s
                        Total time: 18462.74s
                               ETA: 1269049.6s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.135s, learning 0.253s)
               Value function loss: 1.2547
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: -39.23
               Mean episode length: 299.95
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 12.39s
                        Total time: 18475.13s
                               ETA: 1269003.3s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.160s, learning 0.295s)
               Value function loss: 1.2458
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: -38.11
               Mean episode length: 299.95
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 12.45s
                        Total time: 18487.58s
                               ETA: 1268961.5s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.622s, learning 0.236s)
               Value function loss: 1.1550
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: -36.26
               Mean episode length: 299.95
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 11.86s
                        Total time: 18499.44s
                               ETA: 1268879.0s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.956s, learning 0.175s)
               Value function loss: 0.8588
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: -32.69
               Mean episode length: 297.27
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 12.13s
                        Total time: 18511.57s
                               ETA: 1268815.2s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.015s, learning 0.203s)
               Value function loss: 1.2079
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: -25.21
               Mean episode length: 297.27
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 12.22s
                        Total time: 18523.79s
                               ETA: 1268757.5s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.213s, learning 0.206s)
               Value function loss: 1.5833
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -22.64
               Mean episode length: 293.59
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 12.42s
                        Total time: 18536.21s
                               ETA: 1268713.5s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.926s, learning 0.208s)
               Value function loss: 1.3646
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: -22.13
               Mean episode length: 291.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 12.13s
                        Total time: 18548.35s
                               ETA: 1268650.2s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.376s, learning 0.247s)
               Value function loss: 1.7992
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: -27.23
               Mean episode length: 289.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 12.62s
                        Total time: 18560.97s
                               ETA: 1268620.3s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.485s, learning 0.286s)
               Value function loss: 1.7390
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: -22.64
               Mean episode length: 289.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 12.77s
                        Total time: 18573.74s
                               ETA: 1268600.6s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.072s, learning 0.184s)
               Value function loss: 1.4200
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: -21.67
               Mean episode length: 287.48
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 12.26s
                        Total time: 18586.00s
                               ETA: 1268545.7s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.132s, learning 0.390s)
               Value function loss: 1.1343
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: -23.31
               Mean episode length: 286.02
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 12.52s
                        Total time: 18598.52s
                               ETA: 1268509.0s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.948s, learning 0.243s)
               Value function loss: 1.6487
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: -32.90
               Mean episode length: 287.42
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 12.19s
                        Total time: 18610.71s
                               ETA: 1268449.8s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.895s, learning 0.222s)
               Value function loss: 1.0513
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: -31.91
               Mean episode length: 287.42
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 12.12s
                        Total time: 18622.83s
                               ETA: 1268385.7s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1329 steps/s (collection: 11.994s, learning 0.329s)
               Value function loss: 1.5970
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: -29.46
               Mean episode length: 289.27
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 12.32s
                        Total time: 18635.15s
                               ETA: 1268335.6s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.439s, learning 0.175s)
               Value function loss: 1.6204
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -34.74
               Mean episode length: 291.10
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 12.61s
                        Total time: 18647.76s
                               ETA: 1268305.3s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.289s, learning 0.411s)
               Value function loss: 1.8461
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: -30.81
               Mean episode length: 292.71
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 12.70s
                        Total time: 18660.46s
                               ETA: 1268280.9s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.299s, learning 0.287s)
               Value function loss: 1.9497
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: -33.61
               Mean episode length: 292.71
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 12.59s
                        Total time: 18673.05s
                               ETA: 1268248.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.462s, learning 0.293s)
               Value function loss: 7.3342
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: -33.76
               Mean episode length: 290.90
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 12.75s
                        Total time: 18685.80s
                               ETA: 1268228.2s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.857s, learning 0.207s)
               Value function loss: 4.9649
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: -33.08
               Mean episode length: 290.90
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 12.06s
                        Total time: 18697.87s
                               ETA: 1268160.8s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.971s, learning 0.215s)
               Value function loss: 11.6556
                    Surrogate loss: 0.0930
             Mean action noise std: 0.74
                       Mean reward: -37.26
               Mean episode length: 290.90
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 12.19s
                        Total time: 18710.06s
                               ETA: 1268101.7s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.899s, learning 0.193s)
               Value function loss: 2.5696
                    Surrogate loss: 0.0027
             Mean action noise std: 0.74
                       Mean reward: -37.26
               Mean episode length: 290.90
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 12.09s
                        Total time: 18722.15s
                               ETA: 1268036.2s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.792s, learning 0.322s)
               Value function loss: 12.2771
                    Surrogate loss: 0.0039
             Mean action noise std: 0.74
                       Mean reward: -40.48
               Mean episode length: 290.92
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 12.11s
                        Total time: 18734.26s
                               ETA: 1267972.3s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.295s, learning 0.178s)
               Value function loss: 9.1660
                    Surrogate loss: 0.0500
             Mean action noise std: 0.74
                       Mean reward: -40.22
               Mean episode length: 290.92
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 12.47s
                        Total time: 18746.73s
                               ETA: 1267932.9s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.190s, learning 0.376s)
               Value function loss: 4.0050
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: -41.96
               Mean episode length: 292.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 12.57s
                        Total time: 18759.30s
                               ETA: 1267899.7s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.209s, learning 0.173s)
               Value function loss: 3.9950
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: -38.39
               Mean episode length: 292.38
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 12.38s
                        Total time: 18771.68s
                               ETA: 1267854.1s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.549s, learning 0.184s)
               Value function loss: 9.1394
                    Surrogate loss: 0.0116
             Mean action noise std: 0.74
                       Mean reward: -37.86
               Mean episode length: 293.71
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 11.73s
                        Total time: 18783.41s
                               ETA: 1267764.7s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.978s, learning 0.186s)
               Value function loss: 5.1888
                    Surrogate loss: 0.0317
             Mean action noise std: 0.74
                       Mean reward: -38.26
               Mean episode length: 293.71
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 12.16s
                        Total time: 18795.58s
                               ETA: 1267704.6s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.355s, learning 0.227s)
               Value function loss: 4.5851
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: -40.60
               Mean episode length: 293.71
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 12.58s
                        Total time: 18808.16s
                               ETA: 1267672.6s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.384s, learning 0.203s)
               Value function loss: 84.4790
                    Surrogate loss: 0.0252
             Mean action noise std: 0.74
                       Mean reward: -18.94
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 12.59s
                        Total time: 18820.75s
                               ETA: 1267641.0s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.520s, learning 0.207s)
               Value function loss: 1.3790
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: -18.33
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 12.73s
                        Total time: 18833.47s
                               ETA: 1267618.8s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.048s, learning 0.266s)
               Value function loss: 0.8312
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: -20.93
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 12.31s
                        Total time: 18845.79s
                               ETA: 1267568.9s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.001s, learning 0.170s)
               Value function loss: 0.7827
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: -25.63
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 12.17s
                        Total time: 18857.96s
                               ETA: 1267509.4s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.884s, learning 0.174s)
               Value function loss: 1.3149
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: -23.83
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 12.06s
                        Total time: 18870.02s
                               ETA: 1267442.5s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.903s, learning 0.194s)
               Value function loss: 1.1275
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: -23.22
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 12.10s
                        Total time: 18882.11s
                               ETA: 1267378.2s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.258s, learning 0.234s)
               Value function loss: 2.0667
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: -22.65
               Mean episode length: 300.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 12.49s
                        Total time: 18894.60s
                               ETA: 1267340.5s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.842s, learning 0.257s)
               Value function loss: 3.4937
                    Surrogate loss: 0.0210
             Mean action noise std: 0.74
                       Mean reward: -32.10
               Mean episode length: 295.13
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 12.10s
                        Total time: 18906.70s
                               ETA: 1267276.5s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.077s, learning 0.191s)
               Value function loss: 2.4732
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: -26.45
               Mean episode length: 295.13
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 12.27s
                        Total time: 18918.97s
                               ETA: 1267223.9s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.482s, learning 0.298s)
               Value function loss: 1.1679
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: -21.11
               Mean episode length: 295.13
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 12.78s
                        Total time: 18931.75s
                               ETA: 1267205.5s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.415s, learning 0.254s)
               Value function loss: 1.0986
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: -28.32
               Mean episode length: 295.13
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 12.67s
                        Total time: 18944.42s
                               ETA: 1267179.8s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.982s, learning 0.213s)
               Value function loss: 1.0123
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: -23.48
               Mean episode length: 297.57
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 12.19s
                        Total time: 18956.62s
                               ETA: 1267122.4s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.201s, learning 0.271s)
               Value function loss: 0.9083
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: -19.02
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 12.47s
                        Total time: 18969.09s
                               ETA: 1267083.6s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.950s, learning 0.203s)
               Value function loss: 1.1918
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: -16.81
               Mean episode length: 298.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 12.15s
                        Total time: 18981.24s
                               ETA: 1267023.6s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.889s, learning 0.200s)
               Value function loss: 1.4478
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: -12.01
               Mean episode length: 298.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 12.09s
                        Total time: 18993.33s
                               ETA: 1266959.3s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.861s, learning 0.205s)
               Value function loss: 1.4413
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: -12.29
               Mean episode length: 298.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 12.07s
                        Total time: 19005.40s
                               ETA: 1266893.5s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.119s, learning 0.204s)
               Value function loss: 1.1219
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: -12.35
               Mean episode length: 298.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 12.32s
                        Total time: 19017.72s
                               ETA: 1266845.0s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1329 steps/s (collection: 11.938s, learning 0.384s)
               Value function loss: 1.6340
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: -11.75
               Mean episode length: 298.08
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 12.32s
                        Total time: 19030.04s
                               ETA: 1266796.4s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.488s, learning 0.340s)
               Value function loss: 1.3188
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: -10.39
               Mean episode length: 298.08
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 12.83s
                        Total time: 19042.87s
                               ETA: 1266781.6s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.579s, learning 0.266s)
               Value function loss: 1.4360
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: -9.90
               Mean episode length: 298.08
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 12.85s
                        Total time: 19055.71s
                               ETA: 1266767.9s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.155s, learning 0.329s)
               Value function loss: 1.2988
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: -12.48
               Mean episode length: 298.08
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 12.48s
                        Total time: 19068.20s
                               ETA: 1266730.1s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.062s, learning 0.355s)
               Value function loss: 1.9215
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: -12.06
               Mean episode length: 300.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 12.42s
                        Total time: 19080.62s
                               ETA: 1266688.0s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.987s, learning 0.216s)
               Value function loss: 1.5712
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: -12.09
               Mean episode length: 300.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 12.20s
                        Total time: 19092.82s
                               ETA: 1266631.8s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.589s, learning 0.168s)
               Value function loss: 1.4270
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: -10.30
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 11.76s
                        Total time: 19104.58s
                               ETA: 1266546.0s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.326s, learning 0.215s)
               Value function loss: 1.5295
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: -6.60
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 12.54s
                        Total time: 19117.12s
                               ETA: 1266512.2s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.543s, learning 0.203s)
               Value function loss: 1.8207
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: -5.01
               Mean episode length: 300.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 12.75s
                        Total time: 19129.86s
                               ETA: 1266492.0s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.199s, learning 0.389s)
               Value function loss: 1.5144
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: -0.85
               Mean episode length: 300.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 12.59s
                        Total time: 19142.45s
                               ETA: 1266461.4s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.596s, learning 0.201s)
               Value function loss: 1.9032
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 0.87
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 12.80s
                        Total time: 19155.25s
                               ETA: 1266444.6s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.638s, learning 0.295s)
               Value function loss: 2.4963
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 3.48
               Mean episode length: 300.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 12.93s
                        Total time: 19168.18s
                               ETA: 1266436.8s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.172s, learning 0.209s)
               Value function loss: 1.4251
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 5.51
               Mean episode length: 300.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 12.38s
                        Total time: 19180.56s
                               ETA: 1266392.7s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.898s, learning 0.276s)
               Value function loss: 2.1543
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 5.68
               Mean episode length: 300.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 12.17s
                        Total time: 19192.73s
                               ETA: 1266334.8s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.326s, learning 0.171s)
               Value function loss: 2.2296
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 4.24
               Mean episode length: 300.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 12.50s
                        Total time: 19205.23s
                               ETA: 1266298.4s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.314s, learning 0.186s)
               Value function loss: 2.0332
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 7.22
               Mean episode length: 300.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 12.50s
                        Total time: 19217.73s
                               ETA: 1266262.2s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.357s, learning 0.176s)
               Value function loss: 2.1971
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 6.53
               Mean episode length: 300.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 12.53s
                        Total time: 19230.27s
                               ETA: 1266228.1s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.080s, learning 0.201s)
               Value function loss: 2.8902
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 7.59
               Mean episode length: 300.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 12.28s
                        Total time: 19242.55s
                               ETA: 1266177.5s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.879s, learning 0.197s)
               Value function loss: 3.4626
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 4.66
               Mean episode length: 300.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 12.08s
                        Total time: 19254.62s
                               ETA: 1266113.5s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.143s, learning 0.178s)
               Value function loss: 3.0706
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 2.11
               Mean episode length: 300.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 12.32s
                        Total time: 19266.94s
                               ETA: 1266065.7s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.292s, learning 0.180s)
               Value function loss: 102.7736
                    Surrogate loss: 0.0374
             Mean action noise std: 0.74
                       Mean reward: -4.60
               Mean episode length: 300.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 12.47s
                        Total time: 19279.42s
                               ETA: 1266027.8s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.901s, learning 0.191s)
               Value function loss: 0.4398
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: -6.18
               Mean episode length: 300.00
                  Mean reward/step: -1.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 12.09s
                        Total time: 19291.51s
                               ETA: 1265965.0s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.043s, learning 0.276s)
               Value function loss: 0.4855
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: -6.61
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 12.32s
                        Total time: 19303.83s
                               ETA: 1265917.1s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.656s, learning 0.182s)
               Value function loss: 0.7339
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: -7.86
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.84s
                        Total time: 19315.66s
                               ETA: 1265837.8s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.190s, learning 0.175s)
               Value function loss: 0.9361
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: -5.88
               Mean episode length: 300.00
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 12.36s
                        Total time: 19328.03s
                               ETA: 1265793.1s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.079s, learning 0.170s)
               Value function loss: 1.2952
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: -6.58
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 12.25s
                        Total time: 19340.28s
                               ETA: 1265740.8s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.863s, learning 0.185s)
               Value function loss: 1.5383
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: -2.34
               Mean episode length: 300.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 12.05s
                        Total time: 19352.33s
                               ETA: 1265675.5s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.424s, learning 0.186s)
               Value function loss: 3.6080
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: -16.63
               Mean episode length: 297.56
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 12.61s
                        Total time: 19364.94s
                               ETA: 1265647.0s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.883s, learning 0.287s)
               Value function loss: 3.4033
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: -15.15
               Mean episode length: 297.56
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 12.17s
                        Total time: 19377.11s
                               ETA: 1265589.7s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.854s, learning 0.247s)
               Value function loss: 1.8749
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: -10.08
               Mean episode length: 297.56
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 12.10s
                        Total time: 19389.21s
                               ETA: 1265528.1s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.833s, learning 0.196s)
               Value function loss: 1.3240
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: -11.06
               Mean episode length: 297.56
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 12.03s
                        Total time: 19401.24s
                               ETA: 1265461.7s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.286s, learning 0.176s)
               Value function loss: 1.4399
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: -9.55
               Mean episode length: 297.56
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 12.46s
                        Total time: 19413.70s
                               ETA: 1265423.7s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.434s, learning 0.217s)
               Value function loss: 1.1677
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: -5.55
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 12.65s
                        Total time: 19426.35s
                               ETA: 1265398.0s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.914s, learning 0.249s)
               Value function loss: 1.2005
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: -9.49
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 12.16s
                        Total time: 19438.51s
                               ETA: 1265340.5s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.846s, learning 0.219s)
               Value function loss: 1.5340
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: -13.58
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 12.06s
                        Total time: 19450.58s
                               ETA: 1265276.7s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.133s, learning 0.176s)
               Value function loss: 1.9549
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: -18.88
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 12.31s
                        Total time: 19462.89s
                               ETA: 1265228.9s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.209s, learning 0.245s)
               Value function loss: 1.9894
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: -19.39
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 12.45s
                        Total time: 19475.34s
                               ETA: 1265190.5s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.187s, learning 0.190s)
               Value function loss: 2.2468
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: -20.86
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 12.38s
                        Total time: 19487.72s
                               ETA: 1265147.2s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.375s, learning 0.212s)
               Value function loss: 2.8406
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: -28.66
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 12.59s
                        Total time: 19500.30s
                               ETA: 1265117.5s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.020s, learning 0.183s)
               Value function loss: 2.2976
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: -25.82
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 12.20s
                        Total time: 19512.51s
                               ETA: 1265063.0s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1334 steps/s (collection: 11.945s, learning 0.332s)
               Value function loss: 2.4157
                    Surrogate loss: -0.0019
             Mean action noise std: 0.74
                       Mean reward: -27.72
               Mean episode length: 297.55
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 12.28s
                        Total time: 19524.78s
                               ETA: 1265013.3s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.945s, learning 0.173s)
               Value function loss: 2.3330
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: -27.65
               Mean episode length: 297.55
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 12.12s
                        Total time: 19536.90s
                               ETA: 1264953.4s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.997s, learning 0.222s)
               Value function loss: 6.9082
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: -25.48
               Mean episode length: 297.55
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 12.22s
                        Total time: 19549.12s
                               ETA: 1264900.0s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.093s, learning 0.202s)
               Value function loss: 3.6317
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: -24.81
               Mean episode length: 297.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 12.30s
                        Total time: 19561.42s
                               ETA: 1264851.7s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.100s, learning 0.176s)
               Value function loss: 4.6643
                    Surrogate loss: 0.0079
             Mean action noise std: 0.74
                       Mean reward: -23.84
               Mean episode length: 297.55
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 12.28s
                        Total time: 19573.69s
                               ETA: 1264802.1s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.902s, learning 0.188s)
               Value function loss: 3.5868
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: -31.09
               Mean episode length: 295.16
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 12.09s
                        Total time: 19585.78s
                               ETA: 1264740.6s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.101s, learning 0.296s)
               Value function loss: 4.1439
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: -32.03
               Mean episode length: 295.16
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 12.40s
                        Total time: 19598.18s
                               ETA: 1264698.9s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.142s, learning 0.179s)
               Value function loss: 6.0406
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: -32.63
               Mean episode length: 295.16
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 12.32s
                        Total time: 19610.50s
                               ETA: 1264652.5s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.363s, learning 0.176s)
               Value function loss: 9.8295
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: -34.84
               Mean episode length: 295.16
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 12.54s
                        Total time: 19623.04s
                               ETA: 1264620.1s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.393s, learning 0.264s)
               Value function loss: 13.9079
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: -33.80
               Mean episode length: 295.16
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 12.66s
                        Total time: 19635.70s
                               ETA: 1264595.3s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.713s, learning 0.184s)
               Value function loss: 14.0064
                    Surrogate loss: 0.0029
             Mean action noise std: 0.74
                       Mean reward: -34.37
               Mean episode length: 295.16
                  Mean reward/step: 0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 11.90s
                        Total time: 19647.59s
                               ETA: 1264521.7s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.095s, learning 0.185s)
               Value function loss: 3.1334
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: -39.17
               Mean episode length: 295.16
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 12.28s
                        Total time: 19659.87s
                               ETA: 1264472.7s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.329s, learning 0.178s)
               Value function loss: 2.5646
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: -42.81
               Mean episode length: 295.16
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 12.51s
                        Total time: 19672.38s
                               ETA: 1264438.4s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.895s, learning 0.186s)
               Value function loss: 2.8004
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: -38.42
               Mean episode length: 297.61
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 12.08s
                        Total time: 19684.46s
                               ETA: 1264376.7s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.876s, learning 0.186s)
               Value function loss: 3.2814
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: -36.64
               Mean episode length: 297.61
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 12.06s
                        Total time: 19696.52s
                               ETA: 1264313.9s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.406s, learning 0.213s)
               Value function loss: 3.7793
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: -33.65
               Mean episode length: 297.61
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 12.62s
                        Total time: 19709.14s
                               ETA: 1264286.9s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.806s, learning 0.206s)
               Value function loss: 3.7731
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: -37.22
               Mean episode length: 297.61
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 12.01s
                        Total time: 19721.15s
                               ETA: 1264221.0s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.115s, learning 0.210s)
               Value function loss: 4.8034
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: -37.31
               Mean episode length: 297.61
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 12.33s
                        Total time: 19733.48s
                               ETA: 1264175.2s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.971s, learning 0.197s)
               Value function loss: 109.3415
                    Surrogate loss: 0.0676
             Mean action noise std: 0.74
                       Mean reward: -16.00
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 12.17s
                        Total time: 19745.65s
                               ETA: 1264119.4s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.106s, learning 0.178s)
               Value function loss: 0.8066
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: -16.22
               Mean episode length: 300.00
                  Mean reward/step: -0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 12.28s
                        Total time: 19757.93s
                               ETA: 1264071.1s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.611s, learning 0.169s)
               Value function loss: 0.7370
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: -16.61
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 11.78s
                        Total time: 19769.71s
                               ETA: 1263990.6s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.055s, learning 0.189s)
               Value function loss: 0.7994
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: -20.39
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 12.24s
                        Total time: 19781.96s
                               ETA: 1263939.9s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.962s, learning 0.190s)
               Value function loss: 1.3599
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: -20.46
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 12.15s
                        Total time: 19794.11s
                               ETA: 1263883.3s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.240s, learning 0.186s)
               Value function loss: 0.9106
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: -19.90
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 12.43s
                        Total time: 19806.53s
                               ETA: 1263844.2s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.492s, learning 0.213s)
               Value function loss: 1.2803
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: -23.92
               Mean episode length: 300.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 11.71s
                        Total time: 19818.24s
                               ETA: 1263759.3s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.387s, learning 0.185s)
               Value function loss: 4.1490
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: -13.90
               Mean episode length: 300.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 11.57s
                        Total time: 19829.81s
                               ETA: 1263665.9s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.025s, learning 0.174s)
               Value function loss: 3.3981
                    Surrogate loss: 0.0044
             Mean action noise std: 0.74
                       Mean reward: -8.94
               Mean episode length: 300.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 12.20s
                        Total time: 19842.01s
                               ETA: 1263612.6s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.923s, learning 0.283s)
               Value function loss: 2.3382
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: -5.74
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 12.21s
                        Total time: 19854.22s
                               ETA: 1263559.8s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.003s, learning 0.170s)
               Value function loss: 1.9733
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: -1.25
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 12.17s
                        Total time: 19866.39s
                               ETA: 1263504.9s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.146s, learning 0.184s)
               Value function loss: 1.8191
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: -6.48
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 12.33s
                        Total time: 19878.72s
                               ETA: 1263460.1s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.752s, learning 0.193s)
               Value function loss: 1.2990
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: -8.08
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 11.95s
                        Total time: 19890.66s
                               ETA: 1263390.8s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.234s, learning 0.219s)
               Value function loss: 2.1234
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: -11.19
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 12.45s
                        Total time: 19903.12s
                               ETA: 1263353.9s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.922s, learning 0.214s)
               Value function loss: 2.5835
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: -11.22
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 12.14s
                        Total time: 19915.25s
                               ETA: 1263296.8s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.738s, learning 0.191s)
               Value function loss: 3.0222
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: -12.46
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 11.93s
                        Total time: 19927.18s
                               ETA: 1263226.7s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.183s, learning 0.198s)
               Value function loss: 2.5795
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: -18.42
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 12.38s
                        Total time: 19939.56s
                               ETA: 1263185.3s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.595s, learning 0.176s)
               Value function loss: 3.3307
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: -18.62
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 11.77s
                        Total time: 19951.33s
                               ETA: 1263105.3s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.717s, learning 0.196s)
               Value function loss: 3.9132
                    Surrogate loss: 0.0052
             Mean action noise std: 0.74
                       Mean reward: -22.81
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 11.91s
                        Total time: 19963.24s
                               ETA: 1263034.4s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.425s, learning 0.186s)
               Value function loss: 3.5677
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: -17.98
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 12.61s
                        Total time: 19975.86s
                               ETA: 1263007.8s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.926s, learning 0.191s)
               Value function loss: 5.7674
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: -15.24
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 12.12s
                        Total time: 19987.97s
                               ETA: 1262949.9s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.310s, learning 0.179s)
               Value function loss: 15.9883
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: -19.10
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 12.49s
                        Total time: 20000.46s
                               ETA: 1262915.6s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.905s, learning 0.216s)
               Value function loss: 37.2221
                    Surrogate loss: 0.0065
             Mean action noise std: 0.74
                       Mean reward: -22.42
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 12.12s
                        Total time: 20012.58s
                               ETA: 1262858.1s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.851s, learning 0.199s)
               Value function loss: 10.7168
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: -22.43
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 12.05s
                        Total time: 20024.63s
                               ETA: 1262796.2s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.121s, learning 0.296s)
               Value function loss: 11.1159
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: -25.50
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 12.42s
                        Total time: 20037.05s
                               ETA: 1262757.5s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.995s, learning 0.210s)
               Value function loss: 13.1396
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: -20.10
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 12.20s
                        Total time: 20049.25s
                               ETA: 1262705.4s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.072s, learning 0.188s)
               Value function loss: 7.1639
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: -23.55
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 12.26s
                        Total time: 20061.51s
                               ETA: 1262656.8s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.961s, learning 0.211s)
               Value function loss: 4.4819
                    Surrogate loss: 0.0019
             Mean action noise std: 0.74
                       Mean reward: -24.10
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 12.17s
                        Total time: 20073.69s
                               ETA: 1262602.8s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.643s, learning 0.170s)
               Value function loss: 4.8168
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: -31.08
               Mean episode length: 299.28
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 11.81s
                        Total time: 20085.50s
                               ETA: 1262526.3s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.996s, learning 0.178s)
               Value function loss: 5.9597
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: -30.63
               Mean episode length: 299.28
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 12.17s
                        Total time: 20097.67s
                               ETA: 1262472.5s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.727s, learning 0.237s)
               Value function loss: 4.5963
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: -32.70
               Mean episode length: 298.11
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 11.96s
                        Total time: 20109.64s
                               ETA: 1262405.6s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.545s, learning 0.296s)
               Value function loss: 4.3261
                    Surrogate loss: 0.0162
             Mean action noise std: 0.74
                       Mean reward: -34.87
               Mean episode length: 298.11
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 12.84s
                        Total time: 20122.48s
                               ETA: 1262393.8s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.552s, learning 0.271s)
               Value function loss: 3.8775
                    Surrogate loss: 0.0031
             Mean action noise std: 0.74
                       Mean reward: -38.12
               Mean episode length: 298.11
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 12.82s
                        Total time: 20135.30s
                               ETA: 1262380.9s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.597s, learning 0.307s)
               Value function loss: 3.7047
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: -37.00
               Mean episode length: 298.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 12.90s
                        Total time: 20148.21s
                               ETA: 1262373.0s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.373s, learning 0.254s)
               Value function loss: 4.2470
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: -36.80
               Mean episode length: 297.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 12.63s
                        Total time: 20160.83s
                               ETA: 1262347.8s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.371s, learning 0.217s)
               Value function loss: 4.7279
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: -39.83
               Mean episode length: 297.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 12.59s
                        Total time: 20173.42s
                               ETA: 1262320.1s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.665s, learning 0.182s)
               Value function loss: 5.9903
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: -36.55
               Mean episode length: 297.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 11.85s
                        Total time: 20185.27s
                               ETA: 1262246.1s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.002s, learning 0.181s)
               Value function loss: 142.2633
                    Surrogate loss: 0.0120
             Mean action noise std: 0.74
                       Mean reward: -54.50
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 12.18s
                        Total time: 20197.45s
                               ETA: 1262193.2s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.059s, learning 0.307s)
               Value function loss: 3.8312
                    Surrogate loss: 0.0744
             Mean action noise std: 0.74
                       Mean reward: -53.23
               Mean episode length: 300.00
                  Mean reward/step: -1.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 12.37s
                        Total time: 20209.82s
                               ETA: 1262151.8s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.650s, learning 0.178s)
               Value function loss: 1.7011
                    Surrogate loss: 0.0121
             Mean action noise std: 0.74
                       Mean reward: -55.01
               Mean episode length: 300.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 11.83s
                        Total time: 20221.65s
                               ETA: 1262076.9s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.909s, learning 0.218s)
               Value function loss: 1.5091
                    Surrogate loss: 0.0009
             Mean action noise std: 0.74
                       Mean reward: -54.54
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 12.13s
                        Total time: 20233.77s
                               ETA: 1262020.6s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.889s, learning 0.302s)
               Value function loss: 1.4175
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: -53.35
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 12.19s
                        Total time: 20245.96s
                               ETA: 1261968.5s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.536s, learning 0.253s)
               Value function loss: 1.7510
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: -50.19
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 12.79s
                        Total time: 20258.75s
                               ETA: 1261953.7s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.122s, learning 0.231s)
               Value function loss: 1.5188
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: -52.96
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 12.35s
                        Total time: 20271.11s
                               ETA: 1261911.6s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.550s, learning 0.214s)
               Value function loss: 2.6250
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: -58.70
               Mean episode length: 300.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 12.76s
                        Total time: 20283.87s
                               ETA: 1261895.2s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.278s, learning 0.252s)
               Value function loss: 2.9418
                    Surrogate loss: 0.0030
             Mean action noise std: 0.74
                       Mean reward: -70.34
               Mean episode length: 300.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 12.53s
                        Total time: 20296.40s
                               ETA: 1261864.2s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.347s, learning 0.285s)
               Value function loss: 1.9207
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: -76.08
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 12.63s
                        Total time: 20309.03s
                               ETA: 1261839.7s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.189s, learning 0.204s)
               Value function loss: 1.6953
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: -81.65
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 12.39s
                        Total time: 20321.42s
                               ETA: 1261800.2s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.098s, learning 0.227s)
               Value function loss: 1.5815
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: -77.24
               Mean episode length: 297.58
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 12.32s
                        Total time: 20333.75s
                               ETA: 1261756.6s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.846s, learning 0.165s)
               Value function loss: 1.2760
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: -80.84
               Mean episode length: 297.58
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 12.01s
                        Total time: 20345.76s
                               ETA: 1261693.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.102s, learning 0.290s)
               Value function loss: 1.1511
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -80.17
               Mean episode length: 297.58
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 12.39s
                        Total time: 20358.15s
                               ETA: 1261654.3s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.340s, learning 0.280s)
               Value function loss: 1.4468
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: -85.17
               Mean episode length: 297.58
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 12.62s
                        Total time: 20370.77s
                               ETA: 1261629.1s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.535s, learning 0.188s)
               Value function loss: 2.0031
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: -75.31
               Mean episode length: 297.58
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 12.72s
                        Total time: 20383.50s
                               ETA: 1261610.2s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.198s, learning 0.175s)
               Value function loss: 2.0096
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: -70.60
               Mean episode length: 297.58
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 12.37s
                        Total time: 20395.87s
                               ETA: 1261569.8s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.411s, learning 0.333s)
               Value function loss: 2.0442
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: -67.35
               Mean episode length: 297.58
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 12.74s
                        Total time: 20408.61s
                               ETA: 1261552.3s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.327s, learning 0.179s)
               Value function loss: 2.3324
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: -69.55
               Mean episode length: 297.58
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 12.51s
                        Total time: 20421.12s
                               ETA: 1261520.2s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.943s, learning 0.175s)
               Value function loss: 2.0749
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: -69.98
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 12.12s
                        Total time: 20433.24s
                               ETA: 1261464.1s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.907s, learning 0.211s)
               Value function loss: 2.1363
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: -66.09
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 12.12s
                        Total time: 20445.36s
                               ETA: 1261408.0s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.416s, learning 0.323s)
               Value function loss: 2.4283
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: -65.86
               Mean episode length: 297.53
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 12.74s
                        Total time: 20458.10s
                               ETA: 1261390.3s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.007s, learning 0.213s)
               Value function loss: 2.1109
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: -64.36
               Mean episode length: 297.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 12.22s
                        Total time: 20470.32s
                               ETA: 1261340.7s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.863s, learning 0.193s)
               Value function loss: 2.5093
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: -65.11
               Mean episode length: 297.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 12.06s
                        Total time: 20482.37s
                               ETA: 1261280.9s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.987s, learning 0.181s)
               Value function loss: 2.8083
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: -75.90
               Mean episode length: 295.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 12.17s
                        Total time: 20494.54s
                               ETA: 1261228.1s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.505s, learning 0.264s)
               Value function loss: 3.2738
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: -72.36
               Mean episode length: 295.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 12.77s
                        Total time: 20507.31s
                               ETA: 1261212.3s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.030s, learning 0.198s)
               Value function loss: 4.2856
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: -72.15
               Mean episode length: 295.10
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 12.23s
                        Total time: 20519.54s
                               ETA: 1261163.3s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.719s, learning 0.177s)
               Value function loss: 12.5496
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: -72.75
               Mean episode length: 295.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 11.90s
                        Total time: 20531.43s
                               ETA: 1261093.9s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.179s, learning 0.299s)
               Value function loss: 10.2994
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: -71.47
               Mean episode length: 295.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 12.48s
                        Total time: 20543.91s
                               ETA: 1261060.3s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.113s, learning 0.194s)
               Value function loss: 3.2390
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: -69.38
               Mean episode length: 295.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 12.31s
                        Total time: 20556.22s
                               ETA: 1261016.3s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.828s, learning 0.206s)
               Value function loss: 2.4284
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: -70.66
               Mean episode length: 295.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 12.03s
                        Total time: 20568.25s
                               ETA: 1260955.6s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.299s, learning 0.179s)
               Value function loss: 3.2388
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: -64.98
               Mean episode length: 295.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 12.48s
                        Total time: 20580.73s
                               ETA: 1260922.1s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.766s, learning 0.229s)
               Value function loss: 4.1403
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: -63.22
               Mean episode length: 295.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 11.99s
                        Total time: 20592.72s
                               ETA: 1260859.1s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.006s, learning 0.201s)
               Value function loss: 4.7231
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: -57.70
               Mean episode length: 295.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 12.21s
                        Total time: 20604.93s
                               ETA: 1260809.1s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.175s, learning 0.194s)
               Value function loss: 6.2294
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: -58.26
               Mean episode length: 297.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 12.37s
                        Total time: 20617.30s
                               ETA: 1260769.1s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.054s, learning 0.219s)
               Value function loss: 2.8355
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: -55.38
               Mean episode length: 297.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 12.27s
                        Total time: 20629.57s
                               ETA: 1260723.2s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.126s, learning 0.204s)
               Value function loss: 2.6256
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: -59.08
               Mean episode length: 297.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 12.33s
                        Total time: 20641.90s
                               ETA: 1260680.8s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.034s, learning 0.211s)
               Value function loss: 2.4838
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: -57.46
               Mean episode length: 297.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 12.24s
                        Total time: 20654.15s
                               ETA: 1260633.3s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.730s, learning 0.263s)
               Value function loss: 78.4145
                    Surrogate loss: 0.0249
             Mean action noise std: 0.74
                       Mean reward: -60.47
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 11.99s
                        Total time: 20666.14s
                               ETA: 1260570.5s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.126s, learning 0.202s)
               Value function loss: 0.5400
                    Surrogate loss: 0.0032
             Mean action noise std: 0.74
                       Mean reward: -61.94
               Mean episode length: 300.00
                  Mean reward/step: -1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 12.33s
                        Total time: 20678.47s
                               ETA: 1260528.2s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.071s, learning 0.187s)
               Value function loss: 0.8094
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: -58.98
               Mean episode length: 297.17
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 12.26s
                        Total time: 20690.73s
                               ETA: 1260481.6s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.884s, learning 0.201s)
               Value function loss: 0.6971
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: -58.58
               Mean episode length: 297.17
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 12.08s
                        Total time: 20702.81s
                               ETA: 1260424.5s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.089s, learning 0.191s)
               Value function loss: 1.0789
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: -57.57
               Mean episode length: 297.17
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 12.28s
                        Total time: 20715.09s
                               ETA: 1260379.4s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.246s, learning 0.320s)
               Value function loss: 1.1451
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: -56.13
               Mean episode length: 297.17
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 12.57s
                        Total time: 20727.66s
                               ETA: 1260351.7s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.440s, learning 0.403s)
               Value function loss: 1.5663
                    Surrogate loss: 0.0065
             Mean action noise std: 0.74
                       Mean reward: -55.47
               Mean episode length: 297.17
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 12.84s
                        Total time: 20740.50s
                               ETA: 1260340.8s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.246s, learning 0.180s)
               Value function loss: 2.5850
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -63.56
               Mean episode length: 292.29
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 12.43s
                        Total time: 20752.93s
                               ETA: 1260304.7s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.539s, learning 0.237s)
               Value function loss: 1.5342
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: -67.54
               Mean episode length: 295.12
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 12.78s
                        Total time: 20765.70s
                               ETA: 1260289.8s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.568s, learning 0.329s)
               Value function loss: 1.0791
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: -65.64
               Mean episode length: 295.12
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 12.90s
                        Total time: 20778.60s
                               ETA: 1260282.3s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.015s, learning 0.178s)
               Value function loss: 1.0283
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: -68.67
               Mean episode length: 295.12
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 12.19s
                        Total time: 20790.79s
                               ETA: 1260232.0s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.707s, learning 0.177s)
               Value function loss: 0.9265
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: -69.71
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 11.88s
                        Total time: 20802.68s
                               ETA: 1260163.1s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.993s, learning 0.182s)
               Value function loss: 0.9165
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: -67.23
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 12.18s
                        Total time: 20814.85s
                               ETA: 1260111.9s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.799s, learning 0.237s)
               Value function loss: 1.3143
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -71.77
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 12.04s
                        Total time: 20826.89s
                               ETA: 1260052.3s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.096s, learning 0.329s)
               Value function loss: 1.5984
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: -72.37
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 12.43s
                        Total time: 20839.31s
                               ETA: 1260016.4s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.582s, learning 0.293s)
               Value function loss: 2.0979
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: -77.90
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 12.87s
                        Total time: 20852.19s
                               ETA: 1260007.6s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1334 steps/s (collection: 11.955s, learning 0.325s)
               Value function loss: 1.6638
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -79.20
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 12.28s
                        Total time: 20864.47s
                               ETA: 1259962.8s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.508s, learning 0.205s)
               Value function loss: 1.9359
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: -85.94
               Mean episode length: 297.27
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 12.71s
                        Total time: 20877.18s
                               ETA: 1259944.3s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.695s, learning 0.302s)
               Value function loss: 2.1061
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: -85.12
               Mean episode length: 294.71
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 13.00s
                        Total time: 20890.18s
                               ETA: 1259942.9s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.585s, learning 0.346s)
               Value function loss: 2.0038
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: -84.45
               Mean episode length: 294.71
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 12.93s
                        Total time: 20903.11s
                               ETA: 1259937.5s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.130s, learning 0.175s)
               Value function loss: 1.8233
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: -86.70
               Mean episode length: 294.71
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 12.30s
                        Total time: 20915.41s
                               ETA: 1259894.3s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.520s, learning 0.195s)
               Value function loss: 1.9837
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: -87.17
               Mean episode length: 294.71
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 12.71s
                        Total time: 20928.13s
                               ETA: 1259875.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.143s, learning 0.179s)
               Value function loss: 1.3851
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: -84.70
               Mean episode length: 294.71
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 12.32s
                        Total time: 20940.45s
                               ETA: 1259833.9s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.049s, learning 0.217s)
               Value function loss: 1.3511
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: -82.80
               Mean episode length: 294.71
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 12.27s
                        Total time: 20952.72s
                               ETA: 1259788.5s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.025s, learning 0.178s)
               Value function loss: 1.4491
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: -80.55
               Mean episode length: 294.71
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 12.20s
                        Total time: 20964.92s
                               ETA: 1259739.4s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.162s, learning 0.192s)
               Value function loss: 1.5676
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: -81.24
               Mean episode length: 297.44
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 12.35s
                        Total time: 20977.27s
                               ETA: 1259699.4s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.015s, learning 0.194s)
               Value function loss: 1.1663
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: -81.91
               Mean episode length: 297.44
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 12.21s
                        Total time: 20989.48s
                               ETA: 1259650.8s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.775s, learning 0.285s)
               Value function loss: 1.0352
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: -77.46
               Mean episode length: 297.44
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 13.06s
                        Total time: 21002.54s
                               ETA: 1259653.1s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.106s, learning 0.210s)
               Value function loss: 1.2828
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: -77.03
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 12.32s
                        Total time: 21014.86s
                               ETA: 1259610.9s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.277s, learning 0.192s)
               Value function loss: 0.9353
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: -78.68
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 12.47s
                        Total time: 21027.33s
                               ETA: 1259577.9s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.874s, learning 0.176s)
               Value function loss: 1.0344
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: -80.84
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 12.05s
                        Total time: 21039.38s
                               ETA: 1259519.9s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.653s, learning 0.216s)
               Value function loss: 1.1903
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: -80.57
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 12.87s
                        Total time: 21052.25s
                               ETA: 1259510.9s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.659s, learning 0.219s)
               Value function loss: 1.0284
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: -78.40
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 12.88s
                        Total time: 21065.13s
                               ETA: 1259502.4s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.352s, learning 0.224s)
               Value function loss: 1.0202
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: -77.52
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 12.58s
                        Total time: 21077.70s
                               ETA: 1259475.9s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.852s, learning 0.212s)
               Value function loss: 1.1854
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: -76.88
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 12.06s
                        Total time: 21089.77s
                               ETA: 1259418.9s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.813s, learning 0.205s)
               Value function loss: 1.1324
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: -76.39
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 12.02s
                        Total time: 21101.79s
                               ETA: 1259359.1s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.563s, learning 0.238s)
               Value function loss: 0.9350
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: -76.52
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 12.80s
                        Total time: 21114.59s
                               ETA: 1259346.1s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.595s, learning 0.177s)
               Value function loss: 78.2697
                    Surrogate loss: 0.0246
             Mean action noise std: 0.74
                       Mean reward: -75.84
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 12.77s
                        Total time: 21127.36s
                               ETA: 1259331.4s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.453s, learning 0.308s)
               Value function loss: 0.4617
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: -75.49
               Mean episode length: 300.00
                  Mean reward/step: -1.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 12.76s
                        Total time: 21140.12s
                               ETA: 1259316.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.610s, learning 0.307s)
               Value function loss: 0.2914
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: -78.14
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 12.92s
                        Total time: 21153.04s
                               ETA: 1259309.9s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.302s, learning 0.264s)
               Value function loss: 0.9795
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: -79.67
               Mean episode length: 297.23
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 12.57s
                        Total time: 21165.60s
                               ETA: 1259282.9s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.940s, learning 0.205s)
               Value function loss: 0.6180
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -78.53
               Mean episode length: 291.77
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 12.15s
                        Total time: 21177.75s
                               ETA: 1259230.9s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.785s, learning 0.219s)
               Value function loss: 1.0247
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: -80.90
               Mean episode length: 291.77
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 12.00s
                        Total time: 21189.75s
                               ETA: 1259170.5s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.903s, learning 0.216s)
               Value function loss: 0.9942
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: -81.07
               Mean episode length: 291.77
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 12.12s
                        Total time: 21201.87s
                               ETA: 1259117.1s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.972s, learning 0.192s)
               Value function loss: 2.4827
                    Surrogate loss: 0.0080
             Mean action noise std: 0.74
                       Mean reward: -77.19
               Mean episode length: 289.34
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 12.16s
                        Total time: 21214.03s
                               ETA: 1259066.4s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.860s, learning 0.188s)
               Value function loss: 2.0890
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: -88.22
               Mean episode length: 287.38
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 12.05s
                        Total time: 21226.08s
                               ETA: 1259008.8s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.095s, learning 0.219s)
               Value function loss: 1.1734
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: -86.75
               Mean episode length: 292.84
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 12.31s
                        Total time: 21238.40s
                               ETA: 1258967.1s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.290s, learning 0.229s)
               Value function loss: 1.0272
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: -85.12
               Mean episode length: 290.24
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 12.52s
                        Total time: 21250.92s
                               ETA: 1258937.5s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.575s, learning 0.170s)
               Value function loss: 1.2979
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: -90.39
               Mean episode length: 287.57
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 12.74s
                        Total time: 21263.66s
                               ETA: 1258921.3s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.338s, learning 0.321s)
               Value function loss: 1.3784
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: -91.49
               Mean episode length: 287.57
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 12.66s
                        Total time: 21276.32s
                               ETA: 1258900.0s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.683s, learning 0.185s)
               Value function loss: 0.9661
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -95.69
               Mean episode length: 285.47
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 12.87s
                        Total time: 21289.19s
                               ETA: 1258891.2s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.318s, learning 0.192s)
               Value function loss: 0.9518
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: -96.18
               Mean episode length: 280.27
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 12.51s
                        Total time: 21301.70s
                               ETA: 1258861.1s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.882s, learning 0.179s)
               Value function loss: 1.0953
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: -94.51
               Mean episode length: 275.04
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 12.06s
                        Total time: 21313.76s
                               ETA: 1258804.6s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.440s, learning 0.287s)
               Value function loss: 0.9659
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: -98.74
               Mean episode length: 275.04
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 12.73s
                        Total time: 21326.48s
                               ETA: 1258787.4s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.246s, learning 0.294s)
               Value function loss: 0.8530
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: -103.17
               Mean episode length: 277.64
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 12.54s
                        Total time: 21339.02s
                               ETA: 1258759.2s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.279s, learning 0.285s)
               Value function loss: 0.9706
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: -103.70
               Mean episode length: 277.64
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 12.56s
                        Total time: 21351.59s
                               ETA: 1258732.5s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.484s, learning 0.292s)
               Value function loss: 0.7416
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: -105.96
               Mean episode length: 280.31
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 12.78s
                        Total time: 21364.36s
                               ETA: 1258718.2s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.226s, learning 0.238s)
               Value function loss: 0.6429
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: -105.10
               Mean episode length: 280.31
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 12.46s
                        Total time: 21376.83s
                               ETA: 1258685.6s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.583s, learning 0.233s)
               Value function loss: 0.8754
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: -97.55
               Mean episode length: 287.39
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 12.82s
                        Total time: 21389.64s
                               ETA: 1258673.7s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.419s, learning 0.255s)
               Value function loss: 0.5560
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: -100.38
               Mean episode length: 286.54
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 12.67s
                        Total time: 21402.32s
                               ETA: 1258653.4s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.719s, learning 0.266s)
               Value function loss: 0.7242
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -99.05
               Mean episode length: 293.82
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 11.98s
                        Total time: 21414.30s
                               ETA: 1258592.7s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.310s, learning 0.271s)
               Value function loss: 0.8107
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: -98.14
               Mean episode length: 296.50
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 12.58s
                        Total time: 21426.88s
                               ETA: 1258567.0s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.297s, learning 0.290s)
               Value function loss: 1.1307
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: -93.63
               Mean episode length: 294.02
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 12.59s
                        Total time: 21439.47s
                               ETA: 1258541.8s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.097s, learning 0.190s)
               Value function loss: 0.5941
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: -89.92
               Mean episode length: 294.02
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 12.29s
                        Total time: 21451.76s
                               ETA: 1258498.9s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.535s, learning 0.169s)
               Value function loss: 0.5339
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: -84.69
               Mean episode length: 294.02
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 12.70s
                        Total time: 21464.46s
                               ETA: 1258480.5s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.360s, learning 0.309s)
               Value function loss: 0.7304
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: -84.17
               Mean episode length: 288.98
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 12.67s
                        Total time: 21477.13s
                               ETA: 1258460.1s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.277s, learning 0.173s)
               Value function loss: 0.6499
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: -86.16
               Mean episode length: 288.98
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 12.45s
                        Total time: 21489.58s
                               ETA: 1258426.8s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.041s, learning 0.235s)
               Value function loss: 0.4236
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: -86.05
               Mean episode length: 288.98
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 12.28s
                        Total time: 21501.86s
                               ETA: 1258383.4s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.153s, learning 0.226s)
               Value function loss: 0.7120
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: -86.48
               Mean episode length: 288.98
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 12.38s
                        Total time: 21514.24s
                               ETA: 1258346.1s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.023s, learning 0.286s)
               Value function loss: 0.6021
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: -86.50
               Mean episode length: 288.98
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 12.31s
                        Total time: 21526.55s
                               ETA: 1258304.7s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.958s, learning 0.198s)
               Value function loss: 0.5832
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: -85.44
               Mean episode length: 286.65
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 12.16s
                        Total time: 21538.70s
                               ETA: 1258254.3s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.851s, learning 0.196s)
               Value function loss: 0.7531
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: -85.98
               Mean episode length: 288.88
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 12.05s
                        Total time: 21550.75s
                               ETA: 1258197.7s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.116s, learning 0.222s)
               Value function loss: 0.6652
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: -84.36
               Mean episode length: 290.15
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 12.34s
                        Total time: 21563.09s
                               ETA: 1258158.0s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.460s, learning 0.184s)
               Value function loss: 0.5553
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: -86.95
               Mean episode length: 285.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 12.64s
                        Total time: 21575.73s
                               ETA: 1258136.3s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.207s, learning 0.325s)
               Value function loss: 0.5723
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: -86.91
               Mean episode length: 285.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 12.53s
                        Total time: 21588.26s
                               ETA: 1258108.1s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.852s, learning 0.211s)
               Value function loss: 69.4752
                    Surrogate loss: 0.0319
             Mean action noise std: 0.74
                       Mean reward: -69.17
               Mean episode length: 297.51
                  Mean reward/step: -0.90
       Mean episode length/episode: 4.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 12.06s
                        Total time: 21600.32s
                               ETA: 1258052.5s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.061s, learning 0.168s)
               Value function loss: 0.4635
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: -69.00
               Mean episode length: 297.51
                  Mean reward/step: -1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 12.23s
                        Total time: 21612.55s
                               ETA: 1258006.7s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.081s, learning 0.212s)
               Value function loss: 0.5728
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: -72.85
               Mean episode length: 297.51
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 12.29s
                        Total time: 21624.85s
                               ETA: 1257964.6s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.336s, learning 0.205s)
               Value function loss: 0.7794
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: -74.33
               Mean episode length: 292.04
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 12.54s
                        Total time: 21637.39s
                               ETA: 1257937.0s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.408s, learning 0.299s)
               Value function loss: 1.1239
                    Surrogate loss: 0.0034
             Mean action noise std: 0.74
                       Mean reward: -76.80
               Mean episode length: 289.41
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 12.71s
                        Total time: 21650.09s
                               ETA: 1257919.1s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.309s, learning 0.380s)
               Value function loss: 1.2271
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: -81.30
               Mean episode length: 284.19
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 12.69s
                        Total time: 21662.78s
                               ETA: 1257900.1s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.678s, learning 0.297s)
               Value function loss: 1.4399
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: -82.06
               Mean episode length: 279.23
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 12.97s
                        Total time: 21675.76s
                               ETA: 1257897.7s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.050s, learning 0.183s)
               Value function loss: 2.0841
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: -87.96
               Mean episode length: 276.82
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 12.23s
                        Total time: 21687.99s
                               ETA: 1257852.3s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.460s, learning 0.330s)
               Value function loss: 1.6111
                    Surrogate loss: 0.0038
             Mean action noise std: 0.74
                       Mean reward: -89.63
               Mean episode length: 280.03
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 12.79s
                        Total time: 21700.78s
                               ETA: 1257839.2s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.372s, learning 0.207s)
               Value function loss: 0.8996
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: -84.58
               Mean episode length: 282.66
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 12.58s
                        Total time: 21713.36s
                               ETA: 1257813.8s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.061s, learning 0.182s)
               Value function loss: 0.8389
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: -79.62
               Mean episode length: 290.19
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 12.24s
                        Total time: 21725.60s
                               ETA: 1257769.1s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.424s, learning 0.242s)
               Value function loss: 0.7690
                    Surrogate loss: -0.0005
             Mean action noise std: 0.74
                       Mean reward: -75.64
               Mean episode length: 290.19
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 11.67s
                        Total time: 21737.27s
                               ETA: 1257691.0s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.687s, learning 0.223s)
               Value function loss: 0.5871
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: -76.63
               Mean episode length: 287.56
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 12.91s
                        Total time: 21750.18s
                               ETA: 1257684.8s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.268s, learning 0.325s)
               Value function loss: 0.6776
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: -73.80
               Mean episode length: 289.97
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 12.59s
                        Total time: 21762.77s
                               ETA: 1257660.4s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.701s, learning 0.305s)
               Value function loss: 1.1991
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: -67.54
               Mean episode length: 289.86
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 13.01s
                        Total time: 21775.78s
                               ETA: 1257659.9s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.786s, learning 0.176s)
               Value function loss: 1.0429
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: -62.18
               Mean episode length: 289.86
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 11.96s
                        Total time: 21787.74s
                               ETA: 1257599.0s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.213s, learning 0.316s)
               Value function loss: 0.7396
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: -63.01
               Mean episode length: 287.55
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 12.53s
                        Total time: 21800.27s
                               ETA: 1257571.0s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.799s, learning 0.219s)
               Value function loss: 0.9655
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: -63.88
               Mean episode length: 285.09
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 12.02s
                        Total time: 21812.29s
                               ETA: 1257513.5s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.112s, learning 0.189s)
               Value function loss: 0.9505
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: -63.42
               Mean episode length: 282.92
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 12.30s
                        Total time: 21824.59s
                               ETA: 1257472.4s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.989s, learning 0.243s)
               Value function loss: 0.8730
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: -63.95
               Mean episode length: 280.85
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 12.23s
                        Total time: 21836.82s
                               ETA: 1257427.3s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.521s, learning 0.321s)
               Value function loss: 0.9032
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: -61.43
               Mean episode length: 280.85
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 12.84s
                        Total time: 21849.66s
                               ETA: 1257417.4s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.230s, learning 0.182s)
               Value function loss: 1.4673
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -58.71
               Mean episode length: 283.27
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 12.41s
                        Total time: 21862.07s
                               ETA: 1257382.6s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.293s, learning 0.183s)
               Value function loss: 0.8509
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: -59.72
               Mean episode length: 285.71
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 12.48s
                        Total time: 21874.55s
                               ETA: 1257351.7s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.411s, learning 0.304s)
               Value function loss: 0.9204
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: -64.16
               Mean episode length: 285.71
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 12.72s
                        Total time: 21887.27s
                               ETA: 1257334.5s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.714s, learning 0.224s)
               Value function loss: 0.9017
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: -61.91
               Mean episode length: 288.02
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 12.94s
                        Total time: 21900.20s
                               ETA: 1257330.0s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.493s, learning 0.204s)
               Value function loss: 1.0772
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: -66.65
               Mean episode length: 288.91
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 12.70s
                        Total time: 21912.90s
                               ETA: 1257311.8s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.810s, learning 0.198s)
               Value function loss: 0.7261
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: -65.83
               Mean episode length: 288.91
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 13.01s
                        Total time: 21925.91s
                               ETA: 1257311.4s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.413s, learning 0.214s)
               Value function loss: 0.6553
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: -65.43
               Mean episode length: 288.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 12.63s
                        Total time: 21938.54s
                               ETA: 1257289.1s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.331s, learning 0.295s)
               Value function loss: 1.0373
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: -63.49
               Mean episode length: 293.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 12.63s
                        Total time: 21951.16s
                               ETA: 1257266.9s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.463s, learning 0.179s)
               Value function loss: 0.5366
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: -63.41
               Mean episode length: 291.09
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 12.64s
                        Total time: 21963.80s
                               ETA: 1257245.5s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.103s, learning 0.262s)
               Value function loss: 0.6263
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: -60.65
               Mean episode length: 295.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 12.37s
                        Total time: 21976.17s
                               ETA: 1257208.3s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.685s, learning 0.181s)
               Value function loss: 0.9726
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: -66.49
               Mean episode length: 295.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 12.87s
                        Total time: 21989.03s
                               ETA: 1257199.7s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.100s, learning 0.163s)
               Value function loss: 0.6350
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: -64.93
               Mean episode length: 295.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 12.26s
                        Total time: 22001.30s
                               ETA: 1257156.7s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.952s, learning 0.199s)
               Value function loss: 0.7541
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: -64.20
               Mean episode length: 295.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 12.15s
                        Total time: 22013.45s
                               ETA: 1257107.4s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.529s, learning 0.202s)
               Value function loss: 0.9675
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: -69.34
               Mean episode length: 295.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 12.73s
                        Total time: 22026.18s
                               ETA: 1257091.2s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.886s, learning 0.314s)
               Value function loss: 1.0071
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: -68.31
               Mean episode length: 295.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 12.20s
                        Total time: 22038.38s
                               ETA: 1257044.6s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.534s, learning 0.314s)
               Value function loss: 0.6774
                    Surrogate loss: 0.0046
             Mean action noise std: 0.74
                       Mean reward: -64.95
               Mean episode length: 295.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 12.85s
                        Total time: 22051.23s
                               ETA: 1257035.1s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.506s, learning 0.184s)
               Value function loss: 83.1304
                    Surrogate loss: 0.0339
             Mean action noise std: 0.74
                       Mean reward: -51.40
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 4.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 12.69s
                        Total time: 22063.92s
                               ETA: 1257016.5s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.339s, learning 0.238s)
               Value function loss: 0.4643
                    Surrogate loss: -0.0009
             Mean action noise std: 0.74
                       Mean reward: -51.68
               Mean episode length: 300.00
                  Mean reward/step: -1.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 12.58s
                        Total time: 22076.49s
                               ETA: 1256991.6s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.641s, learning 0.189s)
               Value function loss: 0.2594
                    Surrogate loss: 0.0043
             Mean action noise std: 0.74
                       Mean reward: -50.79
               Mean episode length: 300.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 11.83s
                        Total time: 22088.32s
                               ETA: 1256924.1s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.797s, learning 0.165s)
               Value function loss: 0.6977
                    Surrogate loss: -0.0000
             Mean action noise std: 0.74
                       Mean reward: -51.80
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 11.96s
                        Total time: 22100.29s
                               ETA: 1256864.2s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.846s, learning 0.203s)
               Value function loss: 0.8354
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: -51.41
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 12.05s
                        Total time: 22112.34s
                               ETA: 1256809.4s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.249s, learning 0.180s)
               Value function loss: 1.1560
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: -58.50
               Mean episode length: 300.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 12.43s
                        Total time: 22124.76s
                               ETA: 1256776.1s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.438s, learning 0.280s)
               Value function loss: 0.7583
                    Surrogate loss: 0.0039
             Mean action noise std: 0.74
                       Mean reward: -58.23
               Mean episode length: 297.57
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 12.72s
                        Total time: 22137.48s
                               ETA: 1256759.3s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.019s, learning 0.195s)
               Value function loss: 2.2583
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: -57.69
               Mean episode length: 297.57
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 12.21s
                        Total time: 22149.70s
                               ETA: 1256713.9s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.725s, learning 0.198s)
               Value function loss: 2.4122
                    Surrogate loss: -0.0014
             Mean action noise std: 0.74
                       Mean reward: -66.22
               Mean episode length: 297.57
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 12.92s
                        Total time: 22162.62s
                               ETA: 1256708.7s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.404s, learning 0.290s)
               Value function loss: 1.0707
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: -65.35
               Mean episode length: 297.57
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 12.69s
                        Total time: 22175.31s
                               ETA: 1256690.6s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.583s, learning 0.293s)
               Value function loss: 0.7839
                    Surrogate loss: -0.0012
             Mean action noise std: 0.74
                       Mean reward: -65.33
               Mean episode length: 297.57
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 12.88s
                        Total time: 22188.19s
                               ETA: 1256682.8s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.105s, learning 0.175s)
               Value function loss: 1.0241
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: -62.96
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 12.28s
                        Total time: 22200.47s
                               ETA: 1256641.2s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.299s, learning 0.282s)
               Value function loss: 0.9744
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: -61.01
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 12.58s
                        Total time: 22213.05s
                               ETA: 1256616.6s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.131s, learning 0.189s)
               Value function loss: 0.5998
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: -57.86
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 12.32s
                        Total time: 22225.37s
                               ETA: 1256577.3s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.087s, learning 0.181s)
               Value function loss: 0.6415
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: -55.38
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 12.27s
                        Total time: 22237.64s
                               ETA: 1256535.2s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.887s, learning 0.181s)
               Value function loss: 1.4992
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: -55.12
               Mean episode length: 294.95
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 12.07s
                        Total time: 22249.71s
                               ETA: 1256481.8s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1334 steps/s (collection: 11.928s, learning 0.347s)
               Value function loss: 0.9153
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: -54.03
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 12.28s
                        Total time: 22261.98s
                               ETA: 1256440.1s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.436s, learning 0.306s)
               Value function loss: 0.9373
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: -56.29
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 12.74s
                        Total time: 22274.72s
                               ETA: 1256424.8s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.212s, learning 0.183s)
               Value function loss: 1.0843
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: -51.21
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 12.40s
                        Total time: 22287.12s
                               ETA: 1256389.9s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.120s, learning 0.228s)
               Value function loss: 1.0122
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: -51.54
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 12.35s
                        Total time: 22299.47s
                               ETA: 1256352.4s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.298s, learning 0.211s)
               Value function loss: 0.9118
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: -51.05
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 12.51s
                        Total time: 22311.97s
                               ETA: 1256324.0s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.373s, learning 0.212s)
               Value function loss: 1.4321
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -54.42
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 12.59s
                        Total time: 22324.56s
                               ETA: 1256299.9s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.656s, learning 0.188s)
               Value function loss: 1.0431
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: -54.88
               Mean episode length: 294.95
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 12.84s
                        Total time: 22337.40s
                               ETA: 1256290.4s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.033s, learning 0.185s)
               Value function loss: 1.2404
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -53.51
               Mean episode length: 297.33
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 12.22s
                        Total time: 22349.62s
                               ETA: 1256245.7s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.143s, learning 0.212s)
               Value function loss: 0.9878
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: -52.75
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 12.35s
                        Total time: 22361.98s
                               ETA: 1256208.7s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.064s, learning 0.205s)
               Value function loss: 1.2672
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: -58.34
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 12.27s
                        Total time: 22374.25s
                               ETA: 1256166.9s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.576s, learning 0.277s)
               Value function loss: 0.8011
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: -59.31
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 12.85s
                        Total time: 22387.10s
                               ETA: 1256157.9s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.086s, learning 0.196s)
               Value function loss: 0.7513
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: -60.51
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 12.28s
                        Total time: 22399.38s
                               ETA: 1256116.9s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.199s, learning 0.178s)
               Value function loss: 1.1912
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: -66.20
               Mean episode length: 297.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 12.38s
                        Total time: 22411.76s
                               ETA: 1256081.3s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.923s, learning 0.237s)
               Value function loss: 0.8638
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: -66.07
               Mean episode length: 297.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 12.16s
                        Total time: 22423.92s
                               ETA: 1256033.5s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.027s, learning 0.307s)
               Value function loss: 0.5162
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: -67.81
               Mean episode length: 297.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 12.33s
                        Total time: 22436.25s
                               ETA: 1255995.5s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.567s, learning 0.292s)
               Value function loss: 0.7762
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: -69.43
               Mean episode length: 297.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 12.86s
                        Total time: 22449.11s
                               ETA: 1255986.9s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.854s, learning 0.190s)
               Value function loss: 0.5937
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: -71.52
               Mean episode length: 297.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 12.04s
                        Total time: 22461.16s
                               ETA: 1255932.7s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.709s, learning 0.174s)
               Value function loss: 0.6468
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -65.30
               Mean episode length: 297.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 11.88s
                        Total time: 22473.04s
                               ETA: 1255869.6s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.107s, learning 0.191s)
               Value function loss: 0.8327
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: -63.83
               Mean episode length: 297.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 12.30s
                        Total time: 22485.34s
                               ETA: 1255829.7s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.130s, learning 0.251s)
               Value function loss: 0.6486
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -64.02
               Mean episode length: 297.20
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 12.38s
                        Total time: 22497.72s
                               ETA: 1255794.5s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.692s, learning 0.169s)
               Value function loss: 0.7162
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: -65.20
               Mean episode length: 297.20
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 12.86s
                        Total time: 22510.58s
                               ETA: 1255786.1s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1235 steps/s (collection: 12.978s, learning 0.278s)
               Value function loss: 0.6552
                    Surrogate loss: 0.0055
             Mean action noise std: 0.73
                       Mean reward: -64.00
               Mean episode length: 297.20
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 13.26s
                        Total time: 22523.83s
                               ETA: 1255799.7s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.451s, learning 0.345s)
               Value function loss: 85.3641
                    Surrogate loss: 0.0544
             Mean action noise std: 0.73
                       Mean reward: -52.55
               Mean episode length: 297.60
                  Mean reward/step: -0.85
       Mean episode length/episode: 4.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 12.80s
                        Total time: 22536.63s
                               ETA: 1255787.6s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.323s, learning 0.328s)
               Value function loss: 0.2553
                    Surrogate loss: 0.0056
             Mean action noise std: 0.73
                       Mean reward: -53.27
               Mean episode length: 297.60
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 12.65s
                        Total time: 22549.28s
                               ETA: 1255767.5s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.494s, learning 0.304s)
               Value function loss: 0.4811
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: -53.22
               Mean episode length: 297.60
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 12.80s
                        Total time: 22562.08s
                               ETA: 1255755.5s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.119s, learning 0.189s)
               Value function loss: 0.7676
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: -54.35
               Mean episode length: 297.60
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 12.31s
                        Total time: 22574.39s
                               ETA: 1255716.3s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.843s, learning 0.191s)
               Value function loss: 1.7791
                    Surrogate loss: 0.0057
             Mean action noise std: 0.73
                       Mean reward: -54.45
               Mean episode length: 297.60
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 12.03s
                        Total time: 22586.42s
                               ETA: 1255661.9s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.159s, learning 0.290s)
               Value function loss: 1.1207
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: -54.76
               Mean episode length: 295.27
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 12.45s
                        Total time: 22598.87s
                               ETA: 1255630.6s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.185s, learning 0.170s)
               Value function loss: 2.1373
                    Surrogate loss: 0.0145
             Mean action noise std: 0.73
                       Mean reward: -55.68
               Mean episode length: 295.27
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 12.35s
                        Total time: 22611.23s
                               ETA: 1255594.1s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.236s, learning 0.205s)
               Value function loss: 2.7963
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: -59.48
               Mean episode length: 295.27
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 12.44s
                        Total time: 22623.67s
                               ETA: 1255562.4s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.827s, learning 0.196s)
               Value function loss: 1.6489
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: -56.08
               Mean episode length: 297.67
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 12.02s
                        Total time: 22635.69s
                               ETA: 1255507.5s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.709s, learning 0.210s)
               Value function loss: 0.9726
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: -55.84
               Mean episode length: 295.42
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 11.92s
                        Total time: 22647.61s
                               ETA: 1255446.9s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.585s, learning 0.227s)
               Value function loss: 0.8650
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: -54.39
               Mean episode length: 295.53
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 12.81s
                        Total time: 22660.42s
                               ETA: 1255435.9s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.295s, learning 0.297s)
               Value function loss: 1.0087
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: -61.15
               Mean episode length: 290.65
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 12.59s
                        Total time: 22673.01s
                               ETA: 1255412.7s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.512s, learning 0.259s)
               Value function loss: 0.6287
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: -57.51
               Mean episode length: 290.65
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 12.77s
                        Total time: 22685.78s
                               ETA: 1255399.4s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.524s, learning 0.261s)
               Value function loss: 0.7661
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: -60.50
               Mean episode length: 290.65
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 12.78s
                        Total time: 22698.57s
                               ETA: 1255386.8s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.435s, learning 0.299s)
               Value function loss: 1.3415
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: -54.95
               Mean episode length: 290.65
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 12.73s
                        Total time: 22711.30s
                               ETA: 1255371.5s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.704s, learning 0.175s)
               Value function loss: 1.5163
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: -55.46
               Mean episode length: 290.65
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 11.88s
                        Total time: 22723.18s
                               ETA: 1255308.9s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.224s, learning 0.179s)
               Value function loss: 0.9563
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: -55.75
               Mean episode length: 292.90
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 12.40s
                        Total time: 22735.59s
                               ETA: 1255275.2s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.900s, learning 0.185s)
               Value function loss: 1.4120
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -61.81
               Mean episode length: 292.76
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 12.08s
                        Total time: 22747.67s
                               ETA: 1255224.1s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.944s, learning 0.283s)
               Value function loss: 1.3370
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -59.91
               Mean episode length: 297.64
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 12.23s
                        Total time: 22759.90s
                               ETA: 1255180.8s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.450s, learning 0.221s)
               Value function loss: 1.8750
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: -58.90
               Mean episode length: 297.64
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 12.67s
                        Total time: 22772.57s
                               ETA: 1255162.1s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.885s, learning 0.172s)
               Value function loss: 1.3822
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -60.69
               Mean episode length: 295.20
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 13.06s
                        Total time: 22785.62s
                               ETA: 1255164.6s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.123s, learning 0.178s)
               Value function loss: 1.5281
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -67.55
               Mean episode length: 290.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 12.30s
                        Total time: 22797.93s
                               ETA: 1255125.5s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.392s, learning 0.225s)
               Value function loss: 1.1593
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: -68.42
               Mean episode length: 289.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 12.62s
                        Total time: 22810.54s
                               ETA: 1255103.8s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.543s, learning 0.192s)
               Value function loss: 1.1167
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: -69.49
               Mean episode length: 286.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 12.73s
                        Total time: 22823.28s
                               ETA: 1255088.6s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.663s, learning 0.350s)
               Value function loss: 1.1272
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -72.26
               Mean episode length: 286.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 13.01s
                        Total time: 22836.29s
                               ETA: 1255088.7s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.143s, learning 0.176s)
               Value function loss: 1.4988
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: -73.43
               Mean episode length: 288.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 12.32s
                        Total time: 22848.61s
                               ETA: 1255050.6s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.669s, learning 0.194s)
               Value function loss: 0.9869
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: -67.40
               Mean episode length: 288.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 11.86s
                        Total time: 22860.47s
                               ETA: 1254987.5s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.016s, learning 0.187s)
               Value function loss: 1.0278
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -65.58
               Mean episode length: 288.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 12.20s
                        Total time: 22872.68s
                               ETA: 1254943.2s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.145s, learning 0.204s)
               Value function loss: 1.3921
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -59.64
               Mean episode length: 288.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 12.35s
                        Total time: 22885.02s
                               ETA: 1254906.9s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.176s, learning 0.282s)
               Value function loss: 0.8968
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: -61.51
               Mean episode length: 288.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 12.46s
                        Total time: 22897.48s
                               ETA: 1254876.6s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.701s, learning 0.331s)
               Value function loss: 0.9766
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: -57.82
               Mean episode length: 291.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 13.03s
                        Total time: 22910.51s
                               ETA: 1254877.8s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.380s, learning 0.283s)
               Value function loss: 1.2101
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: -57.91
               Mean episode length: 291.36
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 12.66s
                        Total time: 22923.18s
                               ETA: 1254858.7s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.268s, learning 0.306s)
               Value function loss: 1.2428
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -56.37
               Mean episode length: 291.36
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 12.57s
                        Total time: 22935.75s
                               ETA: 1254834.8s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.125s, learning 0.313s)
               Value function loss: 1.4519
                    Surrogate loss: 0.0090
             Mean action noise std: 0.73
                       Mean reward: -53.61
               Mean episode length: 291.36
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 12.44s
                        Total time: 22948.19s
                               ETA: 1254803.5s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.579s, learning 0.308s)
               Value function loss: 1.1527
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: -49.04
               Mean episode length: 296.28
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 12.89s
                        Total time: 22961.08s
                               ETA: 1254796.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.173s, learning 0.173s)
               Value function loss: 1.2688
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: -47.38
               Mean episode length: 297.54
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 12.35s
                        Total time: 22973.42s
                               ETA: 1254760.4s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.157s, learning 0.288s)
               Value function loss: 0.9100
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: -45.16
               Mean episode length: 300.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 12.44s
                        Total time: 22985.87s
                               ETA: 1254729.5s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.405s, learning 0.183s)
               Value function loss: 85.2774
                    Surrogate loss: 0.0417
             Mean action noise std: 0.73
                       Mean reward: -45.27
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 4.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 12.59s
                        Total time: 22998.46s
                               ETA: 1254706.4s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.737s, learning 0.222s)
               Value function loss: 0.5392
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -44.65
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 11.96s
                        Total time: 23010.42s
                               ETA: 1254649.0s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.809s, learning 0.217s)
               Value function loss: 0.6532
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: -46.52
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 12.03s
                        Total time: 23022.44s
                               ETA: 1254595.3s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.647s, learning 0.203s)
               Value function loss: 0.7641
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: -45.49
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 11.85s
                        Total time: 23034.29s
                               ETA: 1254532.1s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.778s, learning 0.180s)
               Value function loss: 0.8189
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -45.78
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 11.96s
                        Total time: 23046.25s
                               ETA: 1254474.9s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1326 steps/s (collection: 11.994s, learning 0.356s)
               Value function loss: 1.0988
                    Surrogate loss: 0.0009
             Mean action noise std: 0.73
                       Mean reward: -42.18
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 12.35s
                        Total time: 23058.60s
                               ETA: 1254439.0s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.477s, learning 0.292s)
               Value function loss: 0.6996
                    Surrogate loss: 0.0126
             Mean action noise std: 0.73
                       Mean reward: -41.55
               Mean episode length: 300.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 12.77s
                        Total time: 23071.37s
                               ETA: 1254425.9s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.537s, learning 0.202s)
               Value function loss: 2.9880
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: -46.32
               Mean episode length: 295.11
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 12.74s
                        Total time: 23084.11s
                               ETA: 1254411.2s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.313s, learning 0.291s)
               Value function loss: 2.5379
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: -48.93
               Mean episode length: 295.11
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 12.60s
                        Total time: 23096.71s
                               ETA: 1254389.2s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.505s, learning 0.373s)
               Value function loss: 1.2221
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: -50.86
               Mean episode length: 295.11
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 12.88s
                        Total time: 23109.59s
                               ETA: 1254382.0s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.221s, learning 0.176s)
               Value function loss: 0.9112
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: -51.46
               Mean episode length: 295.11
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 12.40s
                        Total time: 23121.99s
                               ETA: 1254348.7s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.377s, learning 0.299s)
               Value function loss: 1.0348
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: -51.37
               Mean episode length: 297.57
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 12.68s
                        Total time: 23134.66s
                               ETA: 1254330.6s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.109s, learning 0.179s)
               Value function loss: 0.9284
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -56.03
               Mean episode length: 297.57
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 12.29s
                        Total time: 23146.95s
                               ETA: 1254291.4s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.849s, learning 0.203s)
               Value function loss: 0.5542
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: -51.22
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 12.05s
                        Total time: 23159.00s
                               ETA: 1254239.6s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.796s, learning 0.308s)
               Value function loss: 0.6730
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: -52.58
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 12.10s
                        Total time: 23171.11s
                               ETA: 1254190.6s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.286s, learning 0.213s)
               Value function loss: 1.3972
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: -53.59
               Mean episode length: 298.17
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 12.50s
                        Total time: 23183.61s
                               ETA: 1254162.9s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.024s, learning 0.175s)
               Value function loss: 1.1443
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -49.96
               Mean episode length: 298.17
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 12.20s
                        Total time: 23195.81s
                               ETA: 1254119.1s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.007s, learning 0.276s)
               Value function loss: 1.0577
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: -55.43
               Mean episode length: 298.17
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 12.28s
                        Total time: 23208.09s
                               ETA: 1254079.8s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.249s, learning 0.268s)
               Value function loss: 1.1853
                    Surrogate loss: 0.0080
             Mean action noise std: 0.73
                       Mean reward: -57.32
               Mean episode length: 293.42
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 12.52s
                        Total time: 23220.61s
                               ETA: 1254053.3s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1231 steps/s (collection: 13.068s, learning 0.238s)
               Value function loss: 0.9544
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -52.07
               Mean episode length: 293.42
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 13.31s
                        Total time: 23233.91s
                               ETA: 1254069.3s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.750s, learning 0.281s)
               Value function loss: 0.8310
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: -50.78
               Mean episode length: 292.02
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 13.03s
                        Total time: 23246.94s
                               ETA: 1254070.5s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.965s, learning 0.203s)
               Value function loss: 1.0281
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: -49.91
               Mean episode length: 289.61
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 12.17s
                        Total time: 23259.11s
                               ETA: 1254025.1s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.182s)
               Value function loss: 0.9206
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -50.97
               Mean episode length: 291.44
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 12.11s
                        Total time: 23271.22s
                               ETA: 1253976.4s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.212s, learning 0.209s)
               Value function loss: 0.8154
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -50.08
               Mean episode length: 291.44
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 12.42s
                        Total time: 23283.64s
                               ETA: 1253944.6s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.677s, learning 0.181s)
               Value function loss: 0.7150
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -49.08
               Mean episode length: 291.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 11.86s
                        Total time: 23295.50s
                               ETA: 1253882.7s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.671s, learning 0.275s)
               Value function loss: 1.1663
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: -47.94
               Mean episode length: 288.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 12.95s
                        Total time: 23308.44s
                               ETA: 1253879.3s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.549s, learning 0.287s)
               Value function loss: 0.6898
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: -46.09
               Mean episode length: 293.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 12.84s
                        Total time: 23321.28s
                               ETA: 1253870.0s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.708s, learning 0.225s)
               Value function loss: 0.5750
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -45.35
               Mean episode length: 293.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 12.93s
                        Total time: 23334.21s
                               ETA: 1253865.9s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.074s, learning 0.172s)
               Value function loss: 0.7267
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: -47.33
               Mean episode length: 293.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 12.25s
                        Total time: 23346.46s
                               ETA: 1253824.9s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.021s, learning 0.193s)
               Value function loss: 0.7272
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -54.31
               Mean episode length: 293.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 12.21s
                        Total time: 23358.67s
                               ETA: 1253782.2s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.935s, learning 0.266s)
               Value function loss: 0.3657
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: -52.63
               Mean episode length: 294.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 12.20s
                        Total time: 23370.87s
                               ETA: 1253738.8s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.133s, learning 0.301s)
               Value function loss: 0.6088
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -55.98
               Mean episode length: 292.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 12.43s
                        Total time: 23383.31s
                               ETA: 1253708.1s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.807s, learning 0.231s)
               Value function loss: 0.5434
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: -60.28
               Mean episode length: 292.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 12.04s
                        Total time: 23395.35s
                               ETA: 1253656.0s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.029s, learning 0.253s)
               Value function loss: 0.5880
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: -59.24
               Mean episode length: 295.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 12.28s
                        Total time: 23407.63s
                               ETA: 1253617.1s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.815s, learning 0.390s)
               Value function loss: 0.8475
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: -57.53
               Mean episode length: 293.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 12.20s
                        Total time: 23419.83s
                               ETA: 1253574.0s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.211s, learning 0.284s)
               Value function loss: 0.6959
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: -57.74
               Mean episode length: 293.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 12.49s
                        Total time: 23432.33s
                               ETA: 1253546.5s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.354s, learning 0.195s)
               Value function loss: 0.8517
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -60.49
               Mean episode length: 293.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 12.55s
                        Total time: 23444.88s
                               ETA: 1253522.0s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.092s, learning 0.224s)
               Value function loss: 0.8154
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -61.38
               Mean episode length: 293.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 12.32s
                        Total time: 23457.19s
                               ETA: 1253485.0s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.941s, learning 0.286s)
               Value function loss: 84.4343
                    Surrogate loss: 0.0233
             Mean action noise std: 0.73
                       Mean reward: -49.47
               Mean episode length: 297.60
                  Mean reward/step: -0.84
       Mean episode length/episode: 4.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 12.23s
                        Total time: 23469.42s
                               ETA: 1253443.2s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.066s, learning 0.251s)
               Value function loss: 0.9138
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: -54.50
               Mean episode length: 297.60
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 12.32s
                        Total time: 23481.74s
                               ETA: 1253406.4s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.807s, learning 0.183s)
               Value function loss: 0.4727
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: -53.86
               Mean episode length: 294.91
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.99s
                        Total time: 23493.73s
                               ETA: 1253352.0s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.632s, learning 0.184s)
               Value function loss: 0.7502
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: -54.57
               Mean episode length: 284.16
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 11.82s
                        Total time: 23505.54s
                               ETA: 1253288.5s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.236s, learning 0.216s)
               Value function loss: 0.7584
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -53.16
               Mean episode length: 284.16
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 12.45s
                        Total time: 23517.99s
                               ETA: 1253258.9s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.972s, learning 0.195s)
               Value function loss: 1.0174
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: -55.77
               Mean episode length: 273.54
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 12.17s
                        Total time: 23530.16s
                               ETA: 1253214.1s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.032s, learning 0.170s)
               Value function loss: 1.1260
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: -56.02
               Mean episode length: 268.55
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 12.20s
                        Total time: 23542.36s
                               ETA: 1253171.3s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.554s, learning 0.206s)
               Value function loss: 2.9032
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: -53.02
               Mean episode length: 266.35
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 11.76s
                        Total time: 23554.12s
                               ETA: 1253104.9s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.711s, learning 0.243s)
               Value function loss: 1.5048
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -55.36
               Mean episode length: 274.62
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 11.95s
                        Total time: 23566.08s
                               ETA: 1253049.0s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.903s, learning 0.217s)
               Value function loss: 0.9074
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -54.50
               Mean episode length: 277.24
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 12.12s
                        Total time: 23578.20s
                               ETA: 1253001.9s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.715s, learning 0.177s)
               Value function loss: 0.9004
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: -50.86
               Mean episode length: 290.23
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 11.89s
                        Total time: 23590.09s
                               ETA: 1252942.7s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.281s, learning 0.297s)
               Value function loss: 1.3765
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: -54.14
               Mean episode length: 290.58
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 12.58s
                        Total time: 23602.67s
                               ETA: 1252920.0s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.170s, learning 0.315s)
               Value function loss: 0.6492
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: -54.44
               Mean episode length: 290.58
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 12.49s
                        Total time: 23615.15s
                               ETA: 1252892.5s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.319s, learning 0.223s)
               Value function loss: 0.6853
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: -51.41
               Mean episode length: 290.58
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 12.54s
                        Total time: 23627.70s
                               ETA: 1252867.9s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.069s, learning 0.185s)
               Value function loss: 1.2083
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: -52.54
               Mean episode length: 292.98
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 12.25s
                        Total time: 23639.95s
                               ETA: 1252828.1s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.465s, learning 0.373s)
               Value function loss: 1.2638
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: -51.45
               Mean episode length: 290.31
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 12.84s
                        Total time: 23652.79s
                               ETA: 1252819.3s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.486s, learning 0.308s)
               Value function loss: 0.6116
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: -55.95
               Mean episode length: 288.05
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 12.79s
                        Total time: 23665.58s
                               ETA: 1252808.0s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.370s, learning 0.249s)
               Value function loss: 1.3671
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: -62.49
               Mean episode length: 288.05
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 12.62s
                        Total time: 23678.20s
                               ETA: 1252787.6s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.258s, learning 0.207s)
               Value function loss: 1.0130
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: -63.70
               Mean episode length: 292.59
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 12.47s
                        Total time: 23690.67s
                               ETA: 1252759.0s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.882s, learning 0.289s)
               Value function loss: 1.0308
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: -61.30
               Mean episode length: 290.22
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 12.17s
                        Total time: 23702.84s
                               ETA: 1252714.9s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.363s, learning 0.345s)
               Value function loss: 0.8999
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -62.43
               Mean episode length: 290.22
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 12.71s
                        Total time: 23715.55s
                               ETA: 1252699.1s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.256s, learning 0.291s)
               Value function loss: 1.4553
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: -62.12
               Mean episode length: 290.22
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 12.55s
                        Total time: 23728.09s
                               ETA: 1252674.9s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.305s, learning 0.302s)
               Value function loss: 0.7415
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: -59.08
               Mean episode length: 290.11
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 12.61s
                        Total time: 23740.70s
                               ETA: 1252653.8s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.231s, learning 0.263s)
               Value function loss: 0.9235
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: -59.71
               Mean episode length: 292.74
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 12.49s
                        Total time: 23753.19s
                               ETA: 1252626.8s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.361s, learning 0.193s)
               Value function loss: 0.7436
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: -59.91
               Mean episode length: 295.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 12.55s
                        Total time: 23765.75s
                               ETA: 1252603.0s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.089s, learning 0.173s)
               Value function loss: 1.3190
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -58.61
               Mean episode length: 292.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 12.26s
                        Total time: 23778.01s
                               ETA: 1252563.8s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.815s, learning 0.196s)
               Value function loss: 0.7350
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: -60.54
               Mean episode length: 289.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 12.01s
                        Total time: 23790.02s
                               ETA: 1252511.4s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.507s, learning 0.284s)
               Value function loss: 0.7829
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -60.92
               Mean episode length: 289.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 12.79s
                        Total time: 23802.81s
                               ETA: 1252500.1s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.864s, learning 0.180s)
               Value function loss: 1.1881
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: -57.98
               Mean episode length: 289.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 12.04s
                        Total time: 23814.86s
                               ETA: 1252449.5s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.830s, learning 0.187s)
               Value function loss: 0.7505
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: -58.22
               Mean episode length: 289.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 12.02s
                        Total time: 23826.87s
                               ETA: 1252397.6s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.799s, learning 0.300s)
               Value function loss: 0.8105
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: -59.57
               Mean episode length: 289.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 12.10s
                        Total time: 23838.97s
                               ETA: 1252350.0s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.354s, learning 0.211s)
               Value function loss: 1.0150
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: -59.34
               Mean episode length: 289.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 12.57s
                        Total time: 23851.54s
                               ETA: 1252326.9s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.163s, learning 0.216s)
               Value function loss: 1.0211
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: -62.79
               Mean episode length: 286.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 12.38s
                        Total time: 23863.92s
                               ETA: 1252294.1s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.841s, learning 0.197s)
               Value function loss: 0.9936
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: -65.91
               Mean episode length: 286.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 12.04s
                        Total time: 23875.95s
                               ETA: 1252243.4s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.516s, learning 0.208s)
               Value function loss: 1.1776
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: -67.71
               Mean episode length: 288.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 11.72s
                        Total time: 23887.68s
                               ETA: 1252176.3s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.349s, learning 0.280s)
               Value function loss: 1.3049
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: -69.92
               Mean episode length: 288.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 12.63s
                        Total time: 23900.31s
                               ETA: 1252156.6s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.565s, learning 0.362s)
               Value function loss: 1.0698
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: -69.74
               Mean episode length: 288.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 12.93s
                        Total time: 23913.23s
                               ETA: 1252152.6s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.145s, learning 0.189s)
               Value function loss: 76.5615
                    Surrogate loss: 0.0411
             Mean action noise std: 0.73
                       Mean reward: -63.12
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 4.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 12.33s
                        Total time: 23925.57s
                               ETA: 1252117.5s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.851s, learning 0.206s)
               Value function loss: 0.5243
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: -60.35
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 12.06s
                        Total time: 23937.63s
                               ETA: 1252068.0s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.945s, learning 0.188s)
               Value function loss: 0.4080
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: -59.62
               Mean episode length: 294.80
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 12.13s
                        Total time: 23949.76s
                               ETA: 1252022.4s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.320s, learning 0.194s)
               Value function loss: 0.6330
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: -54.05
               Mean episode length: 294.80
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 12.51s
                        Total time: 23962.27s
                               ETA: 1251996.8s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.257s, learning 0.276s)
               Value function loss: 0.7975
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: -50.31
               Mean episode length: 292.06
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 12.53s
                        Total time: 23974.80s
                               ETA: 1251972.2s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.117s, learning 0.222s)
               Value function loss: 1.3220
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: -58.71
               Mean episode length: 286.76
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 12.34s
                        Total time: 23987.14s
                               ETA: 1251937.5s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.732s, learning 0.250s)
               Value function loss: 1.0991
                    Surrogate loss: 0.0063
             Mean action noise std: 0.73
                       Mean reward: -59.14
               Mean episode length: 286.76
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 11.98s
                        Total time: 23999.13s
                               ETA: 1251884.2s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.997s, learning 0.202s)
               Value function loss: 2.0322
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: -53.56
               Mean episode length: 286.76
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 12.20s
                        Total time: 24011.32s
                               ETA: 1251842.3s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.508s, learning 0.193s)
               Value function loss: 1.9549
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -60.44
               Mean episode length: 292.33
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 11.70s
                        Total time: 24023.03s
                               ETA: 1251774.4s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.975s, learning 0.212s)
               Value function loss: 1.0745
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -61.36
               Mean episode length: 297.63
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 12.19s
                        Total time: 24035.21s
                               ETA: 1251731.9s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.934s, learning 0.186s)
               Value function loss: 0.6887
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -55.51
               Mean episode length: 297.63
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 12.12s
                        Total time: 24047.33s
                               ETA: 1251686.0s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.190s, learning 0.197s)
               Value function loss: 0.9068
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: -55.20
               Mean episode length: 297.63
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 12.39s
                        Total time: 24059.72s
                               ETA: 1251654.0s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.018s, learning 0.188s)
               Value function loss: 0.8071
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -57.12
               Mean episode length: 297.63
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 12.21s
                        Total time: 24071.93s
                               ETA: 1251612.6s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.557s, learning 0.224s)
               Value function loss: 0.5688
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: -60.78
               Mean episode length: 297.63
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 12.78s
                        Total time: 24084.71s
                               ETA: 1251601.1s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.917s, learning 0.350s)
               Value function loss: 0.7523
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: -63.81
               Mean episode length: 297.63
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 12.27s
                        Total time: 24096.97s
                               ETA: 1251562.9s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.844s, learning 0.181s)
               Value function loss: 1.4424
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: -59.60
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 12.03s
                        Total time: 24109.00s
                               ETA: 1251512.2s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.057s, learning 0.274s)
               Value function loss: 1.6050
                    Surrogate loss: 0.0087
             Mean action noise std: 0.73
                       Mean reward: -59.38
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 12.33s
                        Total time: 24121.33s
                               ETA: 1251477.3s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.954s, learning 0.182s)
               Value function loss: 2.0780
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: -68.31
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 12.14s
                        Total time: 24133.47s
                               ETA: 1251432.5s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.250s, learning 0.263s)
               Value function loss: 2.0227
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: -70.33
               Mean episode length: 300.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 12.51s
                        Total time: 24145.98s
                               ETA: 1251407.2s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1329 steps/s (collection: 11.982s, learning 0.342s)
               Value function loss: 1.9266
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -70.45
               Mean episode length: 297.59
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 12.32s
                        Total time: 24158.30s
                               ETA: 1251372.1s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.222s, learning 0.187s)
               Value function loss: 1.9223
                    Surrogate loss: 0.0134
             Mean action noise std: 0.73
                       Mean reward: -73.96
               Mean episode length: 297.59
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 12.41s
                        Total time: 24170.71s
                               ETA: 1251341.4s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.970s, learning 0.228s)
               Value function loss: 1.6377
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: -73.14
               Mean episode length: 297.59
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 12.20s
                        Total time: 24182.91s
                               ETA: 1251299.8s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.323s, learning 0.293s)
               Value function loss: 1.2639
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: -78.83
               Mean episode length: 297.59
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 12.62s
                        Total time: 24195.53s
                               ETA: 1251279.9s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.288s, learning 0.336s)
               Value function loss: 1.3087
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: -83.55
               Mean episode length: 295.02
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 12.62s
                        Total time: 24208.15s
                               ETA: 1251260.4s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.820s, learning 0.201s)
               Value function loss: 1.2497
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: -83.13
               Mean episode length: 295.02
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 13.02s
                        Total time: 24221.17s
                               ETA: 1251261.5s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.839s, learning 0.180s)
               Value function loss: 1.5322
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: -78.18
               Mean episode length: 295.02
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 12.02s
                        Total time: 24233.19s
                               ETA: 1251210.7s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.238s, learning 0.290s)
               Value function loss: 1.0001
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: -79.24
               Mean episode length: 295.02
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 12.53s
                        Total time: 24245.72s
                               ETA: 1251186.3s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.040s, learning 0.229s)
               Value function loss: 0.8896
                    Surrogate loss: 0.0091
             Mean action noise std: 0.73
                       Mean reward: -79.75
               Mean episode length: 295.02
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 12.27s
                        Total time: 24257.99s
                               ETA: 1251148.6s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.225s, learning 0.283s)
               Value function loss: 1.0973
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: -77.73
               Mean episode length: 295.02
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 12.51s
                        Total time: 24270.50s
                               ETA: 1251123.1s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.264s, learning 0.322s)
               Value function loss: 1.2126
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -76.27
               Mean episode length: 297.43
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 12.59s
                        Total time: 24283.08s
                               ETA: 1251101.7s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.129s, learning 0.189s)
               Value function loss: 1.0195
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: -73.13
               Mean episode length: 297.43
                  Mean reward/step: -0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 12.32s
                        Total time: 24295.40s
                               ETA: 1251066.5s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.011s, learning 0.214s)
               Value function loss: 1.3288
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: -68.26
               Mean episode length: 297.43
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 12.22s
                        Total time: 24307.63s
                               ETA: 1251026.6s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.292s, learning 0.180s)
               Value function loss: 1.4126
                    Surrogate loss: 0.0196
             Mean action noise std: 0.73
                       Mean reward: -69.80
               Mean episode length: 297.43
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 12.47s
                        Total time: 24320.10s
                               ETA: 1250999.3s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.174s, learning 0.291s)
               Value function loss: 1.6541
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: -74.30
               Mean episode length: 297.43
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 12.46s
                        Total time: 24332.56s
                               ETA: 1250971.7s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.968s, learning 0.221s)
               Value function loss: 1.8523
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -72.02
               Mean episode length: 297.43
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 12.19s
                        Total time: 24344.75s
                               ETA: 1250930.0s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.939s, learning 0.177s)
               Value function loss: 2.1130
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: -73.06
               Mean episode length: 297.43
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 12.12s
                        Total time: 24356.87s
                               ETA: 1250884.6s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.125s, learning 0.177s)
               Value function loss: 2.3659
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: -69.20
               Mean episode length: 294.71
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 12.30s
                        Total time: 24369.17s
                               ETA: 1250848.8s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.182s, learning 0.206s)
               Value function loss: 2.2791
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: -69.31
               Mean episode length: 297.28
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 12.39s
                        Total time: 24381.56s
                               ETA: 1250817.3s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.434s, learning 0.283s)
               Value function loss: 65.9007
                    Surrogate loss: 0.0352
             Mean action noise std: 0.73
                       Mean reward: -71.32
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 12.72s
                        Total time: 24394.28s
                               ETA: 1250802.8s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.410s, learning 0.232s)
               Value function loss: 0.3824
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: -69.95
               Mean episode length: 300.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 12.64s
                        Total time: 24406.92s
                               ETA: 1250784.4s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.142s, learning 0.169s)
               Value function loss: 0.5822
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -69.93
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 12.31s
                        Total time: 24419.23s
                               ETA: 1250749.1s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.980s, learning 0.188s)
               Value function loss: 0.7776
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -75.03
               Mean episode length: 297.28
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 12.17s
                        Total time: 24431.40s
                               ETA: 1250706.5s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.808s, learning 0.180s)
               Value function loss: 0.7965
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: -74.83
               Mean episode length: 294.64
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 11.99s
                        Total time: 24443.39s
                               ETA: 1250654.7s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.323s, learning 0.199s)
               Value function loss: 0.9052
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: -75.70
               Mean episode length: 294.64
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 12.52s
                        Total time: 24455.91s
                               ETA: 1250630.2s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.143s, learning 0.312s)
               Value function loss: 1.5558
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -75.34
               Mean episode length: 294.64
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 12.46s
                        Total time: 24468.36s
                               ETA: 1250602.4s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.412s, learning 0.217s)
               Value function loss: 2.6704
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -78.41
               Mean episode length: 294.64
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 12.63s
                        Total time: 24480.99s
                               ETA: 1250583.5s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.567s, learning 0.292s)
               Value function loss: 1.3989
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: -75.43
               Mean episode length: 297.36
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 12.86s
                        Total time: 24493.85s
                               ETA: 1250576.3s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.073s, learning 0.190s)
               Value function loss: 0.8791
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: -74.58
               Mean episode length: 300.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 12.26s
                        Total time: 24506.11s
                               ETA: 1250538.6s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.415s, learning 0.241s)
               Value function loss: 0.7215
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: -74.36
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 12.66s
                        Total time: 24518.77s
                               ETA: 1250521.0s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.077s, learning 0.262s)
               Value function loss: 0.7965
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: -74.73
               Mean episode length: 297.30
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 12.34s
                        Total time: 24531.11s
                               ETA: 1250487.3s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.078s, learning 0.197s)
               Value function loss: 0.5719
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: -74.34
               Mean episode length: 297.30
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 12.27s
                        Total time: 24543.38s
                               ETA: 1250450.4s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.838s, learning 0.255s)
               Value function loss: 0.6279
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: -77.87
               Mean episode length: 297.30
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 12.09s
                        Total time: 24555.48s
                               ETA: 1250404.2s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.874s, learning 0.186s)
               Value function loss: 1.3649
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: -74.57
               Mean episode length: 297.30
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 12.06s
                        Total time: 24567.54s
                               ETA: 1250356.3s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.343s, learning 0.281s)
               Value function loss: 1.5394
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: -80.27
               Mean episode length: 294.98
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 12.62s
                        Total time: 24580.16s
                               ETA: 1250337.2s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.814s, learning 0.196s)
               Value function loss: 1.0585
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: -81.76
               Mean episode length: 293.31
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 12.01s
                        Total time: 24592.17s
                               ETA: 1250286.9s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.665s, learning 0.190s)
               Value function loss: 1.6248
                    Surrogate loss: 0.0261
             Mean action noise std: 0.73
                       Mean reward: -82.73
               Mean episode length: 293.31
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 11.86s
                        Total time: 24604.03s
                               ETA: 1250228.7s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.704s, learning 0.297s)
               Value function loss: 1.0788
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: -82.82
               Mean episode length: 296.01
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 12.00s
                        Total time: 24616.03s
                               ETA: 1250178.0s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.297s, learning 0.321s)
               Value function loss: 0.9225
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: -80.69
               Mean episode length: 296.01
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 12.62s
                        Total time: 24628.65s
                               ETA: 1250158.7s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.673s, learning 0.309s)
               Value function loss: 0.7886
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -84.43
               Mean episode length: 296.01
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 12.98s
                        Total time: 24641.63s
                               ETA: 1250157.9s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.667s, learning 0.254s)
               Value function loss: 1.0188
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -80.93
               Mean episode length: 296.01
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 12.92s
                        Total time: 24654.55s
                               ETA: 1250153.9s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.952s, learning 0.185s)
               Value function loss: 0.7684
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: -75.17
               Mean episode length: 296.01
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 12.14s
                        Total time: 24666.69s
                               ETA: 1250110.2s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.875s, learning 0.189s)
               Value function loss: 0.7038
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: -74.56
               Mean episode length: 296.01
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 12.06s
                        Total time: 24678.75s
                               ETA: 1250062.8s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.307s, learning 0.211s)
               Value function loss: 0.7617
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: -74.99
               Mean episode length: 298.33
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 12.52s
                        Total time: 24691.27s
                               ETA: 1250038.5s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1337 steps/s (collection: 11.955s, learning 0.295s)
               Value function loss: 1.1670
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: -74.17
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 12.25s
                        Total time: 24703.52s
                               ETA: 1250000.6s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.263s, learning 0.292s)
               Value function loss: 0.8305
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: -73.35
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 12.55s
                        Total time: 24716.07s
                               ETA: 1249978.1s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.991s, learning 0.196s)
               Value function loss: 0.8382
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: -73.68
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 12.19s
                        Total time: 24728.26s
                               ETA: 1249937.1s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.000s, learning 0.201s)
               Value function loss: 1.2557
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -73.28
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 12.20s
                        Total time: 24740.46s
                               ETA: 1249896.8s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.835s, learning 0.173s)
               Value function loss: 1.0244
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: -73.12
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 12.01s
                        Total time: 24752.47s
                               ETA: 1249846.7s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.152s, learning 0.208s)
               Value function loss: 1.1820
                    Surrogate loss: 0.0096
             Mean action noise std: 0.73
                       Mean reward: -69.12
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 12.36s
                        Total time: 24764.83s
                               ETA: 1249814.5s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.943s, learning 0.176s)
               Value function loss: 1.4826
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: -69.62
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 12.12s
                        Total time: 24776.95s
                               ETA: 1249770.2s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.770s, learning 0.279s)
               Value function loss: 1.6612
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -71.53
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 12.05s
                        Total time: 24789.00s
                               ETA: 1249722.4s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.906s, learning 0.202s)
               Value function loss: 2.2861
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: -69.48
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 12.11s
                        Total time: 24801.11s
                               ETA: 1249677.6s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.802s, learning 0.188s)
               Value function loss: 2.3947
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: -69.09
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 11.99s
                        Total time: 24813.10s
                               ETA: 1249626.8s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.034s, learning 0.302s)
               Value function loss: 2.8967
                    Surrogate loss: 0.0047
             Mean action noise std: 0.73
                       Mean reward: -73.15
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 12.34s
                        Total time: 24825.43s
                               ETA: 1249593.5s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.404s, learning 0.279s)
               Value function loss: 2.4384
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: -72.33
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 12.68s
                        Total time: 24838.12s
                               ETA: 1249577.7s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.668s, learning 0.225s)
               Value function loss: 65.8769
                    Surrogate loss: 0.0186
             Mean action noise std: 0.73
                       Mean reward: -65.42
               Mean episode length: 300.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 11.89s
                        Total time: 24850.01s
                               ETA: 1249522.2s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.067s, learning 0.214s)
               Value function loss: 0.4908
                    Surrogate loss: 0.0069
             Mean action noise std: 0.73
                       Mean reward: -64.68
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 12.28s
                        Total time: 24862.29s
                               ETA: 1249486.2s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.208s, learning 0.217s)
               Value function loss: 0.5860
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: -67.75
               Mean episode length: 300.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 12.43s
                        Total time: 24874.72s
                               ETA: 1249457.5s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.971s, learning 0.181s)
               Value function loss: 0.5290
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: -70.50
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 12.15s
                        Total time: 24886.87s
                               ETA: 1249415.0s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.954s, learning 0.178s)
               Value function loss: 0.7573
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: -68.10
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 12.13s
                        Total time: 24899.00s
                               ETA: 1249371.6s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.925s, learning 0.295s)
               Value function loss: 1.1561
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: -69.43
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 12.22s
                        Total time: 24911.22s
                               ETA: 1249332.7s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.990s, learning 0.167s)
               Value function loss: 0.9165
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: -67.37
               Mean episode length: 300.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 12.16s
                        Total time: 24923.38s
                               ETA: 1249290.6s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.266s, learning 0.234s)
               Value function loss: 1.8626
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: -68.05
               Mean episode length: 300.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 12.50s
                        Total time: 24935.88s
                               ETA: 1249265.7s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.315s, learning 0.335s)
               Value function loss: 1.5036
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -61.46
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 12.65s
                        Total time: 24948.53s
                               ETA: 1249248.4s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.529s, learning 0.179s)
               Value function loss: 0.8531
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: -63.99
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 12.71s
                        Total time: 24961.23s
                               ETA: 1249234.0s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.929s, learning 0.185s)
               Value function loss: 0.7408
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: -64.84
               Mean episode length: 297.75
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 12.11s
                        Total time: 24973.35s
                               ETA: 1249189.8s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.027s, learning 0.174s)
               Value function loss: 0.8554
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: -60.35
               Mean episode length: 297.75
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 12.20s
                        Total time: 24985.55s
                               ETA: 1249150.0s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.045s, learning 0.198s)
               Value function loss: 0.8099
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: -61.12
               Mean episode length: 295.31
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 12.24s
                        Total time: 24997.79s
                               ETA: 1249112.4s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.002s, learning 0.220s)
               Value function loss: 0.6555
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: -60.01
               Mean episode length: 295.31
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 12.22s
                        Total time: 25010.01s
                               ETA: 1249073.7s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.221s, learning 0.319s)
               Value function loss: 0.7165
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -61.59
               Mean episode length: 295.31
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 12.54s
                        Total time: 25022.55s
                               ETA: 1249050.9s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.146s, learning 0.275s)
               Value function loss: 1.3508
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: -59.45
               Mean episode length: 293.07
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 12.42s
                        Total time: 25034.97s
                               ETA: 1249022.2s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.180s, learning 0.304s)
               Value function loss: 0.9699
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: -57.18
               Mean episode length: 293.07
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 12.48s
                        Total time: 25047.46s
                               ETA: 1248996.7s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.497s, learning 0.255s)
               Value function loss: 0.8992
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: -54.43
               Mean episode length: 295.32
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 12.75s
                        Total time: 25060.21s
                               ETA: 1248984.6s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.531s, learning 0.297s)
               Value function loss: 0.8347
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -55.48
               Mean episode length: 295.32
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 12.83s
                        Total time: 25073.04s
                               ETA: 1248976.2s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.701s, learning 0.203s)
               Value function loss: 0.8617
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: -53.19
               Mean episode length: 297.76
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 12.90s
                        Total time: 25085.94s
                               ETA: 1248971.6s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.348s, learning 0.317s)
               Value function loss: 0.7787
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: -52.64
               Mean episode length: 297.76
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 12.66s
                        Total time: 25098.61s
                               ETA: 1248955.1s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.284s, learning 0.191s)
               Value function loss: 1.0122
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: -54.13
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 12.47s
                        Total time: 25111.08s
                               ETA: 1248929.1s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.620s, learning 0.254s)
               Value function loss: 0.9056
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: -52.84
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 12.87s
                        Total time: 25123.95s
                               ETA: 1248923.0s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.601s, learning 0.282s)
               Value function loss: 1.1074
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: -53.80
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 12.88s
                        Total time: 25136.84s
                               ETA: 1248917.3s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.271s, learning 0.187s)
               Value function loss: 1.0591
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: -52.01
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 12.46s
                        Total time: 25149.29s
                               ETA: 1248890.5s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.798s, learning 0.253s)
               Value function loss: 1.6416
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: -54.27
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 12.05s
                        Total time: 25161.35s
                               ETA: 1248843.6s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.214s, learning 0.237s)
               Value function loss: 1.2785
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -53.51
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 12.45s
                        Total time: 25173.80s
                               ETA: 1248816.5s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.961s, learning 0.210s)
               Value function loss: 1.4291
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: -53.00
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 12.17s
                        Total time: 25185.97s
                               ETA: 1248775.6s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.116s, learning 0.192s)
               Value function loss: 1.7191
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: -54.47
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 12.31s
                        Total time: 25198.28s
                               ETA: 1248741.4s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.613s, learning 0.206s)
               Value function loss: 1.8756
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: -55.42
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 11.82s
                        Total time: 25210.09s
                               ETA: 1248683.1s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.953s, learning 0.268s)
               Value function loss: 2.1530
                    Surrogate loss: 0.0196
             Mean action noise std: 0.73
                       Mean reward: -55.41
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 12.22s
                        Total time: 25222.32s
                               ETA: 1248644.7s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.270s, learning 0.306s)
               Value function loss: 2.1703
                    Surrogate loss: 0.0130
             Mean action noise std: 0.73
                       Mean reward: -54.85
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 12.58s
                        Total time: 25234.89s
                               ETA: 1248624.0s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.393s, learning 0.272s)
               Value function loss: 2.3439
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: -54.78
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 12.66s
                        Total time: 25247.56s
                               ETA: 1248607.5s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.822s, learning 0.242s)
               Value function loss: 2.6470
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: -54.81
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 12.06s
                        Total time: 25259.62s
                               ETA: 1248561.5s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.551s, learning 0.278s)
               Value function loss: 3.7996
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: -55.49
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 12.83s
                        Total time: 25272.45s
                               ETA: 1248553.2s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.189s, learning 0.198s)
               Value function loss: 5.6346
                    Surrogate loss: 0.0062
             Mean action noise std: 0.73
                       Mean reward: -57.57
               Mean episode length: 299.78
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 12.39s
                        Total time: 25284.84s
                               ETA: 1248523.2s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.772s, learning 0.168s)
               Value function loss: 8.9961
                    Surrogate loss: 0.0194
             Mean action noise std: 0.73
                       Mean reward: -58.38
               Mean episode length: 299.78
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 11.94s
                        Total time: 25296.78s
                               ETA: 1248471.1s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.854s, learning 0.224s)
               Value function loss: 3.2297
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: -59.83
               Mean episode length: 299.78
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 12.08s
                        Total time: 25308.85s
                               ETA: 1248425.8s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.111s, learning 0.267s)
               Value function loss: 53.8852
                    Surrogate loss: 0.0475
             Mean action noise std: 0.73
                       Mean reward: -72.29
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 12.38s
                        Total time: 25321.23s
                               ETA: 1248395.3s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.629s, learning 0.199s)
               Value function loss: 0.6059
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: -73.29
               Mean episode length: 300.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 12.83s
                        Total time: 25334.06s
                               ETA: 1248387.1s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.915s, learning 0.171s)
               Value function loss: 0.5989
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: -74.37
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 12.09s
                        Total time: 25346.15s
                               ETA: 1248342.3s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.086s, learning 0.194s)
               Value function loss: 0.8539
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: -74.24
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 12.28s
                        Total time: 25358.43s
                               ETA: 1248307.0s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.871s, learning 0.206s)
               Value function loss: 0.8355
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: -72.97
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 12.08s
                        Total time: 25370.50s
                               ETA: 1248261.9s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.473s, learning 0.173s)
               Value function loss: 0.9359
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: -71.55
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 11.65s
                        Total time: 25382.15s
                               ETA: 1248195.5s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.496s, learning 0.219s)
               Value function loss: 1.2677
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: -71.51
               Mean episode length: 300.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 12.71s
                        Total time: 25394.86s
                               ETA: 1248181.8s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.396s, learning 0.213s)
               Value function loss: 2.1492
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: -72.67
               Mean episode length: 300.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 12.61s
                        Total time: 25407.47s
                               ETA: 1248162.8s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.301s, learning 0.184s)
               Value function loss: 1.3289
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: -77.02
               Mean episode length: 297.62
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 12.49s
                        Total time: 25419.96s
                               ETA: 1248137.8s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.013s, learning 0.184s)
               Value function loss: 0.9156
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: -79.47
               Mean episode length: 297.62
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 12.20s
                        Total time: 25432.16s
                               ETA: 1248098.7s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.025s, learning 0.184s)
               Value function loss: 0.9987
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: -80.74
               Mean episode length: 297.62
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 12.21s
                        Total time: 25444.37s
                               ETA: 1248060.1s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.946s, learning 0.210s)
               Value function loss: 1.0184
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: -76.38
               Mean episode length: 297.62
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 12.16s
                        Total time: 25456.52s
                               ETA: 1248019.0s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.929s, learning 0.179s)
               Value function loss: 0.6059
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: -79.92
               Mean episode length: 297.62
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 12.11s
                        Total time: 25468.63s
                               ETA: 1247975.6s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.843s, learning 0.212s)
               Value function loss: 0.8249
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -82.08
               Mean episode length: 297.62
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 12.05s
                        Total time: 25480.68s
                               ETA: 1247929.6s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.317s, learning 0.275s)
               Value function loss: 1.2612
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: -81.10
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 12.59s
                        Total time: 25493.28s
                               ETA: 1247909.9s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.354s, learning 0.325s)
               Value function loss: 1.1146
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: -82.92
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 12.68s
                        Total time: 25505.96s
                               ETA: 1247894.5s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.333s, learning 0.168s)
               Value function loss: 0.9391
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: -83.62
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 12.50s
                        Total time: 25518.46s
                               ETA: 1247870.3s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.379s, learning 0.213s)
               Value function loss: 1.0378
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: -89.22
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 12.59s
                        Total time: 25531.05s
                               ETA: 1247850.7s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.417s, learning 0.179s)
               Value function loss: 0.8950
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: -92.86
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 12.60s
                        Total time: 25543.64s
                               ETA: 1247831.2s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.848s, learning 0.227s)
               Value function loss: 0.9800
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: -94.14
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 12.08s
                        Total time: 25555.72s
                               ETA: 1247786.3s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.918s, learning 0.307s)
               Value function loss: 1.0845
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: -95.83
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 12.23s
                        Total time: 25567.94s
                               ETA: 1247748.8s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.905s, learning 0.177s)
               Value function loss: 1.2570
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -100.70
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 12.08s
                        Total time: 25580.03s
                               ETA: 1247704.3s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.725s, learning 0.180s)
               Value function loss: 1.0901
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: -98.78
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 11.91s
                        Total time: 25591.93s
                               ETA: 1247651.3s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.853s, learning 0.201s)
               Value function loss: 1.1080
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: -95.38
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 12.05s
                        Total time: 25603.99s
                               ETA: 1247605.5s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.983s, learning 0.214s)
               Value function loss: 1.2346
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: -94.41
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 12.20s
                        Total time: 25616.18s
                               ETA: 1247566.7s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.345s, learning 0.185s)
               Value function loss: 1.6368
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: -88.95
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 12.53s
                        Total time: 25628.71s
                               ETA: 1247544.2s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.168s, learning 0.174s)
               Value function loss: 1.4143
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: -88.83
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 12.34s
                        Total time: 25641.06s
                               ETA: 1247512.5s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.979s, learning 0.176s)
               Value function loss: 1.6148
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: -87.76
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 12.15s
                        Total time: 25653.21s
                               ETA: 1247471.7s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.681s, learning 0.166s)
               Value function loss: 2.4122
                    Surrogate loss: 0.0080
             Mean action noise std: 0.73
                       Mean reward: -83.21
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 11.85s
                        Total time: 25665.06s
                               ETA: 1247416.0s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.260s, learning 0.177s)
               Value function loss: 1.5029
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: -83.11
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 12.44s
                        Total time: 25677.49s
                               ETA: 1247389.0s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.917s, learning 0.177s)
               Value function loss: 1.7343
                    Surrogate loss: 0.0092
             Mean action noise std: 0.73
                       Mean reward: -78.63
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 12.09s
                        Total time: 25689.59s
                               ETA: 1247345.4s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.006s, learning 0.186s)
               Value function loss: 1.9368
                    Surrogate loss: 0.0134
             Mean action noise std: 0.73
                       Mean reward: -76.96
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 12.19s
                        Total time: 25701.78s
                               ETA: 1247306.5s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.168s, learning 0.216s)
               Value function loss: 1.9756
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -75.52
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 12.38s
                        Total time: 25714.16s
                               ETA: 1247277.0s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.244s, learning 0.184s)
               Value function loss: 2.1468
                    Surrogate loss: 0.0109
             Mean action noise std: 0.73
                       Mean reward: -77.01
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 12.43s
                        Total time: 25726.59s
                               ETA: 1247249.7s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.725s, learning 0.194s)
               Value function loss: 2.3527
                    Surrogate loss: 0.0220
             Mean action noise std: 0.73
                       Mean reward: -81.13
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 11.92s
                        Total time: 25738.51s
                               ETA: 1247197.7s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.180s, learning 0.181s)
               Value function loss: 2.6312
                    Surrogate loss: 0.0177
             Mean action noise std: 0.73
                       Mean reward: -80.38
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 12.36s
                        Total time: 25750.87s
                               ETA: 1247167.1s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.096s, learning 0.252s)
               Value function loss: 2.7687
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: -79.00
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 12.35s
                        Total time: 25763.22s
                               ETA: 1247135.9s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.247s, learning 0.189s)
               Value function loss: 76.4501
                    Surrogate loss: 0.0368
             Mean action noise std: 0.73
                       Mean reward: -58.95
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 12.44s
                        Total time: 25775.66s
                               ETA: 1247109.0s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.249s, learning 0.337s)
               Value function loss: 0.5874
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: -58.42
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 12.59s
                        Total time: 25788.24s
                               ETA: 1247089.4s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.264s, learning 0.202s)
               Value function loss: 0.5296
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: -60.37
               Mean episode length: 297.17
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 12.47s
                        Total time: 25800.71s
                               ETA: 1247064.0s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.906s, learning 0.210s)
               Value function loss: 0.6730
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: -59.81
               Mean episode length: 297.17
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 12.12s
                        Total time: 25812.83s
                               ETA: 1247021.7s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.100s, learning 0.179s)
               Value function loss: 0.9960
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: -58.35
               Mean episode length: 297.17
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 12.28s
                        Total time: 25825.11s
                               ETA: 1246987.3s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.070s, learning 0.230s)
               Value function loss: 1.1605
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: -57.99
               Mean episode length: 294.53
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 12.30s
                        Total time: 25837.41s
                               ETA: 1246953.9s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.026s, learning 0.193s)
               Value function loss: 1.0669
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -57.39
               Mean episode length: 294.53
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 12.22s
                        Total time: 25849.62s
                               ETA: 1246916.7s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.139s, learning 0.277s)
               Value function loss: 2.3909
                    Surrogate loss: 0.0052
             Mean action noise std: 0.73
                       Mean reward: -64.49
               Mean episode length: 294.53
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 12.42s
                        Total time: 25862.04s
                               ETA: 1246888.9s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.046s, learning 0.186s)
               Value function loss: 1.6883
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: -68.89
               Mean episode length: 297.36
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 12.23s
                        Total time: 25874.27s
                               ETA: 1246852.3s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.948s, learning 0.203s)
               Value function loss: 1.1321
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: -69.15
               Mean episode length: 297.67
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 12.15s
                        Total time: 25886.42s
                               ETA: 1246811.8s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.940s, learning 0.223s)
               Value function loss: 0.8275
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: -69.24
               Mean episode length: 297.67
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 12.16s
                        Total time: 25898.59s
                               ETA: 1246772.0s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.616s, learning 0.308s)
               Value function loss: 1.0629
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -73.58
               Mean episode length: 297.67
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 12.92s
                        Total time: 25911.51s
                               ETA: 1246768.7s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.716s, learning 0.187s)
               Value function loss: 1.0348
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: -66.93
               Mean episode length: 297.67
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 11.90s
                        Total time: 25923.41s
                               ETA: 1246716.4s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.907s, learning 0.175s)
               Value function loss: 0.7289
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -69.40
               Mean episode length: 297.67
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 12.08s
                        Total time: 25935.50s
                               ETA: 1246672.7s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.224s, learning 0.174s)
               Value function loss: 0.8942
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: -68.88
               Mean episode length: 297.67
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 12.40s
                        Total time: 25947.89s
                               ETA: 1246644.2s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.782s, learning 0.195s)
               Value function loss: 1.4234
                    Surrogate loss: 0.0028
             Mean action noise std: 0.73
                       Mean reward: -63.44
               Mean episode length: 297.67
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 11.98s
                        Total time: 25959.87s
                               ETA: 1246595.5s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.313s, learning 0.247s)
               Value function loss: 1.1913
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -60.73
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 12.56s
                        Total time: 25972.43s
                               ETA: 1246574.9s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.000s, learning 0.220s)
               Value function loss: 0.9868
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: -62.13
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 12.22s
                        Total time: 25984.65s
                               ETA: 1246537.9s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.228s, learning 0.186s)
               Value function loss: 1.1764
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: -57.42
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 12.41s
                        Total time: 25997.06s
                               ETA: 1246510.2s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.306s, learning 0.200s)
               Value function loss: 1.0240
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: -56.09
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 12.51s
                        Total time: 26009.57s
                               ETA: 1246487.0s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.345s, learning 0.183s)
               Value function loss: 1.1787
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: -53.97
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 12.53s
                        Total time: 26022.10s
                               ETA: 1246464.9s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.538s, learning 0.261s)
               Value function loss: 1.3375
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: -56.06
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 12.80s
                        Total time: 26034.90s
                               ETA: 1246455.6s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.296s, learning 0.214s)
               Value function loss: 1.4210
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: -57.56
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 12.51s
                        Total time: 26047.41s
                               ETA: 1246432.7s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.045s, learning 0.214s)
               Value function loss: 1.5920
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: -59.03
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 12.26s
                        Total time: 26059.67s
                               ETA: 1246397.6s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.075s, learning 0.171s)
               Value function loss: 1.5531
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: -60.99
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 12.25s
                        Total time: 26071.91s
                               ETA: 1246362.1s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.117s, learning 0.190s)
               Value function loss: 2.1046
                    Surrogate loss: 0.0105
             Mean action noise std: 0.73
                       Mean reward: -63.67
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 12.31s
                        Total time: 26084.22s
                               ETA: 1246329.4s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.999s, learning 0.205s)
               Value function loss: 1.6869
                    Surrogate loss: 0.0069
             Mean action noise std: 0.73
                       Mean reward: -64.70
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 12.20s
                        Total time: 26096.42s
                               ETA: 1246291.8s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.777s, learning 0.184s)
               Value function loss: 1.6324
                    Surrogate loss: 0.0047
             Mean action noise std: 0.73
                       Mean reward: -63.01
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 11.96s
                        Total time: 26108.38s
                               ETA: 1246242.7s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.242s, learning 0.202s)
               Value function loss: 1.9366
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: -64.55
               Mean episode length: 297.64
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 12.44s
                        Total time: 26120.83s
                               ETA: 1246216.6s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.154s, learning 0.172s)
               Value function loss: 1.9765
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: -69.94
               Mean episode length: 297.64
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 12.33s
                        Total time: 26133.15s
                               ETA: 1246184.9s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.885s, learning 0.180s)
               Value function loss: 1.8425
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: -69.70
               Mean episode length: 297.64
                  Mean reward/step: -0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 12.06s
                        Total time: 26145.22s
                               ETA: 1246140.8s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.957s, learning 0.172s)
               Value function loss: 4.3147
                    Surrogate loss: 0.0255
             Mean action noise std: 0.73
                       Mean reward: -70.67
               Mean episode length: 297.64
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 12.13s
                        Total time: 26157.35s
                               ETA: 1246099.8s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.240s, learning 0.302s)
               Value function loss: 2.2487
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: -71.14
               Mean episode length: 297.64
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 12.54s
                        Total time: 26169.89s
                               ETA: 1246078.5s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.975s, learning 0.200s)
               Value function loss: 3.6585
                    Surrogate loss: 0.0077
             Mean action noise std: 0.73
                       Mean reward: -71.34
               Mean episode length: 297.64
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 12.17s
                        Total time: 26182.06s
                               ETA: 1246039.7s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.000s, learning 0.179s)
               Value function loss: 2.5063
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: -70.28
               Mean episode length: 297.64
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 12.18s
                        Total time: 26194.24s
                               ETA: 1246001.2s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.140s, learning 0.190s)
               Value function loss: 3.0618
                    Surrogate loss: 0.0263
             Mean action noise std: 0.73
                       Mean reward: -70.39
               Mean episode length: 297.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 12.33s
                        Total time: 26206.57s
                               ETA: 1245969.8s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.960s, learning 0.209s)
               Value function loss: 1.9827
                    Surrogate loss: 0.0123
             Mean action noise std: 0.73
                       Mean reward: -71.24
               Mean episode length: 297.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 12.17s
                        Total time: 26218.74s
                               ETA: 1245930.8s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.163s, learning 0.181s)
               Value function loss: 1.9729
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: -70.83
               Mean episode length: 297.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 12.34s
                        Total time: 26231.08s
                               ETA: 1245900.2s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.022s, learning 0.189s)
               Value function loss: 69.7187
                    Surrogate loss: 0.0456
             Mean action noise std: 0.73
                       Mean reward: -55.43
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 4.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 12.21s
                        Total time: 26243.30s
                               ETA: 1245863.2s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.945s, learning 0.215s)
               Value function loss: 0.4531
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: -58.38
               Mean episode length: 299.22
                  Mean reward/step: -1.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 12.16s
                        Total time: 26255.45s
                               ETA: 1245823.9s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.012s, learning 0.184s)
               Value function loss: 0.7398
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: -60.81
               Mean episode length: 299.22
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 12.20s
                        Total time: 26267.65s
                               ETA: 1245786.3s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.710s, learning 0.175s)
               Value function loss: 1.1918
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: -63.39
               Mean episode length: 299.22
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 11.89s
                        Total time: 26279.54s
                               ETA: 1245734.0s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.990s, learning 0.209s)
               Value function loss: 0.9714
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -67.38
               Mean episode length: 299.22
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 12.20s
                        Total time: 26291.74s
                               ETA: 1245696.6s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.104s, learning 0.216s)
               Value function loss: 1.0013
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -72.03
               Mean episode length: 299.22
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 12.32s
                        Total time: 26304.06s
                               ETA: 1245664.9s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.027s, learning 0.188s)
               Value function loss: 1.3110
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -72.31
               Mean episode length: 296.75
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 12.21s
                        Total time: 26316.27s
                               ETA: 1245628.3s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.643s, learning 0.254s)
               Value function loss: 3.1599
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: -69.04
               Mean episode length: 292.69
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 11.90s
                        Total time: 26328.17s
                               ETA: 1245576.7s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.710s, learning 0.192s)
               Value function loss: 1.8395
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: -66.33
               Mean episode length: 287.99
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 11.90s
                        Total time: 26340.07s
                               ETA: 1245525.3s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.806s, learning 0.186s)
               Value function loss: 1.2725
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: -65.00
               Mean episode length: 285.73
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 11.99s
                        Total time: 26352.06s
                               ETA: 1245478.3s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.780s, learning 0.220s)
               Value function loss: 0.9360
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: -66.00
               Mean episode length: 288.20
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 12.00s
                        Total time: 26364.06s
                               ETA: 1245431.6s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.910s, learning 0.205s)
               Value function loss: 1.1897
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: -68.34
               Mean episode length: 288.20
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 12.12s
                        Total time: 26376.18s
                               ETA: 1245390.5s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.126s, learning 0.176s)
               Value function loss: 1.0271
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: -65.29
               Mean episode length: 287.98
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 12.30s
                        Total time: 26388.48s
                               ETA: 1245358.2s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.872s, learning 0.201s)
               Value function loss: 0.9302
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: -67.79
               Mean episode length: 290.39
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 12.07s
                        Total time: 26400.55s
                               ETA: 1245315.1s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.856s, learning 0.171s)
               Value function loss: 1.7605
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: -69.82
               Mean episode length: 290.40
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 12.03s
                        Total time: 26412.58s
                               ETA: 1245269.8s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.380s, learning 0.205s)
               Value function loss: 1.7340
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: -63.10
               Mean episode length: 290.35
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 12.58s
                        Total time: 26425.16s
                               ETA: 1245250.9s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.182s)
               Value function loss: 1.0481
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -60.34
               Mean episode length: 292.61
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 12.11s
                        Total time: 26437.27s
                               ETA: 1245209.4s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.039s, learning 0.318s)
               Value function loss: 1.5826
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: -63.19
               Mean episode length: 292.61
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 12.36s
                        Total time: 26449.63s
                               ETA: 1245179.8s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.188s, learning 0.344s)
               Value function loss: 1.1740
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: -63.01
               Mean episode length: 292.61
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 12.53s
                        Total time: 26462.16s
                               ETA: 1245158.4s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.977s, learning 0.206s)
               Value function loss: 1.2937
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: -64.04
               Mean episode length: 295.26
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 12.18s
                        Total time: 26474.34s
                               ETA: 1245120.7s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.792s, learning 0.201s)
               Value function loss: 1.5473
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: -64.04
               Mean episode length: 295.26
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 11.99s
                        Total time: 26486.34s
                               ETA: 1245074.0s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.985s, learning 0.189s)
               Value function loss: 2.3584
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: -60.57
               Mean episode length: 297.63
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 12.17s
                        Total time: 26498.51s
                               ETA: 1245035.8s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.596s, learning 0.176s)
               Value function loss: 1.8430
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: -63.52
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 11.77s
                        Total time: 26510.28s
                               ETA: 1244978.8s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.944s, learning 0.166s)
               Value function loss: 1.9373
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: -70.26
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 12.11s
                        Total time: 26522.39s
                               ETA: 1244937.7s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.015s, learning 0.182s)
               Value function loss: 2.2000
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: -71.48
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 12.20s
                        Total time: 26534.59s
                               ETA: 1244900.7s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.284s, learning 0.282s)
               Value function loss: 2.3397
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -71.41
               Mean episode length: 297.56
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 12.57s
                        Total time: 26547.16s
                               ETA: 1244881.1s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.561s, learning 0.240s)
               Value function loss: 1.7272
                    Surrogate loss: 0.0369
             Mean action noise std: 0.73
                       Mean reward: -63.44
               Mean episode length: 297.56
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 12.80s
                        Total time: 26559.96s
                               ETA: 1244872.4s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.767s, learning 0.167s)
               Value function loss: 1.7953
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: -64.40
               Mean episode length: 297.56
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 11.93s
                        Total time: 26571.89s
                               ETA: 1244823.2s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.908s, learning 0.185s)
               Value function loss: 2.0713
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: -67.60
               Mean episode length: 297.56
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 12.09s
                        Total time: 26583.98s
                               ETA: 1244781.4s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.002s, learning 0.275s)
               Value function loss: 1.7912
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: -69.97
               Mean episode length: 297.56
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 12.28s
                        Total time: 26596.26s
                               ETA: 1244748.2s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.190s, learning 0.269s)
               Value function loss: 1.9270
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: -69.48
               Mean episode length: 295.14
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 12.46s
                        Total time: 26608.72s
                               ETA: 1244723.6s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.448s, learning 0.178s)
               Value function loss: 2.1307
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: -78.66
               Mean episode length: 295.14
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 12.63s
                        Total time: 26621.34s
                               ETA: 1244706.8s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.543s, learning 0.199s)
               Value function loss: 2.3653
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: -83.14
               Mean episode length: 295.14
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 12.74s
                        Total time: 26634.09s
                               ETA: 1244695.4s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.544s, learning 0.283s)
               Value function loss: 2.1838
                    Surrogate loss: 0.0839
             Mean action noise std: 0.73
                       Mean reward: -83.96
               Mean episode length: 295.14
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 12.83s
                        Total time: 26646.91s
                               ETA: 1244688.0s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.086s, learning 0.281s)
               Value function loss: 2.2931
                    Surrogate loss: 0.0132
             Mean action noise std: 0.73
                       Mean reward: -83.37
               Mean episode length: 295.14
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 12.37s
                        Total time: 26659.28s
                               ETA: 1244659.1s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.972s, learning 0.189s)
               Value function loss: 2.3356
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: -84.89
               Mean episode length: 295.14
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 12.16s
                        Total time: 26671.44s
                               ETA: 1244620.7s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.906s, learning 0.190s)
               Value function loss: 2.1593
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: -81.98
               Mean episode length: 295.14
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 12.10s
                        Total time: 26683.54s
                               ETA: 1244579.2s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.024s, learning 0.179s)
               Value function loss: 74.9990
                    Surrogate loss: 0.0539
             Mean action noise std: 0.73
                       Mean reward: -81.36
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 4.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 12.20s
                        Total time: 26695.74s
                               ETA: 1244542.7s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.065s, learning 0.260s)
               Value function loss: 0.5510
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: -82.30
               Mean episode length: 300.00
                  Mean reward/step: -1.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 12.33s
                        Total time: 26708.07s
                               ETA: 1244512.0s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.744s, learning 0.214s)
               Value function loss: 0.5906
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: -79.06
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 11.96s
                        Total time: 26720.02s
                               ETA: 1244464.2s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.822s, learning 0.277s)
               Value function loss: 0.8566
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: -79.82
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 12.10s
                        Total time: 26732.12s
                               ETA: 1244422.9s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1334 steps/s (collection: 11.977s, learning 0.298s)
               Value function loss: 1.0634
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: -81.51
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 12.27s
                        Total time: 26744.40s
                               ETA: 1244389.9s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.043s, learning 0.207s)
               Value function loss: 1.3619
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -89.93
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 12.25s
                        Total time: 26756.65s
                               ETA: 1244355.7s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.855s, learning 0.201s)
               Value function loss: 0.7675
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: -88.27
               Mean episode length: 300.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 12.06s
                        Total time: 26768.70s
                               ETA: 1244312.6s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.710s, learning 0.210s)
               Value function loss: 2.9168
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -87.71
               Mean episode length: 300.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 12.92s
                        Total time: 26781.62s
                               ETA: 1244309.6s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.010s, learning 0.171s)
               Value function loss: 2.8555
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -81.07
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 12.18s
                        Total time: 26793.81s
                               ETA: 1244272.3s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.141s, learning 0.186s)
               Value function loss: 1.5313
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: -74.63
               Mean episode length: 300.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 12.33s
                        Total time: 26806.13s
                               ETA: 1244241.8s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.016s, learning 0.170s)
               Value function loss: 1.0470
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: -72.60
               Mean episode length: 300.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 12.19s
                        Total time: 26818.32s
                               ETA: 1244204.8s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.821s, learning 0.195s)
               Value function loss: 1.1739
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: -75.13
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 12.02s
                        Total time: 26830.34s
                               ETA: 1244159.9s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.262s, learning 0.285s)
               Value function loss: 1.1137
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: -86.80
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 12.55s
                        Total time: 26842.88s
                               ETA: 1244139.7s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.332s, learning 0.170s)
               Value function loss: 0.8800
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: -88.16
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 12.50s
                        Total time: 26855.39s
                               ETA: 1244117.4s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.096s, learning 0.232s)
               Value function loss: 0.8321
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: -94.05
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 12.33s
                        Total time: 26867.71s
                               ETA: 1244087.0s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.948s, learning 0.183s)
               Value function loss: 1.4174
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: -100.96
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 12.13s
                        Total time: 26879.84s
                               ETA: 1244047.5s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.120s, learning 0.308s)
               Value function loss: 1.2987
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: -104.08
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 12.43s
                        Total time: 26892.27s
                               ETA: 1244021.8s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.950s, learning 0.291s)
               Value function loss: 1.1699
                    Surrogate loss: 0.0089
             Mean action noise std: 0.72
                       Mean reward: -105.62
               Mean episode length: 297.94
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 12.24s
                        Total time: 26904.51s
                               ETA: 1243987.5s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.331s, learning 0.187s)
               Value function loss: 1.1097
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: -98.70
               Mean episode length: 295.64
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 12.52s
                        Total time: 26917.03s
                               ETA: 1243965.9s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.371s, learning 0.238s)
               Value function loss: 0.8829
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: -97.19
               Mean episode length: 295.64
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 12.61s
                        Total time: 26929.64s
                               ETA: 1243948.6s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.453s, learning 0.296s)
               Value function loss: 0.9842
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: -92.63
               Mean episode length: 295.64
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 12.75s
                        Total time: 26942.39s
                               ETA: 1243937.8s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.472s, learning 0.181s)
               Value function loss: 1.1816
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: -95.30
               Mean episode length: 295.64
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 12.65s
                        Total time: 26955.04s
                               ETA: 1243922.5s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.681s, learning 0.215s)
               Value function loss: 0.9489
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: -90.75
               Mean episode length: 295.64
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 11.90s
                        Total time: 26966.94s
                               ETA: 1243872.3s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.020s, learning 0.304s)
               Value function loss: 1.0760
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: -93.33
               Mean episode length: 295.64
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 12.32s
                        Total time: 26979.26s
                               ETA: 1243841.8s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.453s, learning 0.185s)
               Value function loss: 0.9448
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: -91.87
               Mean episode length: 295.64
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 12.64s
                        Total time: 26991.90s
                               ETA: 1243825.9s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.581s, learning 0.314s)
               Value function loss: 1.5079
                    Surrogate loss: 0.0019
             Mean action noise std: 0.72
                       Mean reward: -87.57
               Mean episode length: 297.70
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 12.89s
                        Total time: 27004.79s
                               ETA: 1243821.8s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.442s, learning 0.284s)
               Value function loss: 1.0435
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: -93.67
               Mean episode length: 297.70
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 12.73s
                        Total time: 27017.52s
                               ETA: 1243809.9s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.673s, learning 0.196s)
               Value function loss: 0.9535
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: -93.19
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 12.87s
                        Total time: 27030.39s
                               ETA: 1243804.7s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.104s, learning 0.312s)
               Value function loss: 1.3707
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: -87.50
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 12.42s
                        Total time: 27042.81s
                               ETA: 1243778.5s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.619s, learning 0.300s)
               Value function loss: 1.6104
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: -87.04
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 12.92s
                        Total time: 27055.73s
                               ETA: 1243775.5s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.118s, learning 0.358s)
               Value function loss: 1.7981
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: -87.18
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 12.48s
                        Total time: 27068.20s
                               ETA: 1243752.1s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.841s, learning 0.319s)
               Value function loss: 2.6489
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: -82.81
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 12.16s
                        Total time: 27080.36s
                               ETA: 1243714.3s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.527s, learning 0.283s)
               Value function loss: 2.0881
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -81.91
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 12.81s
                        Total time: 27093.17s
                               ETA: 1243706.2s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.979s, learning 0.186s)
               Value function loss: 1.5789
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: -80.68
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 12.16s
                        Total time: 27105.34s
                               ETA: 1243668.6s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.456s, learning 0.291s)
               Value function loss: 1.6250
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -82.29
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 12.75s
                        Total time: 27118.08s
                               ETA: 1243657.7s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.306s, learning 0.184s)
               Value function loss: 1.5822
                    Surrogate loss: 0.0148
             Mean action noise std: 0.72
                       Mean reward: -81.80
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 12.49s
                        Total time: 27130.57s
                               ETA: 1243635.0s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.720s, learning 0.287s)
               Value function loss: 1.6209
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -79.24
               Mean episode length: 297.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 13.01s
                        Total time: 27143.58s
                               ETA: 1243636.0s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.162s, learning 0.188s)
               Value function loss: 1.4833
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: -79.54
               Mean episode length: 297.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 12.35s
                        Total time: 27155.93s
                               ETA: 1243607.0s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.600s, learning 0.176s)
               Value function loss: 71.8815
                    Surrogate loss: 0.0394
             Mean action noise std: 0.72
                       Mean reward: -59.71
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 4.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 11.78s
                        Total time: 27167.71s
                               ETA: 1243551.6s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.827s, learning 0.317s)
               Value function loss: 0.4671
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -58.41
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 12.14s
                        Total time: 27179.85s
                               ETA: 1243513.1s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.477s, learning 0.305s)
               Value function loss: 0.6141
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: -58.61
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 12.78s
                        Total time: 27192.63s
                               ETA: 1243503.9s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.461s, learning 0.335s)
               Value function loss: 1.1123
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: -67.51
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 12.80s
                        Total time: 27205.43s
                               ETA: 1243495.2s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.755s, learning 0.196s)
               Value function loss: 1.0886
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: -68.09
               Mean episode length: 297.35
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 11.95s
                        Total time: 27217.38s
                               ETA: 1243448.0s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.714s, learning 0.175s)
               Value function loss: 1.1036
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -70.16
               Mean episode length: 294.74
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 11.89s
                        Total time: 27229.27s
                               ETA: 1243398.0s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.197s, learning 0.292s)
               Value function loss: 1.3122
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -70.86
               Mean episode length: 292.25
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 12.49s
                        Total time: 27241.76s
                               ETA: 1243375.3s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.181s, learning 0.287s)
               Value function loss: 2.2964
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: -86.56
               Mean episode length: 284.94
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 12.47s
                        Total time: 27254.23s
                               ETA: 1243351.8s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.371s, learning 0.282s)
               Value function loss: 1.6519
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: -77.72
               Mean episode length: 284.86
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 12.65s
                        Total time: 27266.88s
                               ETA: 1243336.7s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.188s, learning 0.278s)
               Value function loss: 0.7997
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: -73.43
               Mean episode length: 287.47
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 12.47s
                        Total time: 27279.34s
                               ETA: 1243313.0s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.503s, learning 0.273s)
               Value function loss: 0.8438
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: -69.55
               Mean episode length: 292.42
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 11.78s
                        Total time: 27291.12s
                               ETA: 1243257.9s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.177s)
               Value function loss: 0.8002
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -64.90
               Mean episode length: 294.87
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 12.10s
                        Total time: 27303.22s
                               ETA: 1243217.8s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.605s, learning 0.254s)
               Value function loss: 0.6050
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: -63.47
               Mean episode length: 294.87
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 12.86s
                        Total time: 27316.08s
                               ETA: 1243212.1s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.279s, learning 0.231s)
               Value function loss: 0.5183
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -57.25
               Mean episode length: 294.87
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 12.51s
                        Total time: 27328.59s
                               ETA: 1243190.5s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.166s, learning 0.333s)
               Value function loss: 1.3579
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -55.77
               Mean episode length: 294.56
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 12.50s
                        Total time: 27341.09s
                               ETA: 1243168.4s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.967s, learning 0.180s)
               Value function loss: 1.3262
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: -54.88
               Mean episode length: 297.29
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 12.15s
                        Total time: 27353.24s
                               ETA: 1243130.3s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.128s, learning 0.176s)
               Value function loss: 0.7607
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: -59.55
               Mean episode length: 294.90
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 12.30s
                        Total time: 27365.54s
                               ETA: 1243099.4s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.363s, learning 0.250s)
               Value function loss: 1.1715
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -65.38
               Mean episode length: 294.90
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 12.61s
                        Total time: 27378.15s
                               ETA: 1243082.6s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.309s, learning 0.201s)
               Value function loss: 0.7412
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -65.95
               Mean episode length: 292.17
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 12.51s
                        Total time: 27390.66s
                               ETA: 1243061.0s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.134s, learning 0.319s)
               Value function loss: 0.7656
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: -65.27
               Mean episode length: 292.17
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 12.45s
                        Total time: 27403.12s
                               ETA: 1243036.9s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.880s, learning 0.231s)
               Value function loss: 0.5122
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: -63.89
               Mean episode length: 294.88
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 13.11s
                        Total time: 27416.23s
                               ETA: 1243042.7s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.227s, learning 0.282s)
               Value function loss: 1.0870
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: -66.42
               Mean episode length: 294.88
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 12.51s
                        Total time: 27428.74s
                               ETA: 1243021.1s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.112s, learning 0.334s)
               Value function loss: 0.6027
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: -66.35
               Mean episode length: 294.88
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 12.45s
                        Total time: 27441.18s
                               ETA: 1242996.7s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1334 steps/s (collection: 11.933s, learning 0.344s)
               Value function loss: 0.6334
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: -66.68
               Mean episode length: 294.88
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 12.28s
                        Total time: 27453.46s
                               ETA: 1242964.6s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.297s, learning 0.227s)
               Value function loss: 0.6600
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: -63.30
               Mean episode length: 297.27
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 12.52s
                        Total time: 27465.98s
                               ETA: 1242943.8s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.079s, learning 0.277s)
               Value function loss: 1.2174
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -57.22
               Mean episode length: 297.27
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 12.36s
                        Total time: 27478.34s
                               ETA: 1242915.3s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.814s, learning 0.231s)
               Value function loss: 0.5562
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -55.82
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 12.05s
                        Total time: 27490.39s
                               ETA: 1242872.9s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.174s)
               Value function loss: 0.5104
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: -54.04
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 12.22s
                        Total time: 27502.60s
                               ETA: 1242838.1s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.005s, learning 0.273s)
               Value function loss: 1.0618
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: -53.45
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 12.28s
                        Total time: 27514.88s
                               ETA: 1242806.2s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.742s, learning 0.235s)
               Value function loss: 0.5400
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: -57.92
               Mean episode length: 297.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 12.98s
                        Total time: 27527.86s
                               ETA: 1242805.8s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.322s, learning 0.415s)
               Value function loss: 0.6878
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: -57.73
               Mean episode length: 297.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 12.74s
                        Total time: 27540.59s
                               ETA: 1242794.6s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.719s, learning 0.178s)
               Value function loss: 0.9385
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: -57.88
               Mean episode length: 297.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 11.90s
                        Total time: 27552.49s
                               ETA: 1242745.5s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1333 steps/s (collection: 11.968s, learning 0.316s)
               Value function loss: 0.7765
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: -57.42
               Mean episode length: 297.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 12.28s
                        Total time: 27564.77s
                               ETA: 1242714.0s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.486s, learning 0.253s)
               Value function loss: 0.8315
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -57.41
               Mean episode length: 297.67
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 12.74s
                        Total time: 27577.51s
                               ETA: 1242702.9s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.120s, learning 0.314s)
               Value function loss: 0.7372
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: -57.01
               Mean episode length: 297.67
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 12.43s
                        Total time: 27589.95s
                               ETA: 1242678.1s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.208s, learning 0.194s)
               Value function loss: 0.9087
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -56.64
               Mean episode length: 297.67
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 12.40s
                        Total time: 27602.35s
                               ETA: 1242651.9s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.134s, learning 0.194s)
               Value function loss: 0.6436
                    Surrogate loss: 0.0040
             Mean action noise std: 0.72
                       Mean reward: -56.83
               Mean episode length: 297.67
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 12.33s
                        Total time: 27614.68s
                               ETA: 1242622.4s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.194s, learning 0.199s)
               Value function loss: 80.5343
                    Surrogate loss: 0.0296
             Mean action noise std: 0.72
                       Mean reward: -48.00
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 4.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 12.39s
                        Total time: 27627.07s
                               ETA: 1242595.7s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.720s, learning 0.183s)
               Value function loss: 0.4265
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: -48.67
               Mean episode length: 300.00
                  Mean reward/step: -1.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 11.90s
                        Total time: 27638.97s
                               ETA: 1242547.1s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.132s, learning 0.165s)
               Value function loss: 0.5022
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: -49.66
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 12.30s
                        Total time: 27651.27s
                               ETA: 1242516.2s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.196s, learning 0.282s)
               Value function loss: 0.8006
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -52.99
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 12.48s
                        Total time: 27663.75s
                               ETA: 1242493.5s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.362s, learning 0.276s)
               Value function loss: 1.0975
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: -55.94
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 12.64s
                        Total time: 27676.39s
                               ETA: 1242478.0s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.768s, learning 0.279s)
               Value function loss: 1.2306
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: -58.73
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 12.05s
                        Total time: 27688.43s
                               ETA: 1242435.9s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.147s, learning 0.201s)
               Value function loss: 0.7358
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: -59.07
               Mean episode length: 297.44
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 12.35s
                        Total time: 27700.78s
                               ETA: 1242407.4s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.850s, learning 0.177s)
               Value function loss: 2.4429
                    Surrogate loss: 0.0120
             Mean action noise std: 0.72
                       Mean reward: -64.63
               Mean episode length: 292.29
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 12.03s
                        Total time: 27712.81s
                               ETA: 1242364.4s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.672s, learning 0.205s)
               Value function loss: 2.3632
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: -63.52
               Mean episode length: 292.29
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.88s
                        Total time: 27724.68s
                               ETA: 1242314.8s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.191s, learning 0.253s)
               Value function loss: 1.0883
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: -62.87
               Mean episode length: 292.29
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 12.44s
                        Total time: 27737.13s
                               ETA: 1242290.6s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.701s, learning 0.213s)
               Value function loss: 0.8672
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -62.60
               Mean episode length: 294.85
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 11.91s
                        Total time: 27749.04s
                               ETA: 1242242.7s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.114s, learning 0.181s)
               Value function loss: 0.8334
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: -58.61
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 12.30s
                        Total time: 27761.34s
                               ETA: 1242211.9s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.798s, learning 0.197s)
               Value function loss: 0.9455
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: -58.69
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 12.00s
                        Total time: 27773.33s
                               ETA: 1242167.7s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.949s, learning 0.190s)
               Value function loss: 0.6437
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: -54.26
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 12.14s
                        Total time: 27785.47s
                               ETA: 1242130.0s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.829s, learning 0.276s)
               Value function loss: 0.7867
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: -56.95
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 12.11s
                        Total time: 27797.58s
                               ETA: 1242090.8s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.378s, learning 0.323s)
               Value function loss: 1.4692
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: -56.64
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 12.70s
                        Total time: 27810.28s
                               ETA: 1242078.2s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.504s, learning 0.306s)
               Value function loss: 1.5608
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: -62.52
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 12.81s
                        Total time: 27823.09s
                               ETA: 1242070.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1333 steps/s (collection: 11.988s, learning 0.297s)
               Value function loss: 0.9818
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: -60.77
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 12.28s
                        Total time: 27835.37s
                               ETA: 1242039.2s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.592s, learning 0.270s)
               Value function loss: 1.1173
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: -65.23
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 12.86s
                        Total time: 27848.23s
                               ETA: 1242033.8s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.378s, learning 0.319s)
               Value function loss: 1.1445
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: -79.87
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 12.70s
                        Total time: 27860.93s
                               ETA: 1242021.0s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.133s, learning 0.232s)
               Value function loss: 1.0203
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: -78.25
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 12.37s
                        Total time: 27873.30s
                               ETA: 1241993.5s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.504s, learning 0.290s)
               Value function loss: 1.2279
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -76.04
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 12.79s
                        Total time: 27886.09s
                               ETA: 1241985.0s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.263s, learning 0.272s)
               Value function loss: 1.1599
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -74.07
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 12.53s
                        Total time: 27898.63s
                               ETA: 1241965.0s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.534s, learning 0.207s)
               Value function loss: 1.3622
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -70.27
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 12.74s
                        Total time: 27911.37s
                               ETA: 1241954.2s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.855s, learning 0.181s)
               Value function loss: 1.3668
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -69.96
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 12.04s
                        Total time: 27923.40s
                               ETA: 1241912.1s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.711s, learning 0.198s)
               Value function loss: 1.9083
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: -74.16
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 11.91s
                        Total time: 27935.31s
                               ETA: 1241864.3s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.588s, learning 0.266s)
               Value function loss: 1.9523
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: -79.16
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 12.85s
                        Total time: 27948.17s
                               ETA: 1241858.5s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.310s, learning 0.292s)
               Value function loss: 2.4317
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: -76.45
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 12.60s
                        Total time: 27960.77s
                               ETA: 1241841.6s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.543s, learning 0.224s)
               Value function loss: 2.7633
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: -69.71
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 12.77s
                        Total time: 27973.53s
                               ETA: 1241831.9s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.403s, learning 0.310s)
               Value function loss: 2.7184
                    Surrogate loss: 0.0077
             Mean action noise std: 0.72
                       Mean reward: -67.63
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 12.71s
                        Total time: 27986.25s
                               ETA: 1241819.9s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.280s, learning 0.265s)
               Value function loss: 1.6401
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -74.44
               Mean episode length: 299.37
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 12.54s
                        Total time: 27998.79s
                               ETA: 1241800.4s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.078s, learning 0.329s)
               Value function loss: 1.8226
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: -74.95
               Mean episode length: 299.37
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 12.41s
                        Total time: 28011.20s
                               ETA: 1241774.8s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.426s, learning 0.228s)
               Value function loss: 1.7224
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: -82.06
               Mean episode length: 299.37
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 12.65s
                        Total time: 28023.85s
                               ETA: 1241760.2s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.350s, learning 0.362s)
               Value function loss: 1.8709
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: -84.26
               Mean episode length: 299.37
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 12.71s
                        Total time: 28036.56s
                               ETA: 1241748.1s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.412s, learning 0.259s)
               Value function loss: 2.7242
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: -84.66
               Mean episode length: 299.37
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 12.67s
                        Total time: 28049.24s
                               ETA: 1241734.2s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.223s, learning 0.293s)
               Value function loss: 5.3541
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -86.99
               Mean episode length: 299.37
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 12.52s
                        Total time: 28061.75s
                               ETA: 1241713.5s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.893s, learning 0.378s)
               Value function loss: 3.2085
                    Surrogate loss: 0.0332
             Mean action noise std: 0.72
                       Mean reward: -88.48
               Mean episode length: 299.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 12.27s
                        Total time: 28074.02s
                               ETA: 1241681.9s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.273s, learning 0.206s)
               Value function loss: 2.2857
                    Surrogate loss: 0.0179
             Mean action noise std: 0.72
                       Mean reward: -88.26
               Mean episode length: 299.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 12.48s
                        Total time: 28086.50s
                               ETA: 1241659.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.019s, learning 0.227s)
               Value function loss: 64.8072
                    Surrogate loss: 0.0398
             Mean action noise std: 0.72
                       Mean reward: -71.47
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 12.25s
                        Total time: 28098.75s
                               ETA: 1241626.9s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.478s, learning 0.225s)
               Value function loss: 0.4827
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: -71.96
               Mean episode length: 300.00
                  Mean reward/step: -0.99
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 12.70s
                        Total time: 28111.45s
                               ETA: 1241614.5s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.037s, learning 0.246s)
               Value function loss: 0.6832
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -72.26
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 12.28s
                        Total time: 28123.73s
                               ETA: 1241583.5s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.359s, learning 0.174s)
               Value function loss: 0.8582
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -75.56
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 12.53s
                        Total time: 28136.27s
                               ETA: 1241563.6s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.696s, learning 0.200s)
               Value function loss: 1.0066
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -73.43
               Mean episode length: 297.30
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.90s
                        Total time: 28148.16s
                               ETA: 1241515.6s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.292s, learning 0.265s)
               Value function loss: 0.9544
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: -78.21
               Mean episode length: 297.30
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 12.56s
                        Total time: 28160.72s
                               ETA: 1241496.7s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.210s, learning 0.304s)
               Value function loss: 1.3219
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: -76.34
               Mean episode length: 297.30
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 12.51s
                        Total time: 28173.23s
                               ETA: 1241476.0s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.160s, learning 0.172s)
               Value function loss: 2.6419
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: -62.34
               Mean episode length: 297.30
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 12.33s
                        Total time: 28185.57s
                               ETA: 1241447.2s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.453s, learning 0.199s)
               Value function loss: 1.6465
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -62.31
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 12.65s
                        Total time: 28198.22s
                               ETA: 1241432.6s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.400s, learning 0.276s)
               Value function loss: 0.9378
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: -60.61
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 12.68s
                        Total time: 28210.89s
                               ETA: 1241419.0s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1331 steps/s (collection: 11.988s, learning 0.313s)
               Value function loss: 0.8857
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: -58.23
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 12.30s
                        Total time: 28223.19s
                               ETA: 1241388.9s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.074s, learning 0.176s)
               Value function loss: 0.9477
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: -61.84
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 12.25s
                        Total time: 28235.44s
                               ETA: 1241356.6s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.952s, learning 0.288s)
               Value function loss: 0.8073
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: -62.86
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 12.24s
                        Total time: 28247.68s
                               ETA: 1241323.9s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.116s, learning 0.198s)
               Value function loss: 0.7659
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: -61.11
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 12.31s
                        Total time: 28260.00s
                               ETA: 1241294.4s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.083s, learning 0.267s)
               Value function loss: 1.3871
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: -67.15
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 12.35s
                        Total time: 28272.35s
                               ETA: 1241266.6s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.953s, learning 0.180s)
               Value function loss: 1.5415
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: -66.65
               Mean episode length: 297.60
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 12.13s
                        Total time: 28284.48s
                               ETA: 1241229.2s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.717s, learning 0.315s)
               Value function loss: 0.9762
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: -65.26
               Mean episode length: 297.60
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 13.03s
                        Total time: 28297.51s
                               ETA: 1241231.2s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.838s, learning 0.276s)
               Value function loss: 1.4562
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: -74.16
               Mean episode length: 297.60
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 12.11s
                        Total time: 28309.63s
                               ETA: 1241193.0s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.897s, learning 0.282s)
               Value function loss: 0.9966
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: -71.98
               Mean episode length: 297.60
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 12.18s
                        Total time: 28321.80s
                               ETA: 1241157.7s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.174s, learning 0.212s)
               Value function loss: 1.1421
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: -74.96
               Mean episode length: 296.13
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 12.39s
                        Total time: 28334.19s
                               ETA: 1241131.5s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.576s, learning 0.180s)
               Value function loss: 0.8975
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: -71.05
               Mean episode length: 296.13
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 11.76s
                        Total time: 28345.95s
                               ETA: 1241077.7s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.382s, learning 0.212s)
               Value function loss: 1.2981
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: -71.00
               Mean episode length: 296.13
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 12.59s
                        Total time: 28358.54s
                               ETA: 1241060.6s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.948s, learning 0.260s)
               Value function loss: 1.0582
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: -71.78
               Mean episode length: 296.13
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 12.21s
                        Total time: 28370.75s
                               ETA: 1241026.7s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.178s)
               Value function loss: 1.0801
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: -71.34
               Mean episode length: 298.53
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 12.10s
                        Total time: 28382.85s
                               ETA: 1240988.1s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.136s, learning 0.253s)
               Value function loss: 1.3924
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: -70.85
               Mean episode length: 298.53
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 12.39s
                        Total time: 28395.24s
                               ETA: 1240962.1s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.258s, learning 0.239s)
               Value function loss: 1.6885
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: -70.24
               Mean episode length: 298.53
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 12.50s
                        Total time: 28407.74s
                               ETA: 1240940.8s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.331s, learning 0.173s)
               Value function loss: 1.2603
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: -73.82
               Mean episode length: 298.53
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 12.50s
                        Total time: 28420.24s
                               ETA: 1240919.8s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.987s, learning 0.186s)
               Value function loss: 1.2109
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: -70.86
               Mean episode length: 298.53
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 12.17s
                        Total time: 28432.41s
                               ETA: 1240884.4s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.353s, learning 0.180s)
               Value function loss: 1.5903
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: -67.24
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 11.53s
                        Total time: 28443.95s
                               ETA: 1240821.1s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.337s, learning 0.176s)
               Value function loss: 1.0986
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: -66.02
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 12.51s
                        Total time: 28456.46s
                               ETA: 1240800.6s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1332 steps/s (collection: 11.968s, learning 0.329s)
               Value function loss: 1.3922
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: -67.57
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 12.30s
                        Total time: 28468.76s
                               ETA: 1240770.7s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.586s, learning 0.221s)
               Value function loss: 1.2983
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: -70.47
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 12.81s
                        Total time: 28481.56s
                               ETA: 1240763.0s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.143s, learning 0.337s)
               Value function loss: 1.2389
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: -67.91
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 12.48s
                        Total time: 28494.04s
                               ETA: 1240741.0s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.388s, learning 0.307s)
               Value function loss: 1.3681
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: -67.75
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 12.69s
                        Total time: 28506.74s
                               ETA: 1240728.4s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.362s, learning 0.170s)
               Value function loss: 1.3657
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: -66.81
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 12.53s
                        Total time: 28519.27s
                               ETA: 1240708.7s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.448s, learning 0.192s)
               Value function loss: 1.5328
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: -67.32
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 12.64s
                        Total time: 28531.91s
                               ETA: 1240693.8s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.953s, learning 0.224s)
               Value function loss: 1.6135
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: -70.67
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 12.18s
                        Total time: 28544.08s
                               ETA: 1240658.7s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.424s, learning 0.342s)
               Value function loss: 76.0228
                    Surrogate loss: 0.0173
             Mean action noise std: 0.72
                       Mean reward: -62.43
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 15.77s
                        Total time: 28559.85s
                               ETA: 1240779.6s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.096s, learning 0.209s)
               Value function loss: 0.5761
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -64.97
               Mean episode length: 300.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 23.30s
                        Total time: 28583.16s
                               ETA: 1241227.7s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.952s, learning 0.199s)
               Value function loss: 0.5682
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: -65.92
               Mean episode length: 298.66
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 24.15s
                        Total time: 28607.31s
                               ETA: 1241712.1s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.211s, learning 0.174s)
               Value function loss: 0.6886
                    Surrogate loss: -0.0122
             Mean action noise std: 0.72
                       Mean reward: -63.35
               Mean episode length: 296.76
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 23.39s
                        Total time: 28630.69s
                               ETA: 1242162.8s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.461s, learning 0.191s)
               Value function loss: 0.9973
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -61.66
               Mean episode length: 296.76
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 23.65s
                        Total time: 28654.34s
                               ETA: 1242624.7s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.844s, learning 0.223s)
               Value function loss: 1.1511
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: -62.05
               Mean episode length: 296.76
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 24.07s
                        Total time: 28678.41s
                               ETA: 1243104.2s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.608s, learning 0.200s)
               Value function loss: 0.7801
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: -62.00
               Mean episode length: 296.76
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 23.81s
                        Total time: 28702.22s
                               ETA: 1243572.0s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.540s, learning 0.193s)
               Value function loss: 2.3104
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: -67.28
               Mean episode length: 296.76
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 23.73s
                        Total time: 28725.95s
                               ETA: 1244036.1s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.124s, learning 0.173s)
               Value function loss: 1.9262
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: -64.44
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 23.30s
                        Total time: 28749.25s
                               ETA: 1244480.9s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.472s, learning 0.218s)
               Value function loss: 1.0553
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: -64.86
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 23.69s
                        Total time: 28772.94s
                               ETA: 1244942.3s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 682 steps/s (collection: 23.774s, learning 0.215s)
               Value function loss: 0.8402
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: -67.71
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 23.99s
                        Total time: 28796.93s
                               ETA: 1245416.2s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 702 steps/s (collection: 22.977s, learning 0.338s)
               Value function loss: 0.8254
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: -68.20
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 23.31s
                        Total time: 28820.24s
                               ETA: 1245860.5s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.582s, learning 0.244s)
               Value function loss: 0.8498
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: -70.81
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 23.83s
                        Total time: 28844.07s
                               ETA: 1246326.5s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 682 steps/s (collection: 23.787s, learning 0.209s)
               Value function loss: 0.6513
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: -70.39
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 24.00s
                        Total time: 28868.07s
                               ETA: 1246799.4s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 679 steps/s (collection: 23.915s, learning 0.187s)
               Value function loss: 0.7837
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: -72.10
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 24.10s
                        Total time: 28892.17s
                               ETA: 1247276.4s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.221s, learning 0.248s)
               Value function loss: 1.4353
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: -74.27
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 23.47s
                        Total time: 28915.64s
                               ETA: 1247725.7s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 676 steps/s (collection: 23.877s, learning 0.331s)
               Value function loss: 1.3679
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: -76.73
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 24.21s
                        Total time: 28939.85s
                               ETA: 1248206.5s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.701s, learning 0.197s)
               Value function loss: 1.1745
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: -76.14
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 23.90s
                        Total time: 28963.74s
                               ETA: 1248673.4s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.286s, learning 0.220s)
               Value function loss: 1.1865
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -74.79
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 23.51s
                        Total time: 28987.25s
                               ETA: 1249123.0s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.506s, learning 0.211s)
               Value function loss: 1.0945
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: -67.77
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 23.72s
                        Total time: 29010.97s
                               ETA: 1249581.2s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.536s, learning 0.197s)
               Value function loss: 1.0948
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: -67.11
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 23.73s
                        Total time: 29034.70s
                               ETA: 1250039.7s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.616s, learning 0.197s)
               Value function loss: 1.1622
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: -67.86
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 23.81s
                        Total time: 29058.51s
                               ETA: 1250501.3s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.605s, learning 0.185s)
               Value function loss: 1.0795
                    Surrogate loss: 0.0053
             Mean action noise std: 0.72
                       Mean reward: -66.86
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 23.79s
                        Total time: 29082.30s
                               ETA: 1250961.4s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.515s, learning 0.208s)
               Value function loss: 1.2255
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -64.87
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 23.72s
                        Total time: 29106.03s
                               ETA: 1251418.2s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 676 steps/s (collection: 23.933s, learning 0.291s)
               Value function loss: 1.1460
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: -64.41
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 24.22s
                        Total time: 29130.25s
                               ETA: 1251896.2s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.775s, learning 0.204s)
               Value function loss: 1.6951
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -67.89
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 23.98s
                        Total time: 29154.23s
                               ETA: 1252363.2s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.404s, learning 0.228s)
               Value function loss: 1.2570
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: -65.38
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 23.63s
                        Total time: 29177.86s
                               ETA: 1252814.8s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.205s, learning 0.211s)
               Value function loss: 1.2436
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -63.76
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 23.42s
                        Total time: 29201.28s
                               ETA: 1253256.7s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.844s, learning 0.182s)
               Value function loss: 1.3660
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: -71.65
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 24.03s
                        Total time: 29225.30s
                               ETA: 1253724.4s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.634s, learning 0.250s)
               Value function loss: 1.5339
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: -73.72
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 23.88s
                        Total time: 29249.19s
                               ETA: 1254185.6s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.840s, learning 0.197s)
               Value function loss: 1.4298
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: -75.86
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 24.04s
                        Total time: 29273.22s
                               ETA: 1254652.9s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.647s, learning 0.183s)
               Value function loss: 1.6983
                    Surrogate loss: 0.0311
             Mean action noise std: 0.72
                       Mean reward: -74.66
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 23.83s
                        Total time: 29297.05s
                               ETA: 1255111.0s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.474s, learning 0.211s)
               Value function loss: 1.2643
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: -75.19
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 23.69s
                        Total time: 29320.74s
                               ETA: 1255562.4s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.568s, learning 0.304s)
               Value function loss: 1.3368
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -76.76
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 23.87s
                        Total time: 29344.61s
                               ETA: 1256021.4s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.475s, learning 0.204s)
               Value function loss: 1.7281
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: -75.02
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 23.68s
                        Total time: 29368.29s
                               ETA: 1256471.7s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.850s, learning 0.216s)
               Value function loss: 1.9106
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -76.43
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 24.07s
                        Total time: 29392.36s
                               ETA: 1256938.1s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.376s, learning 0.211s)
               Value function loss: 2.0533
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: -76.11
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 23.59s
                        Total time: 29415.95s
                               ETA: 1257383.7s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.940s, learning 0.206s)
               Value function loss: 2.0229
                    Surrogate loss: 0.0092
             Mean action noise std: 0.72
                       Mean reward: -76.46
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 24.15s
                        Total time: 29440.09s
                               ETA: 1257852.7s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.243s, learning 0.179s)
               Value function loss: 65.9777
                    Surrogate loss: 0.0582
             Mean action noise std: 0.72
                       Mean reward: -56.65
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 16.42s
                        Total time: 29456.51s
                               ETA: 1257991.4s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.347s, learning 0.349s)
               Value function loss: 0.4189
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: -56.50
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 12.70s
                        Total time: 29469.21s
                               ETA: 1257970.9s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.251s, learning 0.277s)
               Value function loss: 0.6269
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: -57.62
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 12.53s
                        Total time: 29481.74s
                               ETA: 1257943.3s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.498s, learning 0.237s)
               Value function loss: 0.7407
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: -61.61
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 12.74s
                        Total time: 29494.47s
                               ETA: 1257924.5s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.794s, learning 0.231s)
               Value function loss: 0.9722
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -64.48
               Mean episode length: 300.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 12.03s
                        Total time: 29506.50s
                               ETA: 1257875.4s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.762s, learning 0.244s)
               Value function loss: 0.9207
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -68.57
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 12.01s
                        Total time: 29518.50s
                               ETA: 1257825.5s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.275s, learning 0.284s)
               Value function loss: 1.2332
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -73.72
               Mean episode length: 300.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 12.56s
                        Total time: 29531.06s
                               ETA: 1257799.2s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.359s, learning 0.254s)
               Value function loss: 2.3584
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: -76.79
               Mean episode length: 300.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 12.61s
                        Total time: 29543.67s
                               ETA: 1257775.3s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.950s, learning 0.321s)
               Value function loss: 1.3263
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -74.83
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 12.27s
                        Total time: 29555.95s
                               ETA: 1257736.8s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.263s, learning 0.223s)
               Value function loss: 0.8352
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: -70.35
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 12.49s
                        Total time: 29568.43s
                               ETA: 1257707.5s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.547s, learning 0.220s)
               Value function loss: 0.6706
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: -70.31
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 12.77s
                        Total time: 29581.20s
                               ETA: 1257690.1s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.160s, learning 0.215s)
               Value function loss: 0.7252
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -72.48
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 12.38s
                        Total time: 29593.57s
                               ETA: 1257656.1s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.254s, learning 0.189s)
               Value function loss: 0.5022
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: -74.55
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 12.44s
                        Total time: 29606.02s
                               ETA: 1257625.0s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.330s, learning 0.203s)
               Value function loss: 0.5692
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: -69.99
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 12.53s
                        Total time: 29618.55s
                               ETA: 1257597.8s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.818s, learning 0.189s)
               Value function loss: 1.1620
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: -75.18
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 12.01s
                        Total time: 29630.56s
                               ETA: 1257548.2s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.893s, learning 0.166s)
               Value function loss: 1.2884
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -72.29
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 12.06s
                        Total time: 29642.62s
                               ETA: 1257500.9s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.159s, learning 0.230s)
               Value function loss: 0.8222
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: -73.39
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 12.39s
                        Total time: 29655.01s
                               ETA: 1257467.5s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.357s, learning 0.194s)
               Value function loss: 1.2045
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -70.93
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 12.55s
                        Total time: 29667.56s
                               ETA: 1257441.1s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.464s, learning 0.213s)
               Value function loss: 0.7150
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: -67.08
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 12.68s
                        Total time: 29680.23s
                               ETA: 1257420.0s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.108s, learning 0.198s)
               Value function loss: 0.8419
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: -68.56
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 12.31s
                        Total time: 29692.54s
                               ETA: 1257383.2s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.942s, learning 0.180s)
               Value function loss: 0.8190
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: -64.27
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 12.12s
                        Total time: 29704.66s
                               ETA: 1257338.6s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.137s, learning 0.330s)
               Value function loss: 1.1346
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: -65.65
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 12.47s
                        Total time: 29717.13s
                               ETA: 1257308.7s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.197s, learning 0.178s)
               Value function loss: 0.8070
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: -64.19
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 12.38s
                        Total time: 29729.51s
                               ETA: 1257274.9s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.655s, learning 0.187s)
               Value function loss: 0.9240
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: -64.79
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 11.84s
                        Total time: 29741.35s
                               ETA: 1257218.6s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.003s, learning 0.177s)
               Value function loss: 0.9239
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: -66.89
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 12.18s
                        Total time: 29753.53s
                               ETA: 1257176.6s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.691s, learning 0.270s)
               Value function loss: 1.3879
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: -65.65
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 12.96s
                        Total time: 29766.49s
                               ETA: 1257167.6s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.854s, learning 0.376s)
               Value function loss: 0.8913
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: -65.68
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 12.23s
                        Total time: 29778.72s
                               ETA: 1257127.7s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.590s, learning 0.296s)
               Value function loss: 0.8402
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: -66.78
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 12.89s
                        Total time: 29791.60s
                               ETA: 1257115.5s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.300s, learning 0.280s)
               Value function loss: 1.3069
                    Surrogate loss: 0.0081
             Mean action noise std: 0.72
                       Mean reward: -65.98
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 12.58s
                        Total time: 29804.18s
                               ETA: 1257090.5s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1240 steps/s (collection: 12.956s, learning 0.252s)
               Value function loss: 1.0269
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: -64.49
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 13.21s
                        Total time: 29817.39s
                               ETA: 1257091.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.248s, learning 0.194s)
               Value function loss: 1.1475
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -64.55
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 12.44s
                        Total time: 29829.83s
                               ETA: 1257061.1s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.706s, learning 0.292s)
               Value function loss: 1.2581
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -64.06
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 13.00s
                        Total time: 29842.83s
                               ETA: 1257053.7s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.551s, learning 0.248s)
               Value function loss: 1.4220
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -64.07
               Mean episode length: 299.60
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 12.80s
                        Total time: 29855.63s
                               ETA: 1257037.9s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.426s, learning 0.306s)
               Value function loss: 1.9795
                    Surrogate loss: 0.0127
             Mean action noise std: 0.72
                       Mean reward: -63.30
               Mean episode length: 299.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 12.73s
                        Total time: 29868.36s
                               ETA: 1257019.3s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.432s, learning 0.171s)
               Value function loss: 1.2608
                    Surrogate loss: 0.0035
             Mean action noise std: 0.72
                       Mean reward: -61.82
               Mean episode length: 299.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 12.60s
                        Total time: 29880.97s
                               ETA: 1256995.3s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.086s, learning 0.220s)
               Value function loss: 1.2714
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: -62.12
               Mean episode length: 299.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 12.31s
                        Total time: 29893.27s
                               ETA: 1256958.7s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.693s, learning 0.295s)
               Value function loss: 1.1346
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: -63.71
               Mean episode length: 299.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.99s
                        Total time: 29905.26s
                               ETA: 1256908.9s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.721s, learning 0.191s)
               Value function loss: 75.1038
                    Surrogate loss: 0.0057
             Mean action noise std: 0.72
                       Mean reward: -55.49
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 12.91s
                        Total time: 29918.17s
                               ETA: 1256897.9s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.454s, learning 0.257s)
               Value function loss: 0.6548
                    Surrogate loss: 0.0153
             Mean action noise std: 0.72
                       Mean reward: -55.78
               Mean episode length: 300.00
                  Mean reward/step: -1.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 12.71s
                        Total time: 29930.88s
                               ETA: 1256878.4s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.462s, learning 0.390s)
               Value function loss: 0.4720
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -55.94
               Mean episode length: 300.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 12.85s
                        Total time: 29943.74s
                               ETA: 1256864.9s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.774s, learning 0.300s)
               Value function loss: 0.5069
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -57.89
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 13.07s
                        Total time: 29956.81s
                               ETA: 1256860.6s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.827s, learning 0.210s)
               Value function loss: 0.8250
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: -58.66
               Mean episode length: 300.00
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 12.04s
                        Total time: 29968.85s
                               ETA: 1256813.0s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.756s, learning 0.174s)
               Value function loss: 0.9928
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: -58.22
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 11.93s
                        Total time: 29980.78s
                               ETA: 1256760.8s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.838s, learning 0.212s)
               Value function loss: 1.3253
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -64.42
               Mean episode length: 300.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 12.05s
                        Total time: 29992.83s
                               ETA: 1256713.7s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.063s, learning 0.258s)
               Value function loss: 2.1600
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -72.33
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 12.32s
                        Total time: 30005.15s
                               ETA: 1256678.0s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.829s, learning 0.199s)
               Value function loss: 1.6933
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: -70.51
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 12.03s
                        Total time: 30017.18s
                               ETA: 1256630.0s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.126s, learning 0.195s)
               Value function loss: 0.7795
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -72.44
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 12.32s
                        Total time: 30029.50s
                               ETA: 1256594.3s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.828s, learning 0.180s)
               Value function loss: 0.6471
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: -64.82
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 12.01s
                        Total time: 30041.51s
                               ETA: 1256545.5s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.966s, learning 0.307s)
               Value function loss: 0.7503
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: -56.96
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 12.27s
                        Total time: 30053.78s
                               ETA: 1256507.9s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.885s, learning 0.194s)
               Value function loss: 0.6839
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: -57.81
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 12.08s
                        Total time: 30065.86s
                               ETA: 1256462.1s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.081s, learning 0.192s)
               Value function loss: 0.5051
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: -56.23
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 12.27s
                        Total time: 30078.13s
                               ETA: 1256424.5s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.846s, learning 0.284s)
               Value function loss: 0.5629
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: -54.89
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 12.13s
                        Total time: 30090.26s
                               ETA: 1256380.9s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.243s, learning 0.191s)
               Value function loss: 1.2863
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: -56.41
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 12.43s
                        Total time: 30102.69s
                               ETA: 1256350.1s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.051s, learning 0.186s)
               Value function loss: 1.2690
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -52.88
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 12.24s
                        Total time: 30114.93s
                               ETA: 1256311.0s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.305s, learning 0.358s)
               Value function loss: 1.0700
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: -55.13
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 12.66s
                        Total time: 30127.59s
                               ETA: 1256289.8s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.259s, learning 0.196s)
               Value function loss: 1.1606
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: -56.96
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 12.46s
                        Total time: 30140.05s
                               ETA: 1256259.9s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.898s, learning 0.178s)
               Value function loss: 1.0918
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: -59.37
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 12.08s
                        Total time: 30152.13s
                               ETA: 1256214.2s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.957s, learning 0.194s)
               Value function loss: 1.0503
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: -61.74
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 12.15s
                        Total time: 30164.28s
                               ETA: 1256171.7s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.099s, learning 0.304s)
               Value function loss: 1.0914
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: -64.46
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 12.40s
                        Total time: 30176.68s
                               ETA: 1256139.7s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.394s, learning 0.399s)
               Value function loss: 0.8387
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: -62.25
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 12.79s
                        Total time: 30189.47s
                               ETA: 1256123.9s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.278s, learning 0.168s)
               Value function loss: 0.9809
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -63.69
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 12.45s
                        Total time: 30201.92s
                               ETA: 1256093.7s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.999s, learning 0.219s)
               Value function loss: 0.9444
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: -65.35
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 12.22s
                        Total time: 30214.14s
                               ETA: 1256054.0s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.767s, learning 0.170s)
               Value function loss: 1.4535
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -66.92
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 11.94s
                        Total time: 30226.07s
                               ETA: 1256002.7s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.018s, learning 0.179s)
               Value function loss: 0.8870
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: -69.77
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 12.20s
                        Total time: 30238.27s
                               ETA: 1255962.2s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.808s, learning 0.185s)
               Value function loss: 0.8030
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: -69.79
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 11.99s
                        Total time: 30250.26s
                               ETA: 1255913.3s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.242s, learning 0.308s)
               Value function loss: 1.0526
                    Surrogate loss: 0.0080
             Mean action noise std: 0.72
                       Mean reward: -68.58
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 12.55s
                        Total time: 30262.81s
                               ETA: 1255887.5s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.635s, learning 0.345s)
               Value function loss: 1.1334
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: -67.39
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 12.98s
                        Total time: 30275.80s
                               ETA: 1255879.6s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.815s, learning 0.170s)
               Value function loss: 0.9247
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -68.36
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 11.98s
                        Total time: 30287.78s
                               ETA: 1255830.4s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.577s, learning 0.190s)
               Value function loss: 1.0210
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: -66.84
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 11.77s
                        Total time: 30299.55s
                               ETA: 1255772.2s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.204s, learning 0.219s)
               Value function loss: 0.8844
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: -66.14
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 12.42s
                        Total time: 30311.97s
                               ETA: 1255741.2s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.816s, learning 0.215s)
               Value function loss: 1.1083
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: -67.25
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 12.03s
                        Total time: 30324.00s
                               ETA: 1255694.0s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.614s, learning 0.194s)
               Value function loss: 1.1769
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -66.37
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 11.81s
                        Total time: 30335.81s
                               ETA: 1255637.6s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.770s, learning 0.182s)
               Value function loss: 1.4144
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -67.95
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 11.95s
                        Total time: 30347.76s
                               ETA: 1255587.2s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.528s, learning 0.268s)
               Value function loss: 1.5769
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: -66.29
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 11.80s
                        Total time: 30359.56s
                               ETA: 1255530.4s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.714s, learning 0.291s)
               Value function loss: 1.3845
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: -65.98
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 13.01s
                        Total time: 30372.56s
                               ETA: 1255523.6s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.569s, learning 0.294s)
               Value function loss: 65.1203
                    Surrogate loss: 0.0738
             Mean action noise std: 0.72
                       Mean reward: -60.18
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 12.86s
                        Total time: 30385.43s
                               ETA: 1255510.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.256s, learning 0.228s)
               Value function loss: 0.4370
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -60.06
               Mean episode length: 300.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 12.48s
                        Total time: 30397.91s
                               ETA: 1255482.6s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.736s, learning 0.178s)
               Value function loss: 0.6276
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: -58.94
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 11.91s
                        Total time: 30409.83s
                               ETA: 1255430.8s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.698s, learning 0.168s)
               Value function loss: 0.7866
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: -61.13
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 11.87s
                        Total time: 30421.69s
                               ETA: 1255376.9s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.891s, learning 0.252s)
               Value function loss: 0.9565
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: -64.55
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 12.14s
                        Total time: 30433.83s
                               ETA: 1255334.6s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.835s, learning 0.178s)
               Value function loss: 0.8364
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: -67.76
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 12.01s
                        Total time: 30445.85s
                               ETA: 1255286.9s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.439s, learning 0.198s)
               Value function loss: 1.1490
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: -68.15
               Mean episode length: 300.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 11.64s
                        Total time: 30457.49s
                               ETA: 1255223.8s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.272s, learning 0.209s)
               Value function loss: 1.9241
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -66.09
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 12.48s
                        Total time: 30469.97s
                               ETA: 1255195.5s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.157s, learning 0.218s)
               Value function loss: 1.2061
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: -63.29
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 12.38s
                        Total time: 30482.34s
                               ETA: 1255162.8s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.184s, learning 0.223s)
               Value function loss: 0.7854
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -60.38
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 12.41s
                        Total time: 30494.75s
                               ETA: 1255131.5s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.295s, learning 0.193s)
               Value function loss: 0.6812
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: -59.61
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 12.49s
                        Total time: 30507.24s
                               ETA: 1255103.4s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.312s, learning 0.208s)
               Value function loss: 0.7353
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -58.67
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 12.52s
                        Total time: 30519.76s
                               ETA: 1255076.8s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.171s, learning 0.216s)
               Value function loss: 0.5979
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: -59.18
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 12.39s
                        Total time: 30532.14s
                               ETA: 1255044.6s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1332 steps/s (collection: 11.999s, learning 0.298s)
               Value function loss: 0.6844
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -58.44
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 12.30s
                        Total time: 30544.44s
                               ETA: 1255008.8s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.181s, learning 0.275s)
               Value function loss: 1.2800
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: -53.90
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 12.46s
                        Total time: 30556.90s
                               ETA: 1254979.6s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.148s, learning 0.274s)
               Value function loss: 1.3386
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -51.41
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 12.42s
                        Total time: 30569.32s
                               ETA: 1254948.9s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.448s, learning 0.229s)
               Value function loss: 0.8103
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: -54.06
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 12.68s
                        Total time: 30582.00s
                               ETA: 1254928.8s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.180s, learning 0.348s)
               Value function loss: 1.1489
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: -53.35
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 12.53s
                        Total time: 30594.52s
                               ETA: 1254902.6s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.806s, learning 0.253s)
               Value function loss: 0.8996
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: -52.72
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 12.06s
                        Total time: 30606.58s
                               ETA: 1254857.1s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.884s, learning 0.219s)
               Value function loss: 1.0685
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -53.59
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 12.10s
                        Total time: 30618.69s
                               ETA: 1254813.4s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.123s, learning 0.241s)
               Value function loss: 1.0096
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: -53.39
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 12.36s
                        Total time: 30631.05s
                               ETA: 1254780.5s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.771s, learning 0.336s)
               Value function loss: 1.3588
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -60.04
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 12.11s
                        Total time: 30643.16s
                               ETA: 1254737.1s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.237s, learning 0.167s)
               Value function loss: 1.1147
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: -59.50
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 12.40s
                        Total time: 30655.56s
                               ETA: 1254705.8s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.148s, learning 0.312s)
               Value function loss: 1.1496
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: -60.35
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 12.46s
                        Total time: 30668.02s
                               ETA: 1254676.9s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.914s, learning 0.177s)
               Value function loss: 1.3686
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: -59.46
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 12.09s
                        Total time: 30680.11s
                               ETA: 1254632.8s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.128s, learning 0.174s)
               Value function loss: 1.7306
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: -59.71
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 12.30s
                        Total time: 30692.42s
                               ETA: 1254597.5s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.743s, learning 0.205s)
               Value function loss: 1.1697
                    Surrogate loss: 0.0037
             Mean action noise std: 0.72
                       Mean reward: -59.20
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 11.95s
                        Total time: 30704.36s
                               ETA: 1254547.6s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.993s, learning 0.178s)
               Value function loss: 0.9008
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: -59.68
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 12.17s
                        Total time: 30716.53s
                               ETA: 1254506.9s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.892s, learning 0.191s)
               Value function loss: 1.3783
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -59.16
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 12.08s
                        Total time: 30728.62s
                               ETA: 1254462.7s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.962s, learning 0.176s)
               Value function loss: 1.0302
                    Surrogate loss: 0.0048
             Mean action noise std: 0.72
                       Mean reward: -58.44
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 12.14s
                        Total time: 30740.75s
                               ETA: 1254420.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.070s, learning 0.199s)
               Value function loss: 1.8055
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -57.91
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 12.27s
                        Total time: 30753.02s
                               ETA: 1254384.1s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.087s, learning 0.182s)
               Value function loss: 5.1081
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: -56.12
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 12.27s
                        Total time: 30765.29s
                               ETA: 1254347.5s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.377s, learning 0.212s)
               Value function loss: 11.0585
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: -55.02
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 11.59s
                        Total time: 30776.88s
                               ETA: 1254283.2s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.210s, learning 0.354s)
               Value function loss: 12.9038
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -52.24
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 12.56s
                        Total time: 30789.45s
                               ETA: 1254258.7s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.066s, learning 0.171s)
               Value function loss: 17.3225
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: -53.10
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 12.24s
                        Total time: 30801.68s
                               ETA: 1254221.0s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.720s, learning 0.181s)
               Value function loss: 27.8526
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: -55.30
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 11.90s
                        Total time: 30813.59s
                               ETA: 1254169.5s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.694s, learning 0.180s)
               Value function loss: 19.2822
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: -55.89
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 11.87s
                        Total time: 30825.46s
                               ETA: 1254116.9s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.870s, learning 0.182s)
               Value function loss: 576.9640
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: -67.80
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 12.05s
                        Total time: 30837.51s
                               ETA: 1254071.7s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.701s, learning 0.185s)
               Value function loss: 0.6959
                    Surrogate loss: 0.0024
             Mean action noise std: 0.72
                       Mean reward: -68.82
               Mean episode length: 300.00
                  Mean reward/step: -1.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 11.89s
                        Total time: 30849.40s
                               ETA: 1254019.7s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.653s, learning 0.300s)
               Value function loss: 0.7511
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: -68.89
               Mean episode length: 294.34
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 11.95s
                        Total time: 30861.35s
                               ETA: 1253970.5s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.293s, learning 0.333s)
               Value function loss: 0.7376
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: -69.09
               Mean episode length: 294.34
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 12.63s
                        Total time: 30873.98s
                               ETA: 1253948.6s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.946s, learning 0.177s)
               Value function loss: 1.0857
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: -71.94
               Mean episode length: 294.34
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 12.12s
                        Total time: 30886.10s
                               ETA: 1253906.3s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.319s, learning 0.174s)
               Value function loss: 1.1304
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: -69.90
               Mean episode length: 294.34
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 12.49s
                        Total time: 30898.59s
                               ETA: 1253879.1s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.466s, learning 0.181s)
               Value function loss: 1.1459
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: -67.88
               Mean episode length: 294.34
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 11.65s
                        Total time: 30910.24s
                               ETA: 1253817.5s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.198s, learning 0.227s)
               Value function loss: 2.3075
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: -63.74
               Mean episode length: 294.34
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 12.42s
                        Total time: 30922.66s
                               ETA: 1253787.5s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.176s)
               Value function loss: 1.8938
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -62.03
               Mean episode length: 300.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 12.22s
                        Total time: 30934.88s
                               ETA: 1253749.1s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.735s, learning 0.185s)
               Value function loss: 0.8436
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -60.93
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 11.92s
                        Total time: 30946.80s
                               ETA: 1253698.7s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.147s, learning 0.257s)
               Value function loss: 0.6853
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: -60.96
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 12.40s
                        Total time: 30959.21s
                               ETA: 1253668.0s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.135s, learning 0.174s)
               Value function loss: 0.7128
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -59.09
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 12.31s
                        Total time: 30971.51s
                               ETA: 1253633.4s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.880s, learning 0.169s)
               Value function loss: 0.7616
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: -59.35
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 12.05s
                        Total time: 30983.56s
                               ETA: 1253588.3s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.617s, learning 0.177s)
               Value function loss: 0.5425
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -58.39
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 11.79s
                        Total time: 30995.36s
                               ETA: 1253532.9s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.765s, learning 0.169s)
               Value function loss: 0.5667
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: -57.95
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 11.93s
                        Total time: 31007.29s
                               ETA: 1253483.2s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.141s, learning 0.172s)
               Value function loss: 1.1693
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -54.93
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 12.31s
                        Total time: 31019.60s
                               ETA: 1253448.9s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.789s, learning 0.210s)
               Value function loss: 1.0848
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -54.17
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 12.00s
                        Total time: 31031.60s
                               ETA: 1253401.9s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.862s, learning 0.192s)
               Value function loss: 0.8516
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: -53.44
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 12.05s
                        Total time: 31043.66s
                               ETA: 1253357.1s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.943s, learning 0.168s)
               Value function loss: 0.9196
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: -53.58
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 12.11s
                        Total time: 31055.77s
                               ETA: 1253314.7s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.608s, learning 0.313s)
               Value function loss: 0.8003
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -52.70
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 11.92s
                        Total time: 31067.69s
                               ETA: 1253264.6s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.507s, learning 0.227s)
               Value function loss: 0.9152
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: -51.92
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 12.73s
                        Total time: 31080.42s
                               ETA: 1253247.4s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.843s, learning 0.236s)
               Value function loss: 1.0726
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: -52.15
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 12.08s
                        Total time: 31092.50s
                               ETA: 1253203.7s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.467s, learning 0.173s)
               Value function loss: 0.8675
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: -51.44
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 12.64s
                        Total time: 31105.14s
                               ETA: 1253182.7s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.921s, learning 0.182s)
               Value function loss: 0.9926
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: -49.59
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 12.10s
                        Total time: 31117.24s
                               ETA: 1253140.1s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.154s, learning 0.232s)
               Value function loss: 0.9020
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -49.48
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 12.39s
                        Total time: 31129.63s
                               ETA: 1253108.9s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.052s, learning 0.208s)
               Value function loss: 1.3468
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: -50.47
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 12.26s
                        Total time: 31141.89s
                               ETA: 1253072.6s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.140s, learning 0.176s)
               Value function loss: 0.8970
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: -50.00
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 12.32s
                        Total time: 31154.21s
                               ETA: 1253038.6s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.727s, learning 0.178s)
               Value function loss: 0.7956
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: -49.30
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 11.90s
                        Total time: 31166.11s
                               ETA: 1252988.1s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.460s, learning 0.283s)
               Value function loss: 1.0572
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: -50.43
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 12.74s
                        Total time: 31178.85s
                               ETA: 1252971.3s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.617s, learning 0.305s)
               Value function loss: 1.0742
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: -52.35
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 12.92s
                        Total time: 31191.77s
                               ETA: 1252961.7s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.593s, learning 0.265s)
               Value function loss: 1.0643
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: -52.46
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 12.86s
                        Total time: 31204.63s
                               ETA: 1252949.5s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.736s, learning 0.171s)
               Value function loss: 1.3231
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -51.36
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 11.91s
                        Total time: 31216.54s
                               ETA: 1252899.2s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.835s, learning 0.176s)
               Value function loss: 2.2534
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: -51.50
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 12.01s
                        Total time: 31228.55s
                               ETA: 1252853.0s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.801s, learning 0.162s)
               Value function loss: 2.0451
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -52.01
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 11.96s
                        Total time: 31240.51s
                               ETA: 1252805.0s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.180s, learning 0.308s)
               Value function loss: 1.0651
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: -53.44
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 12.49s
                        Total time: 31253.00s
                               ETA: 1252778.1s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.332s, learning 0.170s)
               Value function loss: 0.9930
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: -53.63
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 12.50s
                        Total time: 31265.50s
                               ETA: 1252751.6s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.135s, learning 0.215s)
               Value function loss: 0.9581
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -51.92
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 12.35s
                        Total time: 31277.86s
                               ETA: 1252719.2s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.782s, learning 0.189s)
               Value function loss: 1.0374
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: -51.54
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 11.97s
                        Total time: 31289.83s
                               ETA: 1252671.6s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.884s, learning 0.169s)
               Value function loss: 65.2132
                    Surrogate loss: 0.1038
             Mean action noise std: 0.72
                       Mean reward: -57.66
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 13.05s
                        Total time: 31302.88s
                               ETA: 1252667.3s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.378s, learning 0.220s)
               Value function loss: 0.3108
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -57.06
               Mean episode length: 300.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 12.60s
                        Total time: 31315.48s
                               ETA: 1252644.8s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.012s, learning 0.169s)
               Value function loss: 0.6358
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: -58.56
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 12.18s
                        Total time: 31327.66s
                               ETA: 1252605.6s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.912s, learning 0.191s)
               Value function loss: 0.6897
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: -58.99
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 12.10s
                        Total time: 31339.76s
                               ETA: 1252563.3s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.313s, learning 0.292s)
               Value function loss: 0.8944
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -60.11
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 12.61s
                        Total time: 31352.37s
                               ETA: 1252541.1s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.349s, learning 0.295s)
               Value function loss: 0.8771
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: -58.87
               Mean episode length: 300.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 12.64s
                        Total time: 31365.01s
                               ETA: 1252520.6s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.386s, learning 0.220s)
               Value function loss: 1.0057
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: -57.78
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 12.61s
                        Total time: 31377.62s
                               ETA: 1252498.4s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.027s, learning 0.173s)
               Value function loss: 1.9470
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: -54.69
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 12.20s
                        Total time: 31389.82s
                               ETA: 1252460.1s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.870s, learning 0.173s)
               Value function loss: 1.2065
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: -51.26
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 12.04s
                        Total time: 31401.86s
                               ETA: 1252415.5s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.188s, learning 0.167s)
               Value function loss: 0.7622
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -49.23
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 12.35s
                        Total time: 31414.21s
                               ETA: 1252383.4s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.481s, learning 0.173s)
               Value function loss: 0.5911
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: -50.17
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 12.65s
                        Total time: 31426.87s
                               ETA: 1252363.2s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.324s, learning 0.230s)
               Value function loss: 0.7219
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: -51.10
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 12.55s
                        Total time: 31439.42s
                               ETA: 1252339.0s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.056s, learning 0.181s)
               Value function loss: 0.5353
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -51.00
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 12.24s
                        Total time: 31451.66s
                               ETA: 1252302.3s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.107s, learning 0.339s)
               Value function loss: 0.6063
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: -50.08
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 12.45s
                        Total time: 31464.10s
                               ETA: 1252273.9s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.079s, learning 0.221s)
               Value function loss: 1.1775
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: -47.44
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 12.30s
                        Total time: 31476.40s
                               ETA: 1252239.7s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.198s, learning 0.193s)
               Value function loss: 1.1533
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -48.11
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 12.39s
                        Total time: 31488.80s
                               ETA: 1252209.1s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.959s, learning 0.178s)
               Value function loss: 0.8319
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -50.36
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 12.14s
                        Total time: 31500.93s
                               ETA: 1252168.5s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.886s, learning 0.183s)
               Value function loss: 1.1137
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -51.20
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 12.07s
                        Total time: 31513.00s
                               ETA: 1252125.2s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.123s, learning 0.180s)
               Value function loss: 0.7572
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: -50.72
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 12.30s
                        Total time: 31525.31s
                               ETA: 1252091.2s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.053s, learning 0.192s)
               Value function loss: 0.8857
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: -49.47
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 12.25s
                        Total time: 31537.55s
                               ETA: 1252054.9s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.187s, learning 0.226s)
               Value function loss: 0.7843
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -50.65
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 12.41s
                        Total time: 31549.96s
                               ETA: 1252025.3s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.108s, learning 0.191s)
               Value function loss: 1.2862
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: -50.89
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 12.30s
                        Total time: 31562.26s
                               ETA: 1251991.2s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.135s, learning 0.200s)
               Value function loss: 0.8510
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: -50.77
               Mean episode length: 300.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 12.33s
                        Total time: 31574.60s
                               ETA: 1251958.5s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.945s, learning 0.251s)
               Value function loss: 0.9610
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: -49.21
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 12.20s
                        Total time: 31586.79s
                               ETA: 1251920.3s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.225s, learning 0.170s)
               Value function loss: 1.0417
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -49.74
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 12.39s
                        Total time: 31599.19s
                               ETA: 1251890.0s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.004s, learning 0.169s)
               Value function loss: 1.3995
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: -50.13
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 12.17s
                        Total time: 31611.36s
                               ETA: 1251851.0s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.996s, learning 0.166s)
               Value function loss: 0.7909
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: -46.45
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 12.16s
                        Total time: 31623.52s
                               ETA: 1251811.5s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.378s, learning 0.252s)
               Value function loss: 0.7649
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: -45.64
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 12.63s
                        Total time: 31636.15s
                               ETA: 1251790.6s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.111s, learning 0.276s)
               Value function loss: 1.1767
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: -46.79
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 12.39s
                        Total time: 31648.54s
                               ETA: 1251760.1s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.001s, learning 0.188s)
               Value function loss: 0.7711
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: -48.17
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 12.19s
                        Total time: 31660.73s
                               ETA: 1251721.7s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.812s, learning 0.179s)
               Value function loss: 0.8575
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: -49.81
               Mean episode length: 297.67
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 11.99s
                        Total time: 31672.72s
                               ETA: 1251675.6s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.534s, learning 0.320s)
               Value function loss: 0.7785
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: -49.18
               Mean episode length: 297.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 12.85s
                        Total time: 31685.57s
                               ETA: 1251663.6s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.061s, learning 0.167s)
               Value function loss: 0.6917
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: -50.12
               Mean episode length: 297.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 12.23s
                        Total time: 31697.80s
                               ETA: 1251626.8s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.927s, learning 0.179s)
               Value function loss: 0.6640
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: -49.15
               Mean episode length: 297.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 12.11s
                        Total time: 31709.91s
                               ETA: 1251585.3s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.060s, learning 0.242s)
               Value function loss: 0.6898
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -49.04
               Mean episode length: 297.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 12.30s
                        Total time: 31722.21s
                               ETA: 1251551.5s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.088s, learning 0.171s)
               Value function loss: 0.8668
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -48.69
               Mean episode length: 297.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 12.26s
                        Total time: 31734.47s
                               ETA: 1251516.1s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.802s, learning 0.214s)
               Value function loss: 0.6969
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -49.84
               Mean episode length: 297.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 12.02s
                        Total time: 31746.48s
                               ETA: 1251471.1s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.290s, learning 0.194s)
               Value function loss: 68.3556
                    Surrogate loss: 0.0151
             Mean action noise std: 0.72
                       Mean reward: -48.01
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 12.48s
                        Total time: 31758.97s
                               ETA: 1251444.5s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.367s, learning 0.276s)
               Value function loss: 0.4057
                    Surrogate loss: 0.0020
             Mean action noise std: 0.72
                       Mean reward: -47.35
               Mean episode length: 300.00
                  Mean reward/step: -1.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 12.64s
                        Total time: 31771.61s
                               ETA: 1251424.2s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.040s, learning 0.167s)
               Value function loss: 0.4126
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -50.76
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 12.21s
                        Total time: 31783.82s
                               ETA: 1251386.8s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.008s, learning 0.172s)
               Value function loss: 0.5283
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: -49.28
               Mean episode length: 300.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 12.18s
                        Total time: 31796.00s
                               ETA: 1251348.3s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.931s, learning 0.167s)
               Value function loss: 0.8047
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: -48.37
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 12.10s
                        Total time: 31808.10s
                               ETA: 1251306.7s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.029s, learning 0.166s)
               Value function loss: 1.0308
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: -52.24
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 12.20s
                        Total time: 31820.29s
                               ETA: 1251268.8s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.138s, learning 0.296s)
               Value function loss: 0.7997
                    Surrogate loss: 0.0561
             Mean action noise std: 0.72
                       Mean reward: -53.50
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 12.43s
                        Total time: 31832.73s
                               ETA: 1251240.4s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1326 steps/s (collection: 11.974s, learning 0.373s)
               Value function loss: 1.4321
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -52.32
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 12.35s
                        Total time: 31845.07s
                               ETA: 1251208.5s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.746s, learning 0.166s)
               Value function loss: 1.5583
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -49.03
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 11.91s
                        Total time: 31856.98s
                               ETA: 1251159.6s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.726s, learning 0.215s)
               Value function loss: 0.8056
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: -46.53
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 11.94s
                        Total time: 31868.93s
                               ETA: 1251111.9s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.765s, learning 0.164s)
               Value function loss: 0.5874
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: -45.08
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 11.93s
                        Total time: 31880.85s
                               ETA: 1251063.7s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.882s, learning 0.168s)
               Value function loss: 0.6116
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -44.00
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 12.05s
                        Total time: 31892.90s
                               ETA: 1251020.3s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.129s, learning 0.219s)
               Value function loss: 0.7590
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: -42.57
               Mean episode length: 298.48
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 12.35s
                        Total time: 31905.25s
                               ETA: 1250988.6s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.318s, learning 0.195s)
               Value function loss: 0.4333
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -43.44
               Mean episode length: 298.48
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 12.51s
                        Total time: 31917.77s
                               ETA: 1250963.4s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.043s, learning 0.172s)
               Value function loss: 0.4398
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -43.05
               Mean episode length: 298.48
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 12.22s
                        Total time: 31929.98s
                               ETA: 1250926.6s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.789s, learning 0.185s)
               Value function loss: 1.1133
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: -42.17
               Mean episode length: 298.48
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 11.97s
                        Total time: 31941.95s
                               ETA: 1250880.3s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.165s, learning 0.241s)
               Value function loss: 0.9723
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: -41.95
               Mean episode length: 298.48
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 12.41s
                        Total time: 31954.36s
                               ETA: 1250850.9s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.839s, learning 0.168s)
               Value function loss: 0.7214
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: -41.59
               Mean episode length: 298.48
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 12.01s
                        Total time: 31966.37s
                               ETA: 1250806.0s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.173s, learning 0.179s)
               Value function loss: 0.7960
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -40.80
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 12.35s
                        Total time: 31978.72s
                               ETA: 1250774.5s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.828s, learning 0.169s)
               Value function loss: 0.6566
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -39.89
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 12.00s
                        Total time: 31990.71s
                               ETA: 1250729.2s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.823s, learning 0.186s)
               Value function loss: 0.6447
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -40.75
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 12.01s
                        Total time: 32002.72s
                               ETA: 1250684.4s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.736s, learning 0.177s)
               Value function loss: 0.8177
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -41.13
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 11.91s
                        Total time: 32014.64s
                               ETA: 1250635.9s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.918s, learning 0.234s)
               Value function loss: 0.6235
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: -41.46
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 12.15s
                        Total time: 32026.79s
                               ETA: 1250596.7s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.721s, learning 0.176s)
               Value function loss: 0.7409
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: -40.95
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 11.90s
                        Total time: 32038.68s
                               ETA: 1250547.6s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.867s, learning 0.173s)
               Value function loss: 0.6632
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: -42.32
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 12.04s
                        Total time: 32050.72s
                               ETA: 1250504.1s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.291s, learning 0.188s)
               Value function loss: 1.1247
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: -42.70
               Mean episode length: 300.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 12.48s
                        Total time: 32063.20s
                               ETA: 1250477.8s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.680s, learning 0.186s)
               Value function loss: 0.6558
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: -42.57
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 11.87s
                        Total time: 32075.07s
                               ETA: 1250427.5s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.048s, learning 0.183s)
               Value function loss: 0.6149
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: -43.01
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 12.23s
                        Total time: 32087.30s
                               ETA: 1250391.6s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.744s, learning 0.181s)
               Value function loss: 0.7641
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: -42.83
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 11.93s
                        Total time: 32099.23s
                               ETA: 1250343.7s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.227s, learning 0.172s)
               Value function loss: 0.8661
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: -41.35
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 12.40s
                        Total time: 32111.63s
                               ETA: 1250314.4s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.947s, learning 0.169s)
               Value function loss: 0.6292
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: -41.08
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 12.12s
                        Total time: 32123.74s
                               ETA: 1250274.0s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.016s, learning 0.272s)
               Value function loss: 0.7024
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -41.78
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 12.29s
                        Total time: 32136.03s
                               ETA: 1250240.3s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.427s, learning 0.264s)
               Value function loss: 0.5905
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: -40.65
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 12.69s
                        Total time: 32148.72s
                               ETA: 1250222.4s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.772s, learning 0.169s)
               Value function loss: 0.7497
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -41.30
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 11.94s
                        Total time: 32160.66s
                               ETA: 1250175.2s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.013s, learning 0.172s)
               Value function loss: 0.6978
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: -42.13
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 12.18s
                        Total time: 32172.85s
                               ETA: 1250137.6s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.869s, learning 0.168s)
               Value function loss: 0.7622
                    Surrogate loss: 0.0033
             Mean action noise std: 0.72
                       Mean reward: -42.18
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 12.04s
                        Total time: 32184.88s
                               ETA: 1250094.2s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.845s, learning 0.211s)
               Value function loss: 0.8129
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -40.69
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 12.06s
                        Total time: 32196.94s
                               ETA: 1250051.6s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.971s, learning 0.208s)
               Value function loss: 1.0061
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -40.59
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 12.18s
                        Total time: 32209.12s
                               ETA: 1250013.8s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.919s, learning 0.200s)
               Value function loss: 66.4490
                    Surrogate loss: 0.0452
             Mean action noise std: 0.72
                       Mean reward: -41.06
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 12.12s
                        Total time: 32221.24s
                               ETA: 1249973.7s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.276s, learning 0.194s)
               Value function loss: 0.4606
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -39.84
               Mean episode length: 300.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 12.47s
                        Total time: 32233.71s
                               ETA: 1249947.3s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.873s, learning 0.254s)
               Value function loss: 0.5891
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: -40.90
               Mean episode length: 300.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 12.13s
                        Total time: 32245.83s
                               ETA: 1249907.5s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.562s, learning 0.188s)
               Value function loss: 0.6897
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -39.88
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 11.75s
                        Total time: 32257.58s
                               ETA: 1249853.2s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.009s, learning 0.214s)
               Value function loss: 0.8375
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: -40.75
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 12.22s
                        Total time: 32269.81s
                               ETA: 1249817.2s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.965s, learning 0.178s)
               Value function loss: 1.1277
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: -41.29
               Mean episode length: 300.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 12.14s
                        Total time: 32281.95s
                               ETA: 1249778.2s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.670s, learning 0.193s)
               Value function loss: 0.6922
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: -41.19
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 11.86s
                        Total time: 32293.81s
                               ETA: 1249728.3s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.832s, learning 0.183s)
               Value function loss: 1.9647
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: -41.71
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 12.02s
                        Total time: 32305.83s
                               ETA: 1249684.3s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.799s, learning 0.199s)
               Value function loss: 1.2893
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -43.60
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 12.00s
                        Total time: 32317.83s
                               ETA: 1249639.7s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.448s, learning 0.225s)
               Value function loss: 0.7631
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -42.96
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 12.67s
                        Total time: 32330.50s
                               ETA: 1249621.2s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.839s, learning 0.330s)
               Value function loss: 0.6363
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -43.54
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 12.17s
                        Total time: 32342.67s
                               ETA: 1249583.3s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.948s, learning 0.264s)
               Value function loss: 0.6822
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: -42.40
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 12.21s
                        Total time: 32354.88s
                               ETA: 1249547.0s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.944s, learning 0.193s)
               Value function loss: 0.5682
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: -41.57
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 12.14s
                        Total time: 32367.02s
                               ETA: 1249507.8s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.914s, learning 0.172s)
               Value function loss: 0.5494
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: -40.71
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 12.09s
                        Total time: 32379.10s
                               ETA: 1249466.8s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.179s, learning 0.224s)
               Value function loss: 1.0920
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -42.50
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 12.40s
                        Total time: 32391.51s
                               ETA: 1249437.9s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.758s, learning 0.177s)
               Value function loss: 1.1859
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: -38.34
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 11.94s
                        Total time: 32403.44s
                               ETA: 1249391.1s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.186s, learning 0.186s)
               Value function loss: 0.8137
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: -38.51
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 12.37s
                        Total time: 32415.81s
                               ETA: 1249361.1s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.059s, learning 0.198s)
               Value function loss: 1.0864
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: -36.41
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 12.26s
                        Total time: 32428.07s
                               ETA: 1249326.7s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.042s, learning 0.228s)
               Value function loss: 0.6099
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: -36.46
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 12.27s
                        Total time: 32440.34s
                               ETA: 1249292.8s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.912s, learning 0.185s)
               Value function loss: 0.6627
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: -35.51
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 12.10s
                        Total time: 32452.44s
                               ETA: 1249252.3s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.031s, learning 0.179s)
               Value function loss: 0.6528
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -39.42
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 12.21s
                        Total time: 32464.65s
                               ETA: 1249216.1s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.611s, learning 0.219s)
               Value function loss: 0.8602
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: -38.74
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 11.83s
                        Total time: 32476.48s
                               ETA: 1249165.3s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.079s, learning 0.253s)
               Value function loss: 0.5280
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: -38.94
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 12.33s
                        Total time: 32488.81s
                               ETA: 1249133.9s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.827s, learning 0.214s)
               Value function loss: 0.5439
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: -40.45
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 12.04s
                        Total time: 32500.85s
                               ETA: 1249091.3s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.983s, learning 0.188s)
               Value function loss: 0.6454
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: -41.30
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 12.17s
                        Total time: 32513.02s
                               ETA: 1249053.7s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.958s, learning 0.184s)
               Value function loss: 1.2329
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: -39.22
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 12.14s
                        Total time: 32525.16s
                               ETA: 1249015.0s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.064s, learning 0.198s)
               Value function loss: 0.5974
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: -40.25
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 12.26s
                        Total time: 32537.43s
                               ETA: 1248980.9s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.587s, learning 0.201s)
               Value function loss: 0.5361
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: -39.71
               Mean episode length: 300.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 11.79s
                        Total time: 32549.21s
                               ETA: 1248928.7s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.901s, learning 0.178s)
               Value function loss: 0.9046
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: -37.86
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 12.08s
                        Total time: 32561.29s
                               ETA: 1248887.7s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.993s, learning 0.166s)
               Value function loss: 0.6822
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: -37.46
               Mean episode length: 300.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 12.16s
                        Total time: 32573.45s
                               ETA: 1248849.8s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.082s, learning 0.178s)
               Value function loss: 1.2054
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: -37.12
               Mean episode length: 300.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 12.26s
                        Total time: 32585.71s
                               ETA: 1248815.7s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.066s, learning 0.173s)
               Value function loss: 0.8358
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: -29.82
               Mean episode length: 300.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 12.24s
                        Total time: 32597.95s
                               ETA: 1248780.9s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.992s, learning 0.245s)
               Value function loss: 0.7396
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: -29.81
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 12.24s
                        Total time: 32610.19s
                               ETA: 1248746.0s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.021s, learning 0.187s)
               Value function loss: 0.8209
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: -31.83
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 12.21s
                        Total time: 32622.40s
                               ETA: 1248710.0s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.052s, learning 0.223s)
               Value function loss: 0.9650
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: -31.19
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 12.27s
                        Total time: 32634.67s
                               ETA: 1248676.6s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.119s, learning 0.198s)
               Value function loss: 1.3245
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: -29.59
               Mean episode length: 300.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 12.32s
                        Total time: 32646.99s
                               ETA: 1248644.9s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.736s, learning 0.185s)
               Value function loss: 1.3500
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: -28.71
               Mean episode length: 300.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 11.92s
                        Total time: 32658.91s
                               ETA: 1248598.0s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.955s, learning 0.201s)
               Value function loss: 69.3078
                    Surrogate loss: 0.0489
             Mean action noise std: 0.71
                       Mean reward: -23.13
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 12.16s
                        Total time: 32671.07s
                               ETA: 1248560.0s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.198s, learning 0.263s)
               Value function loss: 0.4141
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: -23.21
               Mean episode length: 300.00
                  Mean reward/step: -1.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 12.46s
                        Total time: 32683.53s
                               ETA: 1248533.8s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.668s, learning 0.175s)
               Value function loss: 0.4445
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: -25.47
               Mean episode length: 300.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 11.84s
                        Total time: 32695.37s
                               ETA: 1248484.0s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.046s, learning 0.187s)
               Value function loss: 0.5302
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: -25.50
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 12.23s
                        Total time: 32707.60s
                               ETA: 1248449.1s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.109s, learning 0.206s)
               Value function loss: 0.8929
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: -24.74
               Mean episode length: 300.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 12.32s
                        Total time: 32719.92s
                               ETA: 1248417.3s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.766s, learning 0.197s)
               Value function loss: 1.2457
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: -27.15
               Mean episode length: 300.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 11.96s
                        Total time: 32731.88s
                               ETA: 1248372.2s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.616s, learning 0.260s)
               Value function loss: 0.5650
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: -27.37
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 11.88s
                        Total time: 32743.76s
                               ETA: 1248323.7s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.266s, learning 0.175s)
               Value function loss: 1.6566
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: -28.44
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 12.44s
                        Total time: 32756.20s
                               ETA: 1248296.9s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.675s, learning 0.204s)
               Value function loss: 1.7312
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: -27.73
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 11.88s
                        Total time: 32768.08s
                               ETA: 1248248.5s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.168s, learning 0.275s)
               Value function loss: 0.8780
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: -26.81
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 12.44s
                        Total time: 32780.52s
                               ETA: 1248221.7s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.921s, learning 0.185s)
               Value function loss: 0.7849
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: -26.25
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 12.11s
                        Total time: 32792.63s
                               ETA: 1248182.1s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.849s, learning 0.263s)
               Value function loss: 0.7941
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: -25.91
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 12.11s
                        Total time: 32804.74s
                               ETA: 1248142.8s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.239s, learning 0.171s)
               Value function loss: 1.0412
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: -27.42
               Mean episode length: 297.92
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 12.41s
                        Total time: 32817.15s
                               ETA: 1248114.8s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.689s, learning 0.284s)
               Value function loss: 0.5717
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: -24.78
               Mean episode length: 297.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 11.97s
                        Total time: 32829.12s
                               ETA: 1248070.2s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.839s, learning 0.193s)
               Value function loss: 0.5512
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: -24.14
               Mean episode length: 297.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 12.03s
                        Total time: 32841.15s
                               ETA: 1248027.9s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.711s, learning 0.192s)
               Value function loss: 1.1200
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: -25.00
               Mean episode length: 297.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 11.90s
                        Total time: 32853.06s
                               ETA: 1247980.7s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.329s, learning 0.303s)
               Value function loss: 1.1135
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: -24.79
               Mean episode length: 297.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 12.63s
                        Total time: 32865.69s
                               ETA: 1247961.2s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.057s, learning 0.322s)
               Value function loss: 0.8481
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: -22.71
               Mean episode length: 297.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 12.38s
                        Total time: 32878.07s
                               ETA: 1247932.1s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.189s, learning 0.228s)
               Value function loss: 0.8741
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: -20.22
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 12.42s
                        Total time: 32890.49s
                               ETA: 1247904.5s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.051s, learning 0.287s)
               Value function loss: 0.6325
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: -21.19
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 12.34s
                        Total time: 32902.82s
                               ETA: 1247873.9s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.588s, learning 0.303s)
               Value function loss: 0.6865
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: -20.94
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 12.89s
                        Total time: 32915.72s
                               ETA: 1247864.2s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.173s, learning 0.182s)
               Value function loss: 0.8669
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: -22.05
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 12.36s
                        Total time: 32928.07s
                               ETA: 1247834.3s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.684s, learning 0.182s)
               Value function loss: 0.6276
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: -19.85
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 11.87s
                        Total time: 32939.94s
                               ETA: 1247785.8s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.947s, learning 0.181s)
               Value function loss: 0.6946
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: -19.31
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 12.13s
                        Total time: 32952.07s
                               ETA: 1247747.3s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.369s, learning 0.193s)
               Value function loss: 0.6412
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: -19.05
               Mean episode length: 300.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 12.56s
                        Total time: 32964.63s
                               ETA: 1247725.2s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.045s, learning 0.180s)
               Value function loss: 1.3961
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: -21.53
               Mean episode length: 300.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 12.22s
                        Total time: 32976.85s
                               ETA: 1247690.4s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.923s, learning 0.226s)
               Value function loss: 0.7158
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: -21.11
               Mean episode length: 300.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 12.15s
                        Total time: 32989.00s
                               ETA: 1247652.7s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.522s, learning 0.310s)
               Value function loss: 0.6887
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: -20.78
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 12.83s
                        Total time: 33001.83s
                               ETA: 1247640.9s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.119s, learning 0.188s)
               Value function loss: 1.0792
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: -18.24
               Mean episode length: 300.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 12.31s
                        Total time: 33014.14s
                               ETA: 1247609.2s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.035s, learning 0.179s)
               Value function loss: 1.1779
                    Surrogate loss: 0.0040
             Mean action noise std: 0.71
                       Mean reward: -19.93
               Mean episode length: 300.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 12.21s
                        Total time: 33026.35s
                               ETA: 1247574.1s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.123s, learning 0.294s)
               Value function loss: 0.7388
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: -21.19
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 12.42s
                        Total time: 33038.77s
                               ETA: 1247546.6s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.581s, learning 0.232s)
               Value function loss: 1.1224
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: -21.27
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 12.81s
                        Total time: 33051.58s
                               ETA: 1247534.0s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.249s, learning 0.272s)
               Value function loss: 1.2316
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: -20.25
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 12.52s
                        Total time: 33064.10s
                               ETA: 1247510.5s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.995s, learning 0.179s)
               Value function loss: 1.3972
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: -20.26
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 12.17s
                        Total time: 33076.28s
                               ETA: 1247473.8s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.045s, learning 0.301s)
               Value function loss: 1.9285
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: -19.71
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 12.35s
                        Total time: 33088.62s
                               ETA: 1247443.7s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.841s, learning 0.185s)
               Value function loss: 1.9660
                    Surrogate loss: 0.0072
             Mean action noise std: 0.71
                       Mean reward: -22.01
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 12.03s
                        Total time: 33100.65s
                               ETA: 1247401.5s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.046s, learning 0.221s)
               Value function loss: 1.9308
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: -21.43
               Mean episode length: 300.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 12.27s
                        Total time: 33112.92s
                               ETA: 1247368.5s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.151s, learning 0.215s)
               Value function loss: 2.2158
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: -21.50
               Mean episode length: 300.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 12.37s
                        Total time: 33125.28s
                               ETA: 1247339.1s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.729s, learning 0.275s)
               Value function loss: 78.8031
                    Surrogate loss: 0.0285
             Mean action noise std: 0.71
                       Mean reward: -16.50
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 12.00s
                        Total time: 33137.29s
                               ETA: 1247296.2s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.818s, learning 0.348s)
               Value function loss: 0.5468
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: -15.34
               Mean episode length: 300.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 12.17s
                        Total time: 33149.45s
                               ETA: 1247259.4s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.390s, learning 0.261s)
               Value function loss: 0.8019
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: -12.29
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 12.65s
                        Total time: 33162.11s
                               ETA: 1247240.9s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.322s, learning 0.299s)
               Value function loss: 0.9609
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: -8.50
               Mean episode length: 300.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 12.62s
                        Total time: 33174.73s
                               ETA: 1247221.2s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.088s, learning 0.358s)
               Value function loss: 1.1999
                    Surrogate loss: -0.0081
             Mean action noise std: 0.71
                       Mean reward: -8.55
               Mean episode length: 300.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 12.45s
                        Total time: 33187.17s
                               ETA: 1247195.0s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.386s, learning 0.181s)
               Value function loss: 1.6793
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: -6.82
               Mean episode length: 300.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 12.57s
                        Total time: 33199.74s
                               ETA: 1247173.2s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.180s, learning 0.191s)
               Value function loss: 0.9758
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: -5.74
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 12.37s
                        Total time: 33212.11s
                               ETA: 1247144.2s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.237s, learning 0.204s)
               Value function loss: 2.1697
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: -4.33
               Mean episode length: 300.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 12.44s
                        Total time: 33224.55s
                               ETA: 1247117.8s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.902s, learning 0.275s)
               Value function loss: 1.3987
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: -9.38
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 12.18s
                        Total time: 33236.73s
                               ETA: 1247081.5s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.271s, learning 0.220s)
               Value function loss: 0.8811
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: -7.78
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 12.49s
                        Total time: 33249.22s
                               ETA: 1247057.0s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.620s, learning 0.225s)
               Value function loss: 0.8143
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: -8.61
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 12.85s
                        Total time: 33262.06s
                               ETA: 1247045.8s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.868s, learning 0.184s)
               Value function loss: 0.9018
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: -11.25
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 12.05s
                        Total time: 33274.12s
                               ETA: 1247004.8s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.006s, learning 0.183s)
               Value function loss: 0.6601
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: -12.67
               Mean episode length: 300.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 12.19s
                        Total time: 33286.31s
                               ETA: 1246969.0s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.788s, learning 0.220s)
               Value function loss: 0.6827
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: -13.50
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 12.01s
                        Total time: 33298.31s
                               ETA: 1246926.5s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.454s, learning 0.306s)
               Value function loss: 1.4922
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: -14.64
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 12.76s
                        Total time: 33311.07s
                               ETA: 1246912.1s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.098s, learning 0.176s)
               Value function loss: 1.3339
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: -17.29
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 12.27s
                        Total time: 33323.35s
                               ETA: 1246879.6s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.970s, learning 0.185s)
               Value function loss: 0.8455
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: -17.22
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 12.16s
                        Total time: 33335.50s
                               ETA: 1246842.6s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.917s, learning 0.230s)
               Value function loss: 0.9936
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: -20.55
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 12.15s
                        Total time: 33347.65s
                               ETA: 1246805.3s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.468s, learning 0.261s)
               Value function loss: 0.7612
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: -18.06
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 11.73s
                        Total time: 33359.38s
                               ETA: 1246752.4s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.510s, learning 0.217s)
               Value function loss: 0.9048
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: -13.79
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 12.73s
                        Total time: 33372.11s
                               ETA: 1246736.8s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.612s, learning 0.205s)
               Value function loss: 0.7203
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: -14.51
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 12.82s
                        Total time: 33384.92s
                               ETA: 1246724.6s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.787s, learning 0.229s)
               Value function loss: 1.1199
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: -10.51
               Mean episode length: 300.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 12.02s
                        Total time: 33396.94s
                               ETA: 1246682.6s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.064s, learning 0.312s)
               Value function loss: 0.7652
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: -5.78
               Mean episode length: 300.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 12.38s
                        Total time: 33409.32s
                               ETA: 1246653.9s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.797s, learning 0.168s)
               Value function loss: 0.7442
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: -6.06
               Mean episode length: 300.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 11.96s
                        Total time: 33421.28s
                               ETA: 1246609.9s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.122s, learning 0.247s)
               Value function loss: 0.8681
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: -5.44
               Mean episode length: 300.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 12.37s
                        Total time: 33433.65s
                               ETA: 1246581.0s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.867s, learning 0.190s)
               Value function loss: 1.2589
                    Surrogate loss: 0.0129
             Mean action noise std: 0.71
                       Mean reward: -2.88
               Mean episode length: 300.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 13.06s
                        Total time: 33446.71s
                               ETA: 1246577.8s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.451s, learning 0.222s)
               Value function loss: 0.8195
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: -4.14
               Mean episode length: 300.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 12.67s
                        Total time: 33459.38s
                               ETA: 1246560.2s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.263s, learning 0.261s)
               Value function loss: 1.0166
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: -4.59
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 12.52s
                        Total time: 33471.90s
                               ETA: 1246537.1s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.172s)
               Value function loss: 1.5212
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: -5.75
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 11.97s
                        Total time: 33483.88s
                               ETA: 1246493.6s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.132s, learning 0.212s)
               Value function loss: 2.9537
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: -8.25
               Mean episode length: 300.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 12.34s
                        Total time: 33496.22s
                               ETA: 1246463.9s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.761s, learning 0.173s)
               Value function loss: 5.6460
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: -8.27
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 11.93s
                        Total time: 33508.16s
                               ETA: 1246418.9s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.421s, learning 0.275s)
               Value function loss: 1.8534
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: -8.23
               Mean episode length: 300.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 12.70s
                        Total time: 33520.85s
                               ETA: 1246402.3s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.038s, learning 0.169s)
               Value function loss: 1.6918
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: -9.47
               Mean episode length: 300.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 12.21s
                        Total time: 33533.06s
                               ETA: 1246367.5s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.925s, learning 0.182s)
               Value function loss: 1.6611
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: -9.53
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 12.11s
                        Total time: 33545.17s
                               ETA: 1246329.0s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.103s, learning 0.165s)
               Value function loss: 1.8700
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: -9.94
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 12.27s
                        Total time: 33557.43s
                               ETA: 1246296.4s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.370s, learning 0.282s)
               Value function loss: 2.3173
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: -7.07
               Mean episode length: 300.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 12.65s
                        Total time: 33570.08s
                               ETA: 1246278.2s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.254s, learning 0.199s)
               Value function loss: 2.3192
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: -8.05
               Mean episode length: 300.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 12.45s
                        Total time: 33582.54s
                               ETA: 1246252.6s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.084s, learning 0.184s)
               Value function loss: 76.7849
                    Surrogate loss: 0.0336
             Mean action noise std: 0.71
                       Mean reward: -9.14
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 12.27s
                        Total time: 33594.80s
                               ETA: 1246220.1s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.080s, learning 0.226s)
               Value function loss: 0.6241
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: -10.33
               Mean episode length: 300.00
                  Mean reward/step: -1.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 12.31s
                        Total time: 33607.11s
                               ETA: 1246189.1s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.871s, learning 0.275s)
               Value function loss: 0.5692
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: -9.53
               Mean episode length: 300.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 12.15s
                        Total time: 33619.26s
                               ETA: 1246152.1s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.116s, learning 0.173s)
               Value function loss: 0.8318
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: -8.28
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 12.29s
                        Total time: 33631.55s
                               ETA: 1246120.5s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.963s, learning 0.201s)
               Value function loss: 1.2095
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: -6.65
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 12.16s
                        Total time: 33643.71s
                               ETA: 1246084.2s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.567s, learning 0.194s)
               Value function loss: 1.6336
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: -8.03
               Mean episode length: 300.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 12.76s
                        Total time: 33656.47s
                               ETA: 1246070.1s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.044s, learning 0.178s)
               Value function loss: 1.3739
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: -7.99
               Mean episode length: 300.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 12.22s
                        Total time: 33668.69s
                               ETA: 1246036.0s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.032s, learning 0.210s)
               Value function loss: 2.0538
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: -11.92
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 12.24s
                        Total time: 33680.94s
                               ETA: 1246002.7s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.099s, learning 0.173s)
               Value function loss: 1.9547
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: -12.67
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 12.27s
                        Total time: 33693.21s
                               ETA: 1245970.5s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.175s, learning 0.207s)
               Value function loss: 1.2527
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: -8.30
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 12.38s
                        Total time: 33705.59s
                               ETA: 1245942.4s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.305s, learning 0.228s)
               Value function loss: 0.8696
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: -10.91
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 12.53s
                        Total time: 33718.12s
                               ETA: 1245919.8s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.347s, learning 0.226s)
               Value function loss: 0.9172
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: -10.53
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 12.57s
                        Total time: 33730.70s
                               ETA: 1245898.8s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.876s, learning 0.179s)
               Value function loss: 0.9800
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: -6.22
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 12.06s
                        Total time: 33742.75s
                               ETA: 1245858.6s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.185s, learning 0.205s)
               Value function loss: 0.7258
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: -5.17
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 12.39s
                        Total time: 33755.14s
                               ETA: 1245830.8s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.443s, learning 0.326s)
               Value function loss: 0.7650
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: -7.72
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 12.77s
                        Total time: 33767.91s
                               ETA: 1245817.0s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.331s, learning 0.206s)
               Value function loss: 1.4400
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: -2.35
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 12.54s
                        Total time: 33780.45s
                               ETA: 1245794.7s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.167s, learning 0.286s)
               Value function loss: 1.2088
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 12.45s
                        Total time: 33792.90s
                               ETA: 1245769.3s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.991s, learning 0.206s)
               Value function loss: 0.9729
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 5.17
               Mean episode length: 300.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 12.20s
                        Total time: 33805.10s
                               ETA: 1245734.4s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.913s, learning 0.225s)
               Value function loss: 1.1055
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 8.06
               Mean episode length: 300.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 12.14s
                        Total time: 33817.23s
                               ETA: 1245697.4s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.219s, learning 0.194s)
               Value function loss: 1.0356
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 9.65
               Mean episode length: 300.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 12.41s
                        Total time: 33829.65s
                               ETA: 1245670.6s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.987s, learning 0.195s)
               Value function loss: 1.0576
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 16.60
               Mean episode length: 300.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 12.18s
                        Total time: 33841.83s
                               ETA: 1245635.2s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.818s, learning 0.186s)
               Value function loss: 1.1740
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 18.13
               Mean episode length: 300.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 12.00s
                        Total time: 33853.83s
                               ETA: 1245593.3s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.519s, learning 0.273s)
               Value function loss: 0.8347
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 18.76
               Mean episode length: 300.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 12.79s
                        Total time: 33866.63s
                               ETA: 1245580.5s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.014s, learning 0.203s)
               Value function loss: 1.0880
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 18.18
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 12.22s
                        Total time: 33878.84s
                               ETA: 1245546.4s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.969s, learning 0.204s)
               Value function loss: 0.9862
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 19.08
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 12.17s
                        Total time: 33891.02s
                               ETA: 1245510.8s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.384s, learning 0.236s)
               Value function loss: 1.6319
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 24.08
               Mean episode length: 300.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 12.62s
                        Total time: 33903.64s
                               ETA: 1245491.7s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.496s, learning 0.197s)
               Value function loss: 0.8949
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 24.72
               Mean episode length: 300.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 11.69s
                        Total time: 33915.33s
                               ETA: 1245438.5s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.852s, learning 0.203s)
               Value function loss: 0.8364
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 25.42
               Mean episode length: 300.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 12.05s
                        Total time: 33927.38s
                               ETA: 1245398.6s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.761s, learning 0.168s)
               Value function loss: 1.1289
                    Surrogate loss: 0.0023
             Mean action noise std: 0.71
                       Mean reward: 24.45
               Mean episode length: 300.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 11.93s
                        Total time: 33939.31s
                               ETA: 1245354.0s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.080s, learning 0.186s)
               Value function loss: 1.2279
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 22.80
               Mean episode length: 300.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 12.27s
                        Total time: 33951.58s
                               ETA: 1245321.9s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.254s, learning 0.191s)
               Value function loss: 1.0494
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 21.89
               Mean episode length: 300.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 12.45s
                        Total time: 33964.02s
                               ETA: 1245296.4s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.862s, learning 0.203s)
               Value function loss: 1.4524
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 22.16
               Mean episode length: 300.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 12.07s
                        Total time: 33976.09s
                               ETA: 1245257.0s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.009s, learning 0.173s)
               Value function loss: 1.3658
                    Surrogate loss: 0.0112
             Mean action noise std: 0.71
                       Mean reward: 23.27
               Mean episode length: 300.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 12.18s
                        Total time: 33988.27s
                               ETA: 1245221.8s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.260s, learning 0.301s)
               Value function loss: 1.8162
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 25.84
               Mean episode length: 300.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 12.56s
                        Total time: 34000.83s
                               ETA: 1245200.6s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.912s, learning 0.194s)
               Value function loss: 2.2480
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 24.16
               Mean episode length: 300.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 12.11s
                        Total time: 34012.94s
                               ETA: 1245162.6s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.873s, learning 0.174s)
               Value function loss: 3.4597
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 24.61
               Mean episode length: 300.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 12.05s
                        Total time: 34024.99s
                               ETA: 1245122.6s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.297s, learning 0.319s)
               Value function loss: 5.5641
                    Surrogate loss: 0.0285
             Mean action noise std: 0.71
                       Mean reward: 23.94
               Mean episode length: 300.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 12.62s
                        Total time: 34037.60s
                               ETA: 1245103.4s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.356s, learning 0.306s)
               Value function loss: 3.9750
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 24.52
               Mean episode length: 300.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 12.66s
                        Total time: 34050.26s
                               ETA: 1245085.9s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.875s, learning 0.195s)
               Value function loss: 83.9459
                    Surrogate loss: 0.1367
             Mean action noise std: 0.71
                       Mean reward: 18.41
               Mean episode length: 300.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 12.07s
                        Total time: 34062.33s
                               ETA: 1245046.7s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.212s, learning 0.198s)
               Value function loss: 1.0227
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 17.92
               Mean episode length: 300.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 12.41s
                        Total time: 34074.74s
                               ETA: 1245020.0s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.701s, learning 0.196s)
               Value function loss: 0.8969
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 17.39
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 11.90s
                        Total time: 34086.64s
                               ETA: 1244974.5s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.782s, learning 0.175s)
               Value function loss: 1.0269
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 21.40
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 11.96s
                        Total time: 34098.60s
                               ETA: 1244931.3s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.033s, learning 0.213s)
               Value function loss: 1.8404
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 25.66
               Mean episode length: 300.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 12.25s
                        Total time: 34110.84s
                               ETA: 1244898.6s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.980s, learning 0.190s)
               Value function loss: 1.4545
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 21.03
               Mean episode length: 300.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 12.17s
                        Total time: 34123.01s
                               ETA: 1244863.2s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.179s)
               Value function loss: 1.8272
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 19.19
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 12.10s
                        Total time: 34135.11s
                               ETA: 1244825.3s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.988s, learning 0.211s)
               Value function loss: 2.9023
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: 25.14
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 12.20s
                        Total time: 34147.31s
                               ETA: 1244791.0s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.522s, learning 0.202s)
               Value function loss: 1.6938
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 16.48
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 12.72s
                        Total time: 34160.04s
                               ETA: 1244775.9s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.157s, learning 0.186s)
               Value function loss: 1.5127
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 25.29
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 12.34s
                        Total time: 34172.38s
                               ETA: 1244746.9s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.807s, learning 0.178s)
               Value function loss: 0.8853
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 23.27
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 11.99s
                        Total time: 34184.37s
                               ETA: 1244704.8s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.993s, learning 0.224s)
               Value function loss: 1.0486
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 21.57
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 12.22s
                        Total time: 34196.58s
                               ETA: 1244671.2s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.413s, learning 0.191s)
               Value function loss: 1.0896
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 23.90
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 12.60s
                        Total time: 34209.19s
                               ETA: 1244651.7s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.137s, learning 0.171s)
               Value function loss: 1.1232
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 12.31s
                        Total time: 34221.49s
                               ETA: 1244621.4s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.759s, learning 0.197s)
               Value function loss: 1.9289
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 30.28
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 11.96s
                        Total time: 34233.45s
                               ETA: 1244578.4s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.797s, learning 0.223s)
               Value function loss: 1.8421
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 33.37
               Mean episode length: 300.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 12.02s
                        Total time: 34245.47s
                               ETA: 1244537.7s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.863s, learning 0.301s)
               Value function loss: 1.9263
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 35.57
               Mean episode length: 300.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 12.16s
                        Total time: 34257.63s
                               ETA: 1244502.2s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.484s, learning 0.297s)
               Value function loss: 3.0411
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 34.04
               Mean episode length: 300.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 12.78s
                        Total time: 34270.42s
                               ETA: 1244489.2s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.760s, learning 0.256s)
               Value function loss: 5.5827
                    Surrogate loss: 0.0239
             Mean action noise std: 0.71
                       Mean reward: 39.83
               Mean episode length: 300.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 13.02s
                        Total time: 34283.43s
                               ETA: 1244484.7s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.286s, learning 0.207s)
               Value function loss: 1.6879
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 40.53
               Mean episode length: 300.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 12.49s
                        Total time: 34295.92s
                               ETA: 1244461.3s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.892s, learning 0.188s)
               Value function loss: 1.2131
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 43.80
               Mean episode length: 300.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 12.08s
                        Total time: 34308.00s
                               ETA: 1244422.8s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.185s, learning 0.268s)
               Value function loss: 1.8552
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 42.46
               Mean episode length: 300.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 12.45s
                        Total time: 34320.46s
                               ETA: 1244397.9s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.187s, learning 0.275s)
               Value function loss: 1.1072
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 44.11
               Mean episode length: 300.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 12.46s
                        Total time: 34332.92s
                               ETA: 1244373.3s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.171s, learning 0.224s)
               Value function loss: 1.1924
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 47.80
               Mean episode length: 300.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 12.40s
                        Total time: 34345.32s
                               ETA: 1244346.4s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.662s, learning 0.195s)
               Value function loss: 1.2897
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 50.44
               Mean episode length: 300.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 11.86s
                        Total time: 34357.17s
                               ETA: 1244299.9s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.733s, learning 0.197s)
               Value function loss: 1.6563
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 53.46
               Mean episode length: 300.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 11.93s
                        Total time: 34369.10s
                               ETA: 1244256.2s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.000s, learning 0.170s)
               Value function loss: 1.3237
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 61.05
               Mean episode length: 300.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 12.17s
                        Total time: 34381.27s
                               ETA: 1244221.1s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.755s, learning 0.197s)
               Value function loss: 1.3956
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 61.86
               Mean episode length: 300.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 11.95s
                        Total time: 34393.23s
                               ETA: 1244178.1s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.118s, learning 0.326s)
               Value function loss: 2.2379
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 65.54
               Mean episode length: 300.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 12.44s
                        Total time: 34405.67s
                               ETA: 1244153.0s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.111s, learning 0.181s)
               Value function loss: 1.8894
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 66.20
               Mean episode length: 300.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 12.29s
                        Total time: 34417.96s
                               ETA: 1244122.4s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.597s, learning 0.173s)
               Value function loss: 4.3776
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 64.81
               Mean episode length: 300.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 11.77s
                        Total time: 34429.73s
                               ETA: 1244072.9s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.053s, learning 0.326s)
               Value function loss: 7.5943
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 64.45
               Mean episode length: 300.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 12.38s
                        Total time: 34442.11s
                               ETA: 1244045.5s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.917s, learning 0.184s)
               Value function loss: 9.6317
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 66.89
               Mean episode length: 300.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 12.10s
                        Total time: 34454.21s
                               ETA: 1244008.0s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.887s, learning 0.291s)
               Value function loss: 3.9344
                    Surrogate loss: 0.0070
             Mean action noise std: 0.71
                       Mean reward: 69.66
               Mean episode length: 300.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 12.18s
                        Total time: 34466.39s
                               ETA: 1243973.3s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.284s, learning 0.293s)
               Value function loss: 4.0778
                    Surrogate loss: 0.0058
             Mean action noise std: 0.71
                       Mean reward: 72.27
               Mean episode length: 300.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 12.58s
                        Total time: 34478.97s
                               ETA: 1243953.0s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.893s, learning 0.175s)
               Value function loss: 5.5556
                    Surrogate loss: 0.0103
             Mean action noise std: 0.71
                       Mean reward: 73.21
               Mean episode length: 300.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 12.07s
                        Total time: 34491.03s
                               ETA: 1243914.4s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.176s, learning 0.202s)
               Value function loss: 6.4446
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 74.46
               Mean episode length: 300.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 12.38s
                        Total time: 34503.41s
                               ETA: 1243887.0s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.334s, learning 0.272s)
               Value function loss: 149.1892
                    Surrogate loss: 0.0461
             Mean action noise std: 0.71
                       Mean reward: 46.06
               Mean episode length: 300.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 12.61s
                        Total time: 34516.02s
                               ETA: 1243867.8s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.486s, learning 0.273s)
               Value function loss: 1.3895
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 45.08
               Mean episode length: 300.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 12.76s
                        Total time: 34528.78s
                               ETA: 1243854.1s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.882s, learning 0.180s)
               Value function loss: 1.4996
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 53.80
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 12.06s
                        Total time: 34540.84s
                               ETA: 1243815.3s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.987s, learning 0.285s)
               Value function loss: 1.3513
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 52.84
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 12.27s
                        Total time: 34553.11s
                               ETA: 1243784.1s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.697s, learning 0.206s)
               Value function loss: 1.6892
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 54.55
               Mean episode length: 300.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 11.90s
                        Total time: 34565.01s
                               ETA: 1243739.7s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.901s, learning 0.211s)
               Value function loss: 2.4789
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 57.07
               Mean episode length: 300.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 12.11s
                        Total time: 34577.12s
                               ETA: 1243702.7s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.109s, learning 0.313s)
               Value function loss: 1.9652
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 58.25
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 12.42s
                        Total time: 34589.55s
                               ETA: 1243677.0s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.610s, learning 0.284s)
               Value function loss: 3.4713
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 57.39
               Mean episode length: 300.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 12.89s
                        Total time: 34602.44s
                               ETA: 1243668.2s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.734s, learning 0.188s)
               Value function loss: 2.8814
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 57.81
               Mean episode length: 300.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 11.92s
                        Total time: 34614.36s
                               ETA: 1243624.5s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.407s, learning 0.337s)
               Value function loss: 2.0262
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 54.84
               Mean episode length: 300.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 12.74s
                        Total time: 34627.11s
                               ETA: 1243610.3s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.836s, learning 0.236s)
               Value function loss: 1.5490
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 54.89
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 12.07s
                        Total time: 34639.18s
                               ETA: 1243572.1s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.737s, learning 0.194s)
               Value function loss: 2.2022
                    Surrogate loss: 0.0069
             Mean action noise std: 0.71
                       Mean reward: 52.09
               Mean episode length: 300.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 11.93s
                        Total time: 34651.11s
                               ETA: 1243528.8s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.539s, learning 0.192s)
               Value function loss: 2.0216
                    Surrogate loss: 0.0171
             Mean action noise std: 0.71
                       Mean reward: 47.96
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 12.73s
                        Total time: 34663.84s
                               ETA: 1243514.1s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.445s, learning 0.298s)
               Value function loss: 2.5994
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 57.65
               Mean episode length: 300.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 12.74s
                        Total time: 34676.58s
                               ETA: 1243500.0s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.016s, learning 0.179s)
               Value function loss: 2.2797
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 50.56
               Mean episode length: 300.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 12.19s
                        Total time: 34688.78s
                               ETA: 1243466.1s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.174s, learning 0.333s)
               Value function loss: 3.6638
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 49.25
               Mean episode length: 300.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 12.51s
                        Total time: 34701.28s
                               ETA: 1243443.5s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.169s, learning 0.197s)
               Value function loss: 4.6832
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 52.20
               Mean episode length: 300.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 12.37s
                        Total time: 34713.65s
                               ETA: 1243415.9s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.351s, learning 0.243s)
               Value function loss: 3.9221
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 54.21
               Mean episode length: 300.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 12.59s
                        Total time: 34726.24s
                               ETA: 1243396.4s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.150s, learning 0.205s)
               Value function loss: 3.3223
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 64.27
               Mean episode length: 300.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 12.36s
                        Total time: 34738.60s
                               ETA: 1243368.3s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.981s, learning 0.174s)
               Value function loss: 4.7137
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 59.24
               Mean episode length: 300.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 12.16s
                        Total time: 34750.75s
                               ETA: 1243333.2s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.863s, learning 0.201s)
               Value function loss: 7.1360
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 64.67
               Mean episode length: 300.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 12.06s
                        Total time: 34762.82s
                               ETA: 1243294.8s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.035s, learning 0.405s)
               Value function loss: 7.6152
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 71.17
               Mean episode length: 300.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 12.44s
                        Total time: 34775.26s
                               ETA: 1243269.8s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.450s, learning 0.282s)
               Value function loss: 3.7588
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 76.19
               Mean episode length: 300.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 12.73s
                        Total time: 34787.99s
                               ETA: 1243255.3s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.166s, learning 0.186s)
               Value function loss: 3.0410
                    Surrogate loss: 0.0326
             Mean action noise std: 0.71
                       Mean reward: 74.68
               Mean episode length: 300.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 12.35s
                        Total time: 34800.34s
                               ETA: 1243227.2s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.021s, learning 0.176s)
               Value function loss: 2.8034
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 77.72
               Mean episode length: 300.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 12.20s
                        Total time: 34812.54s
                               ETA: 1243193.6s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.895s, learning 0.174s)
               Value function loss: 4.1901
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 77.27
               Mean episode length: 300.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 12.07s
                        Total time: 34824.61s
                               ETA: 1243155.5s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.736s, learning 0.194s)
               Value function loss: 3.9299
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 77.93
               Mean episode length: 300.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 11.93s
                        Total time: 34836.54s
                               ETA: 1243112.4s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.751s, learning 0.295s)
               Value function loss: 5.1305
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 77.81
               Mean episode length: 300.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 12.05s
                        Total time: 34848.59s
                               ETA: 1243073.5s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.047s, learning 0.210s)
               Value function loss: 4.5978
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 82.42
               Mean episode length: 300.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 12.26s
                        Total time: 34860.84s
                               ETA: 1243042.1s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.473s, learning 0.216s)
               Value function loss: 5.0110
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 81.91
               Mean episode length: 300.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 11.69s
                        Total time: 34872.53s
                               ETA: 1242990.4s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.715s, learning 0.183s)
               Value function loss: 7.4020
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 84.75
               Mean episode length: 300.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 11.90s
                        Total time: 34884.43s
                               ETA: 1242946.3s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.102s, learning 0.217s)
               Value function loss: 10.6577
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 84.90
               Mean episode length: 300.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 12.32s
                        Total time: 34896.75s
                               ETA: 1242917.1s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.072s, learning 0.190s)
               Value function loss: 10.8786
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 84.34
               Mean episode length: 300.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 12.26s
                        Total time: 34909.01s
                               ETA: 1242886.0s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.929s, learning 0.172s)
               Value function loss: 17.1506
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 87.82
               Mean episode length: 300.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 12.10s
                        Total time: 34921.11s
                               ETA: 1242849.1s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.820s, learning 0.283s)
               Value function loss: 26.3613
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 89.88
               Mean episode length: 300.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 12.10s
                        Total time: 34933.21s
                               ETA: 1242812.3s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.422s, learning 0.367s)
               Value function loss: 45.6518
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 91.37
               Mean episode length: 299.78
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 12.79s
                        Total time: 34946.00s
                               ETA: 1242800.0s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.875s, learning 0.195s)
               Value function loss: 28.2681
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 91.09
               Mean episode length: 299.78
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 12.07s
                        Total time: 34958.07s
                               ETA: 1242762.0s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.987s, learning 0.177s)
               Value function loss: 10.8613
                    Surrogate loss: 0.0159
             Mean action noise std: 0.71
                       Mean reward: 94.25
               Mean episode length: 299.78
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 12.16s
                        Total time: 34970.24s
                               ETA: 1242727.5s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.835s, learning 0.208s)
               Value function loss: 181.4606
                    Surrogate loss: 0.0697
             Mean action noise std: 0.71
                       Mean reward: 100.39
               Mean episode length: 300.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 12.04s
                        Total time: 34982.28s
                               ETA: 1242688.7s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.594s, learning 0.311s)
               Value function loss: 2.0141
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 104.78
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 12.91s
                        Total time: 34995.19s
                               ETA: 1242680.4s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.656s, learning 0.241s)
               Value function loss: 2.0516
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 99.35
               Mean episode length: 297.17
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 11.90s
                        Total time: 35007.08s
                               ETA: 1242636.5s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.451s, learning 0.201s)
               Value function loss: 2.6578
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 104.28
               Mean episode length: 297.17
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 11.65s
                        Total time: 35018.74s
                               ETA: 1242583.8s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.073s, learning 0.317s)
               Value function loss: 2.7617
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 105.39
               Mean episode length: 297.17
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 12.39s
                        Total time: 35031.13s
                               ETA: 1242557.3s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.479s, learning 0.209s)
               Value function loss: 3.6133
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: 110.67
               Mean episode length: 297.17
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 12.69s
                        Total time: 35043.81s
                               ETA: 1242541.5s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.262s, learning 0.272s)
               Value function loss: 3.5661
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 119.96
               Mean episode length: 297.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 12.53s
                        Total time: 35056.35s
                               ETA: 1242520.1s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.252s, learning 0.293s)
               Value function loss: 6.5689
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 139.98
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 12.55s
                        Total time: 35068.89s
                               ETA: 1242499.2s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.946s, learning 0.216s)
               Value function loss: 3.8021
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 142.28
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 12.16s
                        Total time: 35081.05s
                               ETA: 1242464.7s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.344s, learning 0.208s)
               Value function loss: 3.4073
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 143.97
               Mean episode length: 300.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 12.55s
                        Total time: 35093.61s
                               ETA: 1242444.0s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.296s, learning 0.291s)
               Value function loss: 3.7110
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 136.00
               Mean episode length: 297.85
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 12.59s
                        Total time: 35106.19s
                               ETA: 1242424.5s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.763s, learning 0.181s)
               Value function loss: 3.0787
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 137.23
               Mean episode length: 297.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 11.94s
                        Total time: 35118.14s
                               ETA: 1242382.3s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.858s, learning 0.196s)
               Value function loss: 2.4700
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 136.63
               Mean episode length: 295.81
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 12.05s
                        Total time: 35130.19s
                               ETA: 1242344.1s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.077s, learning 0.335s)
               Value function loss: 2.6856
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 129.29
               Mean episode length: 295.81
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 12.41s
                        Total time: 35142.60s
                               ETA: 1242318.5s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.455s, learning 0.283s)
               Value function loss: 4.5312
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 128.41
               Mean episode length: 295.81
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 12.74s
                        Total time: 35155.34s
                               ETA: 1242304.4s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.855s, learning 0.286s)
               Value function loss: 3.8527
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 129.61
               Mean episode length: 295.68
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 12.14s
                        Total time: 35167.48s
                               ETA: 1242269.3s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.243s, learning 0.337s)
               Value function loss: 3.3701
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 128.37
               Mean episode length: 295.68
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 12.58s
                        Total time: 35180.06s
                               ETA: 1242249.6s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.138s, learning 0.202s)
               Value function loss: 3.9670
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 139.60
               Mean episode length: 297.83
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 12.34s
                        Total time: 35192.40s
                               ETA: 1242221.5s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.719s, learning 0.228s)
               Value function loss: 3.3811
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 145.81
               Mean episode length: 299.87
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 11.95s
                        Total time: 35204.35s
                               ETA: 1242179.5s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.168s, learning 0.234s)
               Value function loss: 3.8143
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 152.05
               Mean episode length: 299.87
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 12.40s
                        Total time: 35216.75s
                               ETA: 1242153.6s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.059s, learning 0.239s)
               Value function loss: 3.2310
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 154.91
               Mean episode length: 299.87
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 12.30s
                        Total time: 35229.05s
                               ETA: 1242124.1s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.850s, learning 0.187s)
               Value function loss: 4.7556
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 156.79
               Mean episode length: 300.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 12.04s
                        Total time: 35241.08s
                               ETA: 1242085.3s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.907s, learning 0.293s)
               Value function loss: 3.2969
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 155.58
               Mean episode length: 300.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 12.20s
                        Total time: 35253.28s
                               ETA: 1242052.4s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.029s, learning 0.405s)
               Value function loss: 3.6247
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 148.77
               Mean episode length: 300.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 12.43s
                        Total time: 35265.72s
                               ETA: 1242027.6s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.063s, learning 0.200s)
               Value function loss: 4.6738
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 151.11
               Mean episode length: 300.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 12.26s
                        Total time: 35277.98s
                               ETA: 1241996.9s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.091s, learning 0.291s)
               Value function loss: 6.6976
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 146.51
               Mean episode length: 300.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 12.38s
                        Total time: 35290.36s
                               ETA: 1241970.4s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.371s, learning 0.309s)
               Value function loss: 6.4850
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 142.37
               Mean episode length: 300.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 12.68s
                        Total time: 35303.04s
                               ETA: 1241954.3s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.042s, learning 0.185s)
               Value function loss: 7.4006
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 134.68
               Mean episode length: 300.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 12.23s
                        Total time: 35315.27s
                               ETA: 1241922.4s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.516s, learning 0.194s)
               Value function loss: 6.6920
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 136.51
               Mean episode length: 300.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 11.71s
                        Total time: 35326.98s
                               ETA: 1241872.3s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.040s, learning 0.178s)
               Value function loss: 5.5195
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 132.37
               Mean episode length: 300.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 12.22s
                        Total time: 35339.20s
                               ETA: 1241840.0s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.004s, learning 0.321s)
               Value function loss: 6.9253
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 128.39
               Mean episode length: 300.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 12.33s
                        Total time: 35351.52s
                               ETA: 1241811.6s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.351s, learning 0.309s)
               Value function loss: 9.1119
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 122.08
               Mean episode length: 300.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 12.66s
                        Total time: 35364.18s
                               ETA: 1241794.9s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.608s, learning 0.365s)
               Value function loss: 13.4314
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 118.51
               Mean episode length: 300.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 12.97s
                        Total time: 35377.15s
                               ETA: 1241789.2s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.433s, learning 0.322s)
               Value function loss: 19.9267
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 118.90
               Mean episode length: 300.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 12.76s
                        Total time: 35389.91s
                               ETA: 1241775.9s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.118s, learning 0.370s)
               Value function loss: 20.9886
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 117.19
               Mean episode length: 300.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 12.49s
                        Total time: 35402.40s
                               ETA: 1241753.2s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.475s, learning 0.221s)
               Value function loss: 22.7213
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 117.18
               Mean episode length: 300.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 12.70s
                        Total time: 35415.10s
                               ETA: 1241737.8s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.658s, learning 0.359s)
               Value function loss: 27.6872
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 113.30
               Mean episode length: 299.88
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 13.02s
                        Total time: 35428.11s
                               ETA: 1241733.6s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.492s, learning 0.330s)
               Value function loss: 130.4547
                    Surrogate loss: 0.0304
             Mean action noise std: 0.71
                       Mean reward: 70.00
               Mean episode length: 300.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 4.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 12.82s
                        Total time: 35440.93s
                               ETA: 1241722.6s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.106s, learning 0.276s)
               Value function loss: 3.9955
                    Surrogate loss: 0.0057
             Mean action noise std: 0.71
                       Mean reward: 66.86
               Mean episode length: 300.00
                  Mean reward/step: -1.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 12.38s
                        Total time: 35453.32s
                               ETA: 1241696.2s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.846s, learning 0.176s)
               Value function loss: 3.1672
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 69.48
               Mean episode length: 294.34
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 12.02s
                        Total time: 35465.34s
                               ETA: 1241657.1s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.955s, learning 0.195s)
               Value function loss: 3.2412
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 68.34
               Mean episode length: 294.34
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 12.15s
                        Total time: 35477.49s
                               ETA: 1241622.6s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.708s, learning 0.320s)
               Value function loss: 2.9976
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 65.47
               Mean episode length: 294.34
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 12.03s
                        Total time: 35489.51s
                               ETA: 1241583.9s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.120s, learning 0.188s)
               Value function loss: 3.5502
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 57.96
               Mean episode length: 294.34
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 12.31s
                        Total time: 35501.82s
                               ETA: 1241554.9s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.068s, learning 0.176s)
               Value function loss: 2.5176
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 62.77
               Mean episode length: 294.34
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 12.24s
                        Total time: 35514.07s
                               ETA: 1241523.8s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.710s, learning 0.219s)
               Value function loss: 6.6662
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 58.16
               Mean episode length: 294.34
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 11.93s
                        Total time: 35526.00s
                               ETA: 1241481.6s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.722s, learning 0.220s)
               Value function loss: 7.8618
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: 64.96
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 11.94s
                        Total time: 35537.94s
                               ETA: 1241439.9s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.709s, learning 0.197s)
               Value function loss: 5.6720
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 69.44
               Mean episode length: 300.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 11.91s
                        Total time: 35549.84s
                               ETA: 1241396.9s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.850s, learning 0.297s)
               Value function loss: 6.4444
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 65.34
               Mean episode length: 300.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 12.15s
                        Total time: 35561.99s
                               ETA: 1241362.5s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.746s, learning 0.200s)
               Value function loss: 6.0378
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 68.65
               Mean episode length: 300.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 11.95s
                        Total time: 35573.94s
                               ETA: 1241321.0s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.170s)
               Value function loss: 5.9464
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 60.20
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 11.65s
                        Total time: 35585.59s
                               ETA: 1241269.1s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.008s, learning 0.324s)
               Value function loss: 8.0370
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 55.96
               Mean episode length: 300.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 12.33s
                        Total time: 35597.92s
                               ETA: 1241241.2s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.311s, learning 0.401s)
               Value function loss: 7.0964
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 56.48
               Mean episode length: 299.99
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 12.71s
                        Total time: 35610.63s
                               ETA: 1241226.4s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.498s, learning 0.298s)
               Value function loss: 25.0786
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 50.90
               Mean episode length: 298.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 12.80s
                        Total time: 35623.42s
                               ETA: 1241214.6s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.483s, learning 0.214s)
               Value function loss: 63.9027
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 47.14
               Mean episode length: 298.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 12.70s
                        Total time: 35636.12s
                               ETA: 1241199.4s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.869s, learning 0.210s)
               Value function loss: 56.2936
                    Surrogate loss: 0.0271
             Mean action noise std: 0.71
                       Mean reward: 39.66
               Mean episode length: 298.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 12.08s
                        Total time: 35648.20s
                               ETA: 1241162.6s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.902s, learning 0.226s)
               Value function loss: 10.8955
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 46.18
               Mean episode length: 298.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 12.13s
                        Total time: 35660.33s
                               ETA: 1241127.5s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.957s, learning 0.279s)
               Value function loss: 7.2011
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 46.48
               Mean episode length: 298.14
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 12.24s
                        Total time: 35672.56s
                               ETA: 1241096.3s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.440s, learning 0.228s)
               Value function loss: 7.0730
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 46.56
               Mean episode length: 298.11
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 12.67s
                        Total time: 35685.23s
                               ETA: 1241080.0s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.601s, learning 0.285s)
               Value function loss: 8.5225
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 45.48
               Mean episode length: 299.96
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 12.89s
                        Total time: 35698.12s
                               ETA: 1241071.4s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.865s, learning 0.176s)
               Value function loss: 5.8020
                    Surrogate loss: 0.0141
             Mean action noise std: 0.71
                       Mean reward: 43.43
               Mean episode length: 299.96
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 12.04s
                        Total time: 35710.16s
                               ETA: 1241033.4s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.134s, learning 0.275s)
               Value function loss: 5.5032
                    Surrogate loss: 0.0252
             Mean action noise std: 0.71
                       Mean reward: 42.70
               Mean episode length: 299.96
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 12.41s
                        Total time: 35722.57s
                               ETA: 1241008.2s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.629s, learning 0.303s)
               Value function loss: 6.0691
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 37.20
               Mean episode length: 299.96
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 12.93s
                        Total time: 35735.50s
                               ETA: 1241001.1s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.262s, learning 0.190s)
               Value function loss: 6.1774
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 25.95
               Mean episode length: 299.96
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 12.45s
                        Total time: 35747.95s
                               ETA: 1240977.4s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.735s, learning 0.170s)
               Value function loss: 5.5244
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 17.74
               Mean episode length: 299.96
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 11.90s
                        Total time: 35759.86s
                               ETA: 1240934.7s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.109s, learning 0.206s)
               Value function loss: 6.1043
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 23.40
               Mean episode length: 299.96
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 12.32s
                        Total time: 35772.17s
                               ETA: 1240906.3s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.031s, learning 0.294s)
               Value function loss: 6.8406
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: 21.16
               Mean episode length: 299.96
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 12.33s
                        Total time: 35784.50s
                               ETA: 1240878.2s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.590s, learning 0.313s)
               Value function loss: 7.2588
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 29.53
               Mean episode length: 299.96
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 12.90s
                        Total time: 35797.40s
                               ETA: 1240870.2s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.961s, learning 0.195s)
               Value function loss: 6.5439
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 24.60
               Mean episode length: 300.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 12.16s
                        Total time: 35809.56s
                               ETA: 1240836.3s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.919s, learning 0.194s)
               Value function loss: 6.6779
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 27.87
               Mean episode length: 300.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 12.11s
                        Total time: 35821.67s
                               ETA: 1240800.8s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.911s, learning 0.196s)
               Value function loss: 7.6507
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 31.45
               Mean episode length: 300.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 12.11s
                        Total time: 35833.78s
                               ETA: 1240765.2s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.088s, learning 0.333s)
               Value function loss: 12.6175
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 34.92
               Mean episode length: 300.00
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 12.42s
                        Total time: 35846.20s
                               ETA: 1240740.5s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.081s, learning 0.353s)
               Value function loss: 13.0154
                    Surrogate loss: 0.0162
             Mean action noise std: 0.71
                       Mean reward: 33.73
               Mean episode length: 300.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 12.43s
                        Total time: 35858.63s
                               ETA: 1240716.3s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.392s, learning 0.199s)
               Value function loss: 11.2804
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 30.86
               Mean episode length: 300.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 11.59s
                        Total time: 35870.22s
                               ETA: 1240662.9s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.262s, learning 0.283s)
               Value function loss: 13.1625
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 33.76
               Mean episode length: 299.89
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 12.54s
                        Total time: 35882.77s
                               ETA: 1240642.5s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.313s, learning 0.240s)
               Value function loss: 16.7027
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 35.41
               Mean episode length: 299.76
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 12.55s
                        Total time: 35895.32s
                               ETA: 1240622.4s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.887s, learning 0.200s)
               Value function loss: 146.7173
                    Surrogate loss: 0.0426
             Mean action noise std: 0.71
                       Mean reward: 8.37
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 4.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 12.09s
                        Total time: 35907.41s
                               ETA: 1240586.3s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.859s, learning 0.210s)
               Value function loss: 2.0175
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: -2.42
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 12.07s
                        Total time: 35919.48s
                               ETA: 1240549.5s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.293s, learning 0.214s)
               Value function loss: 2.4801
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 7.26
               Mean episode length: 297.17
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 12.51s
                        Total time: 35931.98s
                               ETA: 1240527.8s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.990s, learning 0.280s)
               Value function loss: 3.0641
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: -0.55
               Mean episode length: 297.17
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 12.27s
                        Total time: 35944.25s
                               ETA: 1240498.0s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.995s, learning 0.172s)
               Value function loss: 2.5942
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: -2.28
               Mean episode length: 297.17
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 12.17s
                        Total time: 35956.42s
                               ETA: 1240464.6s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.028s, learning 0.285s)
               Value function loss: 2.6925
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: -12.00
               Mean episode length: 297.17
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 12.31s
                        Total time: 35968.73s
                               ETA: 1240436.3s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.174s, learning 0.198s)
               Value function loss: 3.9141
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: -16.47
               Mean episode length: 295.21
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 12.37s
                        Total time: 35981.11s
                               ETA: 1240410.0s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.830s, learning 0.200s)
               Value function loss: 6.6767
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: -23.85
               Mean episode length: 295.63
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 12.03s
                        Total time: 35993.14s
                               ETA: 1240372.0s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.586s, learning 0.200s)
               Value function loss: 5.6374
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: -22.16
               Mean episode length: 295.63
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 11.79s
                        Total time: 36004.92s
                               ETA: 1240325.5s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.451s, learning 0.329s)
               Value function loss: 3.9740
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: -29.84
               Mean episode length: 295.63
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 11.78s
                        Total time: 36016.70s
                               ETA: 1240278.9s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.416s, learning 0.285s)
               Value function loss: 5.4715
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: -34.93
               Mean episode length: 297.59
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 12.70s
                        Total time: 36029.40s
                               ETA: 1240264.0s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.910s, learning 0.177s)
               Value function loss: 5.2192
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: -31.86
               Mean episode length: 293.34
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 12.09s
                        Total time: 36041.49s
                               ETA: 1240228.0s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.347s, learning 0.213s)
               Value function loss: 4.4491
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: -33.69
               Mean episode length: 293.74
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 11.56s
                        Total time: 36053.05s
                               ETA: 1240173.8s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.852s, learning 0.279s)
               Value function loss: 5.4425
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: -35.49
               Mean episode length: 289.84
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 12.13s
                        Total time: 36065.18s
                               ETA: 1240139.4s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.943s, learning 0.197s)
               Value function loss: 7.4473
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: -51.51
               Mean episode length: 289.84
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 12.14s
                        Total time: 36077.32s
                               ETA: 1240105.2s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.163s, learning 0.202s)
               Value function loss: 7.9352
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: -54.05
               Mean episode length: 288.03
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 12.36s
                        Total time: 36089.68s
                               ETA: 1240078.8s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.454s, learning 0.206s)
               Value function loss: 7.2107
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: -47.57
               Mean episode length: 288.03
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 12.66s
                        Total time: 36102.34s
                               ETA: 1240062.5s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.422s, learning 0.214s)
               Value function loss: 5.8566
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: -44.79
               Mean episode length: 292.28
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 12.64s
                        Total time: 36114.98s
                               ETA: 1240045.5s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.026s, learning 0.186s)
               Value function loss: 6.0822
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: -41.34
               Mean episode length: 294.29
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 12.21s
                        Total time: 36127.19s
                               ETA: 1240013.8s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.992s, learning 0.206s)
               Value function loss: 6.3232
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: -43.19
               Mean episode length: 298.19
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 12.20s
                        Total time: 36139.39s
                               ETA: 1239981.7s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.035s, learning 0.246s)
               Value function loss: 10.3650
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: -42.85
               Mean episode length: 298.19
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 12.28s
                        Total time: 36151.67s
                               ETA: 1239952.5s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.865s, learning 0.192s)
               Value function loss: 20.1645
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: -37.67
               Mean episode length: 300.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 12.06s
                        Total time: 36163.73s
                               ETA: 1239915.6s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.244s, learning 0.299s)
               Value function loss: 14.5095
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: -47.27
               Mean episode length: 300.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 12.54s
                        Total time: 36176.27s
                               ETA: 1239895.4s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.412s, learning 0.219s)
               Value function loss: 5.9126
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: -49.67
               Mean episode length: 300.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 12.63s
                        Total time: 36188.90s
                               ETA: 1239878.1s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.031s, learning 0.180s)
               Value function loss: 8.3436
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: -54.04
               Mean episode length: 300.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 12.21s
                        Total time: 36201.11s
                               ETA: 1239846.5s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.656s, learning 0.199s)
               Value function loss: 6.7030
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: -60.69
               Mean episode length: 300.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 11.85s
                        Total time: 36212.96s
                               ETA: 1239802.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.419s, learning 0.281s)
               Value function loss: 7.7218
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: -73.72
               Mean episode length: 299.12
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 12.70s
                        Total time: 36225.66s
                               ETA: 1239788.0s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.385s, learning 0.176s)
               Value function loss: 6.2964
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: -78.92
               Mean episode length: 297.03
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 12.56s
                        Total time: 36238.23s
                               ETA: 1239768.4s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.127s, learning 0.213s)
               Value function loss: 5.3004
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: -83.79
               Mean episode length: 297.03
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 12.34s
                        Total time: 36250.57s
                               ETA: 1239741.3s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.194s, learning 0.166s)
               Value function loss: 5.0517
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: -92.59
               Mean episode length: 297.03
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 12.36s
                        Total time: 36262.93s
                               ETA: 1239714.8s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.742s, learning 0.301s)
               Value function loss: 6.5525
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: -76.83
               Mean episode length: 297.03
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 12.04s
                        Total time: 36274.97s
                               ETA: 1239677.6s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.402s, learning 0.291s)
               Value function loss: 6.0962
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: -84.00
               Mean episode length: 295.19
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 12.69s
                        Total time: 36287.66s
                               ETA: 1239662.6s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.248s, learning 0.299s)
               Value function loss: 6.8529
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: -83.89
               Mean episode length: 295.19
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 12.55s
                        Total time: 36300.21s
                               ETA: 1239642.5s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.306s, learning 0.328s)
               Value function loss: 8.1114
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: -78.64
               Mean episode length: 295.19
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 12.63s
                        Total time: 36312.84s
                               ETA: 1239625.5s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.596s, learning 0.204s)
               Value function loss: 8.9306
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: -72.42
               Mean episode length: 295.19
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 11.80s
                        Total time: 36324.64s
                               ETA: 1239580.0s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.888s, learning 0.178s)
               Value function loss: 10.3266
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: -68.03
               Mean episode length: 295.04
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 12.07s
                        Total time: 36336.71s
                               ETA: 1239543.6s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.809s, learning 0.179s)
               Value function loss: 11.9440
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: -68.76
               Mean episode length: 294.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 11.99s
                        Total time: 36348.70s
                               ETA: 1239504.6s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.180s, learning 0.186s)
               Value function loss: 122.5914
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: -18.05
               Mean episode length: 300.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 4.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 12.37s
                        Total time: 36361.06s
                               ETA: 1239478.5s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.964s, learning 0.279s)
               Value function loss: 2.1154
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: -22.93
               Mean episode length: 300.00
                  Mean reward/step: -1.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 12.24s
                        Total time: 36373.31s
                               ETA: 1239448.2s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.855s, learning 0.203s)
               Value function loss: 2.0202
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: -22.62
               Mean episode length: 294.34
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 12.06s
                        Total time: 36385.36s
                               ETA: 1239411.6s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.030s, learning 0.193s)
               Value function loss: 2.3448
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: -29.61
               Mean episode length: 294.34
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 12.22s
                        Total time: 36397.59s
                               ETA: 1239380.6s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.402s, learning 0.229s)
               Value function loss: 3.0293
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: -44.03
               Mean episode length: 294.27
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 12.63s
                        Total time: 36410.22s
                               ETA: 1239363.5s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.312s, learning 0.307s)
               Value function loss: 2.8732
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: -49.23
               Mean episode length: 294.27
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 12.62s
                        Total time: 36422.84s
                               ETA: 1239346.0s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.916s, learning 0.190s)
               Value function loss: 3.1817
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: -52.18
               Mean episode length: 294.27
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 12.11s
                        Total time: 36434.94s
                               ETA: 1239311.1s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.596s, learning 0.183s)
               Value function loss: 4.1693
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: -65.19
               Mean episode length: 294.08
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 11.78s
                        Total time: 36446.72s
                               ETA: 1239265.1s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.744s, learning 0.203s)
               Value function loss: 4.6852
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: -60.09
               Mean episode length: 299.81
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 11.95s
                        Total time: 36458.67s
                               ETA: 1239224.8s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.196s, learning 0.297s)
               Value function loss: 3.1096
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: -68.34
               Mean episode length: 299.81
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 12.49s
                        Total time: 36471.16s
                               ETA: 1239203.1s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.274s, learning 0.290s)
               Value function loss: 3.2925
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: -75.08
               Mean episode length: 299.81
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 12.56s
                        Total time: 36483.73s
                               ETA: 1239183.8s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.151s, learning 0.188s)
               Value function loss: 4.0298
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: -67.30
               Mean episode length: 297.87
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 12.34s
                        Total time: 36496.06s
                               ETA: 1239156.8s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.076s, learning 0.197s)
               Value function loss: 3.8917
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: -61.60
               Mean episode length: 297.87
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 12.27s
                        Total time: 36508.34s
                               ETA: 1239127.7s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.119s, learning 0.202s)
               Value function loss: 3.8525
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: -62.36
               Mean episode length: 297.87
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 12.32s
                        Total time: 36520.66s
                               ETA: 1239100.1s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.106s, learning 0.173s)
               Value function loss: 4.0398
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: -68.56
               Mean episode length: 294.05
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 12.28s
                        Total time: 36532.94s
                               ETA: 1239071.2s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.711s, learning 0.175s)
               Value function loss: 5.8314
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: -63.90
               Mean episode length: 292.35
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 11.89s
                        Total time: 36544.82s
                               ETA: 1239029.0s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.630s, learning 0.260s)
               Value function loss: 5.9857
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: -59.59
               Mean episode length: 292.35
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 11.89s
                        Total time: 36556.71s
                               ETA: 1238986.9s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.337s, learning 0.301s)
               Value function loss: 6.5170
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: -59.49
               Mean episode length: 292.85
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 12.64s
                        Total time: 36569.35s
                               ETA: 1238970.1s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.230s, learning 0.192s)
               Value function loss: 7.7417
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: -75.02
               Mean episode length: 292.52
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 12.42s
                        Total time: 36581.77s
                               ETA: 1238946.1s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1330 steps/s (collection: 11.971s, learning 0.346s)
               Value function loss: 9.2607
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: -58.17
               Mean episode length: 294.44
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 12.32s
                        Total time: 36594.09s
                               ETA: 1238918.5s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.975s, learning 0.292s)
               Value function loss: 10.6730
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: -76.73
               Mean episode length: 290.27
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 12.27s
                        Total time: 36606.36s
                               ETA: 1238889.2s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.740s, learning 0.320s)
               Value function loss: 7.5706
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: -81.04
               Mean episode length: 288.98
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 12.06s
                        Total time: 36618.42s
                               ETA: 1238853.0s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.931s, learning 0.213s)
               Value function loss: 7.2696
                    Surrogate loss: 0.0116
             Mean action noise std: 0.71
                       Mean reward: -85.52
               Mean episode length: 287.74
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 12.14s
                        Total time: 36630.56s
                               ETA: 1238819.6s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.962s, learning 0.192s)
               Value function loss: 6.9780
                    Surrogate loss: 0.0212
             Mean action noise std: 0.71
                       Mean reward: -103.67
               Mean episode length: 287.67
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 12.15s
                        Total time: 36642.71s
                               ETA: 1238786.5s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.959s, learning 0.164s)
               Value function loss: 6.9277
                    Surrogate loss: 0.0116
             Mean action noise std: 0.71
                       Mean reward: -109.34
               Mean episode length: 287.62
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 12.12s
                        Total time: 36654.84s
                               ETA: 1238752.4s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.792s, learning 0.195s)
               Value function loss: 9.1287
                    Surrogate loss: 0.0159
             Mean action noise std: 0.71
                       Mean reward: -117.31
               Mean episode length: 289.25
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 11.99s
                        Total time: 36666.82s
                               ETA: 1238713.7s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.044s, learning 0.178s)
               Value function loss: 8.8463
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: -114.81
               Mean episode length: 289.25
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 12.22s
                        Total time: 36679.05s
                               ETA: 1238683.0s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.986s, learning 0.188s)
               Value function loss: 9.9254
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: -117.42
               Mean episode length: 289.35
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 12.17s
                        Total time: 36691.22s
                               ETA: 1238650.7s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.656s, learning 0.171s)
               Value function loss: 10.5167
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: -116.64
               Mean episode length: 290.52
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 11.83s
                        Total time: 36703.05s
                               ETA: 1238606.7s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.929s, learning 0.192s)
               Value function loss: 10.9082
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: -119.38
               Mean episode length: 294.67
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 12.12s
                        Total time: 36715.17s
                               ETA: 1238572.6s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.266s, learning 0.328s)
               Value function loss: 10.5450
                    Surrogate loss: 0.0069
             Mean action noise std: 0.71
                       Mean reward: -127.53
               Mean episode length: 296.61
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 12.59s
                        Total time: 36727.76s
                               ETA: 1238554.4s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.544s, learning 0.259s)
               Value function loss: 11.1502
                    Surrogate loss: 0.0120
             Mean action noise std: 0.71
                       Mean reward: -136.86
               Mean episode length: 292.55
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 12.80s
                        Total time: 36740.56s
                               ETA: 1238543.4s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.978s, learning 0.194s)
               Value function loss: 11.2123
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: -149.32
               Mean episode length: 290.49
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 12.17s
                        Total time: 36752.74s
                               ETA: 1238511.1s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.086s, learning 0.261s)
               Value function loss: 12.3369
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: -163.26
               Mean episode length: 285.63
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 12.35s
                        Total time: 36765.08s
                               ETA: 1238484.7s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.374s, learning 0.313s)
               Value function loss: 11.0048
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: -215.40
               Mean episode length: 277.32
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 12.69s
                        Total time: 36777.77s
                               ETA: 1238469.7s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.547s, learning 0.209s)
               Value function loss: 11.0171
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: -256.06
               Mean episode length: 276.18
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 12.76s
                        Total time: 36790.53s
                               ETA: 1238457.1s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.002s, learning 0.176s)
               Value function loss: 12.8299
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: -254.42
               Mean episode length: 284.35
                  Mean reward/step: -0.99
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 12.18s
                        Total time: 36802.70s
                               ETA: 1238425.0s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.434s, learning 0.313s)
               Value function loss: 11.0120
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: -220.55
               Mean episode length: 290.69
                  Mean reward/step: -1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 12.75s
                        Total time: 36815.45s
                               ETA: 1238412.1s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.358s, learning 0.205s)
               Value function loss: 129.2450
                    Surrogate loss: 0.0504
             Mean action noise std: 0.71
                       Mean reward: -197.65
               Mean episode length: 299.54
                  Mean reward/step: -1.30
       Mean episode length/episode: 4.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 12.56s
                        Total time: 36828.01s
                               ETA: 1238392.9s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.514s, learning 0.265s)
               Value function loss: 2.9495
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: -200.03
               Mean episode length: 298.10
                  Mean reward/step: -1.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 12.78s
                        Total time: 36840.79s
                               ETA: 1238381.1s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.497s, learning 0.198s)
               Value function loss: 3.4283
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -189.20
               Mean episode length: 289.05
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 12.69s
                        Total time: 36853.49s
                               ETA: 1238366.4s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.058s, learning 0.220s)
               Value function loss: 2.9518
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: -185.13
               Mean episode length: 288.35
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 12.28s
                        Total time: 36865.76s
                               ETA: 1238337.7s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.706s, learning 0.224s)
               Value function loss: 2.6024
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: -188.10
               Mean episode length: 287.27
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 11.93s
                        Total time: 36877.69s
                               ETA: 1238297.4s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.368s, learning 0.299s)
               Value function loss: 2.7423
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: -199.52
               Mean episode length: 286.03
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 12.67s
                        Total time: 36890.36s
                               ETA: 1238281.8s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.644s, learning 0.297s)
               Value function loss: 3.5840
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: -201.90
               Mean episode length: 285.35
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 12.94s
                        Total time: 36903.30s
                               ETA: 1238275.4s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.047s, learning 0.263s)
               Value function loss: 5.6022
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: -212.25
               Mean episode length: 296.04
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 12.31s
                        Total time: 36915.61s
                               ETA: 1238247.9s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.243s, learning 0.378s)
               Value function loss: 3.6247
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: -209.21
               Mean episode length: 296.79
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 12.62s
                        Total time: 36928.23s
                               ETA: 1238230.7s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.808s, learning 0.296s)
               Value function loss: 3.0456
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: -195.54
               Mean episode length: 296.47
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 13.10s
                        Total time: 36941.34s
                               ETA: 1238229.8s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.219s, learning 0.242s)
               Value function loss: 2.7610
                    Surrogate loss: -0.0255
             Mean action noise std: 0.71
                       Mean reward: -184.32
               Mean episode length: 296.61
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 12.46s
                        Total time: 36953.80s
                               ETA: 1238207.3s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.248s, learning 0.202s)
               Value function loss: 3.9516
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: -201.20
               Mean episode length: 294.65
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 12.45s
                        Total time: 36966.25s
                               ETA: 1238184.4s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.011s, learning 0.193s)
               Value function loss: 3.4390
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: -204.27
               Mean episode length: 292.97
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 12.20s
                        Total time: 36978.45s
                               ETA: 1238153.3s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.137s, learning 0.191s)
               Value function loss: 3.5664
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: -191.30
               Mean episode length: 293.10
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 12.33s
                        Total time: 36990.78s
                               ETA: 1238126.4s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.164s, learning 0.292s)
               Value function loss: 4.8086
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: -186.80
               Mean episode length: 292.90
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 12.46s
                        Total time: 37003.24s
                               ETA: 1238103.8s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.661s, learning 0.282s)
               Value function loss: 4.4560
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: -187.71
               Mean episode length: 292.69
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 12.94s
                        Total time: 37016.18s
                               ETA: 1238097.5s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.172s, learning 0.303s)
               Value function loss: 4.1241
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: -193.63
               Mean episode length: 290.61
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 12.47s
                        Total time: 37028.65s
                               ETA: 1238075.5s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.447s, learning 0.370s)
               Value function loss: 5.1385
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: -179.49
               Mean episode length: 292.22
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 12.82s
                        Total time: 37041.47s
                               ETA: 1238064.9s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.866s, learning 0.183s)
               Value function loss: 5.1829
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: -178.60
               Mean episode length: 290.30
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 12.05s
                        Total time: 37053.52s
                               ETA: 1238028.7s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.764s, learning 0.179s)
               Value function loss: 5.0479
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: -195.59
               Mean episode length: 287.32
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 11.94s
                        Total time: 37065.46s
                               ETA: 1237989.0s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.415s, learning 0.301s)
               Value function loss: 4.6645
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: -191.50
               Mean episode length: 285.72
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 12.72s
                        Total time: 37078.18s
                               ETA: 1237975.1s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.108s, learning 0.272s)
               Value function loss: 5.4032
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: -202.23
               Mean episode length: 287.49
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 12.38s
                        Total time: 37090.56s
                               ETA: 1237950.0s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.131s, learning 0.452s)
               Value function loss: 5.3537
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: -201.02
               Mean episode length: 287.49
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 12.58s
                        Total time: 37103.14s
                               ETA: 1237931.7s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.598s, learning 0.191s)
               Value function loss: 5.9726
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: -200.03
               Mean episode length: 289.19
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 11.79s
                        Total time: 37114.93s
                               ETA: 1237886.9s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.872s, learning 0.204s)
               Value function loss: 7.5950
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: -200.65
               Mean episode length: 289.54
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 12.08s
                        Total time: 37127.01s
                               ETA: 1237851.6s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.878s, learning 0.289s)
               Value function loss: 10.3172
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: -204.53
               Mean episode length: 290.39
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 12.17s
                        Total time: 37139.17s
                               ETA: 1237819.5s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.582s, learning 0.192s)
               Value function loss: 10.5770
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: -206.54
               Mean episode length: 292.57
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 11.77s
                        Total time: 37150.95s
                               ETA: 1237774.2s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.103s, learning 0.212s)
               Value function loss: 10.9167
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: -199.55
               Mean episode length: 290.77
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 12.32s
                        Total time: 37163.26s
                               ETA: 1237747.0s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.040s, learning 0.284s)
               Value function loss: 12.8346
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: -205.75
               Mean episode length: 292.66
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 12.32s
                        Total time: 37175.59s
                               ETA: 1237720.1s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.824s, learning 0.182s)
               Value function loss: 14.5564
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: -209.05
               Mean episode length: 293.87
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 12.01s
                        Total time: 37187.59s
                               ETA: 1237682.6s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.299s, learning 0.176s)
               Value function loss: 14.1376
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: -216.40
               Mean episode length: 291.01
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 12.48s
                        Total time: 37200.07s
                               ETA: 1237660.8s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.841s, learning 0.208s)
               Value function loss: 19.8964
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: -226.88
               Mean episode length: 283.52
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 12.05s
                        Total time: 37212.12s
                               ETA: 1237624.8s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.993s, learning 0.184s)
               Value function loss: 27.8119
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: -232.12
               Mean episode length: 275.83
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 12.18s
                        Total time: 37224.29s
                               ETA: 1237593.0s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.054s, learning 0.178s)
               Value function loss: 20.5806
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -215.71
               Mean episode length: 278.60
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 12.23s
                        Total time: 37236.53s
                               ETA: 1237563.1s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.590s, learning 0.197s)
               Value function loss: 13.6242
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: -204.52
               Mean episode length: 285.39
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 11.79s
                        Total time: 37248.31s
                               ETA: 1237518.5s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.902s, learning 0.193s)
               Value function loss: 13.0559
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: -216.48
               Mean episode length: 290.29
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 12.10s
                        Total time: 37260.41s
                               ETA: 1237484.1s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.918s, learning 0.204s)
               Value function loss: 12.1476
                    Surrogate loss: 0.0107
             Mean action noise std: 0.71
                       Mean reward: -186.62
               Mean episode length: 295.13
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 12.12s
                        Total time: 37272.53s
                               ETA: 1237450.6s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.137s, learning 0.352s)
               Value function loss: 116.7393
                    Surrogate loss: 0.0897
             Mean action noise std: 0.71
                       Mean reward: -163.35
               Mean episode length: 300.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 4.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 12.49s
                        Total time: 37285.02s
                               ETA: 1237429.3s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.931s, learning 0.192s)
               Value function loss: 2.2666
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: -159.89
               Mean episode length: 299.66
                  Mean reward/step: -1.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 12.12s
                        Total time: 37297.14s
                               ETA: 1237395.8s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.841s, learning 0.211s)
               Value function loss: 3.6039
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: -154.60
               Mean episode length: 293.98
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 12.05s
                        Total time: 37309.20s
                               ETA: 1237360.0s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.001s, learning 0.285s)
               Value function loss: 2.6814
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: -162.55
               Mean episode length: 293.45
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 12.29s
                        Total time: 37321.48s
                               ETA: 1237332.0s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.169s, learning 0.226s)
               Value function loss: 2.9243
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: -159.63
               Mean episode length: 292.95
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 12.40s
                        Total time: 37333.88s
                               ETA: 1237307.6s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.370s, learning 0.295s)
               Value function loss: 2.9010
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -145.71
               Mean episode length: 292.95
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 12.66s
                        Total time: 37346.54s
                               ETA: 1237292.2s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.247s, learning 0.243s)
               Value function loss: 3.3611
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: -145.01
               Mean episode length: 292.95
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 12.49s
                        Total time: 37359.03s
                               ETA: 1237271.0s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.239s, learning 0.169s)
               Value function loss: 4.1034
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: -157.98
               Mean episode length: 293.31
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 12.41s
                        Total time: 37371.44s
                               ETA: 1237247.0s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.983s, learning 0.282s)
               Value function loss: 4.5658
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: -137.46
               Mean episode length: 299.31
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 12.26s
                        Total time: 37383.70s
                               ETA: 1237218.3s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.118s, learning 0.196s)
               Value function loss: 2.9991
                    Surrogate loss: -0.0245
             Mean action noise std: 0.71
                       Mean reward: -157.61
               Mean episode length: 299.16
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 12.31s
                        Total time: 37396.02s
                               ETA: 1237191.3s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.731s, learning 0.269s)
               Value function loss: 2.7236
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: -145.96
               Mean episode length: 299.12
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 12.00s
                        Total time: 37408.02s
                               ETA: 1237153.9s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.561s, learning 0.187s)
               Value function loss: 3.0990
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: -141.88
               Mean episode length: 299.07
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 11.75s
                        Total time: 37419.77s
                               ETA: 1237108.2s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.300s, learning 0.183s)
               Value function loss: 3.0615
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: -146.81
               Mean episode length: 298.68
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 12.48s
                        Total time: 37432.25s
                               ETA: 1237086.7s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.756s, learning 0.164s)
               Value function loss: 3.6267
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: -139.27
               Mean episode length: 298.36
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 11.92s
                        Total time: 37444.17s
                               ETA: 1237046.7s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.883s, learning 0.274s)
               Value function loss: 2.9951
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: -150.70
               Mean episode length: 298.56
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 12.16s
                        Total time: 37456.32s
                               ETA: 1237014.6s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.230s, learning 0.258s)
               Value function loss: 3.9450
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: -144.95
               Mean episode length: 298.09
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 12.49s
                        Total time: 37468.81s
                               ETA: 1236993.4s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.726s, learning 0.307s)
               Value function loss: 3.7587
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: -149.94
               Mean episode length: 298.02
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 12.03s
                        Total time: 37480.85s
                               ETA: 1236957.1s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.037s, learning 0.167s)
               Value function loss: 4.2507
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: -150.63
               Mean episode length: 298.46
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 12.20s
                        Total time: 37493.05s
                               ETA: 1236926.6s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1328 steps/s (collection: 11.995s, learning 0.338s)
               Value function loss: 4.6993
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: -145.95
               Mean episode length: 298.46
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 12.33s
                        Total time: 37505.38s
                               ETA: 1236900.2s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.054s, learning 0.168s)
               Value function loss: 5.3349
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: -154.79
               Mean episode length: 299.04
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 12.22s
                        Total time: 37517.60s
                               ETA: 1236870.3s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.844s, learning 0.198s)
               Value function loss: 5.3491
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: -145.84
               Mean episode length: 299.44
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 12.04s
                        Total time: 37529.65s
                               ETA: 1236834.4s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.079s, learning 0.185s)
               Value function loss: 7.1037
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: -136.61
               Mean episode length: 299.89
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 12.26s
                        Total time: 37541.91s
                               ETA: 1236805.9s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.946s, learning 0.275s)
               Value function loss: 9.6194
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: -138.66
               Mean episode length: 299.89
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 12.22s
                        Total time: 37554.13s
                               ETA: 1236776.0s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.769s, learning 0.197s)
               Value function loss: 8.1108
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: -140.70
               Mean episode length: 299.89
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 11.97s
                        Total time: 37566.10s
                               ETA: 1236737.7s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.034s, learning 0.165s)
               Value function loss: 8.3681
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: -134.88
               Mean episode length: 299.60
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 12.20s
                        Total time: 37578.30s
                               ETA: 1236707.0s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.929s, learning 0.197s)
               Value function loss: 9.6928
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: -137.59
               Mean episode length: 299.57
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 12.13s
                        Total time: 37590.42s
                               ETA: 1236674.0s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.784s, learning 0.216s)
               Value function loss: 11.1189
                    Surrogate loss: 0.0214
             Mean action noise std: 0.71
                       Mean reward: -131.71
               Mean episode length: 299.12
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 12.00s
                        Total time: 37602.42s
                               ETA: 1236636.8s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.731s, learning 0.341s)
               Value function loss: 9.5942
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: -128.08
               Mean episode length: 299.12
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 12.07s
                        Total time: 37614.50s
                               ETA: 1236602.1s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.316s, learning 0.281s)
               Value function loss: 9.8234
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: -119.88
               Mean episode length: 299.12
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 12.60s
                        Total time: 37627.09s
                               ETA: 1236584.6s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.094s, learning 0.170s)
               Value function loss: 10.6684
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: -124.14
               Mean episode length: 299.12
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 12.26s
                        Total time: 37639.36s
                               ETA: 1236556.1s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.708s, learning 0.239s)
               Value function loss: 8.2451
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: -125.85
               Mean episode length: 298.26
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 11.95s
                        Total time: 37651.30s
                               ETA: 1236517.2s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.001s, learning 0.264s)
               Value function loss: 13.8055
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: -119.16
               Mean episode length: 297.65
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 12.27s
                        Total time: 37663.57s
                               ETA: 1236488.9s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.220s, learning 0.255s)
               Value function loss: 14.9823
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: -128.38
               Mean episode length: 298.18
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 12.48s
                        Total time: 37676.04s
                               ETA: 1236467.4s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.336s, learning 0.334s)
               Value function loss: 8.4561
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: -148.58
               Mean episode length: 298.64
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 12.67s
                        Total time: 37688.71s
                               ETA: 1236452.3s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.410s, learning 0.282s)
               Value function loss: 9.3593
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: -172.03
               Mean episode length: 298.37
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 12.69s
                        Total time: 37701.41s
                               ETA: 1236438.0s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.244s, learning 0.284s)
               Value function loss: 10.6279
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: -197.82
               Mean episode length: 297.70
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 12.53s
                        Total time: 37713.94s
                               ETA: 1236418.2s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.004s, learning 0.189s)
               Value function loss: 11.8760
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: -206.13
               Mean episode length: 299.46
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 12.19s
                        Total time: 37726.13s
                               ETA: 1236387.5s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.226s, learning 0.337s)
               Value function loss: 12.5281
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: -185.06
               Mean episode length: 299.46
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 12.56s
                        Total time: 37738.69s
                               ETA: 1236369.0s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.332s, learning 0.225s)
               Value function loss: 50.6639
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: -182.28
               Mean episode length: 300.00
                  Mean reward/step: -1.17
       Mean episode length/episode: 5.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 12.56s
                        Total time: 37751.25s
                               ETA: 1236350.2s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.224s, learning 0.297s)
               Value function loss: 1.2298
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: -183.89
               Mean episode length: 300.00
                  Mean reward/step: -1.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 12.52s
                        Total time: 37763.77s
                               ETA: 1236330.3s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.163s, learning 0.306s)
               Value function loss: 1.7923
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: -191.62
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 12.47s
                        Total time: 37776.24s
                               ETA: 1236308.7s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.455s, learning 0.186s)
               Value function loss: 1.9461
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: -197.43
               Mean episode length: 300.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 11.64s
                        Total time: 37787.88s
                               ETA: 1236259.9s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.271s, learning 0.190s)
               Value function loss: 1.6477
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: -205.96
               Mean episode length: 299.70
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 12.46s
                        Total time: 37800.34s
                               ETA: 1236238.1s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.109s, learning 0.292s)
               Value function loss: 2.0461
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: -217.54
               Mean episode length: 299.70
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 12.40s
                        Total time: 37812.74s
                               ETA: 1236214.2s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.219s, learning 0.219s)
               Value function loss: 2.2006
                    Surrogate loss: -0.0240
             Mean action noise std: 0.71
                       Mean reward: -229.97
               Mean episode length: 299.70
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 12.44s
                        Total time: 37825.18s
                               ETA: 1236191.6s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.990s, learning 0.204s)
               Value function loss: 3.6377
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: -227.38
               Mean episode length: 299.67
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 12.19s
                        Total time: 37837.37s
                               ETA: 1236161.0s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.207s, learning 0.282s)
               Value function loss: 2.9269
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: -217.12
               Mean episode length: 299.87
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 11.49s
                        Total time: 37848.86s
                               ETA: 1236107.4s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.119s, learning 0.253s)
               Value function loss: 2.5760
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: -215.40
               Mean episode length: 299.87
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 12.37s
                        Total time: 37861.23s
                               ETA: 1236082.7s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.060s, learning 0.285s)
               Value function loss: 2.2781
                    Surrogate loss: -0.0259
             Mean action noise std: 0.71
                       Mean reward: -212.48
               Mean episode length: 295.67
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 12.34s
                        Total time: 37873.58s
                               ETA: 1236057.1s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.875s, learning 0.179s)
               Value function loss: 2.4622
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: -218.55
               Mean episode length: 295.67
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 12.05s
                        Total time: 37885.63s
                               ETA: 1236022.0s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.488s, learning 0.263s)
               Value function loss: 2.4739
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: -211.95
               Mean episode length: 295.70
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 12.75s
                        Total time: 37898.39s
                               ETA: 1236009.6s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.422s, learning 0.277s)
               Value function loss: 2.9468
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: -209.10
               Mean episode length: 295.14
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 12.70s
                        Total time: 37911.08s
                               ETA: 1235995.6s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.391s, learning 0.390s)
               Value function loss: 3.2643
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: -213.18
               Mean episode length: 293.38
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 12.78s
                        Total time: 37923.87s
                               ETA: 1235984.2s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.947s, learning 0.283s)
               Value function loss: 3.1151
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: -206.92
               Mean episode length: 295.15
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 12.23s
                        Total time: 37936.09s
                               ETA: 1235954.9s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.069s, learning 0.266s)
               Value function loss: 3.0222
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: -196.50
               Mean episode length: 295.19
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 12.34s
                        Total time: 37948.43s
                               ETA: 1235929.0s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.009s, learning 0.297s)
               Value function loss: 3.4959
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: -196.43
               Mean episode length: 293.45
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 12.31s
                        Total time: 37960.74s
                               ETA: 1235902.2s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.539s, learning 0.230s)
               Value function loss: 3.9146
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: -218.28
               Mean episode length: 293.49
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 12.77s
                        Total time: 37973.50s
                               ETA: 1235890.4s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.198s, learning 0.196s)
               Value function loss: 4.2830
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: -211.15
               Mean episode length: 294.01
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 12.39s
                        Total time: 37985.90s
                               ETA: 1235866.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.846s, learning 0.182s)
               Value function loss: 4.9187
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -208.30
               Mean episode length: 295.87
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 12.03s
                        Total time: 37997.93s
                               ETA: 1235830.7s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.810s, learning 0.282s)
               Value function loss: 5.6773
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: -215.57
               Mean episode length: 295.87
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 12.09s
                        Total time: 38010.02s
                               ETA: 1235796.9s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.216s, learning 0.182s)
               Value function loss: 5.4679
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -224.57
               Mean episode length: 295.87
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 12.40s
                        Total time: 38022.42s
                               ETA: 1235773.1s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.913s, learning 0.212s)
               Value function loss: 6.2027
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: -225.69
               Mean episode length: 296.28
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 12.13s
                        Total time: 38034.54s
                               ETA: 1235740.4s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.094s, learning 0.282s)
               Value function loss: 6.7419
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: -229.16
               Mean episode length: 298.26
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 12.38s
                        Total time: 38046.92s
                               ETA: 1235716.0s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1263 steps/s (collection: 12.682s, learning 0.286s)
               Value function loss: 6.8839
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: -234.85
               Mean episode length: 300.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 12.97s
                        Total time: 38059.88s
                               ETA: 1235710.7s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.089s, learning 0.380s)
               Value function loss: 6.6848
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: -226.93
               Mean episode length: 300.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 12.47s
                        Total time: 38072.35s
                               ETA: 1235689.2s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.030s, learning 0.208s)
               Value function loss: 6.7781
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: -223.16
               Mean episode length: 299.93
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 12.24s
                        Total time: 38084.59s
                               ETA: 1235660.3s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.117s, learning 0.365s)
               Value function loss: 6.8711
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: -225.75
               Mean episode length: 299.43
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 12.48s
                        Total time: 38097.07s
                               ETA: 1235639.3s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.826s, learning 0.193s)
               Value function loss: 7.1677
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: -216.50
               Mean episode length: 299.43
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 12.02s
                        Total time: 38109.09s
                               ETA: 1235603.2s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.026s, learning 0.317s)
               Value function loss: 9.6465
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: -215.33
               Mean episode length: 298.79
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 12.34s
                        Total time: 38121.43s
                               ETA: 1235577.7s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.316s, learning 0.297s)
               Value function loss: 7.0658
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: -206.18
               Mean episode length: 297.04
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 12.61s
                        Total time: 38134.05s
                               ETA: 1235561.0s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1246 steps/s (collection: 12.838s, learning 0.302s)
               Value function loss: 8.5222
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: -205.89
               Mean episode length: 296.36
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 13.14s
                        Total time: 38147.19s
                               ETA: 1235561.3s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.324s, learning 0.309s)
               Value function loss: 10.5978
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: -211.20
               Mean episode length: 298.08
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 12.63s
                        Total time: 38159.82s
                               ETA: 1235545.2s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.032s, learning 0.200s)
               Value function loss: 10.8658
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: -213.26
               Mean episode length: 298.91
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 12.23s
                        Total time: 38172.05s
                               ETA: 1235516.1s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.224s, learning 0.404s)
               Value function loss: 9.8668
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: -206.95
               Mean episode length: 299.42
                  Mean reward/step: -0.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 12.63s
                        Total time: 38184.68s
                               ETA: 1235499.9s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.305s, learning 0.275s)
               Value function loss: 9.6607
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: -208.95
               Mean episode length: 299.62
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 12.58s
                        Total time: 38197.26s
                               ETA: 1235482.1s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.158s, learning 0.322s)
               Value function loss: 80.3741
                    Surrogate loss: 0.0196
             Mean action noise std: 0.71
                       Mean reward: -190.54
               Mean episode length: 300.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 5.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 12.48s
                        Total time: 38209.74s
                               ETA: 1235461.1s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.185s, learning 0.220s)
               Value function loss: 1.8129
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: -196.33
               Mean episode length: 300.00
                  Mean reward/step: -1.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 12.40s
                        Total time: 38222.15s
                               ETA: 1235437.6s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.592s, learning 0.190s)
               Value function loss: 1.5528
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: -206.75
               Mean episode length: 300.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 12.78s
                        Total time: 38234.93s
                               ETA: 1235426.3s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.134s, learning 0.212s)
               Value function loss: 1.3073
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: -205.73
               Mean episode length: 300.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 12.35s
                        Total time: 38247.28s
                               ETA: 1235401.0s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.835s, learning 0.235s)
               Value function loss: 1.6191
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: -202.27
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 12.07s
                        Total time: 38259.35s
                               ETA: 1235366.8s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.342s, learning 0.215s)
               Value function loss: 1.4696
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: -209.32
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 12.56s
                        Total time: 38271.90s
                               ETA: 1235348.2s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.954s, learning 0.203s)
               Value function loss: 1.3986
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: -212.56
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 12.16s
                        Total time: 38284.06s
                               ETA: 1235316.8s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.980s, learning 0.195s)
               Value function loss: 2.5253
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: -220.23
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 12.17s
                        Total time: 38296.23s
                               ETA: 1235286.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.049s, learning 0.170s)
               Value function loss: 2.6673
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: -222.94
               Mean episode length: 300.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 12.22s
                        Total time: 38308.45s
                               ETA: 1235256.6s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.967s, learning 0.190s)
               Value function loss: 1.5823
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: -221.68
               Mean episode length: 300.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 12.16s
                        Total time: 38320.61s
                               ETA: 1235225.2s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.956s, learning 0.263s)
               Value function loss: 1.2405
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: -223.58
               Mean episode length: 300.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 12.22s
                        Total time: 38332.83s
                               ETA: 1235195.8s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.749s, learning 0.179s)
               Value function loss: 1.8910
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: -217.33
               Mean episode length: 297.88
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 11.93s
                        Total time: 38344.76s
                               ETA: 1235157.1s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.787s, learning 0.203s)
               Value function loss: 1.7532
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: -219.65
               Mean episode length: 297.64
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 11.99s
                        Total time: 38356.75s
                               ETA: 1235120.3s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.927s, learning 0.186s)
               Value function loss: 1.6437
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: -221.82
               Mean episode length: 297.64
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 12.11s
                        Total time: 38368.86s
                               ETA: 1235087.6s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.043s, learning 0.178s)
               Value function loss: 1.5952
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: -224.46
               Mean episode length: 297.64
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 12.22s
                        Total time: 38381.08s
                               ETA: 1235058.3s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.686s, learning 0.176s)
               Value function loss: 2.3807
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: -226.70
               Mean episode length: 297.64
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 11.86s
                        Total time: 38392.94s
                               ETA: 1235017.5s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.811s, learning 0.181s)
               Value function loss: 2.1585
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: -232.14
               Mean episode length: 296.97
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 11.99s
                        Total time: 38404.93s
                               ETA: 1234980.9s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.666s, learning 0.219s)
               Value function loss: 2.3396
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: -240.06
               Mean episode length: 299.09
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 11.89s
                        Total time: 38416.82s
                               ETA: 1234940.9s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.970s, learning 0.199s)
               Value function loss: 2.9616
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: -247.12
               Mean episode length: 299.33
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 12.17s
                        Total time: 38428.99s
                               ETA: 1234910.1s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.912s, learning 0.179s)
               Value function loss: 3.4892
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: -245.85
               Mean episode length: 299.33
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 12.09s
                        Total time: 38441.08s
                               ETA: 1234876.7s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.947s, learning 0.191s)
               Value function loss: 4.1035
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: -240.99
               Mean episode length: 299.33
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 12.14s
                        Total time: 38453.22s
                               ETA: 1234844.9s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.986s, learning 0.167s)
               Value function loss: 4.6175
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: -236.23
               Mean episode length: 299.33
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 12.15s
                        Total time: 38465.37s
                               ETA: 1234813.6s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.885s, learning 0.211s)
               Value function loss: 5.7949
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: -235.28
               Mean episode length: 299.33
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 12.10s
                        Total time: 38477.47s
                               ETA: 1234780.4s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.494s, learning 0.207s)
               Value function loss: 7.1495
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: -231.52
               Mean episode length: 300.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 12.70s
                        Total time: 38490.17s
                               ETA: 1234766.7s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.091s, learning 0.185s)
               Value function loss: 9.9876
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: -220.70
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 12.28s
                        Total time: 38502.45s
                               ETA: 1234739.3s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.725s, learning 0.185s)
               Value function loss: 22.1951
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: -219.15
               Mean episode length: 300.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 11.91s
                        Total time: 38514.36s
                               ETA: 1234700.2s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.836s, learning 0.197s)
               Value function loss: 21.1931
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: -210.73
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 12.03s
                        Total time: 38526.39s
                               ETA: 1234665.1s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.025s, learning 0.250s)
               Value function loss: 20.4928
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: -205.40
               Mean episode length: 300.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 12.27s
                        Total time: 38538.66s
                               ETA: 1234637.7s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.036s, learning 0.176s)
               Value function loss: 10.4585
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: -208.87
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 12.21s
                        Total time: 38550.88s
                               ETA: 1234608.3s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.915s, learning 0.198s)
               Value function loss: 7.7132
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: -212.15
               Mean episode length: 299.69
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 12.11s
                        Total time: 38562.99s
                               ETA: 1234575.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.824s, learning 0.179s)
               Value function loss: 8.0152
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: -207.92
               Mean episode length: 297.59
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 12.00s
                        Total time: 38574.99s
                               ETA: 1234539.8s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.634s, learning 0.171s)
               Value function loss: 6.7436
                    Surrogate loss: 0.0116
             Mean action noise std: 0.71
                       Mean reward: -215.88
               Mean episode length: 297.59
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.80s
                        Total time: 38586.80s
                               ETA: 1234497.4s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.906s, learning 0.197s)
               Value function loss: 6.9500
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: -220.92
               Mean episode length: 297.59
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 12.10s
                        Total time: 38598.90s
                               ETA: 1234464.6s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.170s, learning 0.224s)
               Value function loss: 7.7898
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: -213.50
               Mean episode length: 298.82
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 12.39s
                        Total time: 38611.29s
                               ETA: 1234441.1s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.991s, learning 0.183s)
               Value function loss: 7.7690
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: -191.41
               Mean episode length: 298.59
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 12.17s
                        Total time: 38623.47s
                               ETA: 1234410.6s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.963s, learning 0.271s)
               Value function loss: 7.5880
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: -186.83
               Mean episode length: 299.13
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 12.23s
                        Total time: 38635.70s
                               ETA: 1234382.0s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.018s, learning 0.172s)
               Value function loss: 8.0908
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: -182.02
               Mean episode length: 299.04
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 12.19s
                        Total time: 38647.89s
                               ETA: 1234352.0s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.032s, learning 0.177s)
               Value function loss: 8.0813
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: -169.10
               Mean episode length: 299.65
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 12.21s
                        Total time: 38660.10s
                               ETA: 1234322.7s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.905s, learning 0.182s)
               Value function loss: 51.0562
                    Surrogate loss: 0.0303
             Mean action noise std: 0.71
                       Mean reward: -183.70
               Mean episode length: 300.00
                  Mean reward/step: -1.09
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 12.09s
                        Total time: 38672.19s
                               ETA: 1234289.5s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.871s, learning 0.197s)
               Value function loss: 1.1804
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: -180.94
               Mean episode length: 299.81
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 12.07s
                        Total time: 38684.26s
                               ETA: 1234255.6s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.527s, learning 0.201s)
               Value function loss: 1.7410
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: -185.14
               Mean episode length: 296.60
                  Mean reward/step: -0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 11.73s
                        Total time: 38695.98s
                               ETA: 1234211.0s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.667s, learning 0.194s)
               Value function loss: 1.5126
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: -185.26
               Mean episode length: 296.55
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 11.86s
                        Total time: 38707.85s
                               ETA: 1234170.6s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.133s, learning 0.320s)
               Value function loss: 1.6011
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: -188.36
               Mean episode length: 296.55
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 12.45s
                        Total time: 38720.30s
                               ETA: 1234149.1s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.273s, learning 0.200s)
               Value function loss: 1.3940
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: -185.27
               Mean episode length: 296.40
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 12.47s
                        Total time: 38732.77s
                               ETA: 1234128.2s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.664s, learning 0.185s)
               Value function loss: 1.6378
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: -184.90
               Mean episode length: 296.40
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 11.85s
                        Total time: 38744.62s
                               ETA: 1234087.4s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.785s, learning 0.166s)
               Value function loss: 2.6710
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: -194.68
               Mean episode length: 299.80
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 11.95s
                        Total time: 38756.57s
                               ETA: 1234050.0s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.655s, learning 0.171s)
               Value function loss: 2.0515
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: -188.01
               Mean episode length: 299.85
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 11.83s
                        Total time: 38768.40s
                               ETA: 1234008.5s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.842s, learning 0.176s)
               Value function loss: 1.9128
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: -186.01
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 12.02s
                        Total time: 38780.42s
                               ETA: 1233973.2s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.320s, learning 0.229s)
               Value function loss: 1.9875
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: -181.63
               Mean episode length: 300.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 12.55s
                        Total time: 38792.96s
                               ETA: 1233954.8s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.475s, learning 0.191s)
               Value function loss: 2.3965
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: -179.48
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 11.67s
                        Total time: 38804.63s
                               ETA: 1233908.4s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.663s, learning 0.168s)
               Value function loss: 2.8368
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: -170.71
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 11.83s
                        Total time: 38816.46s
                               ETA: 1233867.2s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.118s, learning 0.311s)
               Value function loss: 3.9222
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: -167.62
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 12.43s
                        Total time: 38828.89s
                               ETA: 1233845.0s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.959s, learning 0.238s)
               Value function loss: 4.3388
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: -161.28
               Mean episode length: 299.54
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 12.20s
                        Total time: 38841.09s
                               ETA: 1233815.4s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.111s, learning 0.183s)
               Value function loss: 4.3823
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: -166.72
               Mean episode length: 299.54
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 12.29s
                        Total time: 38853.38s
                               ETA: 1233789.0s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.786s, learning 0.181s)
               Value function loss: 5.7634
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: -162.24
               Mean episode length: 299.54
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 11.97s
                        Total time: 38865.35s
                               ETA: 1233752.2s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.765s, learning 0.201s)
               Value function loss: 6.5595
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: -163.34
               Mean episode length: 299.54
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 11.97s
                        Total time: 38877.32s
                               ETA: 1233715.3s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.675s, learning 0.177s)
               Value function loss: 6.1260
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: -166.35
               Mean episode length: 299.54
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 11.85s
                        Total time: 38889.17s
                               ETA: 1233674.9s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.989s, learning 0.195s)
               Value function loss: 6.1474
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: -172.39
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 12.18s
                        Total time: 38901.35s
                               ETA: 1233645.0s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.331s, learning 0.186s)
               Value function loss: 5.4040
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: -170.37
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 12.52s
                        Total time: 38913.87s
                               ETA: 1233625.7s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.735s, learning 0.187s)
               Value function loss: 4.7958
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: -173.89
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 11.92s
                        Total time: 38925.79s
                               ETA: 1233587.5s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.159s, learning 0.239s)
               Value function loss: 11.4419
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: -172.83
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 12.40s
                        Total time: 38938.19s
                               ETA: 1233564.4s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.096s, learning 0.178s)
               Value function loss: 9.3879
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: -174.08
               Mean episode length: 300.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 12.27s
                        Total time: 38950.46s
                               ETA: 1233537.4s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.925s, learning 0.167s)
               Value function loss: 10.0154
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: -174.49
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 12.09s
                        Total time: 38962.56s
                               ETA: 1233504.6s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.989s, learning 0.188s)
               Value function loss: 13.1908
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: -172.14
               Mean episode length: 300.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 12.18s
                        Total time: 38974.73s
                               ETA: 1233474.6s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.016s, learning 0.173s)
               Value function loss: 12.8789
                    Surrogate loss: 0.0160
             Mean action noise std: 0.71
                       Mean reward: -173.94
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 12.19s
                        Total time: 38986.92s
                               ETA: 1233444.9s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.902s, learning 0.201s)
               Value function loss: 4.8237
                    Surrogate loss: 0.0037
             Mean action noise std: 0.71
                       Mean reward: -174.03
               Mean episode length: 300.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 12.10s
                        Total time: 38999.02s
                               ETA: 1233412.5s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.952s, learning 0.278s)
               Value function loss: 4.4532
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: -174.00
               Mean episode length: 300.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 12.23s
                        Total time: 39011.25s
                               ETA: 1233384.2s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.943s, learning 0.200s)
               Value function loss: 3.7710
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: -173.25
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 12.14s
                        Total time: 39023.40s
                               ETA: 1233353.1s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.000s, learning 0.182s)
               Value function loss: 4.0678
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: -170.85
               Mean episode length: 300.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 12.18s
                        Total time: 39035.58s
                               ETA: 1233323.3s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.907s, learning 0.168s)
               Value function loss: 4.7162
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: -180.77
               Mean episode length: 300.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 12.07s
                        Total time: 39047.65s
                               ETA: 1233290.0s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.022s, learning 0.176s)
               Value function loss: 5.0133
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: -179.61
               Mean episode length: 300.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 12.20s
                        Total time: 39059.85s
                               ETA: 1233260.7s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.313s, learning 0.279s)
               Value function loss: 6.4926
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: -174.90
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 12.59s
                        Total time: 39072.44s
                               ETA: 1233243.9s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.009s, learning 0.229s)
               Value function loss: 5.7257
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: -175.68
               Mean episode length: 299.71
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 12.24s
                        Total time: 39084.68s
                               ETA: 1233215.9s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.580s, learning 0.226s)
               Value function loss: 7.0874
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: -166.22
               Mean episode length: 300.00
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 11.81s
                        Total time: 39096.49s
                               ETA: 1233174.2s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.771s, learning 0.180s)
               Value function loss: 5.8721
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: -158.89
               Mean episode length: 300.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 11.95s
                        Total time: 39108.44s
                               ETA: 1233137.2s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.140s, learning 0.186s)
               Value function loss: 54.3570
                    Surrogate loss: 0.0414
             Mean action noise std: 0.71
                       Mean reward: -149.14
               Mean episode length: 300.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 5.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 12.33s
                        Total time: 39120.77s
                               ETA: 1233112.0s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.907s, learning 0.173s)
               Value function loss: 1.0366
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: -150.10
               Mean episode length: 300.00
                  Mean reward/step: -1.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 12.08s
                        Total time: 39132.85s
                               ETA: 1233079.0s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.962s, learning 0.187s)
               Value function loss: 1.2380
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: -154.35
               Mean episode length: 299.99
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 12.15s
                        Total time: 39144.99s
                               ETA: 1233048.2s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.382s, learning 0.210s)
               Value function loss: 1.1722
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: -159.39
               Mean episode length: 299.99
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 12.59s
                        Total time: 39157.59s
                               ETA: 1233031.4s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.901s, learning 0.190s)
               Value function loss: 1.3293
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: -162.26
               Mean episode length: 299.99
                  Mean reward/step: -0.77
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 12.09s
                        Total time: 39169.68s
                               ETA: 1232998.8s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.214s, learning 0.337s)
               Value function loss: 1.4503
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: -163.39
               Mean episode length: 299.99
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 12.55s
                        Total time: 39182.23s
                               ETA: 1232980.7s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.177s, learning 0.183s)
               Value function loss: 1.4925
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: -162.97
               Mean episode length: 299.99
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 12.36s
                        Total time: 39194.59s
                               ETA: 1232956.6s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.463s, learning 0.219s)
               Value function loss: 2.7246
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: -168.96
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 12.68s
                        Total time: 39207.27s
                               ETA: 1232942.7s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.944s, learning 0.218s)
               Value function loss: 3.1608
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: -171.93
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 12.16s
                        Total time: 39219.43s
                               ETA: 1232912.3s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.990s, learning 0.213s)
               Value function loss: 2.5115
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: -167.38
               Mean episode length: 300.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 12.20s
                        Total time: 39231.63s
                               ETA: 1232883.4s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.806s, learning 0.185s)
               Value function loss: 2.6872
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: -170.72
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 11.99s
                        Total time: 39243.63s
                               ETA: 1232847.7s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.211s, learning 0.307s)
               Value function loss: 3.3213
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: -168.54
               Mean episode length: 300.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 12.52s
                        Total time: 39256.14s
                               ETA: 1232828.6s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.555s, learning 0.309s)
               Value function loss: 3.4390
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: -168.78
               Mean episode length: 300.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 12.86s
                        Total time: 39269.01s
                               ETA: 1232820.4s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.553s, learning 0.358s)
               Value function loss: 3.8648
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: -172.54
               Mean episode length: 300.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 12.91s
                        Total time: 39281.92s
                               ETA: 1232813.7s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.159s, learning 0.191s)
               Value function loss: 4.4438
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: -165.91
               Mean episode length: 300.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 12.35s
                        Total time: 39294.27s
                               ETA: 1232789.3s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.489s, learning 0.266s)
               Value function loss: 4.1913
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: -160.76
               Mean episode length: 298.18
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 12.76s
                        Total time: 39307.02s
                               ETA: 1232777.7s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.076s, learning 0.198s)
               Value function loss: 3.4433
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: -156.81
               Mean episode length: 298.18
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 12.27s
                        Total time: 39319.30s
                               ETA: 1232751.0s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.830s, learning 0.193s)
               Value function loss: 3.0162
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: -147.23
               Mean episode length: 298.18
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 12.02s
                        Total time: 39331.32s
                               ETA: 1232716.4s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.752s, learning 0.256s)
               Value function loss: 3.1361
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: -143.39
               Mean episode length: 298.18
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 12.01s
                        Total time: 39343.33s
                               ETA: 1232681.4s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.902s, learning 0.216s)
               Value function loss: 3.0304
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: -136.85
               Mean episode length: 298.18
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 12.12s
                        Total time: 39355.45s
                               ETA: 1232649.8s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.276s, learning 0.250s)
               Value function loss: 3.3007
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: -141.43
               Mean episode length: 298.18
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 12.53s
                        Total time: 39367.98s
                               ETA: 1232631.0s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1336 steps/s (collection: 11.966s, learning 0.295s)
               Value function loss: 3.3506
                    Surrogate loss: 0.0201
             Mean action noise std: 0.71
                       Mean reward: -134.47
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 12.26s
                        Total time: 39380.24s
                               ETA: 1232603.9s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.682s, learning 0.218s)
               Value function loss: 2.7104
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: -126.25
               Mean episode length: 300.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 12.90s
                        Total time: 39393.14s
                               ETA: 1232596.9s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.196s, learning 0.221s)
               Value function loss: 2.7316
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: -123.87
               Mean episode length: 300.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 12.42s
                        Total time: 39405.55s
                               ETA: 1232574.7s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.748s, learning 0.189s)
               Value function loss: 3.0713
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: -128.44
               Mean episode length: 300.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 11.94s
                        Total time: 39417.49s
                               ETA: 1232537.5s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.187s, learning 0.188s)
               Value function loss: 3.8392
                    Surrogate loss: 0.0077
             Mean action noise std: 0.71
                       Mean reward: -139.86
               Mean episode length: 300.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 12.37s
                        Total time: 39429.87s
                               ETA: 1232514.0s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.362s, learning 0.301s)
               Value function loss: 3.0577
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: -146.41
               Mean episode length: 300.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 12.66s
                        Total time: 39442.53s
                               ETA: 1232499.5s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.193s, learning 0.279s)
               Value function loss: 3.2910
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: -152.05
               Mean episode length: 300.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 12.47s
                        Total time: 39455.00s
                               ETA: 1232479.1s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.731s, learning 0.307s)
               Value function loss: 3.1990
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: -149.81
               Mean episode length: 300.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 13.04s
                        Total time: 39468.04s
                               ETA: 1232476.3s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.525s, learning 0.231s)
               Value function loss: 3.5225
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: -156.55
               Mean episode length: 299.49
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 12.76s
                        Total time: 39480.80s
                               ETA: 1232464.8s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.256s, learning 0.254s)
               Value function loss: 4.0228
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: -162.71
               Mean episode length: 299.49
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 12.51s
                        Total time: 39493.31s
                               ETA: 1232445.5s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.182s, learning 0.302s)
               Value function loss: 4.1608
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: -172.18
               Mean episode length: 299.49
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 12.48s
                        Total time: 39505.79s
                               ETA: 1232425.5s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.544s, learning 0.222s)
               Value function loss: 5.6809
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: -169.23
               Mean episode length: 299.49
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 12.77s
                        Total time: 39518.56s
                               ETA: 1232414.2s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.340s, learning 0.220s)
               Value function loss: 6.8757
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: -167.78
               Mean episode length: 300.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 12.56s
                        Total time: 39531.12s
                               ETA: 1232396.5s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.891s, learning 0.216s)
               Value function loss: 7.0524
                    Surrogate loss: 0.0294
             Mean action noise std: 0.71
                       Mean reward: -176.27
               Mean episode length: 299.80
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 13.11s
                        Total time: 39544.22s
                               ETA: 1232395.9s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.764s, learning 0.177s)
               Value function loss: 5.8239
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: -171.44
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 12.94s
                        Total time: 39557.16s
                               ETA: 1232390.1s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.420s, learning 0.199s)
               Value function loss: 6.9697
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: -162.50
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 12.62s
                        Total time: 39569.78s
                               ETA: 1232374.2s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.266s, learning 0.315s)
               Value function loss: 10.0442
                    Surrogate loss: 0.0227
             Mean action noise std: 0.71
                       Mean reward: -159.20
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 12.58s
                        Total time: 39582.36s
                               ETA: 1232357.2s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.079s, learning 0.173s)
               Value function loss: 48.9715
                    Surrogate loss: 0.0099
             Mean action noise std: 0.71
                       Mean reward: -140.92
               Mean episode length: 300.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 5.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 12.25s
                        Total time: 39594.62s
                               ETA: 1232329.9s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.185s, learning 0.260s)
               Value function loss: 1.2140
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: -140.32
               Mean episode length: 300.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 12.44s
                        Total time: 39607.06s
                               ETA: 1232308.7s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.125s, learning 0.173s)
               Value function loss: 1.8118
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: -132.69
               Mean episode length: 300.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 12.30s
                        Total time: 39619.36s
                               ETA: 1232282.9s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.653s, learning 0.170s)
               Value function loss: 1.4218
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: -133.62
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 11.82s
                        Total time: 39631.18s
                               ETA: 1232242.3s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.003s, learning 0.172s)
               Value function loss: 1.6903
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: -136.20
               Mean episode length: 300.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 12.18s
                        Total time: 39643.36s
                               ETA: 1232212.7s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.645s, learning 0.210s)
               Value function loss: 1.4772
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: -130.86
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 12.86s
                        Total time: 39656.21s
                               ETA: 1232204.2s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.214s, learning 0.242s)
               Value function loss: 2.0770
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: -132.50
               Mean episode length: 300.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 12.46s
                        Total time: 39668.67s
                               ETA: 1232183.4s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.727s, learning 0.190s)
               Value function loss: 3.1462
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: -148.64
               Mean episode length: 300.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 11.92s
                        Total time: 39680.59s
                               ETA: 1232145.8s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.048s, learning 0.166s)
               Value function loss: 2.9259
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: -149.90
               Mean episode length: 300.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 12.21s
                        Total time: 39692.80s
                               ETA: 1232117.4s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.718s, learning 0.186s)
               Value function loss: 3.0178
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: -148.22
               Mean episode length: 300.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 11.90s
                        Total time: 39704.70s
                               ETA: 1232079.4s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.793s, learning 0.202s)
               Value function loss: 3.4411
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: -153.38
               Mean episode length: 300.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 12.00s
                        Total time: 39716.70s
                               ETA: 1232044.3s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.009s, learning 0.306s)
               Value function loss: 3.8686
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: -147.01
               Mean episode length: 300.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 12.31s
                        Total time: 39729.01s
                               ETA: 1232019.1s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.027s, learning 0.177s)
               Value function loss: 3.6478
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: -144.34
               Mean episode length: 300.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 12.20s
                        Total time: 39741.22s
                               ETA: 1231990.4s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.910s, learning 0.170s)
               Value function loss: 3.9219
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: -138.55
               Mean episode length: 300.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 12.08s
                        Total time: 39753.30s
                               ETA: 1231958.0s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.827s, learning 0.176s)
               Value function loss: 4.0957
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: -138.76
               Mean episode length: 300.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 12.00s
                        Total time: 39765.30s
                               ETA: 1231923.2s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.866s, learning 0.186s)
               Value function loss: 3.5300
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: -134.58
               Mean episode length: 300.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 12.05s
                        Total time: 39777.35s
                               ETA: 1231889.8s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.897s, learning 0.202s)
               Value function loss: 2.8386
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: -129.59
               Mean episode length: 300.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 12.10s
                        Total time: 39789.45s
                               ETA: 1231858.0s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.909s, learning 0.181s)
               Value function loss: 2.5282
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: -128.28
               Mean episode length: 300.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 12.09s
                        Total time: 39801.54s
                               ETA: 1231825.9s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.190s, learning 0.189s)
               Value function loss: 2.2058
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: -131.25
               Mean episode length: 300.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 12.38s
                        Total time: 39813.92s
                               ETA: 1231802.8s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.515s, learning 0.178s)
               Value function loss: 2.5399
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: -132.47
               Mean episode length: 300.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 11.69s
                        Total time: 39825.61s
                               ETA: 1231758.4s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.898s, learning 0.192s)
               Value function loss: 2.6318
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: -135.64
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 12.09s
                        Total time: 39837.70s
                               ETA: 1231726.4s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.296s, learning 0.210s)
               Value function loss: 2.7983
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: -132.53
               Mean episode length: 300.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 12.51s
                        Total time: 39850.21s
                               ETA: 1231707.2s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.399s, learning 0.237s)
               Value function loss: 2.2746
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: -127.87
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 12.64s
                        Total time: 39862.85s
                               ETA: 1231692.0s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.043s, learning 0.173s)
               Value function loss: 2.2397
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: -126.03
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 12.22s
                        Total time: 39875.06s
                               ETA: 1231663.9s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.978s, learning 0.177s)
               Value function loss: 2.7108
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: -124.07
               Mean episode length: 300.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 12.16s
                        Total time: 39887.22s
                               ETA: 1231633.9s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.426s, learning 0.292s)
               Value function loss: 4.2502
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: -111.36
               Mean episode length: 300.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 12.72s
                        Total time: 39899.93s
                               ETA: 1231621.2s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.169s, learning 0.172s)
               Value function loss: 3.1912
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: -102.42
               Mean episode length: 300.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 12.34s
                        Total time: 39912.28s
                               ETA: 1231596.9s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.095s, learning 0.189s)
               Value function loss: 3.6098
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: -101.60
               Mean episode length: 300.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 12.28s
                        Total time: 39924.56s
                               ETA: 1231570.9s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.953s, learning 0.211s)
               Value function loss: 8.9323
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: -87.13
               Mean episode length: 300.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 12.16s
                        Total time: 39936.72s
                               ETA: 1231541.2s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.247s, learning 0.264s)
               Value function loss: 15.2487
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: -87.33
               Mean episode length: 300.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 12.51s
                        Total time: 39949.23s
                               ETA: 1231522.2s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.148s, learning 0.320s)
               Value function loss: 19.4255
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: -93.91
               Mean episode length: 300.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 12.47s
                        Total time: 39961.70s
                               ETA: 1231501.9s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.378s, learning 0.174s)
               Value function loss: 34.9922
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: -94.50
               Mean episode length: 300.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 12.55s
                        Total time: 39974.25s
                               ETA: 1231484.2s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.130s, learning 0.176s)
               Value function loss: 11.7175
                    Surrogate loss: 0.0268
             Mean action noise std: 0.71
                       Mean reward: -107.18
               Mean episode length: 300.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 12.31s
                        Total time: 39986.56s
                               ETA: 1231458.9s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.714s, learning 0.179s)
               Value function loss: 8.5846
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: -107.47
               Mean episode length: 300.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 11.89s
                        Total time: 39998.45s
                               ETA: 1231420.9s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.872s, learning 0.182s)
               Value function loss: 8.7923
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: -82.09
               Mean episode length: 300.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 12.05s
                        Total time: 40010.51s
                               ETA: 1231387.9s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.749s, learning 0.211s)
               Value function loss: 8.6055
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: -91.48
               Mean episode length: 300.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 11.96s
                        Total time: 40022.47s
                               ETA: 1231352.0s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.100s, learning 0.196s)
               Value function loss: 8.5669
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: -95.58
               Mean episode length: 300.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 12.30s
                        Total time: 40034.76s
                               ETA: 1231326.4s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.355s, learning 0.332s)
               Value function loss: 59.3065
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: -82.42
               Mean episode length: 300.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 5.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 12.69s
                        Total time: 40047.45s
                               ETA: 1231312.9s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.235s, learning 0.296s)
               Value function loss: 1.3667
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: -88.97
               Mean episode length: 300.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 12.53s
                        Total time: 40059.98s
                               ETA: 1231294.6s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.302s, learning 0.365s)
               Value function loss: 1.4206
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: -91.04
               Mean episode length: 300.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 12.67s
                        Total time: 40072.65s
                               ETA: 1231280.4s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.153s, learning 0.173s)
               Value function loss: 1.5417
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: -88.96
               Mean episode length: 300.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 12.33s
                        Total time: 40084.97s
                               ETA: 1231255.8s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.541s, learning 0.190s)
               Value function loss: 1.4354
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: -88.21
               Mean episode length: 300.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 11.73s
                        Total time: 40096.70s
                               ETA: 1231212.9s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.559s, learning 0.237s)
               Value function loss: 1.6495
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: -92.59
               Mean episode length: 300.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 11.80s
                        Total time: 40108.50s
                               ETA: 1231172.0s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.865s, learning 0.187s)
               Value function loss: 2.0496
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: -79.63
               Mean episode length: 300.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 12.05s
                        Total time: 40120.55s
                               ETA: 1231139.0s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.941s, learning 0.181s)
               Value function loss: 2.7020
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: -71.88
               Mean episode length: 300.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 12.12s
                        Total time: 40132.67s
                               ETA: 1231108.2s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.530s, learning 0.268s)
               Value function loss: 3.1679
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: -91.22
               Mean episode length: 300.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 11.80s
                        Total time: 40144.47s
                               ETA: 1231067.4s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.456s, learning 0.296s)
               Value function loss: 3.1389
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: -89.66
               Mean episode length: 300.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 12.75s
                        Total time: 40157.22s
                               ETA: 1231056.0s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.416s, learning 0.323s)
               Value function loss: 3.3063
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: -95.73
               Mean episode length: 300.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 12.74s
                        Total time: 40169.96s
                               ETA: 1231044.1s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.002s, learning 0.218s)
               Value function loss: 3.7629
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: -92.81
               Mean episode length: 300.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 12.22s
                        Total time: 40182.18s
                               ETA: 1231016.3s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.808s, learning 0.192s)
               Value function loss: 3.6451
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: -94.37
               Mean episode length: 300.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 12.00s
                        Total time: 40194.18s
                               ETA: 1230981.8s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.086s, learning 0.222s)
               Value function loss: 3.8203
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: -77.07
               Mean episode length: 300.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 12.31s
                        Total time: 40206.49s
                               ETA: 1230956.7s

/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041915-2hn2aix0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandDoorOpenOutward_ppo_20221020041913
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/2hn2aix0
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:5
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Using VHACD cache directory '/data/zihan/.isaacgym/vhacd'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj': 4 hulls
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=5, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=5, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:5', seed=None, sim_device='cuda:5', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandDoorOpenOutward', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_door_open_outward', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 250, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.5, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandDoorOpenOutward', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandDoorOpenOutward_ppo_20221020041913', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 2223
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:5
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 708 steps/s (collection: 22.885s, learning 0.251s)
               Value function loss: 7.3958
                    Surrogate loss: 0.0527
             Mean action noise std: 0.80
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 23.14s
                        Total time: 23.14s
                               ETA: 2313627.5s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 715 steps/s (collection: 21.910s, learning 1.002s)
               Value function loss: 0.8499
                    Surrogate loss: -0.0187
             Mean action noise std: 0.80
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 22.91s
                        Total time: 46.05s
                               ETA: 2302354.9s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 725 steps/s (collection: 22.331s, learning 0.248s)
               Value function loss: 0.4269
                    Surrogate loss: -0.0349
             Mean action noise std: 0.80
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 22.58s
                        Total time: 68.63s
                               ETA: 2287499.0s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 715 steps/s (collection: 22.745s, learning 0.163s)
               Value function loss: 0.2729
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 22.91s
                        Total time: 91.53s
                               ETA: 2288276.9s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 714 steps/s (collection: 22.770s, learning 0.168s)
               Value function loss: 0.3583
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 22.94s
                        Total time: 114.47s
                               ETA: 2289346.7s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 707 steps/s (collection: 22.967s, learning 0.181s)
               Value function loss: 0.4901
                    Surrogate loss: -0.0087
             Mean action noise std: 0.80
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 23.15s
                        Total time: 137.62s
                               ETA: 2293546.4s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 695 steps/s (collection: 22.981s, learning 0.568s)
               Value function loss: 0.6856
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                  Mean reward/step: 0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 23.55s
                        Total time: 161.17s
                               ETA: 2302267.6s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 723 steps/s (collection: 22.475s, learning 0.180s)
               Value function loss: 0.6963
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                  Mean reward/step: 0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 22.65s
                        Total time: 183.82s
                               ETA: 2297631.3s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 739 steps/s (collection: 21.979s, learning 0.185s)
               Value function loss: 1.2937
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 22.16s
                        Total time: 205.99s
                               ETA: 2288572.2s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 739 steps/s (collection: 21.998s, learning 0.165s)
               Value function loss: 1.6332
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                  Mean reward/step: 0.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 22.16s
                        Total time: 228.15s
                               ETA: 2281298.3s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.488s, learning 0.162s)
               Value function loss: 2.5520
                    Surrogate loss: -0.0016
             Mean action noise std: 0.80
                  Mean reward/step: 0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 22.65s
                        Total time: 250.80s
                               ETA: 2279771.5s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 722 steps/s (collection: 22.464s, learning 0.199s)
               Value function loss: 2.5435
                    Surrogate loss: -0.0068
             Mean action noise std: 0.80
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 22.66s
                        Total time: 273.46s
                               ETA: 2278607.6s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 721 steps/s (collection: 22.531s, learning 0.177s)
               Value function loss: 2.9761
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 22.71s
                        Total time: 296.17s
                               ETA: 2277966.5s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 718 steps/s (collection: 22.646s, learning 0.164s)
               Value function loss: 3.0358
                    Surrogate loss: -0.0090
             Mean action noise std: 0.80
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 22.81s
                        Total time: 318.98s
                               ETA: 2278138.9s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 716 steps/s (collection: 22.689s, learning 0.185s)
               Value function loss: 3.4255
                    Surrogate loss: 0.0227
             Mean action noise std: 0.80
                  Mean reward/step: 1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 22.87s
                        Total time: 341.86s
                               ETA: 2278717.2s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 711 steps/s (collection: 22.857s, learning 0.160s)
               Value function loss: 2.7102
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 23.02s
                        Total time: 364.87s
                               ETA: 2280113.4s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 742 steps/s (collection: 21.911s, learning 0.169s)
               Value function loss: 1.9293
                    Surrogate loss: 0.0322
             Mean action noise std: 0.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 22.08s
                        Total time: 386.95s
                               ETA: 2275827.5s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 720 steps/s (collection: 22.541s, learning 0.184s)
               Value function loss: 1.9763
                    Surrogate loss: -0.0078
             Mean action noise std: 0.80
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 22.72s
                        Total time: 409.68s
                               ETA: 2275598.8s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 733 steps/s (collection: 22.168s, learning 0.170s)
               Value function loss: 1.3930
                    Surrogate loss: 0.0062
             Mean action noise std: 0.80
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 22.34s
                        Total time: 432.02s
                               ETA: 2273356.7s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 713 steps/s (collection: 22.810s, learning 0.165s)
               Value function loss: 2.3581
                    Surrogate loss: 0.0046
             Mean action noise std: 0.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 22.98s
                        Total time: 454.99s
                               ETA: 2274521.3s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.494s, learning 0.161s)
               Value function loss: 1.7943
                    Surrogate loss: -0.0074
             Mean action noise std: 0.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 22.65s
                        Total time: 477.65s
                               ETA: 2274046.3s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 719 steps/s (collection: 22.597s, learning 0.174s)
               Value function loss: 1.7750
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 22.77s
                        Total time: 500.42s
                               ETA: 2274142.8s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 702 steps/s (collection: 23.147s, learning 0.169s)
               Value function loss: 1.9170
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 23.32s
                        Total time: 523.73s
                               ETA: 2276598.0s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 718 steps/s (collection: 22.635s, learning 0.179s)
               Value function loss: 0.9621
                    Surrogate loss: -0.0163
             Mean action noise std: 0.80
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 22.81s
                        Total time: 546.55s
                               ETA: 2276755.4s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 710 steps/s (collection: 22.883s, learning 0.163s)
               Value function loss: 0.9555
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 23.05s
                        Total time: 569.59s
                               ETA: 2277826.2s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.354s, learning 0.162s)
               Value function loss: 0.8295
                    Surrogate loss: -0.0045
             Mean action noise std: 0.80
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 22.52s
                        Total time: 592.11s
                               ETA: 2276772.0s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.485s, learning 0.170s)
               Value function loss: 0.9314
                    Surrogate loss: -0.0264
             Mean action noise std: 0.80
                       Mean reward: 161.77
               Mean episode length: 213.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 22.66s
                        Total time: 614.76s
                               ETA: 2276311.4s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.464s, learning 0.166s)
               Value function loss: 0.7537
                    Surrogate loss: -0.0422
             Mean action noise std: 0.80
                       Mean reward: 161.77
               Mean episode length: 213.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 22.63s
                        Total time: 637.39s
                               ETA: 2275792.7s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 729 steps/s (collection: 22.287s, learning 0.166s)
               Value function loss: 0.8498
                    Surrogate loss: 0.0006
             Mean action noise std: 0.80
                       Mean reward: 161.77
               Mean episode length: 213.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 22.45s
                        Total time: 659.85s
                               ETA: 2274697.2s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 711 steps/s (collection: 22.848s, learning 0.165s)
               Value function loss: 1.0615
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 161.77
               Mean episode length: 213.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 23.01s
                        Total time: 682.86s
                               ETA: 2275540.0s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 721 steps/s (collection: 22.532s, learning 0.162s)
               Value function loss: 1.4795
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                       Mean reward: 161.77
               Mean episode length: 213.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 22.69s
                        Total time: 705.55s
                               ETA: 2275297.6s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 716 steps/s (collection: 22.678s, learning 0.176s)
               Value function loss: 62.4492
                    Surrogate loss: 0.0055
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 22.85s
                        Total time: 728.41s
                               ETA: 2275568.2s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 722 steps/s (collection: 22.512s, learning 0.172s)
               Value function loss: 3.9057
                    Surrogate loss: 0.0393
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 22.68s
                        Total time: 751.09s
                               ETA: 2275306.2s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 735 steps/s (collection: 22.093s, learning 0.173s)
               Value function loss: 0.6381
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 22.27s
                        Total time: 773.36s
                               ETA: 2273829.3s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 733 steps/s (collection: 22.153s, learning 0.178s)
               Value function loss: 0.2387
                    Surrogate loss: -0.0371
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.63
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 22.33s
                        Total time: 795.69s
                               ETA: 2272621.5s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 761 steps/s (collection: 21.341s, learning 0.178s)
               Value function loss: 0.3617
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 21.52s
                        Total time: 817.21s
                               ETA: 2269223.2s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 744 steps/s (collection: 21.799s, learning 0.205s)
               Value function loss: 1.4988
                    Surrogate loss: 0.0106
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 22.00s
                        Total time: 839.21s
                               ETA: 2267319.0s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.498s, learning 0.182s)
               Value function loss: 6.6535
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 16.68s
                        Total time: 855.89s
                               ETA: 2251508.0s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.744s, learning 0.180s)
               Value function loss: 5.5558
                    Surrogate loss: 0.0122
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 11.92s
                        Total time: 867.81s
                               ETA: 2224318.4s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.236s, learning 0.271s)
               Value function loss: 2.6565
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 11.51s
                        Total time: 879.32s
                               ETA: 2197446.0s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.331s, learning 0.162s)
               Value function loss: 2.8540
                    Surrogate loss: -0.0120
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 11.49s
                        Total time: 890.82s
                               ETA: 2171850.6s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.868s, learning 0.185s)
               Value function loss: 3.1599
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 11.05s
                        Total time: 901.87s
                               ETA: 2146426.3s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.496s, learning 0.178s)
               Value function loss: 5.0078
                    Surrogate loss: 0.0095
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 11.67s
                        Total time: 913.54s
                               ETA: 2123626.8s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.042s, learning 0.169s)
               Value function loss: 1.6424
                    Surrogate loss: 0.0185
             Mean action noise std: 0.80
                       Mean reward: 183.28
               Mean episode length: 249.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 11.21s
                        Total time: 924.75s
                               ETA: 2100809.4s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.231s, learning 0.194s)
               Value function loss: 1.6413
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 182.59
               Mean episode length: 247.58
                  Mean reward/step: 1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 11.43s
                        Total time: 936.18s
                               ETA: 2079482.3s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.371s, learning 0.169s)
               Value function loss: 2.2130
                    Surrogate loss: -0.0140
             Mean action noise std: 0.80
                       Mean reward: 180.66
               Mean episode length: 243.65
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 11.54s
                        Total time: 947.72s
                               ETA: 2059331.2s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.469s, learning 0.201s)
               Value function loss: 1.8445
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                       Mean reward: 179.78
               Mean episode length: 241.16
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 11.67s
                        Total time: 959.39s
                               ETA: 2040312.9s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.571s, learning 0.174s)
               Value function loss: 1.5811
                    Surrogate loss: -0.0030
             Mean action noise std: 0.80
                       Mean reward: 179.16
               Mean episode length: 239.96
                  Mean reward/step: 1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 11.74s
                        Total time: 971.13s
                               ETA: 2022242.7s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.205s, learning 0.187s)
               Value function loss: 1.8287
                    Surrogate loss: 0.0136
             Mean action noise std: 0.80
                       Mean reward: 179.16
               Mean episode length: 239.96
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 11.39s
                        Total time: 982.53s
                               ETA: 2004191.5s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.657s, learning 0.194s)
               Value function loss: 3.2388
                    Surrogate loss: 0.0206
             Mean action noise std: 0.80
                       Mean reward: 179.27
               Mean episode length: 237.94
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 11.85s
                        Total time: 994.38s
                               ETA: 1987779.7s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.448s, learning 0.170s)
               Value function loss: 5.1078
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: 179.37
               Mean episode length: 237.01
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 11.62s
                        Total time: 1006.00s
                               ETA: 1971554.5s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1464 steps/s (collection: 10.989s, learning 0.200s)
               Value function loss: 4.3527
                    Surrogate loss: -0.0034
             Mean action noise std: 0.80
                       Mean reward: 179.37
               Mean episode length: 237.01
                  Mean reward/step: 0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 11.19s
                        Total time: 1017.18s
                               ETA: 1955126.6s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.402s, learning 0.179s)
               Value function loss: 5.4541
                    Surrogate loss: -0.0067
             Mean action noise std: 0.80
                       Mean reward: 177.34
               Mean episode length: 233.85
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 11.58s
                        Total time: 1028.77s
                               ETA: 1940057.5s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.344s, learning 0.183s)
               Value function loss: 8.5276
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 172.16
               Mean episode length: 226.26
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 11.53s
                        Total time: 1040.29s
                               ETA: 1925447.0s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.041s, learning 0.166s)
               Value function loss: 9.6122
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                       Mean reward: 160.18
               Mean episode length: 209.07
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 11.21s
                        Total time: 1051.50s
                               ETA: 1910786.9s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.150s, learning 0.161s)
               Value function loss: 20.3134
                    Surrogate loss: -0.0095
             Mean action noise std: 0.80
                       Mean reward: 145.25
               Mean episode length: 192.61
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 11.31s
                        Total time: 1062.81s
                               ETA: 1896834.4s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.338s, learning 0.186s)
               Value function loss: 16.6658
                    Surrogate loss: -0.0184
             Mean action noise std: 0.80
                       Mean reward: 148.99
               Mean episode length: 203.25
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 11.52s
                        Total time: 1074.34s
                               ETA: 1883743.2s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.071s, learning 0.174s)
               Value function loss: 12.3607
                    Surrogate loss: 0.0093
             Mean action noise std: 0.80
                       Mean reward: 159.42
               Mean episode length: 213.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 11.24s
                        Total time: 1085.58s
                               ETA: 1870622.8s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.130s, learning 0.197s)
               Value function loss: 17.4044
                    Surrogate loss: -0.0125
             Mean action noise std: 0.80
                       Mean reward: 166.18
               Mean episode length: 222.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 11.33s
                        Total time: 1096.91s
                               ETA: 1858086.0s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.435s, learning 0.163s)
               Value function loss: 12.1533
                    Surrogate loss: -0.0180
             Mean action noise std: 0.80
                       Mean reward: 175.05
               Mean episode length: 229.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 11.60s
                        Total time: 1108.50s
                               ETA: 1846417.3s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.378s, learning 0.171s)
               Value function loss: 9.0292
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 182.25
               Mean episode length: 238.34
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 11.55s
                        Total time: 1120.05s
                               ETA: 1835051.7s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.094s, learning 0.176s)
               Value function loss: 6.5722
                    Surrogate loss: -0.0247
             Mean action noise std: 0.80
                       Mean reward: 183.76
               Mean episode length: 245.52
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 11.27s
                        Total time: 1131.32s
                               ETA: 1823602.4s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.038s, learning 0.177s)
               Value function loss: 8.3405
                    Surrogate loss: -0.0211
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 6.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 11.22s
                        Total time: 1142.54s
                               ETA: 1812429.7s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1457 steps/s (collection: 10.966s, learning 0.279s)
               Value function loss: 2.5607
                    Surrogate loss: 0.0063
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 11.24s
                        Total time: 1153.78s
                               ETA: 1801651.6s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.983s, learning 0.164s)
               Value function loss: 3.4760
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 11.15s
                        Total time: 1164.93s
                               ETA: 1791054.8s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.005s, learning 0.174s)
               Value function loss: 3.1348
                    Surrogate loss: 0.0238
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 11.18s
                        Total time: 1176.11s
                               ETA: 1780827.8s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.090s, learning 0.265s)
               Value function loss: 3.5840
                    Surrogate loss: 0.0013
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 11.36s
                        Total time: 1187.47s
                               ETA: 1771167.9s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.161s)
               Value function loss: 2.9675
                    Surrogate loss: 0.0117
             Mean action noise std: 0.80
                       Mean reward: 217.28
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 11.33s
                        Total time: 1198.79s
                               ETA: 1761749.1s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.623s, learning 0.174s)
               Value function loss: 4.8229
                    Surrogate loss: 0.0183
             Mean action noise std: 0.80
                       Mean reward: 216.09
               Mean episode length: 248.27
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 11.80s
                        Total time: 1210.59s
                               ETA: 1753285.0s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.603s, learning 0.167s)
               Value function loss: 4.1515
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: 212.37
               Mean episode length: 242.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 11.77s
                        Total time: 1222.36s
                               ETA: 1745022.9s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.434s, learning 0.179s)
               Value function loss: 2.6227
                    Surrogate loss: 0.0080
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 242.18
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 11.61s
                        Total time: 1233.97s
                               ETA: 1736773.2s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.636s, learning 0.218s)
               Value function loss: 2.1608
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 242.18
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 11.85s
                        Total time: 1245.83s
                               ETA: 1729087.3s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.265s, learning 0.171s)
               Value function loss: 2.6161
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 209.21
               Mean episode length: 238.70
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 11.44s
                        Total time: 1257.26s
                               ETA: 1721038.5s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.159s)
               Value function loss: 2.1955
                    Surrogate loss: -0.0250
             Mean action noise std: 0.80
                       Mean reward: 206.23
               Mean episode length: 236.41
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 11.33s
                        Total time: 1268.60s
                               ETA: 1713070.3s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.162s)
               Value function loss: 2.3085
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 205.73
               Mean episode length: 235.23
                  Mean reward/step: 1.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 11.84s
                        Total time: 1280.44s
                               ETA: 1705991.7s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.149s, learning 0.165s)
               Value function loss: 2.2989
                    Surrogate loss: -0.0251
             Mean action noise std: 0.80
                       Mean reward: 204.00
               Mean episode length: 233.07
                  Mean reward/step: 1.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 11.31s
                        Total time: 1291.75s
                               ETA: 1698402.9s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.253s, learning 0.263s)
               Value function loss: 2.6044
                    Surrogate loss: -0.0180
             Mean action noise std: 0.80
                       Mean reward: 200.59
               Mean episode length: 229.74
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 12.52s
                        Total time: 1304.27s
                               ETA: 1692571.9s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.522s, learning 0.178s)
               Value function loss: 3.4936
                    Surrogate loss: -0.0146
             Mean action noise std: 0.80
                       Mean reward: 198.69
               Mean episode length: 225.20
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 11.70s
                        Total time: 1315.97s
                               ETA: 1685843.7s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.496s, learning 0.183s)
               Value function loss: 5.2524
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 193.97
               Mean episode length: 218.83
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 11.68s
                        Total time: 1327.65s
                               ETA: 1679259.5s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.400s, learning 0.177s)
               Value function loss: 7.6540
                    Surrogate loss: -0.0163
             Mean action noise std: 0.80
                       Mean reward: 173.53
               Mean episode length: 197.63
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 11.58s
                        Total time: 1339.23s
                               ETA: 1672712.3s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.201s, learning 0.197s)
               Value function loss: 9.6390
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                       Mean reward: 156.41
               Mean episode length: 177.15
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 11.40s
                        Total time: 1350.63s
                               ETA: 1666105.1s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.163s, learning 0.169s)
               Value function loss: 13.7954
                    Surrogate loss: -0.0051
             Mean action noise std: 0.80
                       Mean reward: 157.23
               Mean episode length: 184.85
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 11.33s
                        Total time: 1361.96s
                               ETA: 1659577.8s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.210s, learning 0.162s)
               Value function loss: 9.0997
                    Surrogate loss: -0.0024
             Mean action noise std: 0.80
                       Mean reward: 165.31
               Mean episode length: 193.77
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 11.37s
                        Total time: 1373.33s
                               ETA: 1653255.9s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.233s, learning 0.184s)
               Value function loss: 8.4589
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                       Mean reward: 169.29
               Mean episode length: 197.50
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 11.42s
                        Total time: 1384.75s
                               ETA: 1647138.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.365s, learning 0.167s)
               Value function loss: 9.5451
                    Surrogate loss: -0.0167
             Mean action noise std: 0.80
                       Mean reward: 175.80
               Mean episode length: 204.90
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 11.53s
                        Total time: 1396.28s
                               ETA: 1641299.9s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.905s, learning 0.200s)
               Value function loss: 10.1956
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: 182.86
               Mean episode length: 209.75
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 11.10s
                        Total time: 1407.38s
                               ETA: 1635100.5s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.210s, learning 0.191s)
               Value function loss: 11.6899
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 190.88
               Mean episode length: 214.35
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 11.40s
                        Total time: 1418.78s
                               ETA: 1629383.5s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.345s, learning 0.238s)
               Value function loss: 9.7291
                    Surrogate loss: -0.0180
             Mean action noise std: 0.80
                       Mean reward: 200.75
               Mean episode length: 220.10
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 11.58s
                        Total time: 1430.37s
                               ETA: 1624003.6s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.231s, learning 0.183s)
               Value function loss: 9.4287
                    Surrogate loss: -0.0278
             Mean action noise std: 0.80
                       Mean reward: 207.78
               Mean episode length: 225.78
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 11.41s
                        Total time: 1441.78s
                               ETA: 1618554.0s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.028s, learning 0.180s)
               Value function loss: 8.0831
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: 224.52
               Mean episode length: 231.68
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 11.21s
                        Total time: 1452.99s
                               ETA: 1612996.2s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.160s)
               Value function loss: 7.0668
                    Surrogate loss: -0.0213
             Mean action noise std: 0.80
                       Mean reward: 232.77
               Mean episode length: 237.13
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 11.36s
                        Total time: 1464.35s
                               ETA: 1607727.7s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.325s, learning 0.161s)
               Value function loss: 4.9292
                    Surrogate loss: -0.0270
             Mean action noise std: 0.80
                       Mean reward: 248.33
               Mean episode length: 241.25
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 11.49s
                        Total time: 1475.84s
                               ETA: 1602710.3s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.302s, learning 0.181s)
               Value function loss: 3.9836
                    Surrogate loss: -0.0278
             Mean action noise std: 0.80
                       Mean reward: 264.63
               Mean episode length: 247.28
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 11.48s
                        Total time: 1487.32s
                               ETA: 1597797.1s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.140s, learning 0.170s)
               Value function loss: 4.4524
                    Surrogate loss: -0.0213
             Mean action noise std: 0.80
                       Mean reward: 279.98
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 11.31s
                        Total time: 1498.63s
                               ETA: 1592804.3s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.993s, learning 0.168s)
               Value function loss: 1.6566
                    Surrogate loss: -0.0243
             Mean action noise std: 0.80
                       Mean reward: 278.13
               Mean episode length: 249.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 11.16s
                        Total time: 1509.79s
                               ETA: 1587759.6s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.703s, learning 0.172s)
               Value function loss: 1.7029
                    Surrogate loss: -0.0247
             Mean action noise std: 0.80
                       Mean reward: 273.53
               Mean episode length: 246.08
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 10.87s
                        Total time: 1520.67s
                               ETA: 1582521.5s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.797s, learning 0.173s)
               Value function loss: 1.8825
                    Surrogate loss: -0.0284
             Mean action noise std: 0.80
                       Mean reward: 266.88
               Mean episode length: 241.75
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 10.97s
                        Total time: 1531.64s
                               ETA: 1577490.1s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.338s, learning 0.197s)
               Value function loss: 2.0624
                    Surrogate loss: -0.0320
             Mean action noise std: 0.80
                       Mean reward: 260.08
               Mean episode length: 236.95
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 11.54s
                        Total time: 1543.17s
                               ETA: 1573136.6s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.449s, learning 0.185s)
               Value function loss: 2.0519
                    Surrogate loss: -0.0150
             Mean action noise std: 0.80
                       Mean reward: 247.81
               Mean episode length: 228.83
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 11.63s
                        Total time: 1554.81s
                               ETA: 1568971.6s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.864s, learning 0.182s)
               Value function loss: 2.4938
                    Surrogate loss: -0.0095
             Mean action noise std: 0.80
                       Mean reward: 230.10
               Mean episode length: 218.20
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 11.05s
                        Total time: 1565.85s
                               ETA: 1564300.8s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.579s, learning 0.175s)
               Value function loss: 2.9798
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 203.19
               Mean episode length: 201.06
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 11.75s
                        Total time: 1577.61s
                               ETA: 1560423.3s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.022s, learning 0.172s)
               Value function loss: 3.4325
                    Surrogate loss: -0.0060
             Mean action noise std: 0.80
                       Mean reward: 154.66
               Mean episode length: 169.57
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 11.19s
                        Total time: 1588.80s
                               ETA: 1556072.4s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.083s, learning 0.170s)
               Value function loss: 3.5885
                    Surrogate loss: 0.0044
             Mean action noise std: 0.80
                       Mean reward: 126.69
               Mean episode length: 152.87
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 11.25s
                        Total time: 1600.05s
                               ETA: 1551863.6s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.143s, learning 0.205s)
               Value function loss: 3.7187
                    Surrogate loss: -0.0105
             Mean action noise std: 0.80
                       Mean reward: 127.58
               Mean episode length: 157.10
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 11.35s
                        Total time: 1611.40s
                               ETA: 1547826.6s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.215s, learning 0.163s)
               Value function loss: 4.2359
                    Surrogate loss: -0.0070
             Mean action noise std: 0.80
                       Mean reward: 131.93
               Mean episode length: 163.70
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 11.38s
                        Total time: 1622.78s
                               ETA: 1543895.7s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.359s, learning 0.161s)
               Value function loss: 4.2604
                    Surrogate loss: -0.0228
             Mean action noise std: 0.80
                       Mean reward: 140.40
               Mean episode length: 167.64
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 11.52s
                        Total time: 1634.30s
                               ETA: 1540171.9s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.444s, learning 0.173s)
               Value function loss: 5.3076
                    Surrogate loss: -0.0196
             Mean action noise std: 0.80
                       Mean reward: 137.80
               Mean episode length: 166.47
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 11.62s
                        Total time: 1645.92s
                               ETA: 1536608.8s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.938s, learning 0.166s)
               Value function loss: 5.1343
                    Surrogate loss: -0.0160
             Mean action noise std: 0.80
                       Mean reward: 138.43
               Mean episode length: 170.40
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 11.10s
                        Total time: 1657.02s
                               ETA: 1532636.1s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.880s, learning 0.168s)
               Value function loss: 4.9185
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 143.53
               Mean episode length: 178.99
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 11.05s
                        Total time: 1668.07s
                               ETA: 1528685.1s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.073s, learning 0.203s)
               Value function loss: 5.0622
                    Surrogate loss: -0.0125
             Mean action noise std: 0.80
                       Mean reward: 143.54
               Mean episode length: 183.55
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 11.28s
                        Total time: 1679.34s
                               ETA: 1525012.2s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.270s, learning 0.184s)
               Value function loss: 4.2260
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 148.33
               Mean episode length: 188.09
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 11.45s
                        Total time: 1690.80s
                               ETA: 1521566.0s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.499s, learning 0.181s)
               Value function loss: 4.3977
                    Surrogate loss: -0.0168
             Mean action noise std: 0.80
                       Mean reward: 151.41
               Mean episode length: 189.36
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 11.68s
                        Total time: 1702.48s
                               ETA: 1518382.7s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.187s, learning 0.261s)
               Value function loss: 4.3842
                    Surrogate loss: -0.0267
             Mean action noise std: 0.80
                       Mean reward: 149.94
               Mean episode length: 193.81
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 11.45s
                        Total time: 1713.93s
                               ETA: 1515050.2s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.075s, learning 0.189s)
               Value function loss: 4.0551
                    Surrogate loss: -0.0281
             Mean action noise std: 0.80
                       Mean reward: 158.72
               Mean episode length: 199.23
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 11.26s
                        Total time: 1725.19s
                               ETA: 1511614.5s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.206s, learning 0.193s)
               Value function loss: 4.7606
                    Surrogate loss: -0.0149
             Mean action noise std: 0.80
                       Mean reward: 152.57
               Mean episode length: 199.44
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 11.40s
                        Total time: 1736.59s
                               ETA: 1508356.3s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.286s, learning 0.187s)
               Value function loss: 4.2257
                    Surrogate loss: -0.0163
             Mean action noise std: 0.80
                       Mean reward: 163.75
               Mean episode length: 206.04
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 11.47s
                        Total time: 1748.06s
                               ETA: 1505218.0s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.469s, learning 0.196s)
               Value function loss: 3.7307
                    Surrogate loss: -0.0240
             Mean action noise std: 0.80
                       Mean reward: 170.74
               Mean episode length: 212.70
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 11.67s
                        Total time: 1759.73s
                               ETA: 1502297.1s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.433s, learning 0.178s)
               Value function loss: 3.8296
                    Surrogate loss: -0.0081
             Mean action noise std: 0.80
                       Mean reward: 169.46
               Mean episode length: 214.99
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 11.61s
                        Total time: 1771.34s
                               ETA: 1499379.1s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.284s, learning 0.181s)
               Value function loss: 4.2714
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                       Mean reward: 180.01
               Mean episode length: 220.85
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 11.46s
                        Total time: 1782.80s
                               ETA: 1496387.3s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.969s, learning 0.172s)
               Value function loss: 3.6557
                    Surrogate loss: -0.0043
             Mean action noise std: 0.80
                       Mean reward: 184.04
               Mean episode length: 226.78
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 11.14s
                        Total time: 1793.95s
                               ETA: 1493175.3s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.192s, learning 0.181s)
               Value function loss: 3.9137
                    Surrogate loss: -0.0230
             Mean action noise std: 0.80
                       Mean reward: 177.14
               Mean episode length: 223.76
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 11.37s
                        Total time: 1805.32s
                               ETA: 1490208.2s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.006s, learning 0.180s)
               Value function loss: 3.8406
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: 186.06
               Mean episode length: 226.75
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 11.19s
                        Total time: 1816.50s
                               ETA: 1487136.3s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.847s, learning 0.199s)
               Value function loss: 3.3540
                    Surrogate loss: -0.0154
             Mean action noise std: 0.80
                       Mean reward: 195.00
               Mean episode length: 232.23
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 12.05s
                        Total time: 1828.55s
                               ETA: 1484812.5s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.108s, learning 0.184s)
               Value function loss: 2.8796
                    Surrogate loss: -0.0291
             Mean action noise std: 0.80
                       Mean reward: 201.27
               Mean episode length: 235.93
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 11.29s
                        Total time: 1839.84s
                               ETA: 1481918.9s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.168s, learning 0.187s)
               Value function loss: 3.3610
                    Surrogate loss: -0.0193
             Mean action noise std: 0.80
                       Mean reward: 197.12
               Mean episode length: 237.22
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 11.35s
                        Total time: 1851.20s
                               ETA: 1479121.3s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.396s, learning 0.191s)
               Value function loss: 3.6940
                    Surrogate loss: -0.0295
             Mean action noise std: 0.80
                       Mean reward: 190.33
               Mean episode length: 233.02
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 11.59s
                        Total time: 1862.78s
                               ETA: 1476552.2s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.355s, learning 0.184s)
               Value function loss: 3.3336
                    Surrogate loss: -0.0209
             Mean action noise std: 0.79
                       Mean reward: 177.20
               Mean episode length: 225.61
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 11.54s
                        Total time: 1874.32s
                               ETA: 1473985.1s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.683s, learning 0.178s)
               Value function loss: 3.0336
                    Surrogate loss: -0.0298
             Mean action noise std: 0.79
                       Mean reward: 159.47
               Mean episode length: 213.73
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 10.86s
                        Total time: 1885.18s
                               ETA: 1470929.7s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.533s, learning 0.160s)
               Value function loss: 3.1972
                    Surrogate loss: -0.0198
             Mean action noise std: 0.79
                       Mean reward: 140.52
               Mean episode length: 200.95
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 11.69s
                        Total time: 1896.88s
                               ETA: 1468565.3s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.917s, learning 0.164s)
               Value function loss: 3.1490
                    Surrogate loss: 0.0124
             Mean action noise std: 0.79
                       Mean reward: 133.60
               Mean episode length: 195.58
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 11.08s
                        Total time: 1907.96s
                               ETA: 1465766.4s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.266s, learning 0.180s)
               Value function loss: 3.5806
                    Surrogate loss: 0.0070
             Mean action noise std: 0.79
                       Mean reward: 139.88
               Mean episode length: 197.86
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 11.45s
                        Total time: 1919.40s
                               ETA: 1463289.3s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.141s, learning 0.234s)
               Value function loss: 3.6508
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 146.42
               Mean episode length: 202.88
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 11.38s
                        Total time: 1930.78s
                               ETA: 1460795.5s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.536s, learning 0.176s)
               Value function loss: 4.2275
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 151.73
               Mean episode length: 212.30
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 11.71s
                        Total time: 1942.49s
                               ETA: 1458592.1s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.367s, learning 0.177s)
               Value function loss: 4.7241
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 160.23
               Mean episode length: 215.94
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 11.54s
                        Total time: 1954.04s
                               ETA: 1456295.9s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.377s, learning 0.170s)
               Value function loss: 5.0628
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 155.03
               Mean episode length: 211.42
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 11.55s
                        Total time: 1965.58s
                               ETA: 1454035.6s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.279s, learning 0.165s)
               Value function loss: 4.8397
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 157.61
               Mean episode length: 215.13
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 11.44s
                        Total time: 1977.03s
                               ETA: 1451732.7s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.961s, learning 0.192s)
               Value function loss: 4.9908
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 152.72
               Mean episode length: 217.88
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 11.15s
                        Total time: 1988.18s
                               ETA: 1449251.3s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.507s, learning 0.173s)
               Value function loss: 5.3654
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 168.01
               Mean episode length: 224.16
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 11.68s
                        Total time: 1999.86s
                               ETA: 1447187.4s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.171s)
               Value function loss: 4.8879
                    Surrogate loss: -0.0187
             Mean action noise std: 0.79
                       Mean reward: 172.25
               Mean episode length: 228.11
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 11.38s
                        Total time: 2011.24s
                               ETA: 1444939.6s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.284s, learning 0.256s)
               Value function loss: 4.9170
                    Surrogate loss: -0.0227
             Mean action noise std: 0.79
                       Mean reward: 170.28
               Mean episode length: 228.15
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 11.54s
                        Total time: 2022.78s
                               ETA: 1442836.1s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.900s, learning 0.175s)
               Value function loss: 5.2561
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 169.01
               Mean episode length: 230.18
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 11.07s
                        Total time: 2033.86s
                               ETA: 1440432.4s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.786s, learning 0.179s)
               Value function loss: 4.5987
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 165.46
               Mean episode length: 228.99
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 10.96s
                        Total time: 2044.82s
                               ETA: 1437985.0s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.209s, learning 0.171s)
               Value function loss: 4.8700
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 159.30
               Mean episode length: 230.77
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 11.38s
                        Total time: 2056.20s
                               ETA: 1435861.6s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.844s, learning 0.179s)
               Value function loss: 4.3164
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 170.35
               Mean episode length: 235.29
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 11.02s
                        Total time: 2067.23s
                               ETA: 1433520.5s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.999s, learning 0.179s)
               Value function loss: 5.0676
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 165.51
               Mean episode length: 235.52
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 11.18s
                        Total time: 2078.40s
                               ETA: 1431317.8s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.173s)
               Value function loss: 4.7929
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 169.67
               Mean episode length: 238.25
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 11.09s
                        Total time: 2089.49s
                               ETA: 1429081.5s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.879s, learning 0.183s)
               Value function loss: 4.1429
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 166.56
               Mean episode length: 243.68
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 11.06s
                        Total time: 2100.55s
                               ETA: 1426860.1s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.162s)
               Value function loss: 5.2726
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 161.76
               Mean episode length: 233.93
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 11.57s
                        Total time: 2112.12s
                               ETA: 1425011.9s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.377s, learning 0.180s)
               Value function loss: 5.2750
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 159.30
               Mean episode length: 235.74
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 11.56s
                        Total time: 2123.68s
                               ETA: 1423179.1s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.352s, learning 0.181s)
               Value function loss: 3.4896
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 157.66
               Mean episode length: 233.84
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 11.53s
                        Total time: 2135.21s
                               ETA: 1421354.5s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.160s, learning 0.162s)
               Value function loss: 3.6429
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 158.16
               Mean episode length: 236.17
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 11.32s
                        Total time: 2146.54s
                               ETA: 1419414.6s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.464s, learning 0.158s)
               Value function loss: 3.1964
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 154.63
               Mean episode length: 229.88
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 11.62s
                        Total time: 2158.16s
                               ETA: 1417696.5s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.299s, learning 0.177s)
               Value function loss: 4.0041
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 158.31
               Mean episode length: 225.13
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 11.48s
                        Total time: 2169.63s
                               ETA: 1415905.9s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.239s, learning 0.183s)
               Value function loss: 3.2662
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 158.88
               Mean episode length: 226.18
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 11.42s
                        Total time: 2181.06s
                               ETA: 1414103.1s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.005s, learning 0.175s)
               Value function loss: 3.3386
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 152.36
               Mean episode length: 215.19
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 11.18s
                        Total time: 2192.24s
                               ETA: 1412167.4s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.098s, learning 0.171s)
               Value function loss: 3.5497
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 143.69
               Mean episode length: 211.99
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 11.27s
                        Total time: 2203.50s
                               ETA: 1410313.1s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.264s, learning 0.182s)
               Value function loss: 3.5994
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 141.87
               Mean episode length: 209.75
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 11.45s
                        Total time: 2214.95s
                               ETA: 1408595.5s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.167s)
               Value function loss: 3.8783
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: 138.52
               Mean episode length: 209.53
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 11.52s
                        Total time: 2226.47s
                               ETA: 1406944.6s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.055s, learning 0.178s)
               Value function loss: 4.7091
                    Surrogate loss: -0.0193
             Mean action noise std: 0.79
                       Mean reward: 133.89
               Mean episode length: 199.95
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 11.23s
                        Total time: 2237.70s
                               ETA: 1405135.4s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.018s, learning 0.180s)
               Value function loss: 5.0641
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 136.29
               Mean episode length: 202.26
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 11.20s
                        Total time: 2248.90s
                               ETA: 1403326.8s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.425s, learning 0.162s)
               Value function loss: 4.0378
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 141.04
               Mean episode length: 209.99
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 11.59s
                        Total time: 2260.49s
                               ETA: 1401782.3s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.938s, learning 0.167s)
               Value function loss: 4.6722
                    Surrogate loss: -0.0219
             Mean action noise std: 0.79
                       Mean reward: 139.66
               Mean episode length: 210.41
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 11.11s
                        Total time: 2271.59s
                               ETA: 1399959.4s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.344s, learning 0.172s)
               Value function loss: 5.2651
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 141.99
               Mean episode length: 213.82
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 11.52s
                        Total time: 2283.11s
                               ETA: 1398410.7s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.236s, learning 0.185s)
               Value function loss: 5.9388
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 147.01
               Mean episode length: 221.16
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 11.42s
                        Total time: 2294.53s
                               ETA: 1396822.6s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.986s, learning 0.178s)
               Value function loss: 5.2470
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 154.28
               Mean episode length: 226.96
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 11.16s
                        Total time: 2305.69s
                               ETA: 1395098.4s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.083s, learning 0.160s)
               Value function loss: 4.8458
                    Surrogate loss: -0.0265
             Mean action noise std: 0.79
                       Mean reward: 156.21
               Mean episode length: 231.50
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 11.24s
                        Total time: 2316.94s
                               ETA: 1393442.2s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.333s, learning 0.184s)
               Value function loss: 4.5304
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 145.27
               Mean episode length: 223.91
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 11.52s
                        Total time: 2328.45s
                               ETA: 1391969.4s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.425s, learning 0.173s)
               Value function loss: 5.1299
                    Surrogate loss: -0.0209
             Mean action noise std: 0.79
                       Mean reward: 149.88
               Mean episode length: 228.60
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 11.60s
                        Total time: 2340.05s
                               ETA: 1390561.9s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.593s, learning 0.179s)
               Value function loss: 4.7719
                    Surrogate loss: -0.0218
             Mean action noise std: 0.79
                       Mean reward: 147.98
               Mean episode length: 229.56
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 11.77s
                        Total time: 2351.82s
                               ETA: 1389274.1s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.411s, learning 0.171s)
               Value function loss: 4.4243
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 148.05
               Mean episode length: 228.36
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 11.58s
                        Total time: 2363.41s
                               ETA: 1387889.1s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.638s, learning 0.169s)
               Value function loss: 4.3902
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 149.65
               Mean episode length: 230.56
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 11.81s
                        Total time: 2375.21s
                               ETA: 1386651.8s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.911s, learning 0.180s)
               Value function loss: 4.1253
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 150.15
               Mean episode length: 231.66
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 11.09s
                        Total time: 2386.30s
                               ETA: 1385013.2s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.453s, learning 0.165s)
               Value function loss: 4.7220
                    Surrogate loss: -0.0217
             Mean action noise std: 0.79
                       Mean reward: 143.94
               Mean episode length: 228.91
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 11.62s
                        Total time: 2397.92s
                               ETA: 1383697.2s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.252s, learning 0.170s)
               Value function loss: 4.1525
                    Surrogate loss: -0.0217
             Mean action noise std: 0.79
                       Mean reward: 144.69
               Mean episode length: 227.75
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 11.42s
                        Total time: 2409.34s
                               ETA: 1382283.9s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.293s, learning 0.173s)
               Value function loss: 3.5925
                    Surrogate loss: -0.0252
             Mean action noise std: 0.79
                       Mean reward: 142.90
               Mean episode length: 227.84
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 11.47s
                        Total time: 2420.81s
                               ETA: 1380912.0s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.088s, learning 0.168s)
               Value function loss: 3.5530
                    Surrogate loss: -0.0250
             Mean action noise std: 0.79
                       Mean reward: 142.18
               Mean episode length: 229.29
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 11.26s
                        Total time: 2432.06s
                               ETA: 1379436.1s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.343s, learning 0.168s)
               Value function loss: 3.6405
                    Surrogate loss: -0.0215
             Mean action noise std: 0.79
                       Mean reward: 138.87
               Mean episode length: 223.88
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 11.51s
                        Total time: 2443.58s
                               ETA: 1378121.3s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.024s, learning 0.159s)
               Value function loss: 3.3361
                    Surrogate loss: -0.0186
             Mean action noise std: 0.79
                       Mean reward: 138.34
               Mean episode length: 222.90
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 11.18s
                        Total time: 2454.76s
                               ETA: 1376636.6s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.157s, learning 0.272s)
               Value function loss: 3.2937
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 131.44
               Mean episode length: 224.62
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 11.43s
                        Total time: 2466.19s
                               ETA: 1375305.3s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.962s, learning 0.176s)
               Value function loss: 3.0473
                    Surrogate loss: -0.0246
             Mean action noise std: 0.79
                       Mean reward: 131.67
               Mean episode length: 222.92
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 11.14s
                        Total time: 2477.32s
                               ETA: 1373827.4s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.477s, learning 0.181s)
               Value function loss: 2.8036
                    Surrogate loss: -0.0312
             Mean action noise std: 0.79
                       Mean reward: 130.59
               Mean episode length: 217.04
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 11.66s
                        Total time: 2488.98s
                               ETA: 1372653.1s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.031s, learning 0.176s)
               Value function loss: 2.8176
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 132.87
               Mean episode length: 220.52
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 11.21s
                        Total time: 2500.19s
                               ETA: 1371244.3s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.216s, learning 0.171s)
               Value function loss: 2.8959
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 133.24
               Mean episode length: 217.80
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 11.39s
                        Total time: 2511.58s
                               ETA: 1369948.6s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.179s, learning 0.183s)
               Value function loss: 3.1062
                    Surrogate loss: -0.0236
             Mean action noise std: 0.79
                       Mean reward: 134.05
               Mean episode length: 215.95
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 11.36s
                        Total time: 2522.94s
                               ETA: 1368653.5s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.703s, learning 0.160s)
               Value function loss: 2.8867
                    Surrogate loss: -0.0275
             Mean action noise std: 0.79
                       Mean reward: 133.85
               Mean episode length: 215.28
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 11.86s
                        Total time: 2534.80s
                               ETA: 1367642.5s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.836s, learning 0.161s)
               Value function loss: 3.3398
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 136.26
               Mean episode length: 216.43
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 11.00s
                        Total time: 2545.80s
                               ETA: 1366177.4s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.760s, learning 0.176s)
               Value function loss: 3.6562
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 134.43
               Mean episode length: 211.77
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 10.94s
                        Total time: 2556.74s
                               ETA: 1364695.3s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.321s, learning 0.181s)
               Value function loss: 4.5829
                    Surrogate loss: -0.0224
             Mean action noise std: 0.79
                       Mean reward: 134.98
               Mean episode length: 208.06
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 11.50s
                        Total time: 2568.24s
                               ETA: 1363528.9s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.066s, learning 0.166s)
               Value function loss: 5.1775
                    Surrogate loss: 0.0083
             Mean action noise std: 0.79
                       Mean reward: 136.19
               Mean episode length: 209.46
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 11.23s
                        Total time: 2579.47s
                               ETA: 1362232.2s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.989s, learning 0.179s)
               Value function loss: 3.6698
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 136.12
               Mean episode length: 215.35
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 11.17s
                        Total time: 2590.64s
                               ETA: 1360915.7s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.266s, learning 0.196s)
               Value function loss: 3.6789
                    Surrogate loss: -0.0279
             Mean action noise std: 0.79
                       Mean reward: 139.08
               Mean episode length: 221.26
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 11.46s
                        Total time: 2602.10s
                               ETA: 1359766.6s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.136s, learning 0.250s)
               Value function loss: 3.5369
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 143.75
               Mean episode length: 223.34
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 11.39s
                        Total time: 2613.48s
                               ETA: 1358589.6s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.455s, learning 0.166s)
               Value function loss: 3.7363
                    Surrogate loss: -0.0248
             Mean action noise std: 0.79
                       Mean reward: 151.08
               Mean episode length: 224.32
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 11.62s
                        Total time: 2625.11s
                               ETA: 1357546.5s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.142s, learning 0.168s)
               Value function loss: 3.8229
                    Surrogate loss: -0.0227
             Mean action noise std: 0.79
                       Mean reward: 158.15
               Mean episode length: 232.07
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 11.31s
                        Total time: 2636.42s
                               ETA: 1356354.0s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.224s, learning 0.182s)
               Value function loss: 4.4057
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 167.66
               Mean episode length: 236.44
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 11.41s
                        Total time: 2647.82s
                               ETA: 1355222.7s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.025s, learning 0.174s)
               Value function loss: 4.2200
                    Surrogate loss: -0.0204
             Mean action noise std: 0.79
                       Mean reward: 170.69
               Mean episode length: 238.19
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 11.20s
                        Total time: 2659.02s
                               ETA: 1353997.6s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.407s, learning 0.162s)
               Value function loss: 4.7913
                    Surrogate loss: -0.0232
             Mean action noise std: 0.79
                       Mean reward: 175.93
               Mean episode length: 238.92
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 11.57s
                        Total time: 2670.59s
                               ETA: 1352972.0s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.968s, learning 0.180s)
               Value function loss: 4.4309
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 179.14
               Mean episode length: 238.54
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 11.15s
                        Total time: 2681.74s
                               ETA: 1351744.6s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.102s, learning 0.172s)
               Value function loss: 4.5255
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 184.31
               Mean episode length: 244.97
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 11.27s
                        Total time: 2693.01s
                               ETA: 1350592.8s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.027s, learning 0.176s)
               Value function loss: 4.3570
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 187.04
               Mean episode length: 240.89
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 11.20s
                        Total time: 2704.22s
                               ETA: 1349416.9s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.234s, learning 0.174s)
               Value function loss: 4.6145
                    Surrogate loss: -0.0235
             Mean action noise std: 0.79
                       Mean reward: 189.20
               Mean episode length: 246.79
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 11.41s
                        Total time: 2715.62s
                               ETA: 1348354.5s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.872s, learning 0.170s)
               Value function loss: 4.3702
                    Surrogate loss: -0.0296
             Mean action noise std: 0.79
                       Mean reward: 197.22
               Mean episode length: 245.60
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 11.04s
                        Total time: 2726.67s
                               ETA: 1347121.3s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.372s, learning 0.180s)
               Value function loss: 4.3385
                    Surrogate loss: -0.0256
             Mean action noise std: 0.79
                       Mean reward: 196.82
               Mean episode length: 244.50
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 11.55s
                        Total time: 2738.22s
                               ETA: 1346150.9s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.300s, learning 0.176s)
               Value function loss: 4.8810
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 207.16
               Mean episode length: 247.01
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 11.48s
                        Total time: 2749.69s
                               ETA: 1345153.1s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.159s, learning 0.194s)
               Value function loss: 4.3537
                    Surrogate loss: -0.0237
             Mean action noise std: 0.79
                       Mean reward: 205.56
               Mean episode length: 246.34
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 11.35s
                        Total time: 2761.05s
                               ETA: 1344104.9s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.491s, learning 0.264s)
               Value function loss: 3.9711
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 210.64
               Mean episode length: 243.21
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 11.76s
                        Total time: 2772.80s
                               ETA: 1343261.6s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.681s, learning 0.210s)
               Value function loss: 3.6653
                    Surrogate loss: -0.0243
             Mean action noise std: 0.79
                       Mean reward: 216.05
               Mean episode length: 246.08
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 11.89s
                        Total time: 2784.69s
                               ETA: 1342491.5s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.322s, learning 0.190s)
               Value function loss: 3.9075
                    Surrogate loss: -0.0289
             Mean action noise std: 0.79
                       Mean reward: 218.96
               Mean episode length: 248.05
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 11.51s
                        Total time: 2796.21s
                               ETA: 1341546.9s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.534s, learning 0.169s)
               Value function loss: 4.0108
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 218.66
               Mean episode length: 243.62
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 11.70s
                        Total time: 2807.91s
                               ETA: 1340702.4s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.447s, learning 0.176s)
               Value function loss: 3.7755
                    Surrogate loss: -0.0228
             Mean action noise std: 0.79
                       Mean reward: 218.26
               Mean episode length: 243.79
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 11.62s
                        Total time: 2819.53s
                               ETA: 1339827.6s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.388s, learning 0.175s)
               Value function loss: 4.2829
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 224.83
               Mean episode length: 242.72
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 11.56s
                        Total time: 2831.09s
                               ETA: 1338933.0s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.024s, learning 0.172s)
               Value function loss: 3.6162
                    Surrogate loss: -0.0256
             Mean action noise std: 0.79
                       Mean reward: 219.83
               Mean episode length: 235.81
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 11.20s
                        Total time: 2842.29s
                               ETA: 1337873.5s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.191s, learning 0.175s)
               Value function loss: 3.4879
                    Surrogate loss: -0.0233
             Mean action noise std: 0.79
                       Mean reward: 221.43
               Mean episode length: 239.19
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 11.37s
                        Total time: 2853.65s
                               ETA: 1336903.6s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.422s, learning 0.275s)
               Value function loss: 3.3462
                    Surrogate loss: -0.0233
             Mean action noise std: 0.79
                       Mean reward: 219.17
               Mean episode length: 240.65
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 11.70s
                        Total time: 2865.35s
                               ETA: 1336097.6s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.429s, learning 0.170s)
               Value function loss: 4.5230
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 225.71
               Mean episode length: 244.96
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 11.60s
                        Total time: 2876.95s
                               ETA: 1335253.2s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.766s, learning 0.178s)
               Value function loss: 4.2507
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 222.38
               Mean episode length: 241.73
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 11.94s
                        Total time: 2888.90s
                               ETA: 1334576.2s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.068s, learning 0.172s)
               Value function loss: 3.7968
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 225.83
               Mean episode length: 240.13
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 11.24s
                        Total time: 2900.14s
                               ETA: 1333581.1s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.613s, learning 0.178s)
               Value function loss: 4.8124
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 234.05
               Mean episode length: 242.83
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 11.79s
                        Total time: 2911.93s
                               ETA: 1332847.5s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.390s, learning 0.280s)
               Value function loss: 4.4922
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 242.64
               Mean episode length: 244.01
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 11.67s
                        Total time: 2923.60s
                               ETA: 1332065.6s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.547s, learning 0.186s)
               Value function loss: 4.7723
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 236.97
               Mean episode length: 241.37
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 11.73s
                        Total time: 2935.33s
                               ETA: 1331319.0s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.104s, learning 0.160s)
               Value function loss: 4.6874
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 238.72
               Mean episode length: 241.17
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 11.26s
                        Total time: 2946.59s
                               ETA: 1330367.6s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.556s, learning 0.174s)
               Value function loss: 6.7998
                    Surrogate loss: -0.0230
             Mean action noise std: 0.79
                       Mean reward: 242.56
               Mean episode length: 241.10
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 11.73s
                        Total time: 2958.32s
                               ETA: 1329633.7s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.440s, learning 0.175s)
               Value function loss: 8.0811
                    Surrogate loss: -0.0239
             Mean action noise std: 0.79
                       Mean reward: 241.01
               Mean episode length: 239.17
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 11.61s
                        Total time: 2969.94s
                               ETA: 1328854.8s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.086s, learning 0.199s)
               Value function loss: 10.1181
                    Surrogate loss: -0.0206
             Mean action noise std: 0.79
                       Mean reward: 240.12
               Mean episode length: 238.13
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 11.29s
                        Total time: 2981.23s
                               ETA: 1327936.2s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.074s, learning 0.174s)
               Value function loss: 8.4214
                    Surrogate loss: -0.0172
             Mean action noise std: 0.79
                       Mean reward: 239.78
               Mean episode length: 238.70
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 11.25s
                        Total time: 2992.47s
                               ETA: 1327008.6s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.472s, learning 0.181s)
               Value function loss: 8.1006
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 246.98
               Mean episode length: 241.86
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 11.65s
                        Total time: 3004.13s
                               ETA: 1326268.2s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.342s, learning 0.192s)
               Value function loss: 7.1512
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 252.60
               Mean episode length: 242.88
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 11.53s
                        Total time: 3015.66s
                               ETA: 1325482.2s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.305s, learning 0.172s)
               Value function loss: 8.3510
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 248.93
               Mean episode length: 242.84
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 11.48s
                        Total time: 3027.14s
                               ETA: 1324677.7s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.156s, learning 0.193s)
               Value function loss: 5.3702
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 247.65
               Mean episode length: 243.09
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 11.35s
                        Total time: 3038.49s
                               ETA: 1323824.3s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.198s, learning 0.160s)
               Value function loss: 6.5211
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 241.62
               Mean episode length: 243.07
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 11.36s
                        Total time: 3049.84s
                               ETA: 1322982.1s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.159s)
               Value function loss: 5.8689
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 242.39
               Mean episode length: 241.86
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 11.50s
                        Total time: 3061.34s
                               ETA: 1322207.7s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.166s)
               Value function loss: 5.9887
                    Surrogate loss: -0.0223
             Mean action noise std: 0.79
                       Mean reward: 250.71
               Mean episode length: 245.57
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 11.34s
                        Total time: 3072.68s
                               ETA: 1321370.8s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.409s, learning 0.186s)
               Value function loss: 5.7490
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 242.69
               Mean episode length: 241.04
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 11.59s
                        Total time: 3084.27s
                               ETA: 1320651.1s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.296s, learning 0.171s)
               Value function loss: 4.9817
                    Surrogate loss: 0.0060
             Mean action noise std: 0.79
                       Mean reward: 246.24
               Mean episode length: 243.60
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 11.47s
                        Total time: 3095.74s
                               ETA: 1319883.4s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.046s, learning 0.163s)
               Value function loss: 5.8307
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 263.77
               Mean episode length: 247.19
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 11.21s
                        Total time: 3106.95s
                               ETA: 1319012.2s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.415s, learning 0.168s)
               Value function loss: 5.5737
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 271.89
               Mean episode length: 247.96
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 11.58s
                        Total time: 3118.53s
                               ETA: 1318306.2s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.164s)
               Value function loss: 5.0639
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 261.00
               Mean episode length: 244.35
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 11.46s
                        Total time: 3129.99s
                               ETA: 1317555.1s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.146s, learning 0.175s)
               Value function loss: 5.1501
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 246.79
               Mean episode length: 243.82
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 11.32s
                        Total time: 3141.31s
                               ETA: 1316751.4s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.599s, learning 0.165s)
               Value function loss: 5.1055
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 249.36
               Mean episode length: 242.42
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 11.76s
                        Total time: 3153.08s
                               ETA: 1316139.1s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.163s)
               Value function loss: 5.4627
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 247.38
               Mean episode length: 240.11
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 11.64s
                        Total time: 3164.71s
                               ETA: 1315478.9s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.031s, learning 0.160s)
               Value function loss: 5.1572
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 246.09
               Mean episode length: 241.67
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 11.19s
                        Total time: 3175.90s
                               ETA: 1314640.0s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.326s, learning 0.166s)
               Value function loss: 5.0526
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 254.49
               Mean episode length: 245.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 11.49s
                        Total time: 3187.40s
                               ETA: 1313931.7s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.110s, learning 0.171s)
               Value function loss: 5.3442
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 246.86
               Mean episode length: 241.32
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 11.28s
                        Total time: 3198.68s
                               ETA: 1313142.8s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.271s, learning 0.165s)
               Value function loss: 3.3780
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 235.45
               Mean episode length: 238.91
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 11.44s
                        Total time: 3210.11s
                               ETA: 1312423.8s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.716s, learning 0.194s)
               Value function loss: 4.5151
                    Surrogate loss: -0.0025
             Mean action noise std: 0.79
                       Mean reward: 218.90
               Mean episode length: 230.53
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 11.91s
                        Total time: 3222.03s
                               ETA: 1311903.4s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.156s, learning 0.161s)
               Value function loss: 3.7505
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 230.71
               Mean episode length: 236.16
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 11.32s
                        Total time: 3233.34s
                               ETA: 1311146.3s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.168s)
               Value function loss: 3.8854
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 233.63
               Mean episode length: 237.21
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 11.12s
                        Total time: 3244.46s
                               ETA: 1310315.9s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.937s, learning 0.167s)
               Value function loss: 3.8777
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 227.09
               Mean episode length: 235.84
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 11.10s
                        Total time: 3255.57s
                               ETA: 1309485.9s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.464s, learning 0.166s)
               Value function loss: 4.9525
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 223.49
               Mean episode length: 232.26
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 11.63s
                        Total time: 3267.20s
                               ETA: 1308872.8s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.028s, learning 0.167s)
               Value function loss: 4.4460
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 214.82
               Mean episode length: 229.98
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 11.20s
                        Total time: 3278.39s
                               ETA: 1308091.3s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.984s, learning 0.171s)
               Value function loss: 4.3030
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 203.95
               Mean episode length: 231.66
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 11.15s
                        Total time: 3289.55s
                               ETA: 1307299.5s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.091s, learning 0.169s)
               Value function loss: 4.2374
                    Surrogate loss: -0.0232
             Mean action noise std: 0.79
                       Mean reward: 202.24
               Mean episode length: 227.01
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 11.26s
                        Total time: 3300.81s
                               ETA: 1306555.7s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.505s, learning 0.167s)
               Value function loss: 4.4179
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: 201.04
               Mean episode length: 229.97
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 11.67s
                        Total time: 3312.48s
                               ETA: 1305980.4s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.143s, learning 0.161s)
               Value function loss: 4.2875
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 211.59
               Mean episode length: 233.43
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 11.30s
                        Total time: 3323.78s
                               ETA: 1305265.2s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.104s, learning 0.193s)
               Value function loss: 4.7300
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 212.92
               Mean episode length: 235.44
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 11.30s
                        Total time: 3335.08s
                               ETA: 1304552.4s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.114s, learning 0.175s)
               Value function loss: 5.0152
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 208.21
               Mean episode length: 236.21
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 11.29s
                        Total time: 3346.37s
                               ETA: 1303841.8s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.067s, learning 0.168s)
               Value function loss: 5.0510
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 203.57
               Mean episode length: 233.34
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 11.24s
                        Total time: 3357.60s
                               ETA: 1303115.9s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.734s, learning 0.188s)
               Value function loss: 5.4680
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 191.44
               Mean episode length: 231.84
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 11.92s
                        Total time: 3369.53s
                               ETA: 1302661.0s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.159s, learning 0.220s)
               Value function loss: 5.3544
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 202.06
               Mean episode length: 234.16
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 11.38s
                        Total time: 3380.90s
                               ETA: 1302000.4s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.193s, learning 0.159s)
               Value function loss: 4.5490
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 202.86
               Mean episode length: 236.01
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 11.35s
                        Total time: 3392.26s
                               ETA: 1301334.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.320s, learning 0.159s)
               Value function loss: 5.3204
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 190.02
               Mean episode length: 232.40
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 11.48s
                        Total time: 3403.73s
                               ETA: 1300722.2s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.162s)
               Value function loss: 5.1868
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 193.03
               Mean episode length: 235.77
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 11.11s
                        Total time: 3414.85s
                               ETA: 1299974.3s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.313s, learning 0.158s)
               Value function loss: 5.3986
                    Surrogate loss: -0.0176
             Mean action noise std: 0.79
                       Mean reward: 188.97
               Mean episode length: 232.11
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 11.47s
                        Total time: 3426.32s
                               ETA: 1299368.6s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.161s)
               Value function loss: 4.1970
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 180.79
               Mean episode length: 228.80
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 11.39s
                        Total time: 3437.70s
                               ETA: 1298735.6s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.192s, learning 0.172s)
               Value function loss: 3.8525
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 186.75
               Mean episode length: 235.47
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 11.36s
                        Total time: 3449.07s
                               ETA: 1298098.7s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.216s, learning 0.197s)
               Value function loss: 4.7707
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 175.21
               Mean episode length: 228.81
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 11.41s
                        Total time: 3460.48s
                               ETA: 1297485.2s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.009s, learning 0.200s)
               Value function loss: 4.2583
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 178.54
               Mean episode length: 231.38
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 11.21s
                        Total time: 3471.69s
                               ETA: 1296799.6s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.167s)
               Value function loss: 4.2406
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 174.83
               Mean episode length: 228.91
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 11.27s
                        Total time: 3482.96s
                               ETA: 1296143.2s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.234s, learning 0.185s)
               Value function loss: 3.9955
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 173.32
               Mean episode length: 223.86
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 11.42s
                        Total time: 3494.38s
                               ETA: 1295545.5s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.354s, learning 0.264s)
               Value function loss: 4.1397
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 176.70
               Mean episode length: 226.05
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 11.62s
                        Total time: 3506.00s
                               ETA: 1295025.5s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.330s, learning 0.184s)
               Value function loss: 4.0789
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 184.67
               Mean episode length: 232.45
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 11.51s
                        Total time: 3517.51s
                               ETA: 1294471.2s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.078s, learning 0.162s)
               Value function loss: 3.9557
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 187.13
               Mean episode length: 232.79
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 11.24s
                        Total time: 3528.75s
                               ETA: 1293820.3s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.191s, learning 0.183s)
               Value function loss: 4.2211
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 184.81
               Mean episode length: 233.59
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 11.37s
                        Total time: 3540.13s
                               ETA: 1293223.0s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.155s, learning 0.165s)
               Value function loss: 4.4850
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 182.13
               Mean episode length: 232.11
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 11.32s
                        Total time: 3551.45s
                               ETA: 1292610.4s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.167s)
               Value function loss: 3.9124
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 176.89
               Mean episode length: 225.89
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 11.45s
                        Total time: 3562.90s
                               ETA: 1292048.9s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.239s, learning 0.171s)
               Value function loss: 5.1852
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 175.10
               Mean episode length: 219.83
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 11.41s
                        Total time: 3574.31s
                               ETA: 1291477.5s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.626s, learning 0.172s)
               Value function loss: 4.0695
                    Surrogate loss: -0.0203
             Mean action noise std: 0.79
                       Mean reward: 178.32
               Mean episode length: 222.09
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 11.80s
                        Total time: 3586.11s
                               ETA: 1291049.8s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.021s, learning 0.170s)
               Value function loss: 4.4423
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 175.32
               Mean episode length: 222.10
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 11.19s
                        Total time: 3597.30s
                               ETA: 1290407.5s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.169s)
               Value function loss: 4.5892
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 168.83
               Mean episode length: 223.81
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 11.44s
                        Total time: 3608.74s
                               ETA: 1289857.8s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.859s, learning 0.178s)
               Value function loss: 4.7875
                    Surrogate loss: -0.0025
             Mean action noise std: 0.79
                       Mean reward: 167.50
               Mean episode length: 224.19
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 11.04s
                        Total time: 3619.77s
                               ETA: 1289169.0s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.377s, learning 0.186s)
               Value function loss: 4.6910
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 165.26
               Mean episode length: 219.88
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 11.56s
                        Total time: 3631.34s
                               ETA: 1288671.7s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.362s, learning 0.194s)
               Value function loss: 4.6907
                    Surrogate loss: -0.0192
             Mean action noise std: 0.79
                       Mean reward: 163.33
               Mean episode length: 220.62
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 11.56s
                        Total time: 3642.89s
                               ETA: 1288175.5s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.255s, learning 0.184s)
               Value function loss: 4.7724
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 160.41
               Mean episode length: 221.67
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 11.44s
                        Total time: 3654.33s
                               ETA: 1287641.3s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.161s)
               Value function loss: 5.8541
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 164.15
               Mean episode length: 222.68
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 11.68s
                        Total time: 3666.01s
                               ETA: 1287194.1s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.393s, learning 0.175s)
               Value function loss: 4.5277
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 168.09
               Mean episode length: 224.67
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 11.57s
                        Total time: 3677.57s
                               ETA: 1286712.5s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.958s, learning 0.165s)
               Value function loss: 4.9333
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 162.55
               Mean episode length: 230.44
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 11.12s
                        Total time: 3688.70s
                               ETA: 1286078.6s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.428s, learning 0.184s)
               Value function loss: 6.0065
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 173.50
               Mean episode length: 228.96
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 11.61s
                        Total time: 3700.31s
                               ETA: 1285619.1s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.945s, learning 0.175s)
               Value function loss: 5.9084
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 176.64
               Mean episode length: 233.65
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 11.12s
                        Total time: 3711.43s
                               ETA: 1284992.3s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.083s, learning 0.177s)
               Value function loss: 8.8744
                    Surrogate loss: -0.0213
             Mean action noise std: 0.79
                       Mean reward: 176.72
               Mean episode length: 231.42
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 11.26s
                        Total time: 3722.69s
                               ETA: 1284418.3s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.307s, learning 0.170s)
               Value function loss: 8.9353
                    Surrogate loss: -0.0217
             Mean action noise std: 0.79
                       Mean reward: 174.73
               Mean episode length: 234.53
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 11.48s
                        Total time: 3734.17s
                               ETA: 1283922.3s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.298s, learning 0.174s)
               Value function loss: 4.8930
                    Surrogate loss: 0.1946
             Mean action noise std: 0.79
                       Mean reward: 177.09
               Mean episode length: 235.36
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 11.47s
                        Total time: 3745.64s
                               ETA: 1283428.1s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.451s, learning 0.174s)
               Value function loss: 4.6804
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 171.90
               Mean episode length: 235.98
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 11.63s
                        Total time: 3757.26s
                               ETA: 1282989.8s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.441s, learning 0.165s)
               Value function loss: 4.6707
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 177.42
               Mean episode length: 239.28
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 11.61s
                        Total time: 3768.87s
                               ETA: 1282547.7s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.161s)
               Value function loss: 4.4927
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: 187.37
               Mean episode length: 238.32
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 11.31s
                        Total time: 3780.18s
                               ETA: 1282006.5s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.496s, learning 0.158s)
               Value function loss: 4.3578
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 177.75
               Mean episode length: 232.07
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 11.65s
                        Total time: 3791.83s
                               ETA: 1281586.6s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.277s, learning 0.161s)
               Value function loss: 3.9509
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 179.66
               Mean episode length: 232.83
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 11.44s
                        Total time: 3803.27s
                               ETA: 1281096.7s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.239s, learning 0.167s)
               Value function loss: 4.5702
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 177.16
               Mean episode length: 229.44
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 11.41s
                        Total time: 3814.67s
                               ETA: 1280599.4s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.162s)
               Value function loss: 4.5490
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 180.82
               Mean episode length: 233.46
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 11.41s
                        Total time: 3826.08s
                               ETA: 1280105.4s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.747s, learning 0.191s)
               Value function loss: 4.8146
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 173.97
               Mean episode length: 233.27
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 10.94s
                        Total time: 3837.02s
                               ETA: 1279458.6s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.259s, learning 0.181s)
               Value function loss: 4.2291
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 169.99
               Mean episode length: 227.57
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 11.44s
                        Total time: 3848.46s
                               ETA: 1278983.1s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.520s, learning 0.174s)
               Value function loss: 5.3753
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 160.77
               Mean episode length: 222.87
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 11.69s
                        Total time: 3860.15s
                               ETA: 1278594.5s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.521s, learning 0.183s)
               Value function loss: 4.5914
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 151.91
               Mean episode length: 220.10
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 11.70s
                        Total time: 3871.85s
                               ETA: 1278211.7s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.345s, learning 0.191s)
               Value function loss: 4.2945
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 156.61
               Mean episode length: 217.94
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 11.54s
                        Total time: 3883.39s
                               ETA: 1277776.1s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.932s, learning 0.165s)
               Value function loss: 4.1212
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 154.15
               Mean episode length: 216.57
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 11.10s
                        Total time: 3894.49s
                               ETA: 1277199.2s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.419s, learning 0.169s)
               Value function loss: 4.6873
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 149.96
               Mean episode length: 215.94
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 11.59s
                        Total time: 3906.07s
                               ETA: 1276786.7s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.405s, learning 0.171s)
               Value function loss: 4.5246
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 150.06
               Mean episode length: 216.08
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 11.58s
                        Total time: 3917.65s
                               ETA: 1276373.2s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.397s, learning 0.182s)
               Value function loss: 4.9738
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 149.16
               Mean episode length: 219.61
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 11.58s
                        Total time: 3929.23s
                               ETA: 1275963.0s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.168s, learning 0.159s)
               Value function loss: 5.4634
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 129.73
               Mean episode length: 210.37
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 11.33s
                        Total time: 3940.56s
                               ETA: 1275473.8s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.119s, learning 0.175s)
               Value function loss: 5.0488
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 128.65
               Mean episode length: 210.17
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 11.29s
                        Total time: 3951.85s
                               ETA: 1274977.3s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.161s)
               Value function loss: 4.7744
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 136.83
               Mean episode length: 218.34
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 11.33s
                        Total time: 3963.18s
                               ETA: 1274495.7s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.162s)
               Value function loss: 5.6544
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 135.91
               Mean episode length: 224.45
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 11.46s
                        Total time: 3974.64s
                               ETA: 1274058.1s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.050s, learning 0.193s)
               Value function loss: 4.9595
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 136.94
               Mean episode length: 223.92
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 11.24s
                        Total time: 3985.88s
                               ETA: 1273554.1s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.276s, learning 0.176s)
               Value function loss: 5.5911
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 124.08
               Mean episode length: 217.63
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 11.45s
                        Total time: 3997.34s
                               ETA: 1273119.8s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.330s, learning 0.190s)
               Value function loss: 5.5333
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 134.56
               Mean episode length: 221.44
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 11.52s
                        Total time: 4008.86s
                               ETA: 1272709.9s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.484s, learning 0.176s)
               Value function loss: 5.6378
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 134.80
               Mean episode length: 223.54
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 11.66s
                        Total time: 4020.52s
                               ETA: 1272346.9s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.273s, learning 0.177s)
               Value function loss: 5.1113
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 137.05
               Mean episode length: 224.72
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 11.45s
                        Total time: 4031.97s
                               ETA: 1271919.7s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.238s, learning 0.173s)
               Value function loss: 4.1380
                    Surrogate loss: -0.0239
             Mean action noise std: 0.78
                       Mean reward: 131.19
               Mean episode length: 226.35
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 11.41s
                        Total time: 4043.38s
                               ETA: 1271483.0s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.976s, learning 0.161s)
               Value function loss: 4.4885
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 130.91
               Mean episode length: 224.88
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 11.14s
                        Total time: 4054.52s
                               ETA: 1270963.1s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.334s, learning 0.168s)
               Value function loss: 4.1559
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 127.06
               Mean episode length: 225.21
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 11.50s
                        Total time: 4066.02s
                               ETA: 1270560.5s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.162s)
               Value function loss: 4.6387
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 125.97
               Mean episode length: 226.14
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 11.41s
                        Total time: 4077.43s
                               ETA: 1270132.5s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.103s, learning 0.162s)
               Value function loss: 3.8534
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 133.72
               Mean episode length: 226.79
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 11.26s
                        Total time: 4088.70s
                               ETA: 1269661.0s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.242s, learning 0.160s)
               Value function loss: 4.0637
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 145.60
               Mean episode length: 235.93
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 11.40s
                        Total time: 4100.10s
                               ETA: 1269235.1s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.112s, learning 0.166s)
               Value function loss: 3.8664
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 148.43
               Mean episode length: 234.49
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 11.28s
                        Total time: 4111.38s
                               ETA: 1268773.4s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.434s, learning 0.195s)
               Value function loss: 3.8109
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 157.06
               Mean episode length: 234.23
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 11.63s
                        Total time: 4123.01s
                               ETA: 1268422.4s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.401s, learning 0.183s)
               Value function loss: 4.2070
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 171.56
               Mean episode length: 238.41
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 11.58s
                        Total time: 4134.59s
                               ETA: 1268059.9s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.161s)
               Value function loss: 3.6575
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 184.19
               Mean episode length: 240.25
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 11.44s
                        Total time: 4146.03s
                               ETA: 1267654.2s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.339s, learning 0.171s)
               Value function loss: 3.6425
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 185.37
               Mean episode length: 242.27
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 11.51s
                        Total time: 4157.54s
                               ETA: 1267273.3s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.220s, learning 0.170s)
               Value function loss: 4.2896
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 185.91
               Mean episode length: 237.88
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 11.39s
                        Total time: 4168.93s
                               ETA: 1266858.1s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.577s, learning 0.160s)
               Value function loss: 4.3286
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 197.86
               Mean episode length: 241.90
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 10.74s
                        Total time: 4179.66s
                               ETA: 1266247.8s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.462s, learning 0.171s)
               Value function loss: 4.1319
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 204.40
               Mean episode length: 240.62
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 11.63s
                        Total time: 4191.30s
                               ETA: 1265911.6s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.168s, learning 0.164s)
               Value function loss: 4.2105
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 199.50
               Mean episode length: 234.92
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 11.33s
                        Total time: 4202.63s
                               ETA: 1265486.6s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.434s, learning 0.161s)
               Value function loss: 4.6367
                    Surrogate loss: 0.0026
             Mean action noise std: 0.78
                       Mean reward: 199.59
               Mean episode length: 231.32
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 11.60s
                        Total time: 4214.22s
                               ETA: 1265143.3s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.775s, learning 0.235s)
               Value function loss: 4.6498
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 217.57
               Mean episode length: 241.81
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 11.01s
                        Total time: 4225.23s
                               ETA: 1264626.5s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.347s, learning 0.186s)
               Value function loss: 3.5190
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 227.96
               Mean episode length: 242.03
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 11.53s
                        Total time: 4236.77s
                               ETA: 1264269.1s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.162s)
               Value function loss: 4.7692
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 224.48
               Mean episode length: 239.25
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 11.41s
                        Total time: 4248.18s
                               ETA: 1263877.2s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.551s, learning 0.163s)
               Value function loss: 4.6739
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 229.55
               Mean episode length: 238.67
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 11.71s
                        Total time: 4259.89s
                               ETA: 1263577.7s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.181s, learning 0.159s)
               Value function loss: 3.8745
                    Surrogate loss: 0.0051
             Mean action noise std: 0.78
                       Mean reward: 242.89
               Mean episode length: 245.60
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 11.34s
                        Total time: 4271.23s
                               ETA: 1263169.3s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.120s, learning 0.166s)
               Value function loss: 4.6577
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 242.15
               Mean episode length: 245.56
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 11.29s
                        Total time: 4282.52s
                               ETA: 1262747.4s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.212s, learning 0.159s)
               Value function loss: 5.2804
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 238.95
               Mean episode length: 246.14
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 11.37s
                        Total time: 4293.89s
                               ETA: 1262352.8s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.221s, learning 0.199s)
               Value function loss: 5.1632
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 232.68
               Mean episode length: 239.69
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 11.42s
                        Total time: 4305.31s
                               ETA: 1261974.8s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.475s, learning 0.239s)
               Value function loss: 5.3609
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 232.03
               Mean episode length: 236.55
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 11.71s
                        Total time: 4317.02s
                               ETA: 1261685.1s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.127s, learning 0.162s)
               Value function loss: 4.0996
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 228.42
               Mean episode length: 233.35
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 11.29s
                        Total time: 4328.31s
                               ETA: 1261273.1s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.065s, learning 0.164s)
               Value function loss: 4.3678
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 221.60
               Mean episode length: 234.70
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 11.23s
                        Total time: 4339.54s
                               ETA: 1260846.0s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.465s, learning 0.169s)
               Value function loss: 5.6701
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 232.86
               Mean episode length: 240.44
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 11.63s
                        Total time: 4351.18s
                               ETA: 1260538.7s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.370s, learning 0.186s)
               Value function loss: 5.9636
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 229.46
               Mean episode length: 238.70
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 11.56s
                        Total time: 4362.73s
                               ETA: 1260210.5s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.006s, learning 0.164s)
               Value function loss: 5.3079
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 227.77
               Mean episode length: 239.47
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 11.17s
                        Total time: 4373.90s
                               ETA: 1259772.8s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.430s, learning 0.187s)
               Value function loss: 6.3038
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 226.59
               Mean episode length: 238.86
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 11.62s
                        Total time: 4385.52s
                               ETA: 1259465.9s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.007s, learning 0.166s)
               Value function loss: 5.9626
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 220.59
               Mean episode length: 234.71
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 11.17s
                        Total time: 4396.69s
                               ETA: 1259033.7s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.307s, learning 0.162s)
               Value function loss: 5.1198
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 219.66
               Mean episode length: 238.54
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 11.47s
                        Total time: 4408.16s
                               ETA: 1258688.2s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.498s, learning 0.159s)
               Value function loss: 4.2158
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 217.86
               Mean episode length: 239.91
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 11.66s
                        Total time: 4419.82s
                               ETA: 1258398.5s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.435s, learning 0.192s)
               Value function loss: 4.7529
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 215.38
               Mean episode length: 235.84
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 11.63s
                        Total time: 4431.45s
                               ETA: 1258101.5s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.161s)
               Value function loss: 4.4547
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 219.64
               Mean episode length: 232.66
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 11.33s
                        Total time: 4442.78s
                               ETA: 1257722.9s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.155s, learning 0.160s)
               Value function loss: 5.2821
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 210.75
               Mean episode length: 228.39
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 11.32s
                        Total time: 4454.09s
                               ETA: 1257341.5s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.361s, learning 0.161s)
               Value function loss: 4.3829
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 211.90
               Mean episode length: 230.55
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 11.52s
                        Total time: 4465.62s
                               ETA: 1257020.3s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.429s, learning 0.168s)
               Value function loss: 4.7507
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 203.90
               Mean episode length: 229.03
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 11.60s
                        Total time: 4477.21s
                               ETA: 1256722.2s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.168s, learning 0.162s)
               Value function loss: 4.2374
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 209.29
               Mean episode length: 228.96
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 11.33s
                        Total time: 4488.54s
                               ETA: 1256350.8s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.323s, learning 0.167s)
               Value function loss: 4.5784
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 215.03
               Mean episode length: 231.43
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 11.49s
                        Total time: 4500.03s
                               ETA: 1256026.2s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.340s, learning 0.168s)
               Value function loss: 4.8448
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 212.21
               Mean episode length: 227.77
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 11.51s
                        Total time: 4511.54s
                               ETA: 1255708.4s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.293s, learning 0.164s)
               Value function loss: 5.1064
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 226.21
               Mean episode length: 234.65
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 11.46s
                        Total time: 4523.00s
                               ETA: 1255377.9s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.196s, learning 0.170s)
               Value function loss: 4.1528
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 207.35
               Mean episode length: 224.10
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 11.37s
                        Total time: 4534.36s
                               ETA: 1255023.9s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.165s)
               Value function loss: 6.1401
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 220.17
               Mean episode length: 231.14
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 11.58s
                        Total time: 4545.94s
                               ETA: 1254730.4s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.155s, learning 0.166s)
               Value function loss: 4.3160
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 211.97
               Mean episode length: 226.10
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 11.32s
                        Total time: 4557.26s
                               ETA: 1254367.8s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.063s, learning 0.187s)
               Value function loss: 5.2030
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 194.83
               Mean episode length: 212.72
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 11.25s
                        Total time: 4568.51s
                               ETA: 1253987.7s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.423s, learning 0.163s)
               Value function loss: 4.8602
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 193.11
               Mean episode length: 210.52
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 11.59s
                        Total time: 4580.10s
                               ETA: 1253701.5s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.162s)
               Value function loss: 4.0007
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 191.08
               Mean episode length: 207.99
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 11.39s
                        Total time: 4591.49s
                               ETA: 1253362.8s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.788s, learning 0.180s)
               Value function loss: 4.9790
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 206.30
               Mean episode length: 222.57
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 11.97s
                        Total time: 4603.45s
                               ETA: 1253183.6s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.124s, learning 0.159s)
               Value function loss: 4.9407
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 207.51
               Mean episode length: 220.33
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 11.28s
                        Total time: 4614.74s
                               ETA: 1252819.6s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.239s, learning 0.159s)
               Value function loss: 4.5594
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 205.10
               Mean episode length: 222.17
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 11.40s
                        Total time: 4626.14s
                               ETA: 1252488.7s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.853s, learning 0.164s)
               Value function loss: 4.7685
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 196.09
               Mean episode length: 214.13
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 11.02s
                        Total time: 4637.15s
                               ETA: 1252056.6s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.164s)
               Value function loss: 5.7650
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 198.04
               Mean episode length: 210.27
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 11.43s
                        Total time: 4648.58s
                               ETA: 1251738.1s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.907s, learning 0.184s)
               Value function loss: 5.2672
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 211.03
               Mean episode length: 219.34
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 11.09s
                        Total time: 4659.68s
                               ETA: 1251330.1s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.206s, learning 0.179s)
               Value function loss: 4.7904
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 180.06
               Mean episode length: 200.82
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 11.38s
                        Total time: 4671.06s
                               ETA: 1251002.8s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.198s, learning 0.176s)
               Value function loss: 4.8821
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 182.81
               Mean episode length: 203.37
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 11.37s
                        Total time: 4682.43s
                               ETA: 1250674.6s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.915s, learning 0.163s)
               Value function loss: 5.5664
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 179.81
               Mean episode length: 199.80
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 11.08s
                        Total time: 4693.51s
                               ETA: 1250268.9s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.897s, learning 0.162s)
               Value function loss: 5.0038
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 195.48
               Mean episode length: 208.86
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 12.06s
                        Total time: 4705.57s
                               ETA: 1250126.1s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.163s)
               Value function loss: 6.2929
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 187.40
               Mean episode length: 204.84
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 11.31s
                        Total time: 4716.88s
                               ETA: 1249784.4s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.197s, learning 0.176s)
               Value function loss: 5.5907
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 181.11
               Mean episode length: 197.07
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 11.37s
                        Total time: 4728.25s
                               ETA: 1249462.0s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.155s, learning 0.173s)
               Value function loss: 5.8548
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 181.88
               Mean episode length: 196.30
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 11.33s
                        Total time: 4739.58s
                               ETA: 1249129.5s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.022s, learning 0.166s)
               Value function loss: 4.9036
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 178.43
               Mean episode length: 194.70
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 11.19s
                        Total time: 4750.77s
                               ETA: 1248761.9s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.867s, learning 0.170s)
               Value function loss: 4.9425
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 185.80
               Mean episode length: 198.19
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 11.04s
                        Total time: 4761.80s
                               ETA: 1248356.6s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.555s, learning 0.189s)
               Value function loss: 5.5186
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 173.36
               Mean episode length: 191.36
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 11.74s
                        Total time: 4773.55s
                               ETA: 1248138.3s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.171s, learning 0.178s)
               Value function loss: 4.1551
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 195.42
               Mean episode length: 206.70
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 11.35s
                        Total time: 4784.90s
                               ETA: 1247818.1s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.179s, learning 0.190s)
               Value function loss: 3.6170
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 194.41
               Mean episode length: 206.26
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 11.37s
                        Total time: 4796.26s
                               ETA: 1247504.6s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.280s, learning 0.160s)
               Value function loss: 3.3835
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 202.42
               Mean episode length: 215.67
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 11.44s
                        Total time: 4807.70s
                               ETA: 1247211.2s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.162s)
               Value function loss: 3.0939
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 200.96
               Mean episode length: 216.67
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 11.63s
                        Total time: 4819.34s
                               ETA: 1246969.5s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.677s, learning 0.167s)
               Value function loss: 4.1540
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 182.66
               Mean episode length: 201.42
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 11.84s
                        Total time: 4831.18s
                               ETA: 1246783.2s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.068s, learning 0.159s)
               Value function loss: 4.0827
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 183.76
               Mean episode length: 207.12
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 11.23s
                        Total time: 4842.41s
                               ETA: 1246438.9s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.122s, learning 0.158s)
               Value function loss: 4.0311
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 183.16
               Mean episode length: 211.02
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 11.28s
                        Total time: 4853.69s
                               ETA: 1246109.9s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.381s, learning 0.170s)
               Value function loss: 4.8581
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 179.10
               Mean episode length: 209.08
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 11.55s
                        Total time: 4865.24s
                               ETA: 1245852.0s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.326s, learning 0.159s)
               Value function loss: 4.0314
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 180.97
               Mean episode length: 208.97
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 11.48s
                        Total time: 4876.73s
                               ETA: 1245578.3s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.384s, learning 0.166s)
               Value function loss: 4.3517
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 180.78
               Mean episode length: 211.95
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 11.55s
                        Total time: 4888.28s
                               ETA: 1245322.6s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.853s, learning 0.172s)
               Value function loss: 3.7122
                    Surrogate loss: 0.0007
             Mean action noise std: 0.78
                       Mean reward: 175.46
               Mean episode length: 210.09
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 11.02s
                        Total time: 4899.30s
                               ETA: 1244934.8s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.754s, learning 0.167s)
               Value function loss: 3.9336
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 174.82
               Mean episode length: 211.08
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 11.92s
                        Total time: 4911.22s
                               ETA: 1244776.0s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.319s, learning 0.167s)
               Value function loss: 4.6105
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 184.44
               Mean episode length: 219.48
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 11.49s
                        Total time: 4922.71s
                               ETA: 1244508.0s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.758s, learning 0.163s)
               Value function loss: 4.6552
                    Surrogate loss: 0.0114
             Mean action noise std: 0.78
                       Mean reward: 199.68
               Mean episode length: 227.41
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 11.92s
                        Total time: 4934.63s
                               ETA: 1244351.0s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.166s)
               Value function loss: 3.9368
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 195.18
               Mean episode length: 226.81
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 12.07s
                        Total time: 4946.70s
                               ETA: 1244232.7s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.167s)
               Value function loss: 3.9068
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 191.43
               Mean episode length: 225.29
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 11.33s
                        Total time: 4958.03s
                               ETA: 1243928.1s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.217s, learning 0.167s)
               Value function loss: 6.4585
                    Surrogate loss: -0.0004
             Mean action noise std: 0.78
                       Mean reward: 194.68
               Mean episode length: 225.05
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 11.38s
                        Total time: 4969.41s
                               ETA: 1243639.0s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.118s, learning 0.162s)
               Value function loss: 4.4332
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 209.72
               Mean episode length: 232.20
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 11.28s
                        Total time: 4980.69s
                               ETA: 1243325.4s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.958s, learning 0.159s)
               Value function loss: 6.2065
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 215.91
               Mean episode length: 236.34
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 11.12s
                        Total time: 4991.81s
                               ETA: 1242972.9s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.546s, learning 0.170s)
               Value function loss: 5.6266
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 211.19
               Mean episode length: 233.90
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 11.72s
                        Total time: 5003.53s
                               ETA: 1242770.8s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.752s, learning 0.177s)
               Value function loss: 5.9747
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 214.34
               Mean episode length: 237.68
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 11.93s
                        Total time: 5015.45s
                               ETA: 1242622.3s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.427s, learning 0.165s)
               Value function loss: 4.7960
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 214.91
               Mean episode length: 237.87
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 11.59s
                        Total time: 5027.05s
                               ETA: 1242391.2s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.140s, learning 0.172s)
               Value function loss: 4.7176
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 217.29
               Mean episode length: 238.87
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 11.31s
                        Total time: 5038.36s
                               ETA: 1242092.3s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.579s, learning 0.167s)
               Value function loss: 4.1621
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 214.69
               Mean episode length: 240.33
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 11.75s
                        Total time: 5050.10s
                               ETA: 1241901.5s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.166s)
               Value function loss: 4.4869
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 215.16
               Mean episode length: 238.81
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 11.47s
                        Total time: 5061.57s
                               ETA: 1241643.6s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.279s, learning 0.170s)
               Value function loss: 5.2968
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 228.92
               Mean episode length: 246.64
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 11.45s
                        Total time: 5073.02s
                               ETA: 1241381.9s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.897s, learning 0.163s)
               Value function loss: 5.4810
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 219.62
               Mean episode length: 242.38
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 11.06s
                        Total time: 5084.08s
                               ETA: 1241026.6s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.443s, learning 0.167s)
               Value function loss: 6.3336
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 215.84
               Mean episode length: 245.94
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 11.61s
                        Total time: 5095.69s
                               ETA: 1240807.0s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.311s, learning 0.167s)
               Value function loss: 4.7994
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 220.25
               Mean episode length: 244.30
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 11.48s
                        Total time: 5107.17s
                               ETA: 1240556.4s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.383s, learning 0.160s)
               Value function loss: 5.1896
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 213.60
               Mean episode length: 243.21
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 11.54s
                        Total time: 5118.71s
                               ETA: 1240322.6s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.550s, learning 0.188s)
               Value function loss: 5.2519
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 204.92
               Mean episode length: 240.83
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 11.74s
                        Total time: 5130.45s
                               ETA: 1240137.2s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.409s, learning 0.158s)
               Value function loss: 4.3687
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: 215.37
               Mean episode length: 243.18
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 11.57s
                        Total time: 5142.02s
                               ETA: 1239911.1s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.063s, learning 0.159s)
               Value function loss: 3.9569
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 217.29
               Mean episode length: 242.63
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 11.22s
                        Total time: 5153.24s
                               ETA: 1239603.1s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.984s, learning 0.162s)
               Value function loss: 3.6614
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 211.96
               Mean episode length: 240.74
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 11.15s
                        Total time: 5164.39s
                               ETA: 1239278.3s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.163s)
               Value function loss: 4.1230
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 213.67
               Mean episode length: 241.18
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 11.36s
                        Total time: 5175.74s
                               ETA: 1239005.7s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.110s, learning 0.173s)
               Value function loss: 3.5989
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 217.91
               Mean episode length: 243.49
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 11.28s
                        Total time: 5187.03s
                               ETA: 1238716.3s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.012s, learning 0.167s)
               Value function loss: 3.9791
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 212.72
               Mean episode length: 240.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 11.18s
                        Total time: 5198.20s
                               ETA: 1238403.7s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.355s, learning 0.168s)
               Value function loss: 3.4900
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 211.95
               Mean episode length: 241.02
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 10.52s
                        Total time: 5208.73s
                               ETA: 1237936.6s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.165s)
               Value function loss: 3.5524
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 204.13
               Mean episode length: 240.86
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 11.42s
                        Total time: 5220.15s
                               ETA: 1237684.6s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.162s)
               Value function loss: 3.5825
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 193.81
               Mean episode length: 235.53
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 11.39s
                        Total time: 5231.54s
                               ETA: 1237426.2s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.207s, learning 0.168s)
               Value function loss: 3.8695
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 196.20
               Mean episode length: 234.86
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 11.38s
                        Total time: 5242.91s
                               ETA: 1237165.7s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.477s, learning 0.200s)
               Value function loss: 4.3262
                    Surrogate loss: -0.0035
             Mean action noise std: 0.78
                       Mean reward: 204.02
               Mean episode length: 233.91
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 11.68s
                        Total time: 5254.59s
                               ETA: 1236977.3s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.410s, learning 0.186s)
               Value function loss: 4.5144
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 201.55
               Mean episode length: 230.63
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 11.60s
                        Total time: 5266.19s
                               ETA: 1236771.0s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.480s, learning 0.259s)
               Value function loss: 4.2876
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 204.23
               Mean episode length: 229.33
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 11.74s
                        Total time: 5277.92s
                               ETA: 1236599.1s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.020s, learning 0.163s)
               Value function loss: 4.9888
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 202.26
               Mean episode length: 232.69
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 11.18s
                        Total time: 5289.11s
                               ETA: 1236298.0s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.392s, learning 0.186s)
               Value function loss: 3.3388
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 209.05
               Mean episode length: 232.87
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 11.58s
                        Total time: 5300.69s
                               ETA: 1236090.1s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.187s, learning 0.166s)
               Value function loss: 5.5403
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: 219.11
               Mean episode length: 234.84
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 11.35s
                        Total time: 5312.04s
                               ETA: 1235830.7s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.431s, learning 0.158s)
               Value function loss: 5.9045
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 214.71
               Mean episode length: 229.27
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 11.59s
                        Total time: 5323.63s
                               ETA: 1235627.6s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.134s, learning 0.165s)
               Value function loss: 4.6449
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 216.90
               Mean episode length: 230.53
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 11.30s
                        Total time: 5334.93s
                               ETA: 1235358.0s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.455s, learning 0.166s)
               Value function loss: 5.9127
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 218.87
               Mean episode length: 232.82
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 11.62s
                        Total time: 5346.55s
                               ETA: 1235164.0s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.163s)
               Value function loss: 5.8176
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: 230.78
               Mean episode length: 234.78
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 11.24s
                        Total time: 5357.79s
                               ETA: 1234882.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.546s, learning 0.161s)
               Value function loss: 5.5917
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 245.72
               Mean episode length: 236.37
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 11.71s
                        Total time: 5369.49s
                               ETA: 1234710.6s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.051s, learning 0.179s)
               Value function loss: 5.5523
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 247.10
               Mean episode length: 236.76
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 11.23s
                        Total time: 5380.72s
                               ETA: 1234429.7s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.166s)
               Value function loss: 7.1380
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 251.11
               Mean episode length: 237.18
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 11.30s
                        Total time: 5392.02s
                               ETA: 1234164.9s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.459s, learning 0.181s)
               Value function loss: 9.1271
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 253.62
               Mean episode length: 237.16
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 11.64s
                        Total time: 5403.66s
                               ETA: 1233980.0s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.289s, learning 0.199s)
               Value function loss: 6.8047
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 265.18
               Mean episode length: 240.19
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 11.49s
                        Total time: 5415.15s
                               ETA: 1233761.2s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.930s, learning 0.176s)
               Value function loss: 8.2715
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 274.61
               Mean episode length: 244.02
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 11.11s
                        Total time: 5426.25s
                               ETA: 1233456.7s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.158s)
               Value function loss: 6.6158
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 274.45
               Mean episode length: 240.99
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 11.35s
                        Total time: 5437.60s
                               ETA: 1233208.1s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.103s, learning 0.169s)
               Value function loss: 5.5323
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 284.88
               Mean episode length: 244.56
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 11.27s
                        Total time: 5448.87s
                               ETA: 1232943.4s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.404s, learning 0.161s)
               Value function loss: 5.0477
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 288.33
               Mean episode length: 244.87
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 11.56s
                        Total time: 5460.44s
                               ETA: 1232746.1s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.309s, learning 0.160s)
               Value function loss: 4.8583
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 299.81
               Mean episode length: 246.16
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 11.47s
                        Total time: 5471.90s
                               ETA: 1232527.9s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.495s, learning 0.170s)
               Value function loss: 6.5116
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 305.03
               Mean episode length: 247.08
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 11.67s
                        Total time: 5483.57s
                               ETA: 1232355.0s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.182s, learning 0.168s)
               Value function loss: 7.1527
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 304.82
               Mean episode length: 245.44
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 11.35s
                        Total time: 5494.92s
                               ETA: 1232112.1s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.591s, learning 0.182s)
               Value function loss: 5.8230
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 296.58
               Mean episode length: 245.58
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 11.77s
                        Total time: 5506.69s
                               ETA: 1231964.8s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.924s, learning 0.184s)
               Value function loss: 5.9103
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 298.62
               Mean episode length: 245.42
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 11.11s
                        Total time: 5517.80s
                               ETA: 1231669.5s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.184s, learning 0.271s)
               Value function loss: 5.5580
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 304.40
               Mean episode length: 246.17
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 11.46s
                        Total time: 5529.26s
                               ETA: 1231453.0s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.292s, learning 0.175s)
               Value function loss: 5.3061
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 308.44
               Mean episode length: 247.44
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 11.47s
                        Total time: 5540.72s
                               ETA: 1231239.9s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.225s, learning 0.199s)
               Value function loss: 7.1624
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 312.35
               Mean episode length: 246.44
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 11.42s
                        Total time: 5552.15s
                               ETA: 1231018.4s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.129s, learning 0.159s)
               Value function loss: 8.1804
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: 321.64
               Mean episode length: 245.15
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 11.29s
                        Total time: 5563.43s
                               ETA: 1230767.7s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.051s, learning 0.183s)
               Value function loss: 7.5294
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 317.26
               Mean episode length: 245.46
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 11.23s
                        Total time: 5574.67s
                               ETA: 1230505.8s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.124s, learning 0.185s)
               Value function loss: 8.3424
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 330.73
               Mean episode length: 248.78
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 11.31s
                        Total time: 5585.98s
                               ETA: 1230261.8s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.073s, learning 0.178s)
               Value function loss: 8.9200
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 332.98
               Mean episode length: 247.94
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 11.25s
                        Total time: 5597.23s
                               ETA: 1230006.0s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.168s, learning 0.177s)
               Value function loss: 10.7991
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 333.98
               Mean episode length: 247.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 11.35s
                        Total time: 5608.57s
                               ETA: 1229772.1s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.247s, learning 0.178s)
               Value function loss: 10.6433
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 338.07
               Mean episode length: 247.15
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 11.42s
                        Total time: 5620.00s
                               ETA: 1229556.4s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.169s)
               Value function loss: 10.9625
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 339.43
               Mean episode length: 248.10
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 11.65s
                        Total time: 5631.64s
                               ETA: 1229390.3s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.392s, learning 0.169s)
               Value function loss: 13.7889
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 343.46
               Mean episode length: 247.88
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 11.56s
                        Total time: 5643.20s
                               ETA: 1229206.1s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.473s, learning 0.197s)
               Value function loss: 11.0481
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 341.97
               Mean episode length: 248.29
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 11.67s
                        Total time: 5654.87s
                               ETA: 1229046.1s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.037s, learning 0.178s)
               Value function loss: 11.4812
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 339.38
               Mean episode length: 244.94
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 11.22s
                        Total time: 5666.09s
                               ETA: 1228788.5s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.352s, learning 0.182s)
               Value function loss: 11.9105
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 332.59
               Mean episode length: 242.95
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 11.53s
                        Total time: 5677.62s
                               ETA: 1228600.8s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.962s, learning 0.180s)
               Value function loss: 10.4205
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 334.08
               Mean episode length: 243.25
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 11.14s
                        Total time: 5688.77s
                               ETA: 1228329.2s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.372s, learning 0.180s)
               Value function loss: 10.1295
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 334.23
               Mean episode length: 243.08
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 11.55s
                        Total time: 5700.32s
                               ETA: 1228146.9s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.458s, learning 0.160s)
               Value function loss: 13.9365
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 337.54
               Mean episode length: 241.88
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 11.62s
                        Total time: 5711.94s
                               ETA: 1227979.8s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.162s)
               Value function loss: 12.3738
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 349.94
               Mean episode length: 243.97
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 11.15s
                        Total time: 5723.08s
                               ETA: 1227712.4s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.084s, learning 0.164s)
               Value function loss: 11.6328
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 328.25
               Mean episode length: 234.94
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 11.25s
                        Total time: 5734.33s
                               ETA: 1227467.5s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.477s, learning 0.156s)
               Value function loss: 10.6734
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 329.10
               Mean episode length: 234.49
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 11.63s
                        Total time: 5745.96s
                               ETA: 1227305.9s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.980s, learning 0.171s)
               Value function loss: 9.0972
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 313.82
               Mean episode length: 230.15
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 11.15s
                        Total time: 5757.12s
                               ETA: 1227042.3s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.253s, learning 0.176s)
               Value function loss: 10.4216
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 317.39
               Mean episode length: 231.77
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 11.43s
                        Total time: 5768.54s
                               ETA: 1226838.7s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.955s, learning 0.158s)
               Value function loss: 10.0063
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 328.97
               Mean episode length: 238.31
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 11.11s
                        Total time: 5779.66s
                               ETA: 1226569.1s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.165s)
               Value function loss: 12.9819
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 327.10
               Mean episode length: 238.28
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 11.08s
                        Total time: 5790.73s
                               ETA: 1226292.8s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.017s, learning 0.187s)
               Value function loss: 10.5392
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 338.57
               Mean episode length: 239.82
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 11.20s
                        Total time: 5801.94s
                               ETA: 1226044.5s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.291s, learning 0.174s)
               Value function loss: 8.2437
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 332.60
               Mean episode length: 241.17
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 11.47s
                        Total time: 5813.40s
                               ETA: 1225852.3s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.164s)
               Value function loss: 7.0464
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 324.32
               Mean episode length: 240.26
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 11.37s
                        Total time: 5824.78s
                               ETA: 1225641.4s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.166s)
               Value function loss: 6.6561
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 319.96
               Mean episode length: 239.64
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 11.31s
                        Total time: 5836.08s
                               ETA: 1225417.5s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.994s, learning 0.170s)
               Value function loss: 5.5543
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 316.53
               Mean episode length: 240.52
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 11.16s
                        Total time: 5847.25s
                               ETA: 1225164.5s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.172s)
               Value function loss: 6.8211
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 317.20
               Mean episode length: 238.30
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 11.51s
                        Total time: 5858.76s
                               ETA: 1224985.6s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.491s, learning 0.182s)
               Value function loss: 5.3395
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 312.86
               Mean episode length: 236.93
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 11.67s
                        Total time: 5870.43s
                               ETA: 1224840.5s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.396s, learning 0.172s)
               Value function loss: 7.2599
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 306.08
               Mean episode length: 238.75
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 11.57s
                        Total time: 5882.00s
                               ETA: 1224674.4s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.159s)
               Value function loss: 5.4761
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 304.09
               Mean episode length: 239.36
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 11.55s
                        Total time: 5893.55s
                               ETA: 1224504.6s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.040s, learning 0.176s)
               Value function loss: 7.1609
                    Surrogate loss: 0.0120
             Mean action noise std: 0.78
                       Mean reward: 305.76
               Mean episode length: 240.41
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 11.22s
                        Total time: 5904.76s
                               ETA: 1224266.8s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.668s, learning 0.165s)
               Value function loss: 7.1974
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 311.26
               Mean episode length: 241.43
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 11.83s
                        Total time: 5916.60s
                               ETA: 1224157.5s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.159s)
               Value function loss: 7.4733
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 308.46
               Mean episode length: 244.40
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 11.30s
                        Total time: 5927.90s
                               ETA: 1223938.8s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.149s, learning 0.192s)
               Value function loss: 6.1589
                    Surrogate loss: 0.0063
             Mean action noise std: 0.78
                       Mean reward: 303.01
               Mean episode length: 245.98
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 11.34s
                        Total time: 5939.24s
                               ETA: 1223729.3s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.267s, learning 0.166s)
               Value function loss: 6.0517
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 286.29
               Mean episode length: 244.04
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 11.43s
                        Total time: 5950.67s
                               ETA: 1223539.5s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.359s, learning 0.161s)
               Value function loss: 6.7818
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 283.54
               Mean episode length: 243.58
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 11.52s
                        Total time: 5962.19s
                               ETA: 1223368.1s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.096s, learning 0.161s)
               Value function loss: 6.1536
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 292.83
               Mean episode length: 244.25
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 11.26s
                        Total time: 5973.45s
                               ETA: 1223143.7s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.892s, learning 0.180s)
               Value function loss: 6.1468
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: 295.17
               Mean episode length: 247.19
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 11.07s
                        Total time: 5984.52s
                               ETA: 1222882.3s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.081s, learning 0.162s)
               Value function loss: 6.5849
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 298.29
               Mean episode length: 245.78
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 11.24s
                        Total time: 5995.76s
                               ETA: 1222656.6s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1467 steps/s (collection: 11.002s, learning 0.161s)
               Value function loss: 5.2418
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 295.42
               Mean episode length: 246.72
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 11.16s
                        Total time: 6006.93s
                               ETA: 1222415.7s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.210s, learning 0.174s)
               Value function loss: 5.1297
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 287.56
               Mean episode length: 245.09
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 11.38s
                        Total time: 6018.31s
                               ETA: 1222220.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.362s, learning 0.178s)
               Value function loss: 6.5049
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 284.24
               Mean episode length: 243.72
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 11.54s
                        Total time: 6029.85s
                               ETA: 1222057.7s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.232s, learning 0.164s)
               Value function loss: 7.0562
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 296.37
               Mean episode length: 247.66
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 11.40s
                        Total time: 6041.25s
                               ETA: 1221866.5s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.162s)
               Value function loss: 5.8260
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 284.92
               Mean episode length: 248.91
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 11.46s
                        Total time: 6052.71s
                               ETA: 1221688.9s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.188s, learning 0.162s)
               Value function loss: 7.2568
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 285.56
               Mean episode length: 248.73
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 11.35s
                        Total time: 6064.06s
                               ETA: 1221489.9s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.178s, learning 0.159s)
               Value function loss: 7.0752
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 288.34
               Mean episode length: 249.66
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 11.34s
                        Total time: 6075.39s
                               ETA: 1221288.9s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.455s, learning 0.197s)
               Value function loss: 7.9635
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 294.61
               Mean episode length: 249.19
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 11.65s
                        Total time: 6087.04s
                               ETA: 1221152.0s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.352s, learning 0.191s)
               Value function loss: 7.5038
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 298.14
               Mean episode length: 249.79
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 11.54s
                        Total time: 6098.59s
                               ETA: 1220993.7s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.147s, learning 0.185s)
               Value function loss: 7.1150
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 299.34
               Mean episode length: 249.55
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 11.33s
                        Total time: 6109.92s
                               ETA: 1220793.8s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.164s)
               Value function loss: 7.0418
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 297.51
               Mean episode length: 249.66
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 11.37s
                        Total time: 6121.29s
                               ETA: 1220602.8s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.911s, learning 0.164s)
               Value function loss: 6.2008
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 292.81
               Mean episode length: 247.86
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 11.07s
                        Total time: 6132.37s
                               ETA: 1220353.3s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.157s, learning 0.189s)
               Value function loss: 8.4151
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 305.73
               Mean episode length: 248.98
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 11.35s
                        Total time: 6143.71s
                               ETA: 1220158.6s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.385s, learning 0.192s)
               Value function loss: 7.0431
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 309.90
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 11.58s
                        Total time: 6155.29s
                               ETA: 1220010.4s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.313s, learning 0.159s)
               Value function loss: 7.3413
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 302.53
               Mean episode length: 249.96
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 11.47s
                        Total time: 6166.76s
                               ETA: 1219842.0s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.480s, learning 0.177s)
               Value function loss: 6.4781
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 312.37
               Mean episode length: 249.89
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 11.66s
                        Total time: 6178.42s
                               ETA: 1219710.6s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.164s)
               Value function loss: 6.0145
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 318.61
               Mean episode length: 249.93
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 11.03s
                        Total time: 6189.45s
                               ETA: 1219455.9s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.945s, learning 0.161s)
               Value function loss: 5.7795
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 326.17
               Mean episode length: 249.96
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 11.11s
                        Total time: 6200.55s
                               ETA: 1219217.4s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.167s)
               Value function loss: 6.5769
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 331.58
               Mean episode length: 249.92
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 11.43s
                        Total time: 6211.98s
                               ETA: 1219043.8s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.940s, learning 0.167s)
               Value function loss: 4.4509
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 333.68
               Mean episode length: 249.96
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 11.11s
                        Total time: 6223.09s
                               ETA: 1218807.2s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.273s, learning 0.162s)
               Value function loss: 6.9219
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 342.97
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 11.43s
                        Total time: 6234.53s
                               ETA: 1218635.5s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.329s, learning 0.172s)
               Value function loss: 5.8821
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 344.74
               Mean episode length: 249.86
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 11.50s
                        Total time: 6246.03s
                               ETA: 1218477.4s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.273s, learning 0.165s)
               Value function loss: 8.8636
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 343.59
               Mean episode length: 249.86
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 11.44s
                        Total time: 6257.46s
                               ETA: 1218307.5s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.994s, learning 0.184s)
               Value function loss: 8.4573
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 349.46
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 11.18s
                        Total time: 6268.64s
                               ETA: 1218087.9s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.165s)
               Value function loss: 8.6581
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 345.86
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 11.55s
                        Total time: 6280.19s
                               ETA: 1217941.2s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.436s, learning 0.185s)
               Value function loss: 9.5498
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 340.17
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 11.62s
                        Total time: 6291.81s
                               ETA: 1217808.9s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.424s, learning 0.166s)
               Value function loss: 7.2845
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 342.36
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 11.59s
                        Total time: 6303.40s
                               ETA: 1217670.9s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.178s, learning 0.258s)
               Value function loss: 7.5786
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 341.74
               Mean episode length: 249.54
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 11.44s
                        Total time: 6314.84s
                               ETA: 1217503.8s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.425s, learning 0.161s)
               Value function loss: 9.4775
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 340.75
               Mean episode length: 248.89
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 11.59s
                        Total time: 6326.43s
                               ETA: 1217366.2s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.798s, learning 0.163s)
               Value function loss: 8.8292
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 338.73
               Mean episode length: 249.53
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 11.96s
                        Total time: 6338.39s
                               ETA: 1217300.9s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.566s, learning 0.166s)
               Value function loss: 9.9545
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 337.16
               Mean episode length: 249.23
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 11.73s
                        Total time: 6350.12s
                               ETA: 1217192.0s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.448s, learning 0.185s)
               Value function loss: 9.5102
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 340.43
               Mean episode length: 249.77
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 11.63s
                        Total time: 6361.75s
                               ETA: 1217064.5s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.696s, learning 0.252s)
               Value function loss: 7.5051
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 343.61
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 11.95s
                        Total time: 6373.70s
                               ETA: 1216997.6s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.160s, learning 0.179s)
               Value function loss: 10.6986
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: 340.72
               Mean episode length: 249.31
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 11.34s
                        Total time: 6385.04s
                               ETA: 1216814.8s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.169s)
               Value function loss: 10.6847
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 341.67
               Mean episode length: 249.89
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 11.42s
                        Total time: 6396.46s
                               ETA: 1216649.1s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.164s)
               Value function loss: 9.2419
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 344.84
               Mean episode length: 249.90
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 11.31s
                        Total time: 6407.77s
                               ETA: 1216461.2s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.255s, learning 0.170s)
               Value function loss: 10.5416
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 340.16
               Mean episode length: 249.06
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 11.43s
                        Total time: 6419.20s
                               ETA: 1216296.9s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.165s)
               Value function loss: 13.8233
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 344.69
               Mean episode length: 249.17
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 11.43s
                        Total time: 6430.63s
                               ETA: 1216134.6s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.524s, learning 0.159s)
               Value function loss: 13.6053
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 346.63
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 11.68s
                        Total time: 6442.31s
                               ETA: 1216019.9s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.304s, learning 0.173s)
               Value function loss: 11.2849
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 346.47
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 11.48s
                        Total time: 6453.79s
                               ETA: 1215866.7s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.177s, learning 0.160s)
               Value function loss: 11.2722
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 342.09
               Mean episode length: 249.92
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 11.34s
                        Total time: 6465.12s
                               ETA: 1215687.9s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.506s, learning 0.172s)
               Value function loss: 10.7873
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 343.41
               Mean episode length: 249.87
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 11.68s
                        Total time: 6476.80s
                               ETA: 1215573.8s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.213s, learning 0.161s)
               Value function loss: 12.4218
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 349.82
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 11.37s
                        Total time: 6488.18s
                               ETA: 1215403.1s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.297s, learning 0.184s)
               Value function loss: 13.8100
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 350.17
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 11.48s
                        Total time: 6499.66s
                               ETA: 1215252.9s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.110s, learning 0.179s)
               Value function loss: 11.3479
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 347.02
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 11.29s
                        Total time: 6510.95s
                               ETA: 1215067.3s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.285s, learning 0.158s)
               Value function loss: 12.9797
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 350.24
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 11.44s
                        Total time: 6522.39s
                               ETA: 1214911.2s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.388s, learning 0.174s)
               Value function loss: 9.9366
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 351.74
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 11.56s
                        Total time: 6533.95s
                               ETA: 1214777.7s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.162s)
               Value function loss: 9.1345
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: 347.46
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 11.22s
                        Total time: 6545.17s
                               ETA: 1214581.7s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.552s, learning 0.162s)
               Value function loss: 9.2300
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 343.10
               Mean episode length: 249.93
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 11.71s
                        Total time: 6556.89s
                               ETA: 1214477.3s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.170s)
               Value function loss: 8.1478
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 339.83
               Mean episode length: 249.82
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 11.74s
                        Total time: 6568.62s
                               ETA: 1214377.3s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.806s, learning 0.237s)
               Value function loss: 7.4394
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 343.47
               Mean episode length: 249.63
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 12.04s
                        Total time: 6580.67s
                               ETA: 1214334.5s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.328s, learning 0.183s)
               Value function loss: 10.2883
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 344.17
               Mean episode length: 249.74
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 11.51s
                        Total time: 6592.18s
                               ETA: 1214193.7s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.169s, learning 0.163s)
               Value function loss: 10.0819
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 339.80
               Mean episode length: 249.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 11.33s
                        Total time: 6603.51s
                               ETA: 1214020.5s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.353s, learning 0.179s)
               Value function loss: 8.4906
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 341.19
               Mean episode length: 249.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 11.53s
                        Total time: 6615.04s
                               ETA: 1213884.6s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.681s, learning 0.170s)
               Value function loss: 10.3651
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 328.76
               Mean episode length: 249.70
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 11.85s
                        Total time: 6626.89s
                               ETA: 1213807.6s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.268s, learning 0.162s)
               Value function loss: 12.5266
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 334.20
               Mean episode length: 249.49
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 11.43s
                        Total time: 6638.32s
                               ETA: 1213653.9s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.452s, learning 0.160s)
               Value function loss: 13.4918
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 333.23
               Mean episode length: 249.08
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 11.61s
                        Total time: 6649.94s
                               ETA: 1213534.0s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.018s, learning 0.179s)
               Value function loss: 9.7805
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 324.19
               Mean episode length: 247.25
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 11.20s
                        Total time: 6661.13s
                               ETA: 1213338.9s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.545s, learning 0.174s)
               Value function loss: 10.2560
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 325.74
               Mean episode length: 249.51
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 11.72s
                        Total time: 6672.85s
                               ETA: 1213239.4s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.472s, learning 0.166s)
               Value function loss: 10.5148
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 326.02
               Mean episode length: 249.93
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 11.64s
                        Total time: 6684.49s
                               ETA: 1213125.4s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.022s, learning 0.162s)
               Value function loss: 9.8110
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 326.36
               Mean episode length: 249.99
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 11.18s
                        Total time: 6695.68s
                               ETA: 1212929.5s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.566s, learning 0.176s)
               Value function loss: 10.0853
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 327.55
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 11.74s
                        Total time: 6707.42s
                               ETA: 1212835.2s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.741s, learning 0.176s)
               Value function loss: 11.3349
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 322.19
               Mean episode length: 249.81
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 11.92s
                        Total time: 6719.33s
                               ETA: 1212772.8s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.848s, learning 0.170s)
               Value function loss: 8.8311
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 325.48
               Mean episode length: 249.81
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 12.02s
                        Total time: 6731.35s
                               ETA: 1212728.9s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.554s, learning 0.169s)
               Value function loss: 9.7480
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 333.81
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 11.72s
                        Total time: 6743.08s
                               ETA: 1212631.9s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.060s, learning 0.179s)
               Value function loss: 11.4528
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 329.25
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 11.24s
                        Total time: 6754.32s
                               ETA: 1212448.3s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.161s)
               Value function loss: 9.2952
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 318.50
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 11.51s
                        Total time: 6765.82s
                               ETA: 1212313.2s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.172s, learning 0.168s)
               Value function loss: 11.6647
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 314.69
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 11.34s
                        Total time: 6777.16s
                               ETA: 1212148.8s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.184s, learning 0.160s)
               Value function loss: 11.8308
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 316.01
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 11.34s
                        Total time: 6788.51s
                               ETA: 1211985.9s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.568s, learning 0.162s)
               Value function loss: 12.1895
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 309.09
               Mean episode length: 249.65
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 11.73s
                        Total time: 6800.24s
                               ETA: 1211892.2s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.245s, learning 0.176s)
               Value function loss: 10.6009
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 309.82
               Mean episode length: 249.82
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 11.42s
                        Total time: 6811.66s
                               ETA: 1211743.7s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.503s, learning 0.172s)
               Value function loss: 8.9139
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 316.51
               Mean episode length: 249.82
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 11.67s
                        Total time: 6823.33s
                               ETA: 1211640.8s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.591s, learning 0.160s)
               Value function loss: 7.2292
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 314.13
               Mean episode length: 249.75
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 11.75s
                        Total time: 6835.08s
                               ETA: 1211551.9s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.243s, learning 0.161s)
               Value function loss: 7.5617
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 320.20
               Mean episode length: 249.81
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 11.40s
                        Total time: 6846.49s
                               ETA: 1211401.7s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.153s, learning 0.168s)
               Value function loss: 8.1342
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 312.41
               Mean episode length: 249.81
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 11.32s
                        Total time: 6857.81s
                               ETA: 1211237.3s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.277s, learning 0.173s)
               Value function loss: 6.9414
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 316.79
               Mean episode length: 249.43
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 11.45s
                        Total time: 6869.26s
                               ETA: 1211096.3s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.561s, learning 0.160s)
               Value function loss: 5.9879
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 304.41
               Mean episode length: 248.41
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 11.72s
                        Total time: 6880.98s
                               ETA: 1211003.5s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.210s, learning 0.161s)
               Value function loss: 6.1202
                    Surrogate loss: -0.0227
             Mean action noise std: 0.77
                       Mean reward: 306.91
               Mean episode length: 248.98
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 11.37s
                        Total time: 6892.35s
                               ETA: 1210849.3s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.618s, learning 0.185s)
               Value function loss: 6.0239
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 320.46
               Mean episode length: 249.54
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 11.80s
                        Total time: 6904.15s
                               ETA: 1210771.3s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.574s, learning 0.163s)
               Value function loss: 5.5006
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 329.83
               Mean episode length: 248.87
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 11.74s
                        Total time: 6915.89s
                               ETA: 1210682.3s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.930s, learning 0.165s)
               Value function loss: 5.6747
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 332.91
               Mean episode length: 249.33
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 11.09s
                        Total time: 6926.98s
                               ETA: 1210481.1s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.177s, learning 0.161s)
               Value function loss: 6.0209
                    Surrogate loss: -0.0200
             Mean action noise std: 0.77
                       Mean reward: 334.27
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 11.34s
                        Total time: 6938.32s
                               ETA: 1210323.0s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.466s, learning 0.162s)
               Value function loss: 6.6707
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 328.94
               Mean episode length: 249.77
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 11.63s
                        Total time: 6949.95s
                               ETA: 1210216.0s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.164s, learning 0.162s)
               Value function loss: 7.5125
                    Surrogate loss: -0.0224
             Mean action noise std: 0.77
                       Mean reward: 332.05
               Mean episode length: 249.53
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 11.33s
                        Total time: 6961.27s
                               ETA: 1210056.9s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.385s, learning 0.182s)
               Value function loss: 6.4444
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 344.62
               Mean episode length: 249.48
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 11.57s
                        Total time: 6972.84s
                               ETA: 1209940.1s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.187s, learning 0.161s)
               Value function loss: 7.8566
                    Surrogate loss: -0.0200
             Mean action noise std: 0.77
                       Mean reward: 354.43
               Mean episode length: 249.72
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 11.35s
                        Total time: 6984.19s
                               ETA: 1209785.7s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.045s, learning 0.165s)
               Value function loss: 8.8679
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 371.75
               Mean episode length: 249.38
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 11.21s
                        Total time: 6995.40s
                               ETA: 1209608.0s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.357s, learning 0.159s)
               Value function loss: 8.8762
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 377.95
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 11.52s
                        Total time: 7006.92s
                               ETA: 1209483.6s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.150s, learning 0.173s)
               Value function loss: 8.1699
                    Surrogate loss: -0.0210
             Mean action noise std: 0.77
                       Mean reward: 388.67
               Mean episode length: 249.42
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 11.32s
                        Total time: 7018.24s
                               ETA: 1209326.4s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.611s, learning 0.162s)
               Value function loss: 9.1152
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 402.09
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 11.77s
                        Total time: 7030.01s
                               ETA: 1209247.1s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.996s, learning 0.181s)
               Value function loss: 10.3532
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 384.00
               Mean episode length: 248.11
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 11.18s
                        Total time: 7041.19s
                               ETA: 1209065.6s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.518s, learning 0.185s)
               Value function loss: 9.8096
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 396.33
               Mean episode length: 249.95
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 11.70s
                        Total time: 7052.89s
                               ETA: 1208975.0s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.270s, learning 0.164s)
               Value function loss: 9.3108
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 400.00
               Mean episode length: 249.95
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 11.43s
                        Total time: 7064.33s
                               ETA: 1208838.6s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.553s, learning 0.169s)
               Value function loss: 10.9920
                    Surrogate loss: -0.0264
             Mean action noise std: 0.77
                       Mean reward: 410.71
               Mean episode length: 249.77
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 11.72s
                        Total time: 7076.05s
                               ETA: 1208751.7s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.162s)
               Value function loss: 8.0474
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 410.47
               Mean episode length: 249.84
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 11.19s
                        Total time: 7087.23s
                               ETA: 1208573.8s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.600s, learning 0.168s)
               Value function loss: 8.9788
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 409.19
               Mean episode length: 249.84
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 11.77s
                        Total time: 7099.00s
                               ETA: 1208495.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.283s, learning 0.162s)
               Value function loss: 10.3499
                    Surrogate loss: -0.0244
             Mean action noise std: 0.77
                       Mean reward: 406.91
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 11.44s
                        Total time: 7110.45s
                               ETA: 1208362.4s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.421s, learning 0.201s)
               Value function loss: 10.1908
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 395.17
               Mean episode length: 249.83
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 11.62s
                        Total time: 7122.07s
                               ETA: 1208260.0s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.094s, learning 0.167s)
               Value function loss: 9.9321
                    Surrogate loss: -0.0215
             Mean action noise std: 0.77
                       Mean reward: 409.67
               Mean episode length: 249.78
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 11.26s
                        Total time: 7133.33s
                               ETA: 1208096.7s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.619s, learning 0.170s)
               Value function loss: 11.4764
                    Surrogate loss: -0.0256
             Mean action noise std: 0.77
                       Mean reward: 413.25
               Mean episode length: 249.95
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 11.79s
                        Total time: 7145.12s
                               ETA: 1208023.1s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1461 steps/s (collection: 10.965s, learning 0.247s)
               Value function loss: 10.2038
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 410.53
               Mean episode length: 249.43
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 11.21s
                        Total time: 7156.33s
                               ETA: 1207852.3s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.406s, learning 0.185s)
               Value function loss: 9.7186
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 393.09
               Mean episode length: 249.35
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 11.59s
                        Total time: 7167.92s
                               ETA: 1207745.9s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.309s, learning 0.173s)
               Value function loss: 9.6265
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 407.87
               Mean episode length: 249.03
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 11.48s
                        Total time: 7179.40s
                               ETA: 1207621.5s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.205s, learning 0.162s)
               Value function loss: 8.5854
                    Surrogate loss: -0.0210
             Mean action noise std: 0.77
                       Mean reward: 410.78
               Mean episode length: 249.34
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 11.37s
                        Total time: 7190.77s
                               ETA: 1207478.2s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.967s, learning 0.172s)
               Value function loss: 12.1052
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 391.31
               Mean episode length: 248.49
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 11.14s
                        Total time: 7201.91s
                               ETA: 1207297.1s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.051s, learning 0.163s)
               Value function loss: 11.2558
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 400.96
               Mean episode length: 248.92
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 11.21s
                        Total time: 7213.12s
                               ETA: 1207129.2s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.886s, learning 0.166s)
               Value function loss: 10.3614
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 420.41
               Mean episode length: 249.91
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 11.05s
                        Total time: 7224.17s
                               ETA: 1206934.7s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.551s, learning 0.165s)
               Value function loss: 9.4641
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 411.75
               Mean episode length: 249.64
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 11.72s
                        Total time: 7235.89s
                               ETA: 1206851.6s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.735s, learning 0.193s)
               Value function loss: 9.0161
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 417.05
               Mean episode length: 249.68
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 10.93s
                        Total time: 7246.82s
                               ETA: 1206637.5s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.789s, learning 0.162s)
               Value function loss: 10.7019
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 405.91
               Mean episode length: 247.76
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 11.95s
                        Total time: 7258.77s
                               ETA: 1206594.2s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.048s, learning 0.159s)
               Value function loss: 6.8406
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 396.07
               Mean episode length: 247.89
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 11.21s
                        Total time: 7269.98s
                               ETA: 1206427.5s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.151s, learning 0.167s)
               Value function loss: 7.5502
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 398.07
               Mean episode length: 248.83
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 11.32s
                        Total time: 7281.29s
                               ETA: 1206279.8s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.310s, learning 0.187s)
               Value function loss: 8.3985
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 385.83
               Mean episode length: 248.47
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 11.50s
                        Total time: 7292.79s
                               ETA: 1206162.1s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.641s, learning 0.168s)
               Value function loss: 7.1886
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 375.97
               Mean episode length: 247.11
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 11.81s
                        Total time: 7304.60s
                               ETA: 1206096.2s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.277s, learning 0.187s)
               Value function loss: 9.4357
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 368.61
               Mean episode length: 245.89
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.46s
                        Total time: 7316.06s
                               ETA: 1205973.6s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.267s, learning 0.170s)
               Value function loss: 7.4033
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 360.92
               Mean episode length: 245.73
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 11.44s
                        Total time: 7327.50s
                               ETA: 1205847.0s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.703s, learning 0.166s)
               Value function loss: 9.1607
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 357.69
               Mean episode length: 246.55
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 10.87s
                        Total time: 7338.37s
                               ETA: 1205627.5s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.874s, learning 0.162s)
               Value function loss: 9.5759
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 357.60
               Mean episode length: 247.61
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 11.04s
                        Total time: 7349.41s
                               ETA: 1205436.0s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.461s, learning 0.162s)
               Value function loss: 8.3590
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 357.94
               Mean episode length: 247.64
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 11.62s
                        Total time: 7361.03s
                               ETA: 1205341.1s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.328s, learning 0.160s)
               Value function loss: 8.1684
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 355.84
               Mean episode length: 247.84
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 11.49s
                        Total time: 7372.52s
                               ETA: 1205224.6s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.314s, learning 0.161s)
               Value function loss: 10.0723
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 350.73
               Mean episode length: 247.66
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 11.48s
                        Total time: 7383.99s
                               ETA: 1205106.3s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.122s, learning 0.171s)
               Value function loss: 9.4718
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 345.58
               Mean episode length: 244.85
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 11.29s
                        Total time: 7395.29s
                               ETA: 1204958.7s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.291s, learning 0.161s)
               Value function loss: 9.3488
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 341.53
               Mean episode length: 244.76
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 11.45s
                        Total time: 7406.74s
                               ETA: 1204837.5s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.252s, learning 0.166s)
               Value function loss: 8.6767
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 342.15
               Mean episode length: 246.45
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 11.42s
                        Total time: 7418.16s
                               ETA: 1204710.9s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.807s, learning 0.158s)
               Value function loss: 10.3178
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 344.10
               Mean episode length: 246.68
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 10.96s
                        Total time: 7429.12s
                               ETA: 1204511.3s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.164s)
               Value function loss: 6.9729
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 338.85
               Mean episode length: 246.21
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 11.20s
                        Total time: 7440.32s
                               ETA: 1204349.9s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.187s, learning 0.183s)
               Value function loss: 7.0142
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 343.63
               Mean episode length: 247.89
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 11.37s
                        Total time: 7451.69s
                               ETA: 1204217.0s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.455s, learning 0.171s)
               Value function loss: 10.6449
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 337.20
               Mean episode length: 249.03
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 11.63s
                        Total time: 7463.31s
                               ETA: 1204125.7s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.350s, learning 0.159s)
               Value function loss: 8.9511
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 336.36
               Mean episode length: 247.64
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 11.51s
                        Total time: 7474.82s
                               ETA: 1204015.9s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.247s, learning 0.187s)
               Value function loss: 8.1921
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 337.25
               Mean episode length: 248.60
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 11.43s
                        Total time: 7486.26s
                               ETA: 1203894.3s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.160s)
               Value function loss: 12.9863
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 341.87
               Mean episode length: 248.91
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 11.16s
                        Total time: 7497.42s
                               ETA: 1203729.7s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.336s, learning 0.160s)
               Value function loss: 8.6689
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 326.16
               Mean episode length: 249.13
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 11.50s
                        Total time: 7508.92s
                               ETA: 1203618.7s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.552s, learning 0.165s)
               Value function loss: 10.7029
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 330.51
               Mean episode length: 249.70
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 11.72s
                        Total time: 7520.63s
                               ETA: 1203543.6s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.164s)
               Value function loss: 10.0780
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 332.14
               Mean episode length: 248.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 11.12s
                        Total time: 7531.75s
                               ETA: 1203372.6s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.152s, learning 0.267s)
               Value function loss: 10.5719
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 347.34
               Mean episode length: 249.90
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 11.42s
                        Total time: 7543.17s
                               ETA: 1203250.5s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.400s, learning 0.160s)
               Value function loss: 11.0317
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 335.23
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 11.56s
                        Total time: 7554.73s
                               ETA: 1203151.1s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.159s)
               Value function loss: 9.6798
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 339.29
               Mean episode length: 249.90
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 11.33s
                        Total time: 7566.06s
                               ETA: 1203016.1s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.121s, learning 0.285s)
               Value function loss: 11.4042
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 350.36
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 11.41s
                        Total time: 7577.47s
                               ETA: 1202893.0s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.329s, learning 0.185s)
               Value function loss: 10.6733
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 348.45
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 11.51s
                        Total time: 7588.98s
                               ETA: 1202787.3s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.566s, learning 0.161s)
               Value function loss: 9.9261
                    Surrogate loss: 0.0023
             Mean action noise std: 0.77
                       Mean reward: 349.11
               Mean episode length: 249.90
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 11.73s
                        Total time: 7600.71s
                               ETA: 1202715.6s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.194s, learning 0.173s)
               Value function loss: 9.1096
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 346.36
               Mean episode length: 249.96
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 11.37s
                        Total time: 7612.08s
                               ETA: 1202587.0s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.222s, learning 0.161s)
               Value function loss: 8.2734
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 357.22
               Mean episode length: 249.96
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 11.38s
                        Total time: 7623.46s
                               ETA: 1202461.6s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.340s, learning 0.165s)
               Value function loss: 7.8691
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 356.10
               Mean episode length: 249.96
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 11.50s
                        Total time: 7634.96s
                               ETA: 1202355.6s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.870s, learning 0.164s)
               Value function loss: 8.8410
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 367.98
               Mean episode length: 249.96
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 11.03s
                        Total time: 7646.00s
                               ETA: 1202175.9s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.135s, learning 0.164s)
               Value function loss: 7.1076
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 382.50
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 11.30s
                        Total time: 7657.30s
                               ETA: 1202038.4s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.750s, learning 0.165s)
               Value function loss: 10.1880
                    Surrogate loss: -0.0008
             Mean action noise std: 0.77
                       Mean reward: 399.36
               Mean episode length: 249.96
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 11.91s
                        Total time: 7669.21s
                               ETA: 1201997.7s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.163s)
               Value function loss: 8.0669
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 397.42
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 11.15s
                        Total time: 7680.36s
                               ETA: 1201837.4s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.406s, learning 0.166s)
               Value function loss: 10.9412
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 398.74
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 11.57s
                        Total time: 7691.93s
                               ETA: 1201743.6s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.492s, learning 0.160s)
               Value function loss: 10.5953
                    Surrogate loss: -0.0208
             Mean action noise std: 0.77
                       Mean reward: 388.02
               Mean episode length: 249.59
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 11.65s
                        Total time: 7703.59s
                               ETA: 1201662.6s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.374s, learning 0.172s)
               Value function loss: 9.5519
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 400.72
               Mean episode length: 249.30
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 11.55s
                        Total time: 7715.13s
                               ETA: 1201565.3s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.160s)
               Value function loss: 9.3527
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 415.06
               Mean episode length: 249.71
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 11.41s
                        Total time: 7726.54s
                               ETA: 1201447.1s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.582s, learning 0.166s)
               Value function loss: 10.9967
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 415.06
               Mean episode length: 249.77
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 11.75s
                        Total time: 7738.29s
                               ETA: 1201381.7s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.067s, learning 0.166s)
               Value function loss: 10.6992
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 406.21
               Mean episode length: 249.88
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 11.23s
                        Total time: 7749.52s
                               ETA: 1201236.6s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.508s, learning 0.158s)
               Value function loss: 10.0644
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 404.72
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 11.67s
                        Total time: 7761.19s
                               ETA: 1201159.0s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1466 steps/s (collection: 10.976s, learning 0.194s)
               Value function loss: 8.6333
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 412.03
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 11.17s
                        Total time: 7772.36s
                               ETA: 1201004.9s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.492s, learning 0.170s)
               Value function loss: 10.2513
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 425.80
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 11.66s
                        Total time: 7784.02s
                               ETA: 1200927.1s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.598s, learning 0.168s)
               Value function loss: 10.0557
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 428.13
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 11.77s
                        Total time: 7795.79s
                               ETA: 1200865.6s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.864s, learning 0.188s)
               Value function loss: 8.3986
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 423.15
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 11.05s
                        Total time: 7806.84s
                               ETA: 1200694.3s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.167s, learning 0.165s)
               Value function loss: 11.2965
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 431.56
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 11.33s
                        Total time: 7818.17s
                               ETA: 1200566.6s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.123s, learning 0.171s)
               Value function loss: 12.1049
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 451.07
               Mean episode length: 249.46
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 11.29s
                        Total time: 7829.47s
                               ETA: 1200433.5s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.328s, learning 0.168s)
               Value function loss: 10.5159
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 446.45
               Mean episode length: 249.29
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 11.50s
                        Total time: 7840.96s
                               ETA: 1200331.7s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.444s, learning 0.173s)
               Value function loss: 10.5932
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 454.88
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 11.62s
                        Total time: 7852.58s
                               ETA: 1200248.6s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.324s, learning 0.192s)
               Value function loss: 12.2820
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 441.20
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 11.52s
                        Total time: 7864.10s
                               ETA: 1200150.3s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.035s, learning 0.163s)
               Value function loss: 12.4816
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 432.19
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 11.20s
                        Total time: 7875.29s
                               ETA: 1200003.9s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.043s, learning 0.162s)
               Value function loss: 11.7743
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 440.65
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 11.20s
                        Total time: 7886.50s
                               ETA: 1199858.8s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.627s, learning 0.177s)
               Value function loss: 10.4799
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 442.65
               Mean episode length: 249.60
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 11.80s
                        Total time: 7898.30s
                               ETA: 1199805.3s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.308s, learning 0.170s)
               Value function loss: 10.3079
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 464.12
               Mean episode length: 250.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 11.48s
                        Total time: 7909.78s
                               ETA: 1199702.3s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.285s, learning 0.175s)
               Value function loss: 13.0053
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 467.41
               Mean episode length: 249.64
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 11.46s
                        Total time: 7921.24s
                               ETA: 1199596.9s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.504s, learning 0.185s)
               Value function loss: 11.9495
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 461.30
               Mean episode length: 249.70
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 11.69s
                        Total time: 7932.93s
                               ETA: 1199526.5s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.111s, learning 0.183s)
               Value function loss: 9.3057
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 458.70
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 11.29s
                        Total time: 7944.22s
                               ETA: 1199396.6s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.373s, learning 0.169s)
               Value function loss: 10.5937
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 455.50
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 11.54s
                        Total time: 7955.77s
                               ETA: 1199304.5s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.488s, learning 0.183s)
               Value function loss: 8.6782
                    Surrogate loss: -0.0221
             Mean action noise std: 0.77
                       Mean reward: 443.58
               Mean episode length: 249.66
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 11.67s
                        Total time: 7967.44s
                               ETA: 1199231.9s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.423s, learning 0.187s)
               Value function loss: 7.7013
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 444.17
               Mean episode length: 249.66
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 11.61s
                        Total time: 7979.05s
                               ETA: 1199150.3s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.221s, learning 0.185s)
               Value function loss: 7.0177
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 444.25
               Mean episode length: 250.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 11.41s
                        Total time: 7990.45s
                               ETA: 1199038.4s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.406s, learning 0.161s)
               Value function loss: 7.6714
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 457.04
               Mean episode length: 249.51
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 11.57s
                        Total time: 8002.02s
                               ETA: 1198950.9s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.165s)
               Value function loss: 7.7423
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 466.05
               Mean episode length: 249.51
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 11.08s
                        Total time: 8013.09s
                               ETA: 1198790.3s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.624s, learning 0.164s)
               Value function loss: 7.8331
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 475.69
               Mean episode length: 250.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 11.79s
                        Total time: 8024.88s
                               ETA: 1198736.5s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.257s, learning 0.167s)
               Value function loss: 8.2021
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 481.39
               Mean episode length: 250.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 11.42s
                        Total time: 8036.31s
                               ETA: 1198628.5s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.104s, learning 0.178s)
               Value function loss: 6.9263
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 470.76
               Mean episode length: 250.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 11.28s
                        Total time: 8047.59s
                               ETA: 1198499.6s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.393s, learning 0.167s)
               Value function loss: 10.7092
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 485.00
               Mean episode length: 250.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 11.56s
                        Total time: 8059.15s
                               ETA: 1198412.4s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.448s, learning 0.163s)
               Value function loss: 8.4688
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 496.62
               Mean episode length: 250.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 11.61s
                        Total time: 8070.76s
                               ETA: 1198333.0s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.248s, learning 0.265s)
               Value function loss: 10.2216
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 496.64
               Mean episode length: 250.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 11.51s
                        Total time: 8082.27s
                               ETA: 1198239.2s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.153s, learning 0.173s)
               Value function loss: 8.7557
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 490.37
               Mean episode length: 250.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 11.33s
                        Total time: 8093.60s
                               ETA: 1198118.1s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.162s, learning 0.160s)
               Value function loss: 8.9366
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 497.39
               Mean episode length: 250.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 11.32s
                        Total time: 8104.92s
                               ETA: 1197996.6s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.775s, learning 0.156s)
               Value function loss: 12.1466
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 496.63
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 11.93s
                        Total time: 8116.85s
                               ETA: 1197965.4s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.163s)
               Value function loss: 10.0891
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 490.07
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 11.40s
                        Total time: 8128.26s
                               ETA: 1197856.6s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.259s, learning 0.171s)
               Value function loss: 8.4293
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 485.96
               Mean episode length: 249.91
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 11.43s
                        Total time: 8139.69s
                               ETA: 1197751.8s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.067s, learning 0.167s)
               Value function loss: 10.0084
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 488.70
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 11.23s
                        Total time: 8150.92s
                               ETA: 1197618.5s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.160s)
               Value function loss: 8.3307
                    Surrogate loss: -0.0221
             Mean action noise std: 0.77
                       Mean reward: 502.82
               Mean episode length: 250.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 11.54s
                        Total time: 8162.46s
                               ETA: 1197530.3s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.161s)
               Value function loss: 8.7318
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 503.88
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 11.50s
                        Total time: 8173.96s
                               ETA: 1197436.4s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.108s, learning 0.166s)
               Value function loss: 9.7132
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 504.53
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 11.27s
                        Total time: 8185.23s
                               ETA: 1197309.8s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.168s)
               Value function loss: 8.2497
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 502.77
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 11.42s
                        Total time: 8196.65s
                               ETA: 1197204.6s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.588s, learning 0.168s)
               Value function loss: 11.4846
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 508.58
               Mean episode length: 248.66
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 11.76s
                        Total time: 8208.40s
                               ETA: 1197149.0s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.777s, learning 0.164s)
               Value function loss: 10.1134
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 504.53
               Mean episode length: 248.66
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 10.94s
                        Total time: 8219.34s
                               ETA: 1196974.9s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.568s, learning 0.175s)
               Value function loss: 10.4834
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 503.81
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 11.74s
                        Total time: 8231.09s
                               ETA: 1196918.0s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.443s, learning 0.166s)
               Value function loss: 9.8170
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 500.59
               Mean episode length: 249.63
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 11.61s
                        Total time: 8242.69s
                               ETA: 1196841.7s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.161s)
               Value function loss: 9.8517
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 508.79
               Mean episode length: 249.63
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 11.43s
                        Total time: 8254.12s
                               ETA: 1196739.1s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.683s, learning 0.181s)
               Value function loss: 10.1788
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 509.88
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 11.86s
                        Total time: 8265.98s
                               ETA: 1196700.1s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.422s, learning 0.189s)
               Value function loss: 15.4091
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 518.12
               Mean episode length: 249.96
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 11.61s
                        Total time: 8277.60s
                               ETA: 1196624.6s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.320s, learning 0.187s)
               Value function loss: 14.1485
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 518.62
               Mean episode length: 248.89
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 11.51s
                        Total time: 8289.10s
                               ETA: 1196534.4s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.169s)
               Value function loss: 11.8026
                    Surrogate loss: -0.0215
             Mean action noise std: 0.77
                       Mean reward: 515.61
               Mean episode length: 248.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 11.08s
                        Total time: 8300.18s
                               ETA: 1196383.1s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.227s, learning 0.168s)
               Value function loss: 11.0296
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 516.77
               Mean episode length: 250.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 11.40s
                        Total time: 8311.58s
                               ETA: 1196277.3s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.140s, learning 0.189s)
               Value function loss: 9.1323
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 521.26
               Mean episode length: 250.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 11.33s
                        Total time: 8322.91s
                               ETA: 1196162.2s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.377s, learning 0.234s)
               Value function loss: 8.7733
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 519.00
               Mean episode length: 249.64
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 11.61s
                        Total time: 8334.52s
                               ETA: 1196087.9s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.638s, learning 0.167s)
               Value function loss: 8.1047
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 519.13
               Mean episode length: 249.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 11.80s
                        Total time: 8346.33s
                               ETA: 1196041.6s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.160s)
               Value function loss: 7.6146
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 511.97
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 11.39s
                        Total time: 8357.72s
                               ETA: 1195936.6s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.424s, learning 0.174s)
               Value function loss: 9.0450
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 511.09
               Mean episode length: 249.63
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 11.60s
                        Total time: 8369.32s
                               ETA: 1195860.9s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.425s, learning 0.171s)
               Value function loss: 9.6916
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 506.40
               Mean episode length: 249.63
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 11.60s
                        Total time: 8380.91s
                               ETA: 1195785.1s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.093s, learning 0.179s)
               Value function loss: 9.9446
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 506.28
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 11.27s
                        Total time: 8392.18s
                               ETA: 1195663.4s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.683s, learning 0.169s)
               Value function loss: 8.6869
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 515.74
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 11.85s
                        Total time: 8404.04s
                               ETA: 1195624.6s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.368s, learning 0.166s)
               Value function loss: 12.4646
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 522.27
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 11.53s
                        Total time: 8415.57s
                               ETA: 1195540.6s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.774s, learning 0.184s)
               Value function loss: 11.2499
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 514.23
               Mean episode length: 248.55
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 11.96s
                        Total time: 8427.53s
                               ETA: 1195517.1s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.520s, learning 0.162s)
               Value function loss: 12.3590
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 513.16
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 11.68s
                        Total time: 8439.21s
                               ETA: 1195454.5s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.433s, learning 0.165s)
               Value function loss: 10.4275
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 506.42
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 11.60s
                        Total time: 8450.81s
                               ETA: 1195380.0s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.167s)
               Value function loss: 13.9765
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 515.91
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 11.27s
                        Total time: 8462.08s
                               ETA: 1195260.0s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.747s, learning 0.165s)
               Value function loss: 13.3725
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 521.13
               Mean episode length: 248.27
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 11.91s
                        Total time: 8473.99s
                               ETA: 1195230.3s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.245s, learning 0.165s)
               Value function loss: 10.8618
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 511.44
               Mean episode length: 248.27
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 11.41s
                        Total time: 8485.40s
                               ETA: 1195130.0s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.001s, learning 0.170s)
               Value function loss: 10.9930
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 516.12
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 11.17s
                        Total time: 8496.57s
                               ETA: 1194996.3s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.297s, learning 0.267s)
               Value function loss: 13.0042
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 521.60
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 11.56s
                        Total time: 8508.14s
                               ETA: 1194918.1s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.169s)
               Value function loss: 9.3910
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 522.81
               Mean episode length: 249.51
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 11.52s
                        Total time: 8519.66s
                               ETA: 1194833.9s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.161s)
               Value function loss: 11.2632
                    Surrogate loss: -0.0238
             Mean action noise std: 0.77
                       Mean reward: 523.86
               Mean episode length: 249.16
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 11.15s
                        Total time: 8530.81s
                               ETA: 1194697.9s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.430s, learning 0.161s)
               Value function loss: 14.0713
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 520.60
               Mean episode length: 249.25
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 11.59s
                        Total time: 8542.40s
                               ETA: 1194624.2s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.052s, learning 0.164s)
               Value function loss: 12.8904
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 501.02
               Mean episode length: 248.90
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 11.22s
                        Total time: 8553.61s
                               ETA: 1194498.3s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.171s)
               Value function loss: 15.9070
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 505.65
               Mean episode length: 249.90
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 11.43s
                        Total time: 8565.05s
                               ETA: 1194402.8s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.332s, learning 0.168s)
               Value function loss: 14.9051
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 513.50
               Mean episode length: 249.84
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 11.50s
                        Total time: 8576.54s
                               ETA: 1194317.0s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.226s, learning 0.181s)
               Value function loss: 14.2790
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 516.96
               Mean episode length: 249.65
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 11.41s
                        Total time: 8587.95s
                               ETA: 1194218.5s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.348s, learning 0.186s)
               Value function loss: 14.0328
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 512.89
               Mean episode length: 249.96
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 11.53s
                        Total time: 8599.49s
                               ETA: 1194137.9s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.846s, learning 0.166s)
               Value function loss: 15.5957
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 527.89
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 11.01s
                        Total time: 8610.50s
                               ETA: 1193985.1s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.192s, learning 0.180s)
               Value function loss: 13.5082
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 520.35
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 11.37s
                        Total time: 8621.87s
                               ETA: 1193882.6s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.134s, learning 0.170s)
               Value function loss: 17.0984
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 514.48
               Mean episode length: 250.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 11.30s
                        Total time: 8633.17s
                               ETA: 1193770.8s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.244s, learning 0.167s)
               Value function loss: 18.7076
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 520.25
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 11.41s
                        Total time: 8644.59s
                               ETA: 1193674.1s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.336s, learning 0.160s)
               Value function loss: 16.9295
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 518.75
               Mean episode length: 250.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 11.50s
                        Total time: 8656.08s
                               ETA: 1193589.4s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.450s, learning 0.160s)
               Value function loss: 14.3350
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 522.82
               Mean episode length: 250.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 11.61s
                        Total time: 8667.69s
                               ETA: 1193520.7s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.500s, learning 0.167s)
               Value function loss: 18.3522
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 510.40
               Mean episode length: 249.50
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 11.67s
                        Total time: 8679.36s
                               ETA: 1193459.9s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.283s, learning 0.162s)
               Value function loss: 14.8325
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 523.43
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 11.44s
                        Total time: 8690.80s
                               ETA: 1193368.7s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.234s, learning 0.161s)
               Value function loss: 11.4398
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 523.72
               Mean episode length: 249.47
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 11.39s
                        Total time: 8702.20s
                               ETA: 1193270.9s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.367s, learning 0.165s)
               Value function loss: 11.5028
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 521.37
               Mean episode length: 249.40
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 11.53s
                        Total time: 8713.73s
                               ETA: 1193192.1s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.159s)
               Value function loss: 12.7056
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 525.32
               Mean episode length: 249.93
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 11.07s
                        Total time: 8724.80s
                               ETA: 1193050.5s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.050s, learning 0.161s)
               Value function loss: 13.4502
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 531.00
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 11.21s
                        Total time: 8736.01s
                               ETA: 1192928.3s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.899s, learning 0.164s)
               Value function loss: 16.6932
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 541.44
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 11.06s
                        Total time: 8747.08s
                               ETA: 1192786.3s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.348s, learning 0.172s)
               Value function loss: 11.5883
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 528.70
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 11.52s
                        Total time: 8758.60s
                               ETA: 1192706.8s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.376s, learning 0.166s)
               Value function loss: 17.7674
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 524.85
               Mean episode length: 249.76
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 11.54s
                        Total time: 8770.14s
                               ETA: 1192630.5s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.965s, learning 0.167s)
               Value function loss: 15.0207
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 534.02
               Mean episode length: 249.88
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 11.13s
                        Total time: 8781.27s
                               ETA: 1192498.7s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.087s, learning 0.161s)
               Value function loss: 17.3927
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 536.20
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 11.25s
                        Total time: 8792.52s
                               ETA: 1192382.9s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.474s, learning 0.182s)
               Value function loss: 16.5179
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 539.73
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 11.66s
                        Total time: 8804.17s
                               ETA: 1192322.8s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.262s, learning 0.162s)
               Value function loss: 22.6897
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 550.03
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 11.42s
                        Total time: 8815.60s
                               ETA: 1192231.4s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.163s)
               Value function loss: 17.9211
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 545.25
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 11.35s
                        Total time: 8826.95s
                               ETA: 1192130.4s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.338s, learning 0.162s)
               Value function loss: 17.7435
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 543.09
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 11.50s
                        Total time: 8838.45s
                               ETA: 1192049.7s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.365s, learning 0.159s)
               Value function loss: 22.5234
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 543.69
               Mean episode length: 249.56
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 11.52s
                        Total time: 8849.97s
                               ETA: 1191972.4s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.333s, learning 0.165s)
               Value function loss: 16.4617
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 545.21
               Mean episode length: 249.56
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 11.50s
                        Total time: 8861.47s
                               ETA: 1191891.8s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.473s, learning 0.161s)
               Value function loss: 15.8221
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 545.62
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 11.63s
                        Total time: 8873.10s
                               ETA: 1191829.7s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.353s, learning 0.163s)
               Value function loss: 17.7863
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 545.37
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 11.52s
                        Total time: 8884.62s
                               ETA: 1191751.8s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.165s)
               Value function loss: 23.4858
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 552.17
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 11.11s
                        Total time: 8895.74s
                               ETA: 1191620.3s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.060s, learning 0.172s)
               Value function loss: 24.0119
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 539.33
               Mean episode length: 248.51
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 11.23s
                        Total time: 8906.97s
                               ETA: 1191504.9s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.302s, learning 0.180s)
               Value function loss: 18.8349
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 548.26
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 11.48s
                        Total time: 8918.45s
                               ETA: 1191423.1s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.261s, learning 0.161s)
               Value function loss: 30.2730
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 541.31
               Mean episode length: 249.86
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 11.42s
                        Total time: 8929.87s
                               ETA: 1191333.6s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.174s, learning 0.178s)
               Value function loss: 29.0407
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 543.13
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 11.35s
                        Total time: 8941.22s
                               ETA: 1191234.8s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.254s, learning 0.170s)
               Value function loss: 30.2159
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 549.34
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 11.42s
                        Total time: 8952.65s
                               ETA: 1191146.0s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.353s, learning 0.165s)
               Value function loss: 25.4643
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 545.22
               Mean episode length: 249.59
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 11.52s
                        Total time: 8964.16s
                               ETA: 1191069.9s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.184s, learning 0.175s)
               Value function loss: 22.9045
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 548.62
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 11.36s
                        Total time: 8975.52s
                               ETA: 1190972.9s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.582s, learning 0.159s)
               Value function loss: 26.7867
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 531.62
               Mean episode length: 249.52
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 11.74s
                        Total time: 8987.27s
                               ETA: 1190926.6s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.045s, learning 0.158s)
               Value function loss: 29.1010
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 545.19
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 11.20s
                        Total time: 8998.47s
                               ETA: 1190809.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.464s, learning 0.215s)
               Value function loss: 27.9352
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 544.62
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 11.68s
                        Total time: 9010.15s
                               ETA: 1190755.2s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.931s, learning 0.162s)
               Value function loss: 24.4906
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 548.38
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 11.09s
                        Total time: 9021.24s
                               ETA: 1190623.9s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.317s, learning 0.171s)
               Value function loss: 25.0303
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 542.13
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 11.49s
                        Total time: 9032.73s
                               ETA: 1190545.0s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.566s, learning 0.164s)
               Value function loss: 22.7349
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 535.01
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 11.73s
                        Total time: 9044.46s
                               ETA: 1190498.1s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.259s, learning 0.177s)
               Value function loss: 20.1644
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 532.94
               Mean episode length: 248.09
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 11.44s
                        Total time: 9055.90s
                               ETA: 1190412.5s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.156s, learning 0.188s)
               Value function loss: 16.5435
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 529.39
               Mean episode length: 247.41
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 11.34s
                        Total time: 9067.24s
                               ETA: 1190315.1s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.422s, learning 0.172s)
               Value function loss: 17.0420
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 525.95
               Mean episode length: 249.48
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 11.59s
                        Total time: 9078.83s
                               ETA: 1190250.7s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.077s, learning 0.186s)
               Value function loss: 13.6826
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 536.88
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 11.26s
                        Total time: 9090.10s
                               ETA: 1190143.1s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.078s, learning 0.166s)
               Value function loss: 22.9701
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 544.10
               Mean episode length: 249.47
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 11.24s
                        Total time: 9101.34s
                               ETA: 1190033.3s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.093s, learning 0.176s)
               Value function loss: 16.1146
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 544.46
               Mean episode length: 249.55
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 11.27s
                        Total time: 9112.61s
                               ETA: 1189927.0s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.981s, learning 0.161s)
               Value function loss: 18.5088
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 537.36
               Mean episode length: 249.27
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 11.14s
                        Total time: 9123.75s
                               ETA: 1189804.4s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.166s)
               Value function loss: 19.7903
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 531.05
               Mean episode length: 249.64
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 11.43s
                        Total time: 9135.18s
                               ETA: 1189719.5s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.197s, learning 0.198s)
               Value function loss: 18.4275
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 534.10
               Mean episode length: 249.92
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 11.40s
                        Total time: 9146.58s
                               ETA: 1189630.4s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.925s, learning 0.170s)
               Value function loss: 18.1388
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 540.89
               Mean episode length: 249.98
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 11.09s
                        Total time: 9157.67s
                               ETA: 1189502.4s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.161s)
               Value function loss: 21.4071
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 548.07
               Mean episode length: 249.98
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 11.57s
                        Total time: 9169.24s
                               ETA: 1189435.8s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.024s, learning 0.181s)
               Value function loss: 19.6796
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 547.35
               Mean episode length: 249.90
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 11.21s
                        Total time: 9180.44s
                               ETA: 1189322.6s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.492s, learning 0.183s)
               Value function loss: 22.8055
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 548.50
               Mean episode length: 249.93
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 11.67s
                        Total time: 9192.12s
                               ETA: 1189270.5s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.319s, learning 0.164s)
               Value function loss: 21.8841
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 547.91
               Mean episode length: 249.90
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 11.48s
                        Total time: 9203.60s
                               ETA: 1189193.8s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.171s)
               Value function loss: 22.9692
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 545.21
               Mean episode length: 249.46
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 11.28s
                        Total time: 9214.87s
                               ETA: 1189090.3s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.198s, learning 0.186s)
               Value function loss: 21.5438
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 548.08
               Mean episode length: 249.63
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 11.38s
                        Total time: 9226.26s
                               ETA: 1189001.1s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.698s, learning 0.232s)
               Value function loss: 17.8917
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 540.80
               Mean episode length: 249.11
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 11.93s
                        Total time: 9238.19s
                               ETA: 1188982.4s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.416s, learning 0.162s)
               Value function loss: 25.1799
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 542.02
               Mean episode length: 247.97
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 11.58s
                        Total time: 9249.77s
                               ETA: 1188918.4s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.243s, learning 0.162s)
               Value function loss: 28.2945
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 545.70
               Mean episode length: 249.29
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 11.41s
                        Total time: 9261.17s
                               ETA: 1188832.5s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.169s)
               Value function loss: 20.3835
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 548.28
               Mean episode length: 249.46
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 11.38s
                        Total time: 9272.55s
                               ETA: 1188743.9s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.217s, learning 0.182s)
               Value function loss: 29.3817
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 543.44
               Mean episode length: 249.12
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 11.40s
                        Total time: 9283.95s
                               ETA: 1188657.5s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.423s, learning 0.197s)
               Value function loss: 29.6513
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 551.00
               Mean episode length: 249.26
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 11.62s
                        Total time: 9295.57s
                               ETA: 1188599.6s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.126s, learning 0.165s)
               Value function loss: 32.8272
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 547.41
               Mean episode length: 248.69
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 11.29s
                        Total time: 9306.86s
                               ETA: 1188499.8s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.517s, learning 0.187s)
               Value function loss: 26.5435
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 539.41
               Mean episode length: 247.61
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 11.70s
                        Total time: 9318.57s
                               ETA: 1188452.9s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.316s, learning 0.264s)
               Value function loss: 27.9231
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 543.67
               Mean episode length: 248.42
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 11.58s
                        Total time: 9330.15s
                               ETA: 1188390.3s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.628s, learning 0.167s)
               Value function loss: 22.2387
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 550.80
               Mean episode length: 249.07
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 11.80s
                        Total time: 9341.94s
                               ETA: 1188355.2s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.376s, learning 0.185s)
               Value function loss: 31.7581
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 548.04
               Mean episode length: 248.91
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 11.56s
                        Total time: 9353.51s
                               ETA: 1188290.4s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.998s, learning 0.169s)
               Value function loss: 31.5088
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 551.85
               Mean episode length: 249.41
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 11.17s
                        Total time: 9364.67s
                               ETA: 1188175.7s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.403s, learning 0.159s)
               Value function loss: 24.3157
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 547.07
               Mean episode length: 248.46
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 11.56s
                        Total time: 9376.23s
                               ETA: 1188111.3s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.348s, learning 0.189s)
               Value function loss: 21.5047
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 552.85
               Mean episode length: 249.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 11.54s
                        Total time: 9387.77s
                               ETA: 1188043.9s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.145s, learning 0.161s)
               Value function loss: 19.8810
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 553.44
               Mean episode length: 248.34
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 11.31s
                        Total time: 9399.08s
                               ETA: 1187947.4s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.163s)
               Value function loss: 17.8807
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 542.92
               Mean episode length: 247.22
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 10.95s
                        Total time: 9410.02s
                               ETA: 1187805.7s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.138s, learning 0.187s)
               Value function loss: 18.1465
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 545.97
               Mean episode length: 248.46
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 11.33s
                        Total time: 9421.35s
                               ETA: 1187712.2s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.168s)
               Value function loss: 17.3982
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 559.47
               Mean episode length: 249.71
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 11.51s
                        Total time: 9432.86s
                               ETA: 1187642.1s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.898s, learning 0.171s)
               Value function loss: 14.4726
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 550.56
               Mean episode length: 248.71
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 11.07s
                        Total time: 9443.93s
                               ETA: 1187516.8s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.187s, learning 0.198s)
               Value function loss: 23.1591
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 548.35
               Mean episode length: 248.60
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 11.39s
                        Total time: 9455.31s
                               ETA: 1187431.4s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.232s, learning 0.162s)
               Value function loss: 20.0319
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 559.86
               Mean episode length: 249.55
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 11.39s
                        Total time: 9466.70s
                               ETA: 1187347.3s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.162s)
               Value function loss: 15.6840
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 555.83
               Mean episode length: 249.68
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 11.44s
                        Total time: 9478.14s
                               ETA: 1187268.8s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.950s, learning 0.178s)
               Value function loss: 23.3661
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 545.30
               Mean episode length: 246.22
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 11.13s
                        Total time: 9489.27s
                               ETA: 1187151.9s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.150s, learning 0.159s)
               Value function loss: 18.5199
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 548.46
               Mean episode length: 248.86
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 11.31s
                        Total time: 9500.58s
                               ETA: 1187057.9s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1467 steps/s (collection: 11.000s, learning 0.162s)
               Value function loss: 19.2656
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 549.83
               Mean episode length: 249.19
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 11.16s
                        Total time: 9511.74s
                               ETA: 1186945.7s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.994s, learning 0.164s)
               Value function loss: 17.8767
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 557.87
               Mean episode length: 249.40
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 11.16s
                        Total time: 9522.90s
                               ETA: 1186833.2s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.934s, learning 0.169s)
               Value function loss: 17.5145
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 552.17
               Mean episode length: 248.42
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 11.10s
                        Total time: 9534.00s
                               ETA: 1186714.2s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.459s, learning 0.161s)
               Value function loss: 20.1751
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 548.38
               Mean episode length: 248.77
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 11.62s
                        Total time: 9545.62s
                               ETA: 1186659.7s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.116s, learning 0.158s)
               Value function loss: 17.0865
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 543.16
               Mean episode length: 248.59
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 11.27s
                        Total time: 9556.90s
                               ETA: 1186562.2s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.483s, learning 0.159s)
               Value function loss: 17.3577
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 535.96
               Mean episode length: 247.42
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 11.64s
                        Total time: 9568.54s
                               ETA: 1186510.8s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1466 steps/s (collection: 10.989s, learning 0.184s)
               Value function loss: 19.8950
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 539.93
               Mean episode length: 248.53
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 11.17s
                        Total time: 9579.71s
                               ETA: 1186401.2s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.169s, learning 0.163s)
               Value function loss: 13.4838
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 539.31
               Mean episode length: 248.37
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 11.33s
                        Total time: 9591.04s
                               ETA: 1186311.5s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.082s, learning 0.161s)
               Value function loss: 17.5175
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 541.05
               Mean episode length: 247.81
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 11.24s
                        Total time: 9602.29s
                               ETA: 1186211.1s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.212s, learning 0.189s)
               Value function loss: 21.0066
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 554.18
               Mean episode length: 249.74
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 11.40s
                        Total time: 9613.69s
                               ETA: 1186130.4s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.217s, learning 0.162s)
               Value function loss: 16.9221
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 527.13
               Mean episode length: 245.24
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 11.38s
                        Total time: 9625.07s
                               ETA: 1186047.2s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.027s, learning 0.158s)
               Value function loss: 20.7397
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 534.80
               Mean episode length: 246.28
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 11.19s
                        Total time: 9636.25s
                               ETA: 1185940.4s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.095s, learning 0.174s)
               Value function loss: 17.7656
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 558.49
               Mean episode length: 249.63
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 11.27s
                        Total time: 9647.52s
                               ETA: 1185844.0s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.418s, learning 0.165s)
               Value function loss: 20.9239
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 556.83
               Mean episode length: 249.30
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 11.58s
                        Total time: 9659.10s
                               ETA: 1185786.4s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.211s, learning 0.178s)
               Value function loss: 18.9991
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 545.50
               Mean episode length: 248.44
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 11.39s
                        Total time: 9670.49s
                               ETA: 1185705.1s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.365s, learning 0.160s)
               Value function loss: 17.8093
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 556.32
               Mean episode length: 249.24
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 11.53s
                        Total time: 9682.02s
                               ETA: 1185640.7s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.132s, learning 0.159s)
               Value function loss: 15.8241
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 556.01
               Mean episode length: 249.39
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 11.29s
                        Total time: 9693.31s
                               ETA: 1185547.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.069s, learning 0.178s)
               Value function loss: 19.8922
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 549.16
               Mean episode length: 249.39
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 11.25s
                        Total time: 9704.55s
                               ETA: 1185449.6s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.033s, learning 0.166s)
               Value function loss: 24.3581
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 556.64
               Mean episode length: 249.90
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 11.20s
                        Total time: 9715.75s
                               ETA: 1185345.7s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.974s, learning 0.175s)
               Value function loss: 16.5740
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 554.51
               Mean episode length: 249.17
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 11.15s
                        Total time: 9726.90s
                               ETA: 1185236.0s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.162s)
               Value function loss: 15.8822
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 553.06
               Mean episode length: 249.17
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 11.51s
                        Total time: 9738.42s
                               ETA: 1185171.1s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.163s)
               Value function loss: 15.7826
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 552.32
               Mean episode length: 249.33
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 11.48s
                        Total time: 9749.89s
                               ETA: 1185101.9s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.107s, learning 0.183s)
               Value function loss: 14.2216
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 540.40
               Mean episode length: 247.92
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 11.29s
                        Total time: 9761.18s
                               ETA: 1185010.0s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.166s, learning 0.182s)
               Value function loss: 13.4721
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 540.03
               Mean episode length: 247.78
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 11.35s
                        Total time: 9772.53s
                               ETA: 1184925.3s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.008s, learning 0.186s)
               Value function loss: 15.0988
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 523.82
               Mean episode length: 245.09
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 11.19s
                        Total time: 9783.73s
                               ETA: 1184822.3s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.946s, learning 0.181s)
               Value function loss: 13.1954
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 520.02
               Mean episode length: 244.66
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 11.13s
                        Total time: 9794.85s
                               ETA: 1184711.2s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.311s, learning 0.184s)
               Value function loss: 16.0966
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 512.73
               Mean episode length: 242.90
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 11.50s
                        Total time: 9806.35s
                               ETA: 1184644.9s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.271s, learning 0.177s)
               Value function loss: 17.8048
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 543.70
               Mean episode length: 248.07
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 11.45s
                        Total time: 9817.80s
                               ETA: 1184573.1s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.504s, learning 0.179s)
               Value function loss: 14.4664
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 535.75
               Mean episode length: 246.42
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 10.68s
                        Total time: 9828.48s
                               ETA: 1184409.3s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.002s, learning 0.172s)
               Value function loss: 19.5013
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 542.88
               Mean episode length: 247.90
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 11.17s
                        Total time: 9839.65s
                               ETA: 1184304.9s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.162s)
               Value function loss: 19.1494
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 526.28
               Mean episode length: 245.54
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 11.34s
                        Total time: 9850.99s
                               ETA: 1184220.8s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.483s, learning 0.174s)
               Value function loss: 17.1092
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 505.64
               Mean episode length: 243.17
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 11.66s
                        Total time: 9862.65s
                               ETA: 1184174.7s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.304s, learning 0.168s)
               Value function loss: 15.2434
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 511.54
               Mean episode length: 243.46
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 11.47s
                        Total time: 9874.12s
                               ETA: 1184106.7s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.112s, learning 0.183s)
               Value function loss: 18.7924
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 515.17
               Mean episode length: 243.73
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 11.30s
                        Total time: 9885.42s
                               ETA: 1184017.5s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.729s, learning 0.175s)
               Value function loss: 15.3394
                    Surrogate loss: -0.0241
             Mean action noise std: 0.76
                       Mean reward: 529.60
               Mean episode length: 245.31
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 10.90s
                        Total time: 9896.32s
                               ETA: 1183881.7s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.574s, learning 0.171s)
               Value function loss: 18.1055
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 508.34
               Mean episode length: 242.65
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 11.74s
                        Total time: 9908.07s
                               ETA: 1183846.7s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.972s, learning 0.181s)
               Value function loss: 13.7385
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 481.15
               Mean episode length: 238.52
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 11.15s
                        Total time: 9919.22s
                               ETA: 1183741.2s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.937s, learning 0.159s)
               Value function loss: 18.9534
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 527.80
               Mean episode length: 245.16
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 11.10s
                        Total time: 9930.31s
                               ETA: 1183629.0s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.000s, learning 0.182s)
               Value function loss: 12.4022
                    Surrogate loss: -0.0233
             Mean action noise std: 0.76
                       Mean reward: 508.05
               Mean episode length: 243.21
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 11.18s
                        Total time: 9941.50s
                               ETA: 1183527.3s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.881s, learning 0.189s)
               Value function loss: 11.7710
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 502.35
               Mean episode length: 243.07
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 11.07s
                        Total time: 9952.57s
                               ETA: 1183412.5s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.139s, learning 0.164s)
               Value function loss: 19.7316
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 526.36
               Mean episode length: 246.82
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 11.30s
                        Total time: 9963.87s
                               ETA: 1183325.8s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.717s, learning 0.167s)
               Value function loss: 15.8361
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 526.85
               Mean episode length: 246.41
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 10.88s
                        Total time: 9974.75s
                               ETA: 1183189.4s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.156s, learning 0.178s)
               Value function loss: 15.3204
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 508.53
               Mean episode length: 244.19
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 11.33s
                        Total time: 9986.09s
                               ETA: 1183106.7s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.971s, learning 0.167s)
               Value function loss: 17.1994
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 516.21
               Mean episode length: 245.62
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 11.14s
                        Total time: 9997.22s
                               ETA: 1183001.0s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.020s, learning 0.169s)
               Value function loss: 16.6098
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 520.79
               Mean episode length: 247.41
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 11.19s
                        Total time: 10008.41s
                               ETA: 1182901.5s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.163s)
               Value function loss: 17.3963
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 528.09
               Mean episode length: 247.86
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 11.36s
                        Total time: 10019.77s
                               ETA: 1182822.0s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.221s, learning 0.164s)
               Value function loss: 17.1448
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 544.53
               Mean episode length: 249.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 11.39s
                        Total time: 10031.16s
                               ETA: 1182746.0s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.291s, learning 0.164s)
               Value function loss: 13.4335
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 541.29
               Mean episode length: 248.93
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 11.45s
                        Total time: 10042.61s
                               ETA: 1182678.4s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.209s, learning 0.191s)
               Value function loss: 20.1148
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 522.56
               Mean episode length: 246.77
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 11.40s
                        Total time: 10054.01s
                               ETA: 1182604.5s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.267s, learning 0.162s)
               Value function loss: 20.1333
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 532.56
               Mean episode length: 248.47
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 11.43s
                        Total time: 10065.44s
                               ETA: 1182534.2s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.356s, learning 0.183s)
               Value function loss: 15.1550
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 516.16
               Mean episode length: 245.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 11.54s
                        Total time: 10076.98s
                               ETA: 1182476.8s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.654s, learning 0.165s)
               Value function loss: 16.1429
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 511.22
               Mean episode length: 246.07
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 11.82s
                        Total time: 10088.80s
                               ETA: 1182452.3s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.161s)
               Value function loss: 14.4009
                    Surrogate loss: -0.0241
             Mean action noise std: 0.76
                       Mean reward: 511.53
               Mean episode length: 245.96
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 11.39s
                        Total time: 10100.19s
                               ETA: 1182378.2s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.162s)
               Value function loss: 17.8547
                    Surrogate loss: -0.0224
             Mean action noise std: 0.76
                       Mean reward: 499.81
               Mean episode length: 244.75
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 11.57s
                        Total time: 10111.76s
                               ETA: 1182324.4s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.980s, learning 0.170s)
               Value function loss: 14.3943
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 511.38
               Mean episode length: 246.84
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 11.15s
                        Total time: 10122.91s
                               ETA: 1182222.1s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.034s, learning 0.175s)
               Value function loss: 13.1824
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 529.92
               Mean episode length: 249.42
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 11.21s
                        Total time: 10134.12s
                               ETA: 1182126.8s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.093s, learning 0.176s)
               Value function loss: 14.9639
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 513.77
               Mean episode length: 248.30
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 11.27s
                        Total time: 10145.39s
                               ETA: 1182038.8s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.103s, learning 0.185s)
               Value function loss: 14.9578
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 502.18
               Mean episode length: 247.21
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 11.29s
                        Total time: 10156.67s
                               ETA: 1181953.1s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.189s, learning 0.195s)
               Value function loss: 15.6286
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 498.46
               Mean episode length: 245.68
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 11.38s
                        Total time: 10168.06s
                               ETA: 1181878.7s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.634s, learning 0.180s)
               Value function loss: 14.6944
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 512.52
               Mean episode length: 247.91
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 11.81s
                        Total time: 10179.87s
                               ETA: 1181854.4s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.145s, learning 0.184s)
               Value function loss: 16.6706
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 508.49
               Mean episode length: 248.23
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 11.33s
                        Total time: 10191.20s
                               ETA: 1181773.9s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.338s, learning 0.188s)
               Value function loss: 15.7902
                    Surrogate loss: 0.0011
             Mean action noise std: 0.76
                       Mean reward: 508.99
               Mean episode length: 249.26
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 11.53s
                        Total time: 10202.73s
                               ETA: 1181716.5s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.354s, learning 0.252s)
               Value function loss: 14.7264
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 490.58
               Mean episode length: 247.73
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 11.61s
                        Total time: 10214.33s
                               ETA: 1181668.4s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.261s, learning 0.183s)
               Value function loss: 13.6900
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 484.76
               Mean episode length: 244.33
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 11.44s
                        Total time: 10225.78s
                               ETA: 1181601.6s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.722s, learning 0.191s)
               Value function loss: 15.3346
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 488.59
               Mean episode length: 244.97
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 10.91s
                        Total time: 10236.69s
                               ETA: 1181473.8s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.224s, learning 0.262s)
               Value function loss: 17.2155
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 490.55
               Mean episode length: 246.81
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 11.49s
                        Total time: 10248.18s
                               ETA: 1181412.2s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.120s, learning 0.175s)
               Value function loss: 18.3427
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 480.06
               Mean episode length: 248.30
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 11.29s
                        Total time: 10259.47s
                               ETA: 1181328.7s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.165s)
               Value function loss: 16.9982
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 471.34
               Mean episode length: 245.89
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 11.38s
                        Total time: 10270.85s
                               ETA: 1181255.2s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.754s, learning 0.265s)
               Value function loss: 14.4131
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 478.14
               Mean episode length: 246.22
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 11.02s
                        Total time: 10281.87s
                               ETA: 1181140.3s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.963s, learning 0.180s)
               Value function loss: 13.0618
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 482.73
               Mean episode length: 246.38
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 11.14s
                        Total time: 10293.01s
                               ETA: 1181039.9s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.163s, learning 0.181s)
               Value function loss: 12.4807
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 464.43
               Mean episode length: 244.69
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 11.34s
                        Total time: 10304.36s
                               ETA: 1180962.8s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.043s, learning 0.185s)
               Value function loss: 13.6895
                    Surrogate loss: -0.0259
             Mean action noise std: 0.76
                       Mean reward: 467.18
               Mean episode length: 245.08
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 11.23s
                        Total time: 10315.59s
                               ETA: 1180872.5s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.004s, learning 0.165s)
               Value function loss: 13.7621
                    Surrogate loss: -0.0281
             Mean action noise std: 0.76
                       Mean reward: 483.44
               Mean episode length: 248.31
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 11.17s
                        Total time: 10326.75s
                               ETA: 1180775.6s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.784s, learning 0.182s)
               Value function loss: 12.6643
                    Surrogate loss: -0.0226
             Mean action noise std: 0.76
                       Mean reward: 484.19
               Mean episode length: 248.11
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 11.97s
                        Total time: 10338.72s
                               ETA: 1180770.0s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.936s, learning 0.181s)
               Value function loss: 14.3669
                    Surrogate loss: -0.0130
             Mean action noise std: 0.76
                       Mean reward: 475.74
               Mean episode length: 246.98
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 11.12s
                        Total time: 10349.84s
                               ETA: 1180667.5s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.006s, learning 0.169s)
               Value function loss: 13.5618
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 478.60
               Mean episode length: 245.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 11.18s
                        Total time: 10361.01s
                               ETA: 1180571.9s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.259s, learning 0.176s)
               Value function loss: 15.7681
                    Surrogate loss: -0.0267
             Mean action noise std: 0.76
                       Mean reward: 475.57
               Mean episode length: 245.05
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 11.44s
                        Total time: 10372.45s
                               ETA: 1180506.0s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.101s, learning 0.166s)
               Value function loss: 12.8078
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 494.36
               Mean episode length: 247.21
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 11.27s
                        Total time: 10383.71s
                               ETA: 1180421.1s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.278s, learning 0.184s)
               Value function loss: 12.9939
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 507.32
               Mean episode length: 248.45
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 11.46s
                        Total time: 10395.18s
                               ETA: 1180358.5s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.569s, learning 0.179s)
               Value function loss: 13.2131
                    Surrogate loss: -0.0235
             Mean action noise std: 0.76
                       Mean reward: 501.93
               Mean episode length: 247.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 11.75s
                        Total time: 10406.92s
                               ETA: 1180328.5s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.550s, learning 0.168s)
               Value function loss: 15.2091
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 507.42
               Mean episode length: 249.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 11.72s
                        Total time: 10418.64s
                               ETA: 1180295.1s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.084s, learning 0.176s)
               Value function loss: 15.2391
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 512.32
               Mean episode length: 249.31
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 11.26s
                        Total time: 10429.90s
                               ETA: 1180209.9s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.103s, learning 0.166s)
               Value function loss: 12.2870
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 514.42
               Mean episode length: 248.08
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 11.27s
                        Total time: 10441.17s
                               ETA: 1180126.0s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.274s, learning 0.171s)
               Value function loss: 12.9167
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 508.08
               Mean episode length: 247.14
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 11.44s
                        Total time: 10452.61s
                               ETA: 1180062.1s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.447s, learning 0.173s)
               Value function loss: 15.2836
                    Surrogate loss: -0.0236
             Mean action noise std: 0.76
                       Mean reward: 500.24
               Mean episode length: 244.66
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 11.62s
                        Total time: 10464.23s
                               ETA: 1180018.0s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.009s, learning 0.182s)
               Value function loss: 14.1983
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 513.67
               Mean episode length: 246.59
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 11.19s
                        Total time: 10475.43s
                               ETA: 1179925.7s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.048s, learning 0.182s)
               Value function loss: 13.1764
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 508.60
               Mean episode length: 244.21
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 11.23s
                        Total time: 10486.66s
                               ETA: 1179838.0s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.870s, learning 0.199s)
               Value function loss: 14.5536
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 492.34
               Mean episode length: 241.90
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 11.07s
                        Total time: 10497.72s
                               ETA: 1179732.4s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.161s, learning 0.178s)
               Value function loss: 12.0247
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 502.83
               Mean episode length: 244.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 11.34s
                        Total time: 10509.06s
                               ETA: 1179657.3s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.183s, learning 0.164s)
               Value function loss: 13.1450
                    Surrogate loss: -0.0223
             Mean action noise std: 0.76
                       Mean reward: 488.27
               Mean episode length: 239.86
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 11.35s
                        Total time: 10520.41s
                               ETA: 1179583.2s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.275s, learning 0.189s)
               Value function loss: 12.4156
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 513.41
               Mean episode length: 244.64
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 11.46s
                        Total time: 10531.87s
                               ETA: 1179522.4s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.302s, learning 0.221s)
               Value function loss: 17.6634
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 495.04
               Mean episode length: 239.62
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 11.52s
                        Total time: 10543.40s
                               ETA: 1179468.3s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.375s, learning 0.180s)
               Value function loss: 14.3890
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 488.39
               Mean episode length: 239.65
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 11.56s
                        Total time: 10554.95s
                               ETA: 1179417.9s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.173s)
               Value function loss: 14.1716
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 486.56
               Mean episode length: 238.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 11.31s
                        Total time: 10566.26s
                               ETA: 1179339.9s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.098s, learning 0.181s)
               Value function loss: 14.0374
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 482.76
               Mean episode length: 236.70
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 11.28s
                        Total time: 10577.54s
                               ETA: 1179258.9s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.358s, learning 0.179s)
               Value function loss: 13.3828
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 481.44
               Mean episode length: 237.56
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 11.54s
                        Total time: 10589.08s
                               ETA: 1179206.8s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.804s, learning 0.173s)
               Value function loss: 14.4862
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 491.03
               Mean episode length: 239.83
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 10.98s
                        Total time: 10600.05s
                               ETA: 1179092.5s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.850s, learning 0.184s)
               Value function loss: 17.7299
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 497.66
               Mean episode length: 241.45
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 11.03s
                        Total time: 10611.09s
                               ETA: 1178984.8s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.202s, learning 0.175s)
               Value function loss: 18.1623
                    Surrogate loss: -0.0221
             Mean action noise std: 0.76
                       Mean reward: 491.15
               Mean episode length: 237.75
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 11.38s
                        Total time: 10622.47s
                               ETA: 1178915.2s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.042s, learning 0.167s)
               Value function loss: 16.9292
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 497.99
               Mean episode length: 238.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 11.21s
                        Total time: 10633.67s
                               ETA: 1178827.2s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.238s, learning 0.171s)
               Value function loss: 18.5763
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 496.26
               Mean episode length: 241.24
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 11.41s
                        Total time: 10645.08s
                               ETA: 1178761.5s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.741s, learning 0.161s)
               Value function loss: 13.1065
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 465.69
               Mean episode length: 233.96
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 10.90s
                        Total time: 10655.98s
                               ETA: 1178639.9s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.159s, learning 0.183s)
               Value function loss: 16.1204
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 475.44
               Mean episode length: 235.91
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 11.34s
                        Total time: 10667.33s
                               ETA: 1178567.2s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.125s, learning 0.165s)
               Value function loss: 16.6723
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 494.77
               Mean episode length: 240.27
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 11.29s
                        Total time: 10678.62s
                               ETA: 1178488.8s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.506s, learning 0.172s)
               Value function loss: 13.3402
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 489.61
               Mean episode length: 239.91
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 11.68s
                        Total time: 10690.29s
                               ETA: 1178453.3s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.260s, learning 0.191s)
               Value function loss: 19.2524
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 485.22
               Mean episode length: 239.39
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 11.45s
                        Total time: 10701.74s
                               ETA: 1178392.9s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.164s)
               Value function loss: 18.8381
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 493.38
               Mean episode length: 240.41
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 11.49s
                        Total time: 10713.24s
                               ETA: 1178337.0s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.206s, learning 0.175s)
               Value function loss: 18.6984
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 489.02
               Mean episode length: 238.65
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 11.38s
                        Total time: 10724.62s
                               ETA: 1178269.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.431s, learning 0.180s)
               Value function loss: 14.6915
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 490.12
               Mean episode length: 239.38
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 11.61s
                        Total time: 10736.23s
                               ETA: 1178226.7s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.167s, learning 0.161s)
               Value function loss: 14.1789
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 494.49
               Mean episode length: 241.84
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 11.33s
                        Total time: 10747.56s
                               ETA: 1178153.2s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.642s, learning 0.173s)
               Value function loss: 13.6386
                    Surrogate loss: -0.0242
             Mean action noise std: 0.76
                       Mean reward: 498.96
               Mean episode length: 241.22
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 10.82s
                        Total time: 10758.37s
                               ETA: 1178023.8s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.367s, learning 0.166s)
               Value function loss: 11.3769
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 500.25
               Mean episode length: 244.23
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 11.53s
                        Total time: 10769.90s
                               ETA: 1177973.1s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 838 steps/s (collection: 19.386s, learning 0.160s)
               Value function loss: 13.9428
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 472.65
               Mean episode length: 237.34
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 19.55s
                        Total time: 10789.45s
                               ETA: 1178798.0s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 738 steps/s (collection: 22.005s, learning 0.170s)
               Value function loss: 13.5286
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 475.16
               Mean episode length: 235.60
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 22.18s
                        Total time: 10811.63s
                               ETA: 1179908.0s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 750 steps/s (collection: 21.656s, learning 0.164s)
               Value function loss: 15.2788
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 482.21
               Mean episode length: 237.91
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 21.82s
                        Total time: 10833.45s
                               ETA: 1180976.6s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.719s, learning 0.169s)
               Value function loss: 13.3041
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 449.54
               Mean episode length: 230.64
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 21.89s
                        Total time: 10855.33s
                               ETA: 1182050.4s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.732s, learning 0.188s)
               Value function loss: 14.4832
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 471.29
               Mean episode length: 236.57
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 21.92s
                        Total time: 10877.25s
                               ETA: 1183125.1s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 750 steps/s (collection: 21.673s, learning 0.171s)
               Value function loss: 13.8273
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 486.94
               Mean episode length: 238.97
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 21.84s
                        Total time: 10899.10s
                               ETA: 1184189.3s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.492s, learning 0.189s)
               Value function loss: 13.7063
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 493.29
               Mean episode length: 238.15
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 21.68s
                        Total time: 10920.78s
                               ETA: 1185233.4s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.054s, learning 0.176s)
               Value function loss: 13.0713
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 490.68
               Mean episode length: 239.51
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 22.23s
                        Total time: 10943.01s
                               ETA: 1186334.6s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.643s, learning 0.173s)
               Value function loss: 15.8193
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 455.60
               Mean episode length: 230.46
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 21.82s
                        Total time: 10964.82s
                               ETA: 1187388.6s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 736 steps/s (collection: 22.051s, learning 0.187s)
               Value function loss: 14.9463
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 470.46
               Mean episode length: 236.83
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 22.24s
                        Total time: 10987.06s
                               ETA: 1188485.9s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 736 steps/s (collection: 22.084s, learning 0.171s)
               Value function loss: 15.3572
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 467.80
               Mean episode length: 237.55
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 22.25s
                        Total time: 11009.32s
                               ETA: 1189582.5s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.782s, learning 0.163s)
               Value function loss: 15.9324
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 451.85
               Mean episode length: 232.68
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 21.94s
                        Total time: 11031.26s
                               ETA: 1190643.2s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.815s, learning 0.166s)
               Value function loss: 18.2118
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 457.95
               Mean episode length: 234.43
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 21.98s
                        Total time: 11053.24s
                               ETA: 1191705.4s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 741 steps/s (collection: 21.923s, learning 0.168s)
               Value function loss: 17.9077
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 480.10
               Mean episode length: 239.86
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 22.09s
                        Total time: 11075.33s
                               ETA: 1192777.1s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 733 steps/s (collection: 22.174s, learning 0.164s)
               Value function loss: 17.3014
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 486.38
               Mean episode length: 241.18
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 22.34s
                        Total time: 11097.67s
                               ETA: 1193873.0s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.758s, learning 0.161s)
               Value function loss: 16.5261
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 497.49
               Mean episode length: 245.28
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 21.92s
                        Total time: 11119.59s
                               ETA: 1194921.6s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.326s, learning 0.169s)
               Value function loss: 17.4110
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 475.04
               Mean episode length: 239.08
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 22.50s
                        Total time: 11142.08s
                               ETA: 1196029.7s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.703s, learning 0.160s)
               Value function loss: 18.4587
                    Surrogate loss: -0.0007
             Mean action noise std: 0.76
                       Mean reward: 472.16
               Mean episode length: 239.86
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 21.86s
                        Total time: 11163.95s
                               ETA: 1197067.5s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 759 steps/s (collection: 21.412s, learning 0.173s)
               Value function loss: 17.5214
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 466.31
               Mean episode length: 239.11
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 21.59s
                        Total time: 11185.53s
                               ETA: 1198073.3s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.808s, learning 0.163s)
               Value function loss: 19.3481
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 473.44
               Mean episode length: 242.25
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 21.97s
                        Total time: 11207.50s
                               ETA: 1199118.1s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.746s, learning 0.163s)
               Value function loss: 13.8970
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 477.76
               Mean episode length: 240.44
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 21.91s
                        Total time: 11229.41s
                               ETA: 1200154.1s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.763s, learning 0.167s)
               Value function loss: 16.4257
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 461.70
               Mean episode length: 238.98
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 21.93s
                        Total time: 11251.34s
                               ETA: 1201189.9s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.023s, learning 0.181s)
               Value function loss: 19.2833
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 472.21
               Mean episode length: 244.66
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 22.20s
                        Total time: 11273.55s
                               ETA: 1202252.8s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 739 steps/s (collection: 21.991s, learning 0.165s)
               Value function loss: 19.9346
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 497.46
               Mean episode length: 244.32
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 22.16s
                        Total time: 11295.70s
                               ETA: 1203308.2s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.813s, learning 0.162s)
               Value function loss: 19.4143
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 467.19
               Mean episode length: 241.59
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 21.98s
                        Total time: 11317.68s
                               ETA: 1204342.0s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 746 steps/s (collection: 21.774s, learning 0.187s)
               Value function loss: 17.5257
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 472.96
               Mean episode length: 245.52
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 21.96s
                        Total time: 11339.64s
                               ETA: 1205372.0s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 730 steps/s (collection: 22.256s, learning 0.184s)
               Value function loss: 21.1084
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 467.92
               Mean episode length: 243.13
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 22.44s
                        Total time: 11362.08s
                               ETA: 1206450.7s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 758 steps/s (collection: 21.435s, learning 0.164s)
               Value function loss: 18.9626
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 482.90
               Mean episode length: 244.20
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 21.60s
                        Total time: 11383.68s
                               ETA: 1207437.9s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 721 steps/s (collection: 22.558s, learning 0.160s)
               Value function loss: 16.4187
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 476.32
               Mean episode length: 244.05
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 22.72s
                        Total time: 11406.40s
                               ETA: 1208541.3s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 738 steps/s (collection: 22.007s, learning 0.173s)
               Value function loss: 17.9702
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 465.82
               Mean episode length: 243.74
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 22.18s
                        Total time: 11428.58s
                               ETA: 1209585.4s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.057s, learning 0.169s)
               Value function loss: 18.5695
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 481.14
               Mean episode length: 246.59
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 22.23s
                        Total time: 11450.80s
                               ETA: 1210632.1s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.957s, learning 0.167s)
               Value function loss: 20.7973
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 477.23
               Mean episode length: 245.06
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 22.12s
                        Total time: 11472.93s
                               ETA: 1211665.9s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 736 steps/s (collection: 22.080s, learning 0.171s)
               Value function loss: 19.1562
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 472.32
               Mean episode length: 244.85
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 22.25s
                        Total time: 11495.18s
                               ETA: 1212710.6s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.105s, learning 0.170s)
               Value function loss: 20.3834
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 461.91
               Mean episode length: 244.52
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 21.27s
                        Total time: 11516.45s
                               ETA: 1213650.3s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.594s, learning 0.175s)
               Value function loss: 17.5766
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 461.02
               Mean episode length: 246.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 21.77s
                        Total time: 11538.22s
                               ETA: 1214640.1s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 741 steps/s (collection: 21.922s, learning 0.169s)
               Value function loss: 20.0709
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 466.32
               Mean episode length: 244.34
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 22.09s
                        Total time: 11560.31s
                               ETA: 1215661.4s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.736s, learning 0.167s)
               Value function loss: 18.3168
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 460.47
               Mean episode length: 240.75
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 21.90s
                        Total time: 11582.22s
                               ETA: 1216660.9s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 784 steps/s (collection: 20.697s, learning 0.175s)
               Value function loss: 17.7045
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 469.40
               Mean episode length: 243.06
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 20.87s
                        Total time: 11603.09s
                               ETA: 1217550.0s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.758s, learning 0.163s)
               Value function loss: 17.3978
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 447.92
               Mean episode length: 240.85
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 10.92s
                        Total time: 11614.01s
                               ETA: 1217394.0s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.152s, learning 0.165s)
               Value function loss: 19.1697
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 454.58
               Mean episode length: 245.24
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 11.32s
                        Total time: 11625.33s
                               ETA: 1217279.9s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.126s, learning 0.280s)
               Value function loss: 19.2859
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 465.98
               Mean episode length: 247.98
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 11.41s
                        Total time: 11636.73s
                               ETA: 1217175.2s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.364s, learning 0.186s)
               Value function loss: 16.8130
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 450.44
               Mean episode length: 244.64
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 11.55s
                        Total time: 11648.28s
                               ETA: 1217085.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.022s, learning 0.166s)
               Value function loss: 19.4425
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 451.02
               Mean episode length: 245.68
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 11.19s
                        Total time: 11659.47s
                               ETA: 1216958.8s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.381s, learning 0.163s)
               Value function loss: 22.9381
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 472.52
               Mean episode length: 246.52
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 11.54s
                        Total time: 11671.01s
                               ETA: 1216869.1s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.243s, learning 0.167s)
               Value function loss: 17.1314
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 457.15
               Mean episode length: 243.87
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 11.41s
                        Total time: 11682.42s
                               ETA: 1216765.7s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.239s, learning 0.163s)
               Value function loss: 16.3931
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 435.19
               Mean episode length: 243.35
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 11.40s
                        Total time: 11693.83s
                               ETA: 1216661.6s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.533s, learning 0.187s)
               Value function loss: 19.7384
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 442.08
               Mean episode length: 244.21
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 11.72s
                        Total time: 11705.55s
                               ETA: 1216590.8s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.966s, learning 0.168s)
               Value function loss: 21.1444
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 441.05
               Mean episode length: 243.53
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 11.13s
                        Total time: 11716.68s
                               ETA: 1216459.3s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.020s, learning 0.167s)
               Value function loss: 17.9612
                    Surrogate loss: -0.0226
             Mean action noise std: 0.76
                       Mean reward: 446.77
               Mean episode length: 245.18
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 11.19s
                        Total time: 11727.87s
                               ETA: 1216333.4s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.776s, learning 0.167s)
               Value function loss: 18.0106
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 446.29
               Mean episode length: 245.63
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 11.94s
                        Total time: 11739.81s
                               ETA: 1216286.2s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.154s, learning 0.168s)
               Value function loss: 17.2946
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 456.70
               Mean episode length: 247.04
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 11.32s
                        Total time: 11751.13s
                               ETA: 1216174.8s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.163s)
               Value function loss: 15.0537
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 442.28
               Mean episode length: 246.63
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 11.68s
                        Total time: 11762.81s
                               ETA: 1216100.3s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.163s)
               Value function loss: 17.5627
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 438.06
               Mean episode length: 246.76
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 11.36s
                        Total time: 11774.18s
                               ETA: 1215993.6s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.587s, learning 0.159s)
               Value function loss: 22.1156
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 462.92
               Mean episode length: 246.14
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 11.75s
                        Total time: 11785.92s
                               ETA: 1215926.5s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.108s, learning 0.161s)
               Value function loss: 22.6337
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 475.57
               Mean episode length: 246.02
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 11.27s
                        Total time: 11797.19s
                               ETA: 1215810.4s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.040s, learning 0.196s)
               Value function loss: 17.4904
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 455.14
               Mean episode length: 245.61
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 11.24s
                        Total time: 11808.43s
                               ETA: 1215691.1s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.720s, learning 0.161s)
               Value function loss: 18.9247
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 438.56
               Mean episode length: 245.63
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 11.88s
                        Total time: 11820.31s
                               ETA: 1215638.3s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1463 steps/s (collection: 10.999s, learning 0.193s)
               Value function loss: 19.8982
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 424.39
               Mean episode length: 244.26
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 11.19s
                        Total time: 11831.50s
                               ETA: 1215514.8s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.096s, learning 0.158s)
               Value function loss: 15.8257
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 439.92
               Mean episode length: 243.03
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 11.25s
                        Total time: 11842.75s
                               ETA: 1215398.0s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.257s, learning 0.159s)
               Value function loss: 17.7464
                    Surrogate loss: -0.0233
             Mean action noise std: 0.76
                       Mean reward: 433.21
               Mean episode length: 244.57
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 11.42s
                        Total time: 11854.17s
                               ETA: 1215297.9s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.169s)
               Value function loss: 15.4680
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 451.37
               Mean episode length: 245.95
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 11.29s
                        Total time: 11865.46s
                               ETA: 1215185.2s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.559s, learning 0.194s)
               Value function loss: 18.1149
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 462.18
               Mean episode length: 244.27
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 11.75s
                        Total time: 11877.21s
                               ETA: 1215120.0s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.187s, learning 0.181s)
               Value function loss: 19.6044
                    Surrogate loss: -0.0224
             Mean action noise std: 0.76
                       Mean reward: 457.86
               Mean episode length: 245.67
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 11.37s
                        Total time: 11888.58s
                               ETA: 1215015.6s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.347s, learning 0.159s)
               Value function loss: 15.6675
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 469.85
               Mean episode length: 248.15
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 11.51s
                        Total time: 11900.09s
                               ETA: 1214925.5s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.246s, learning 0.160s)
               Value function loss: 15.7614
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 438.69
               Mean episode length: 243.15
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 11.41s
                        Total time: 11911.50s
                               ETA: 1214825.3s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.160s)
               Value function loss: 18.4469
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 433.02
               Mean episode length: 245.14
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 11.33s
                        Total time: 11922.83s
                               ETA: 1214717.9s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.182s, learning 0.192s)
               Value function loss: 19.3648
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 447.58
               Mean episode length: 246.26
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 11.37s
                        Total time: 11934.20s
                               ETA: 1214614.8s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.165s)
               Value function loss: 16.5487
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 440.62
               Mean episode length: 245.36
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 11.37s
                        Total time: 11945.57s
                               ETA: 1214511.1s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.919s, learning 0.192s)
               Value function loss: 18.9200
                    Surrogate loss: -0.0134
             Mean action noise std: 0.76
                       Mean reward: 459.96
               Mean episode length: 245.42
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 11.11s
                        Total time: 11956.68s
                               ETA: 1214381.6s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.423s, learning 0.180s)
               Value function loss: 21.3854
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 442.30
               Mean episode length: 245.76
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 11.60s
                        Total time: 11968.28s
                               ETA: 1214302.3s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.936s, learning 0.171s)
               Value function loss: 20.6819
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 454.64
               Mean episode length: 247.27
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 11.11s
                        Total time: 11979.39s
                               ETA: 1214172.9s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.235s, learning 0.271s)
               Value function loss: 21.1531
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 448.30
               Mean episode length: 247.25
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 11.51s
                        Total time: 11990.89s
                               ETA: 1214084.2s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.459s, learning 0.163s)
               Value function loss: 17.3814
                    Surrogate loss: -0.0236
             Mean action noise std: 0.76
                       Mean reward: 436.23
               Mean episode length: 245.58
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 11.62s
                        Total time: 12002.52s
                               ETA: 1214007.2s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.123s, learning 0.162s)
               Value function loss: 19.6308
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 440.35
               Mean episode length: 243.88
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 11.28s
                        Total time: 12013.80s
                               ETA: 1213896.4s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.564s, learning 0.165s)
               Value function loss: 18.3589
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 464.05
               Mean episode length: 246.68
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 11.73s
                        Total time: 12025.53s
                               ETA: 1213830.6s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.157s, learning 0.162s)
               Value function loss: 18.2446
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 445.34
               Mean episode length: 245.85
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 11.32s
                        Total time: 12036.85s
                               ETA: 1213723.6s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.279s, learning 0.172s)
               Value function loss: 21.1982
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 466.54
               Mean episode length: 246.29
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 11.45s
                        Total time: 12048.30s
                               ETA: 1213630.2s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.163s, learning 0.163s)
               Value function loss: 18.4440
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 455.25
               Mean episode length: 244.55
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 11.33s
                        Total time: 12059.63s
                               ETA: 1213524.3s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.167s)
               Value function loss: 25.2109
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 456.71
               Mean episode length: 243.75
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 11.38s
                        Total time: 12071.01s
                               ETA: 1213424.3s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.383s, learning 0.166s)
               Value function loss: 18.1300
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 477.39
               Mean episode length: 247.16
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 11.55s
                        Total time: 12082.56s
                               ETA: 1213341.2s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.586s, learning 0.191s)
               Value function loss: 21.4721
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 456.51
               Mean episode length: 245.26
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 11.78s
                        Total time: 12094.33s
                               ETA: 1213281.1s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.618s, learning 0.164s)
               Value function loss: 24.7881
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 442.46
               Mean episode length: 242.36
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 11.78s
                        Total time: 12106.12s
                               ETA: 1213221.6s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.586s, learning 0.245s)
               Value function loss: 15.5820
                    Surrogate loss: -0.0234
             Mean action noise std: 0.76
                       Mean reward: 480.17
               Mean episode length: 248.25
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 11.83s
                        Total time: 12117.95s
                               ETA: 1213167.1s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.568s, learning 0.164s)
               Value function loss: 15.2577
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 462.35
               Mean episode length: 247.05
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 11.73s
                        Total time: 12129.68s
                               ETA: 1213102.8s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.604s, learning 0.160s)
               Value function loss: 19.2545
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 470.41
               Mean episode length: 246.09
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 11.76s
                        Total time: 12141.44s
                               ETA: 1213041.7s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.182s, learning 0.164s)
               Value function loss: 19.4799
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 458.65
               Mean episode length: 243.31
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 11.35s
                        Total time: 12152.79s
                               ETA: 1212939.1s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.162s)
               Value function loss: 16.6108
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 445.24
               Mean episode length: 242.91
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 11.45s
                        Total time: 12164.24s
                               ETA: 1212846.9s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.212s, learning 0.189s)
               Value function loss: 20.1815
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 458.21
               Mean episode length: 246.71
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 11.40s
                        Total time: 12175.64s
                               ETA: 1212750.1s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.709s, learning 0.184s)
               Value function loss: 18.1736
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 468.91
               Mean episode length: 246.34
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 11.89s
                        Total time: 12187.53s
                               ETA: 1212702.3s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.137s, learning 0.164s)
               Value function loss: 19.8575
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 477.95
               Mean episode length: 247.44
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 11.30s
                        Total time: 12198.83s
                               ETA: 1212595.8s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.017s, learning 0.161s)
               Value function loss: 18.9953
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 485.22
               Mean episode length: 248.11
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 11.18s
                        Total time: 12210.01s
                               ETA: 1212477.4s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.476s, learning 0.166s)
               Value function loss: 16.4973
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 483.96
               Mean episode length: 247.35
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 11.64s
                        Total time: 12221.65s
                               ETA: 1212405.1s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.056s, learning 0.181s)
               Value function loss: 21.3129
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 478.96
               Mean episode length: 246.40
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 11.24s
                        Total time: 12232.89s
                               ETA: 1212292.9s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.162s)
               Value function loss: 17.4871
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 457.90
               Mean episode length: 244.09
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 11.29s
                        Total time: 12244.18s
                               ETA: 1212186.4s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.231s, learning 0.171s)
               Value function loss: 18.3014
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 464.94
               Mean episode length: 244.47
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 11.40s
                        Total time: 12255.58s
                               ETA: 1212090.8s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.915s, learning 0.172s)
               Value function loss: 18.4278
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 462.90
               Mean episode length: 244.31
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 11.09s
                        Total time: 12266.67s
                               ETA: 1211964.3s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.301s, learning 0.170s)
               Value function loss: 17.8594
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 466.25
               Mean episode length: 244.24
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 11.47s
                        Total time: 12278.14s
                               ETA: 1211875.9s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.100s, learning 0.160s)
               Value function loss: 19.2512
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 475.15
               Mean episode length: 244.46
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 11.26s
                        Total time: 12289.40s
                               ETA: 1211766.9s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.324s, learning 0.169s)
               Value function loss: 17.8473
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 501.21
               Mean episode length: 248.34
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 11.49s
                        Total time: 12300.90s
                               ETA: 1211681.1s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.170s, learning 0.172s)
               Value function loss: 17.3833
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 484.37
               Mean episode length: 246.69
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 11.34s
                        Total time: 12312.24s
                               ETA: 1211580.5s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.273s, learning 0.168s)
               Value function loss: 13.9702
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 484.40
               Mean episode length: 245.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 11.44s
                        Total time: 12323.68s
                               ETA: 1211489.8s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.734s, learning 0.168s)
               Value function loss: 12.6452
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 499.76
               Mean episode length: 247.77
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.90s
                        Total time: 12335.58s
                               ETA: 1211444.7s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.286s, learning 0.264s)
               Value function loss: 17.8120
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 488.03
               Mean episode length: 246.43
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 11.55s
                        Total time: 12347.13s
                               ETA: 1211365.0s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.069s, learning 0.163s)
               Value function loss: 14.9421
                    Surrogate loss: -0.0234
             Mean action noise std: 0.76
                       Mean reward: 491.30
               Mean episode length: 246.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 11.23s
                        Total time: 12358.36s
                               ETA: 1211254.3s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.238s, learning 0.177s)
               Value function loss: 18.9073
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 507.69
               Mean episode length: 249.10
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 11.41s
                        Total time: 12369.78s
                               ETA: 1211161.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.267s, learning 0.181s)
               Value function loss: 17.0668
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 519.52
               Mean episode length: 249.01
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 11.45s
                        Total time: 12381.23s
                               ETA: 1211072.4s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.113s, learning 0.175s)
               Value function loss: 17.1224
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 498.25
               Mean episode length: 246.62
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 11.29s
                        Total time: 12392.52s
                               ETA: 1210967.7s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.045s, learning 0.177s)
               Value function loss: 17.0485
                    Surrogate loss: -0.0237
             Mean action noise std: 0.76
                       Mean reward: 503.15
               Mean episode length: 246.98
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 11.22s
                        Total time: 12403.74s
                               ETA: 1210856.7s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.268s, learning 0.181s)
               Value function loss: 14.5168
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 498.18
               Mean episode length: 247.09
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 11.45s
                        Total time: 12415.19s
                               ETA: 1210768.0s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.453s, learning 0.173s)
               Value function loss: 16.6405
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 504.27
               Mean episode length: 246.76
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 11.63s
                        Total time: 12426.81s
                               ETA: 1210696.8s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.448s, learning 0.171s)
               Value function loss: 15.8120
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 504.63
               Mean episode length: 247.14
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 11.62s
                        Total time: 12438.43s
                               ETA: 1210625.1s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.112s, learning 0.165s)
               Value function loss: 16.9751
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 497.31
               Mean episode length: 247.21
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 11.28s
                        Total time: 12449.71s
                               ETA: 1210520.2s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.421s, learning 0.172s)
               Value function loss: 18.3328
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 504.52
               Mean episode length: 248.17
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 11.59s
                        Total time: 12461.30s
                               ETA: 1210446.2s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.237s, learning 0.158s)
               Value function loss: 17.0923
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 518.87
               Mean episode length: 248.84
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 11.40s
                        Total time: 12472.70s
                               ETA: 1210353.0s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.205s, learning 0.172s)
               Value function loss: 17.5050
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 521.31
               Mean episode length: 248.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 11.38s
                        Total time: 12484.07s
                               ETA: 1210258.3s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.342s, learning 0.162s)
               Value function loss: 21.1597
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 514.83
               Mean episode length: 248.13
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 11.50s
                        Total time: 12495.58s
                               ETA: 1210176.0s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.507s, learning 0.165s)
               Value function loss: 17.5217
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 496.78
               Mean episode length: 246.79
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 11.67s
                        Total time: 12507.25s
                               ETA: 1210110.1s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.285s, learning 0.164s)
               Value function loss: 19.7620
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 490.05
               Mean episode length: 245.60
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 11.45s
                        Total time: 12518.70s
                               ETA: 1210022.8s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.523s, learning 0.165s)
               Value function loss: 18.5057
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 510.38
               Mean episode length: 248.42
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 11.69s
                        Total time: 12530.39s
                               ETA: 1209958.7s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.123s, learning 0.170s)
               Value function loss: 18.9326
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 506.33
               Mean episode length: 246.30
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 11.29s
                        Total time: 12541.68s
                               ETA: 1209856.6s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.160s)
               Value function loss: 19.4784
                    Surrogate loss: -0.0182
             Mean action noise std: 0.76
                       Mean reward: 492.60
               Mean episode length: 246.40
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 11.47s
                        Total time: 12553.16s
                               ETA: 1209772.2s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.130s, learning 0.160s)
               Value function loss: 16.3562
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 494.40
               Mean episode length: 246.48
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 11.29s
                        Total time: 12564.45s
                               ETA: 1209670.1s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.192s, learning 0.191s)
               Value function loss: 14.1887
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 498.59
               Mean episode length: 247.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 11.38s
                        Total time: 12575.83s
                               ETA: 1209577.1s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.221s, learning 0.163s)
               Value function loss: 16.3230
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 488.70
               Mean episode length: 247.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 11.38s
                        Total time: 12587.21s
                               ETA: 1209484.5s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.039s, learning 0.159s)
               Value function loss: 18.6296
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 494.71
               Mean episode length: 246.45
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 11.20s
                        Total time: 12598.41s
                               ETA: 1209374.1s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.165s)
               Value function loss: 16.8325
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 497.68
               Mean episode length: 247.41
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 11.52s
                        Total time: 12609.93s
                               ETA: 1209294.4s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.558s, learning 0.164s)
               Value function loss: 18.5673
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 506.19
               Mean episode length: 246.41
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 11.72s
                        Total time: 12621.65s
                               ETA: 1209234.5s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.225s, learning 0.171s)
               Value function loss: 18.1243
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 521.86
               Mean episode length: 248.96
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 11.40s
                        Total time: 12633.04s
                               ETA: 1209143.6s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.905s, learning 0.163s)
               Value function loss: 19.3048
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 528.37
               Mean episode length: 249.04
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 11.07s
                        Total time: 12644.11s
                               ETA: 1209021.4s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.197s, learning 0.171s)
               Value function loss: 13.3831
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 510.79
               Mean episode length: 249.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 11.37s
                        Total time: 12655.48s
                               ETA: 1208928.2s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.224s, learning 0.175s)
               Value function loss: 15.6608
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 507.09
               Mean episode length: 248.39
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 11.40s
                        Total time: 12666.88s
                               ETA: 1208838.0s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.665s, learning 0.164s)
               Value function loss: 15.3944
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 513.05
               Mean episode length: 248.31
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 11.83s
                        Total time: 12678.71s
                               ETA: 1208789.0s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.089s, learning 0.169s)
               Value function loss: 11.3531
                    Surrogate loss: -0.0238
             Mean action noise std: 0.76
                       Mean reward: 513.96
               Mean episode length: 248.50
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 11.26s
                        Total time: 12689.97s
                               ETA: 1208685.7s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.887s, learning 0.160s)
               Value function loss: 15.9531
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 512.27
               Mean episode length: 248.32
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 11.05s
                        Total time: 12701.01s
                               ETA: 1208562.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.394s, learning 0.156s)
               Value function loss: 14.2664
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 515.42
               Mean episode length: 248.72
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 11.55s
                        Total time: 12712.56s
                               ETA: 1208487.3s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.349s, learning 0.183s)
               Value function loss: 15.4480
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 530.40
               Mean episode length: 249.88
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 11.53s
                        Total time: 12724.09s
                               ETA: 1208410.5s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.581s, learning 0.164s)
               Value function loss: 14.7505
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 525.33
               Mean episode length: 249.88
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 11.74s
                        Total time: 12735.84s
                               ETA: 1208354.0s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.240s, learning 0.162s)
               Value function loss: 19.2619
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 521.99
               Mean episode length: 249.49
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 11.40s
                        Total time: 12747.24s
                               ETA: 1208265.1s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.486s, learning 0.163s)
               Value function loss: 17.9996
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 526.55
               Mean episode length: 249.69
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 11.65s
                        Total time: 12758.89s
                               ETA: 1208199.9s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.087s, learning 0.175s)
               Value function loss: 14.2835
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 524.59
               Mean episode length: 249.25
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 11.26s
                        Total time: 12770.15s
                               ETA: 1208098.1s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.176s, learning 0.192s)
               Value function loss: 16.5993
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 505.10
               Mean episode length: 246.38
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 11.37s
                        Total time: 12781.52s
                               ETA: 1208006.5s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.302s, learning 0.175s)
               Value function loss: 16.3836
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 505.59
               Mean episode length: 243.43
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 11.48s
                        Total time: 12793.00s
                               ETA: 1207925.3s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.163s)
               Value function loss: 19.1336
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 520.33
               Mean episode length: 247.04
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 11.45s
                        Total time: 12804.45s
                               ETA: 1207842.0s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.383s, learning 0.159s)
               Value function loss: 16.7339
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 522.00
               Mean episode length: 249.22
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 11.54s
                        Total time: 12815.99s
                               ETA: 1207767.2s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.103s, learning 0.160s)
               Value function loss: 17.8301
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 508.18
               Mean episode length: 247.28
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 11.26s
                        Total time: 12827.26s
                               ETA: 1207666.2s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.118s, learning 0.187s)
               Value function loss: 15.2305
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 518.13
               Mean episode length: 247.91
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 11.30s
                        Total time: 12838.56s
                               ETA: 1207569.3s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.387s, learning 0.168s)
               Value function loss: 17.9648
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 515.59
               Mean episode length: 249.10
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 11.55s
                        Total time: 12850.12s
                               ETA: 1207496.1s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.222s, learning 0.170s)
               Value function loss: 18.8719
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 533.77
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 11.39s
                        Total time: 12861.51s
                               ETA: 1207407.7s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.823s, learning 0.161s)
               Value function loss: 21.2623
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 535.62
               Mean episode length: 249.01
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 11.98s
                        Total time: 12873.49s
                               ETA: 1207374.9s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.106s, learning 0.187s)
               Value function loss: 23.3233
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 529.56
               Mean episode length: 247.66
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 11.29s
                        Total time: 12884.79s
                               ETA: 1207277.6s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.213s, learning 0.159s)
               Value function loss: 21.0230
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 532.01
               Mean episode length: 249.27
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 11.37s
                        Total time: 12896.16s
                               ETA: 1207187.7s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.778s, learning 0.166s)
               Value function loss: 22.6727
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 530.07
               Mean episode length: 249.23
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 10.94s
                        Total time: 12907.10s
                               ETA: 1207057.9s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.396s, learning 0.189s)
               Value function loss: 20.0881
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 531.98
               Mean episode length: 249.26
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 11.58s
                        Total time: 12918.69s
                               ETA: 1206988.3s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.177s, learning 0.162s)
               Value function loss: 18.6458
                    Surrogate loss: -0.0131
             Mean action noise std: 0.76
                       Mean reward: 535.80
               Mean episode length: 249.42
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 11.34s
                        Total time: 12930.03s
                               ETA: 1206895.9s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.160s)
               Value function loss: 18.0493
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 530.52
               Mean episode length: 249.34
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 11.12s
                        Total time: 12941.15s
                               ETA: 1206783.5s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.484s, learning 0.167s)
               Value function loss: 21.3464
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 529.48
               Mean episode length: 249.77
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 11.65s
                        Total time: 12952.80s
                               ETA: 1206720.3s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.188s, learning 0.182s)
               Value function loss: 22.2423
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 525.50
               Mean episode length: 249.66
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 11.37s
                        Total time: 12964.17s
                               ETA: 1206631.2s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.592s, learning 0.163s)
               Value function loss: 21.7702
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 530.58
               Mean episode length: 249.88
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 11.75s
                        Total time: 12975.92s
                               ETA: 1206577.9s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.305s, learning 0.195s)
               Value function loss: 20.5361
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 538.31
               Mean episode length: 249.46
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 11.50s
                        Total time: 12987.42s
                               ETA: 1206501.1s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.295s, learning 0.173s)
               Value function loss: 18.5576
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 538.83
               Mean episode length: 249.14
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 11.47s
                        Total time: 12998.89s
                               ETA: 1206421.5s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.366s, learning 0.162s)
               Value function loss: 17.9818
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 538.23
               Mean episode length: 249.49
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 11.53s
                        Total time: 13010.42s
                               ETA: 1206347.6s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.225s, learning 0.162s)
               Value function loss: 12.8394
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 522.90
               Mean episode length: 249.31
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 11.39s
                        Total time: 13021.81s
                               ETA: 1206260.7s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.407s, learning 0.163s)
               Value function loss: 14.7081
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 518.11
               Mean episode length: 248.02
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 11.57s
                        Total time: 13033.38s
                               ETA: 1206190.8s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.167s)
               Value function loss: 15.8622
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 530.86
               Mean episode length: 248.98
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 11.18s
                        Total time: 13044.56s
                               ETA: 1206085.3s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.164s)
               Value function loss: 18.0952
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 532.94
               Mean episode length: 248.64
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 11.52s
                        Total time: 13056.07s
                               ETA: 1206010.7s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.673s, learning 0.163s)
               Value function loss: 15.3978
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 536.55
               Mean episode length: 249.38
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 11.84s
                        Total time: 13067.91s
                               ETA: 1205965.8s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.169s)
               Value function loss: 15.0063
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 540.44
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 11.43s
                        Total time: 13079.34s
                               ETA: 1205883.5s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.911s, learning 0.168s)
               Value function loss: 16.5864
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 537.06
               Mean episode length: 249.55
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 11.08s
                        Total time: 13090.42s
                               ETA: 1205769.0s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.160s, learning 0.186s)
               Value function loss: 20.8189
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 531.96
               Mean episode length: 248.97
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 11.35s
                        Total time: 13101.77s
                               ETA: 1205679.3s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.242s, learning 0.164s)
               Value function loss: 15.8001
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 548.10
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 11.41s
                        Total time: 13113.17s
                               ETA: 1205595.2s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.132s, learning 0.197s)
               Value function loss: 17.3341
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 546.86
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 11.33s
                        Total time: 13124.50s
                               ETA: 1205504.3s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.691s, learning 0.170s)
               Value function loss: 16.5714
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 546.43
               Mean episode length: 249.48
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 10.86s
                        Total time: 13135.36s
                               ETA: 1205370.5s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.028s, learning 0.174s)
               Value function loss: 17.8503
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 546.64
               Mean episode length: 248.78
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 11.20s
                        Total time: 13146.57s
                               ETA: 1205268.3s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.580s, learning 0.164s)
               Value function loss: 19.5063
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 551.44
               Mean episode length: 249.28
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 10.74s
                        Total time: 13157.31s
                               ETA: 1205124.3s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.116s, learning 0.184s)
               Value function loss: 19.2382
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 551.25
               Mean episode length: 248.82
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 11.30s
                        Total time: 13168.61s
                               ETA: 1205031.3s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.478s, learning 0.161s)
               Value function loss: 23.6854
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 555.61
               Mean episode length: 249.08
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 11.64s
                        Total time: 13180.25s
                               ETA: 1204969.5s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.267s, learning 0.187s)
               Value function loss: 14.8668
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 556.64
               Mean episode length: 249.68
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 11.45s
                        Total time: 13191.70s
                               ETA: 1204890.9s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.733s, learning 0.163s)
               Value function loss: 15.9986
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 546.53
               Mean episode length: 248.36
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 10.90s
                        Total time: 13202.60s
                               ETA: 1204761.5s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.168s, learning 0.172s)
               Value function loss: 21.5218
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 536.55
               Mean episode length: 247.45
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 11.34s
                        Total time: 13213.94s
                               ETA: 1204672.8s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.923s, learning 0.167s)
               Value function loss: 21.1594
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 555.47
               Mean episode length: 249.52
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 11.09s
                        Total time: 13225.03s
                               ETA: 1204561.4s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.163s)
               Value function loss: 19.2739
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 555.32
               Mean episode length: 249.32
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 11.48s
                        Total time: 13236.51s
                               ETA: 1204485.7s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.935s, learning 0.187s)
               Value function loss: 18.9388
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 545.18
               Mean episode length: 249.37
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 11.12s
                        Total time: 13247.63s
                               ETA: 1204377.6s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.164s)
               Value function loss: 16.4735
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 544.84
               Mean episode length: 249.45
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 11.64s
                        Total time: 13259.26s
                               ETA: 1204316.3s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.165s)
               Value function loss: 21.2665
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 557.41
               Mean episode length: 249.70
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 11.40s
                        Total time: 13270.66s
                               ETA: 1204233.5s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.162s)
               Value function loss: 17.6313
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 548.36
               Mean episode length: 247.68
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 11.40s
                        Total time: 13282.07s
                               ETA: 1204151.4s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.771s, learning 0.189s)
               Value function loss: 16.7539
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 548.07
               Mean episode length: 247.33
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 10.96s
                        Total time: 13293.03s
                               ETA: 1204029.3s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.167s)
               Value function loss: 18.7208
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 549.43
               Mean episode length: 248.62
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 11.38s
                        Total time: 13304.41s
                               ETA: 1203945.5s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.125s, learning 0.177s)
               Value function loss: 22.6979
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 547.08
               Mean episode length: 249.03
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 11.30s
                        Total time: 13315.71s
                               ETA: 1203854.6s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.823s, learning 0.167s)
               Value function loss: 19.7026
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 558.71
               Mean episode length: 249.49
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 10.99s
                        Total time: 13326.70s
                               ETA: 1203735.7s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.077s, learning 0.170s)
               Value function loss: 19.1716
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 550.61
               Mean episode length: 249.88
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 11.25s
                        Total time: 13337.95s
                               ETA: 1203640.1s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.779s, learning 0.166s)
               Value function loss: 20.7739
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 559.72
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 10.94s
                        Total time: 13348.89s
                               ETA: 1203517.5s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.966s, learning 0.170s)
               Value function loss: 23.4108
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 554.45
               Mean episode length: 249.86
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 11.14s
                        Total time: 13360.03s
                               ETA: 1203412.3s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.189s, learning 0.264s)
               Value function loss: 17.0751
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 564.57
               Mean episode length: 249.96
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 11.45s
                        Total time: 13371.48s
                               ETA: 1203335.8s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.938s, learning 0.177s)
               Value function loss: 19.3658
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 559.87
               Mean episode length: 249.17
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 11.12s
                        Total time: 13382.59s
                               ETA: 1203229.1s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.028s, learning 0.180s)
               Value function loss: 16.6043
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 550.38
               Mean episode length: 248.61
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 11.21s
                        Total time: 13393.80s
                               ETA: 1203130.9s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.517s, learning 0.165s)
               Value function loss: 16.9007
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 545.35
               Mean episode length: 248.16
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 11.68s
                        Total time: 13405.49s
                               ETA: 1203075.4s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.170s, learning 0.165s)
               Value function loss: 19.4907
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 550.32
               Mean episode length: 249.47
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 11.33s
                        Total time: 13416.82s
                               ETA: 1202988.8s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.467s, learning 0.170s)
               Value function loss: 16.9960
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 552.67
               Mean episode length: 249.26
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 11.64s
                        Total time: 13428.46s
                               ETA: 1202929.4s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.270s, learning 0.259s)
               Value function loss: 24.5687
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 549.10
               Mean episode length: 248.83
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 11.53s
                        Total time: 13439.99s
                               ETA: 1202860.5s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.754s, learning 0.160s)
               Value function loss: 22.4927
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 551.96
               Mean episode length: 248.91
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 10.91s
                        Total time: 13450.90s
                               ETA: 1202736.7s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.173s, learning 0.170s)
               Value function loss: 18.5202
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 553.70
               Mean episode length: 249.48
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 11.34s
                        Total time: 13462.24s
                               ETA: 1202651.4s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.092s, learning 0.161s)
               Value function loss: 23.3661
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 551.00
               Mean episode length: 247.93
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 11.25s
                        Total time: 13473.50s
                               ETA: 1202558.2s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.376s, learning 0.167s)
               Value function loss: 21.5931
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 558.77
               Mean episode length: 248.45
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 11.54s
                        Total time: 13485.04s
                               ETA: 1202491.1s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.100s, learning 0.174s)
               Value function loss: 25.5556
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 557.00
               Mean episode length: 249.59
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 11.27s
                        Total time: 13496.32s
                               ETA: 1202400.1s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.488s, learning 0.183s)
               Value function loss: 18.2252
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 560.38
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 11.67s
                        Total time: 13507.99s
                               ETA: 1202344.5s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.968s, learning 0.168s)
               Value function loss: 23.9448
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 566.24
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 11.14s
                        Total time: 13519.12s
                               ETA: 1202241.5s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.069s, learning 0.172s)
               Value function loss: 24.9797
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 552.90
               Mean episode length: 249.04
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 11.24s
                        Total time: 13530.36s
                               ETA: 1202148.0s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.296s, learning 0.176s)
               Value function loss: 18.6503
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 560.60
               Mean episode length: 249.40
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 11.47s
                        Total time: 13541.84s
                               ETA: 1202075.0s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.556s, learning 0.180s)
               Value function loss: 18.5812
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 556.30
               Mean episode length: 249.40
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 11.74s
                        Total time: 13553.57s
                               ETA: 1202025.6s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.703s, learning 0.157s)
               Value function loss: 28.6445
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 556.67
               Mean episode length: 249.49
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 10.86s
                        Total time: 13564.43s
                               ETA: 1201898.6s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.164s)
               Value function loss: 23.2562
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 566.14
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 11.52s
                        Total time: 13575.96s
                               ETA: 1201830.7s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.700s, learning 0.158s)
               Value function loss: 20.0862
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 556.12
               Mean episode length: 249.41
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 11.86s
                        Total time: 13587.81s
                               ETA: 1201792.3s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.120s, learning 0.168s)
               Value function loss: 20.1093
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 561.88
               Mean episode length: 249.88
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 11.29s
                        Total time: 13599.10s
                               ETA: 1201703.7s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.164s)
               Value function loss: 20.5066
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 569.76
               Mean episode length: 249.50
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 11.40s
                        Total time: 13610.50s
                               ETA: 1201624.8s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.503s, learning 0.195s)
               Value function loss: 25.2698
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 569.81
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 11.70s
                        Total time: 13622.20s
                               ETA: 1201572.6s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.432s, learning 0.159s)
               Value function loss: 22.2617
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 561.67
               Mean episode length: 249.88
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 11.59s
                        Total time: 13633.79s
                               ETA: 1201511.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.312s, learning 0.165s)
               Value function loss: 15.0959
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 564.63
               Mean episode length: 249.57
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 11.48s
                        Total time: 13645.27s
                               ETA: 1201439.5s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.368s, learning 0.172s)
               Value function loss: 21.5564
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 556.87
               Mean episode length: 249.28
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 11.54s
                        Total time: 13656.80s
                               ETA: 1201373.6s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.132s, learning 0.187s)
               Value function loss: 20.2753
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 555.91
               Mean episode length: 249.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 11.32s
                        Total time: 13668.12s
                               ETA: 1201288.3s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.333s, learning 0.166s)
               Value function loss: 22.1995
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 559.31
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 11.50s
                        Total time: 13679.62s
                               ETA: 1201219.1s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.692s, learning 0.192s)
               Value function loss: 22.5044
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 566.21
               Mean episode length: 249.73
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 11.88s
                        Total time: 13691.51s
                               ETA: 1201183.7s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.163s)
               Value function loss: 22.3236
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 563.40
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 11.36s
                        Total time: 13702.86s
                               ETA: 1201102.3s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.131s, learning 0.165s)
               Value function loss: 24.1100
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 568.04
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 11.30s
                        Total time: 13714.16s
                               ETA: 1201015.5s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.172s, learning 0.169s)
               Value function loss: 16.0158
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 559.73
               Mean episode length: 248.86
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 11.34s
                        Total time: 13725.50s
                               ETA: 1200932.8s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.241s, learning 0.198s)
               Value function loss: 18.7240
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 559.82
               Mean episode length: 249.43
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 11.44s
                        Total time: 13736.94s
                               ETA: 1200858.9s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.254s, learning 0.190s)
               Value function loss: 16.9192
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 565.68
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 11.44s
                        Total time: 13748.38s
                               ETA: 1200785.4s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.202s, learning 0.167s)
               Value function loss: 15.5667
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 557.69
               Mean episode length: 248.04
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 11.37s
                        Total time: 13759.75s
                               ETA: 1200705.5s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.101s, learning 0.270s)
               Value function loss: 23.2844
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 551.01
               Mean episode length: 248.95
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 11.37s
                        Total time: 13771.12s
                               ETA: 1200625.9s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.158s, learning 0.188s)
               Value function loss: 14.9782
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 550.58
               Mean episode length: 248.41
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 11.35s
                        Total time: 13782.47s
                               ETA: 1200544.3s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.162s)
               Value function loss: 20.3783
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 557.61
               Mean episode length: 249.11
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 11.36s
                        Total time: 13793.83s
                               ETA: 1200463.7s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.210s, learning 0.166s)
               Value function loss: 22.6177
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 566.45
               Mean episode length: 249.40
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 11.38s
                        Total time: 13805.20s
                               ETA: 1200384.9s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.162s)
               Value function loss: 20.8005
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 567.34
               Mean episode length: 249.31
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 11.17s
                        Total time: 13816.38s
                               ETA: 1200288.8s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.233s, learning 0.174s)
               Value function loss: 22.5065
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 563.62
               Mean episode length: 249.23
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 11.41s
                        Total time: 13827.79s
                               ETA: 1200212.9s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.181s, learning 0.266s)
               Value function loss: 17.0418
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 570.12
               Mean episode length: 249.56
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 11.45s
                        Total time: 13839.23s
                               ETA: 1200140.6s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.282s, learning 0.178s)
               Value function loss: 22.3131
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 563.52
               Mean episode length: 248.90
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 11.46s
                        Total time: 13850.69s
                               ETA: 1200069.6s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.160s)
               Value function loss: 22.0259
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 564.45
               Mean episode length: 249.18
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 11.30s
                        Total time: 13861.99s
                               ETA: 1199985.0s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.027s, learning 0.165s)
               Value function loss: 25.4744
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 562.63
               Mean episode length: 249.66
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 11.19s
                        Total time: 13873.19s
                               ETA: 1199891.1s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.294s, learning 0.161s)
               Value function loss: 24.9071
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 554.90
               Mean episode length: 248.39
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 11.45s
                        Total time: 13884.64s
                               ETA: 1199819.9s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.002s, learning 0.198s)
               Value function loss: 21.5617
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 572.12
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 11.20s
                        Total time: 13895.84s
                               ETA: 1199726.9s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.292s, learning 0.177s)
               Value function loss: 20.2089
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 565.30
               Mean episode length: 249.72
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 11.47s
                        Total time: 13907.31s
                               ETA: 1199657.1s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.974s, learning 0.182s)
               Value function loss: 30.9045
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 572.49
               Mean episode length: 249.92
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 11.16s
                        Total time: 13918.46s
                               ETA: 1199560.5s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.143s, learning 0.159s)
               Value function loss: 27.5835
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 561.52
               Mean episode length: 248.77
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 11.30s
                        Total time: 13929.77s
                               ETA: 1199476.7s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.445s, learning 0.163s)
               Value function loss: 27.6029
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 566.70
               Mean episode length: 248.37
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 11.61s
                        Total time: 13941.37s
                               ETA: 1199419.3s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.162s)
               Value function loss: 25.5404
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 572.65
               Mean episode length: 249.36
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 11.50s
                        Total time: 13952.88s
                               ETA: 1199352.8s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.304s, learning 0.165s)
               Value function loss: 29.6689
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 567.41
               Mean episode length: 249.03
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 11.47s
                        Total time: 13964.34s
                               ETA: 1199283.6s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.240s, learning 0.161s)
               Value function loss: 25.8622
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 557.45
               Mean episode length: 248.63
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 11.40s
                        Total time: 13975.75s
                               ETA: 1199208.7s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.556s, learning 0.171s)
               Value function loss: 28.7296
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 556.86
               Mean episode length: 248.81
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 11.73s
                        Total time: 13987.47s
                               ETA: 1199161.9s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.676s, learning 0.162s)
               Value function loss: 24.2182
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 555.69
               Mean episode length: 248.43
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 11.84s
                        Total time: 13999.31s
                               ETA: 1199124.6s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.270s, learning 0.161s)
               Value function loss: 26.4294
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 565.37
               Mean episode length: 248.93
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 11.43s
                        Total time: 14010.74s
                               ETA: 1199052.6s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.163s)
               Value function loss: 30.8660
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 553.38
               Mean episode length: 248.06
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 11.39s
                        Total time: 14022.13s
                               ETA: 1198977.1s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.223s, learning 0.159s)
               Value function loss: 24.3457
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 562.23
               Mean episode length: 248.90
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 11.38s
                        Total time: 14033.51s
                               ETA: 1198901.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.165s)
               Value function loss: 25.7654
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 568.13
               Mean episode length: 249.14
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 11.49s
                        Total time: 14045.00s
                               ETA: 1198834.0s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.362s, learning 0.182s)
               Value function loss: 26.9166
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 558.09
               Mean episode length: 248.99
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 11.54s
                        Total time: 14056.54s
                               ETA: 1198772.1s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.348s, learning 0.171s)
               Value function loss: 30.9002
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 550.53
               Mean episode length: 247.65
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 11.52s
                        Total time: 14068.06s
                               ETA: 1198708.0s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.501s, learning 0.168s)
               Value function loss: 23.9834
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 545.91
               Mean episode length: 247.24
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 11.67s
                        Total time: 14079.73s
                               ETA: 1198656.8s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.377s, learning 0.166s)
               Value function loss: 30.6116
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 551.88
               Mean episode length: 247.51
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 11.54s
                        Total time: 14091.27s
                               ETA: 1198595.0s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.478s, learning 0.163s)
               Value function loss: 24.1904
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 555.88
               Mean episode length: 248.02
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 11.64s
                        Total time: 14102.92s
                               ETA: 1198541.6s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.288s, learning 0.188s)
               Value function loss: 20.6949
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 560.12
               Mean episode length: 249.46
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 11.48s
                        Total time: 14114.39s
                               ETA: 1198474.3s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.085s, learning 0.172s)
               Value function loss: 28.2628
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 561.58
               Mean episode length: 249.70
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 11.26s
                        Total time: 14125.65s
                               ETA: 1198388.4s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.159s)
               Value function loss: 21.6371
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 552.90
               Mean episode length: 247.80
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 11.33s
                        Total time: 14136.98s
                               ETA: 1198309.3s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.160s)
               Value function loss: 23.2132
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 554.28
               Mean episode length: 248.46
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 11.39s
                        Total time: 14148.37s
                               ETA: 1198234.7s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.161s, learning 0.159s)
               Value function loss: 25.0347
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 554.51
               Mean episode length: 247.44
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 11.32s
                        Total time: 14159.69s
                               ETA: 1198154.5s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.384s, learning 0.187s)
               Value function loss: 30.2854
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 554.72
               Mean episode length: 249.40
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 11.57s
                        Total time: 14171.26s
                               ETA: 1198095.8s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.159s)
               Value function loss: 26.4108
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 551.63
               Mean episode length: 248.75
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 11.37s
                        Total time: 14182.63s
                               ETA: 1198019.7s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.124s, learning 0.268s)
               Value function loss: 22.6506
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 546.48
               Mean episode length: 248.67
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 11.39s
                        Total time: 14194.02s
                               ETA: 1197946.0s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.165s)
               Value function loss: 21.2282
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 551.12
               Mean episode length: 248.72
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 11.27s
                        Total time: 14205.29s
                               ETA: 1197862.1s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.251s, learning 0.159s)
               Value function loss: 22.2060
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 545.10
               Mean episode length: 248.01
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 11.41s
                        Total time: 14216.70s
                               ETA: 1197790.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.325s, learning 0.195s)
               Value function loss: 28.5250
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 540.16
               Mean episode length: 247.24
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 11.52s
                        Total time: 14228.22s
                               ETA: 1197727.5s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.229s, learning 0.171s)
               Value function loss: 26.9772
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 551.19
               Mean episode length: 248.58
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 11.40s
                        Total time: 14239.62s
                               ETA: 1197654.9s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.159s)
               Value function loss: 20.1441
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 542.47
               Mean episode length: 248.52
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 11.30s
                        Total time: 14250.92s
                               ETA: 1197573.9s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.166s)
               Value function loss: 19.6480
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 550.42
               Mean episode length: 248.43
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 11.43s
                        Total time: 14262.34s
                               ETA: 1197503.8s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.221s, learning 0.169s)
               Value function loss: 22.5523
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 542.52
               Mean episode length: 247.78
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 11.39s
                        Total time: 14273.73s
                               ETA: 1197430.6s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.230s, learning 0.162s)
               Value function loss: 22.7336
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 547.94
               Mean episode length: 249.15
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 11.39s
                        Total time: 14285.13s
                               ETA: 1197357.8s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.845s, learning 0.161s)
               Value function loss: 23.6347
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 548.71
               Mean episode length: 247.59
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 11.01s
                        Total time: 14296.13s
                               ETA: 1197252.6s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.349s, learning 0.160s)
               Value function loss: 29.4690
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 548.60
               Mean episode length: 247.41
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 11.51s
                        Total time: 14307.64s
                               ETA: 1197189.8s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.306s, learning 0.181s)
               Value function loss: 26.1543
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 544.24
               Mean episode length: 246.81
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 11.49s
                        Total time: 14319.13s
                               ETA: 1197125.2s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.868s, learning 0.189s)
               Value function loss: 26.4854
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 554.82
               Mean episode length: 247.70
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 11.06s
                        Total time: 14330.19s
                               ETA: 1197024.8s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.160s, learning 0.160s)
               Value function loss: 21.1822
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 543.96
               Mean episode length: 246.03
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 11.32s
                        Total time: 14341.51s
                               ETA: 1196946.4s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.973s, learning 0.167s)
               Value function loss: 21.2987
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 550.81
               Mean episode length: 248.13
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 11.14s
                        Total time: 14352.65s
                               ETA: 1196853.2s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.592s, learning 0.160s)
               Value function loss: 20.9372
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 544.58
               Mean episode length: 246.32
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 11.75s
                        Total time: 14364.40s
                               ETA: 1196811.1s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.946s, learning 0.160s)
               Value function loss: 22.1206
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 546.85
               Mean episode length: 247.01
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 11.11s
                        Total time: 14375.50s
                               ETA: 1196715.3s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.881s, learning 0.186s)
               Value function loss: 19.7813
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 547.67
               Mean episode length: 246.58
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 12.07s
                        Total time: 14387.57s
                               ETA: 1196699.5s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.185s, learning 0.163s)
               Value function loss: 20.0278
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 538.22
               Mean episode length: 245.60
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 11.35s
                        Total time: 14398.92s
                               ETA: 1196624.0s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.164s)
               Value function loss: 17.0342
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 541.10
               Mean episode length: 245.65
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 11.37s
                        Total time: 14410.29s
                               ETA: 1196550.8s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.203s, learning 0.164s)
               Value function loss: 20.0595
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 540.64
               Mean episode length: 245.77
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 11.37s
                        Total time: 14421.66s
                               ETA: 1196477.0s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.162s)
               Value function loss: 19.0339
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 532.47
               Mean episode length: 243.64
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 11.17s
                        Total time: 14432.83s
                               ETA: 1196387.4s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.446s, learning 0.159s)
               Value function loss: 15.4748
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 538.97
               Mean episode length: 246.02
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 11.61s
                        Total time: 14444.44s
                               ETA: 1196333.6s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.281s, learning 0.164s)
               Value function loss: 14.7060
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 546.40
               Mean episode length: 247.65
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 11.45s
                        Total time: 14455.88s
                               ETA: 1196266.7s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.817s, learning 0.157s)
               Value function loss: 15.3298
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 534.89
               Mean episode length: 244.53
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 10.97s
                        Total time: 14466.86s
                               ETA: 1196161.0s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.532s, learning 0.164s)
               Value function loss: 14.9357
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 540.05
               Mean episode length: 245.98
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 11.70s
                        Total time: 14478.55s
                               ETA: 1196115.0s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.395s, learning 0.159s)
               Value function loss: 15.8513
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 546.62
               Mean episode length: 249.81
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 11.55s
                        Total time: 14490.11s
                               ETA: 1196057.3s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.878s, learning 0.160s)
               Value function loss: 14.5422
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 528.01
               Mean episode length: 244.52
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 11.04s
                        Total time: 14501.15s
                               ETA: 1195957.2s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.160s)
               Value function loss: 15.1173
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 526.76
               Mean episode length: 244.83
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 11.45s
                        Total time: 14512.59s
                               ETA: 1195890.9s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.159s)
               Value function loss: 14.8672
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 515.38
               Mean episode length: 243.70
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 10.87s
                        Total time: 14523.46s
                               ETA: 1195776.9s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.163s)
               Value function loss: 14.0799
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 514.81
               Mean episode length: 244.22
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 11.42s
                        Total time: 14534.88s
                               ETA: 1195708.9s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.354s, learning 0.159s)
               Value function loss: 15.1005
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 511.55
               Mean episode length: 242.83
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 11.51s
                        Total time: 14546.40s
                               ETA: 1195648.3s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.460s, learning 0.159s)
               Value function loss: 14.9464
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 519.14
               Mean episode length: 245.53
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 11.62s
                        Total time: 14558.01s
                               ETA: 1195596.6s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.168s)
               Value function loss: 16.0957
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 517.94
               Mean episode length: 247.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 11.38s
                        Total time: 14569.40s
                               ETA: 1195525.5s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.357s, learning 0.164s)
               Value function loss: 14.8579
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 514.32
               Mean episode length: 245.99
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 11.52s
                        Total time: 14580.92s
                               ETA: 1195465.8s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.403s, learning 0.173s)
               Value function loss: 16.6606
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 517.26
               Mean episode length: 244.77
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 11.58s
                        Total time: 14592.49s
                               ETA: 1195410.8s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.979s, learning 0.179s)
               Value function loss: 15.7412
                    Surrogate loss: 0.0138
             Mean action noise std: 0.75
                       Mean reward: 518.76
               Mean episode length: 245.60
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 11.16s
                        Total time: 14603.65s
                               ETA: 1195321.6s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.776s, learning 0.171s)
               Value function loss: 13.6711
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 512.22
               Mean episode length: 245.52
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 10.95s
                        Total time: 14614.60s
                               ETA: 1195215.3s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.511s, learning 0.163s)
               Value function loss: 14.4254
                    Surrogate loss: 0.0043
             Mean action noise std: 0.75
                       Mean reward: 506.19
               Mean episode length: 244.49
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 11.67s
                        Total time: 14626.27s
                               ETA: 1195168.5s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.020s, learning 0.178s)
               Value function loss: 15.1168
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 498.49
               Mean episode length: 244.50
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 11.20s
                        Total time: 14637.47s
                               ETA: 1195082.9s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.050s, learning 0.181s)
               Value function loss: 14.1551
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 483.19
               Mean episode length: 244.39
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 11.23s
                        Total time: 14648.70s
                               ETA: 1195000.1s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.375s, learning 0.178s)
               Value function loss: 13.7773
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 467.70
               Mean episode length: 240.10
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 11.55s
                        Total time: 14660.25s
                               ETA: 1194943.8s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.268s, learning 0.170s)
               Value function loss: 12.3916
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 470.06
               Mean episode length: 240.70
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 11.44s
                        Total time: 14671.69s
                               ETA: 1194878.1s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.929s, learning 0.160s)
               Value function loss: 13.8533
                    Surrogate loss: 0.0020
             Mean action noise std: 0.75
                       Mean reward: 489.85
               Mean episode length: 244.12
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 11.09s
                        Total time: 14682.78s
                               ETA: 1194784.1s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.466s, learning 0.163s)
               Value function loss: 11.7241
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 482.51
               Mean episode length: 243.04
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 11.63s
                        Total time: 14694.41s
                               ETA: 1194734.2s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.375s, learning 0.168s)
               Value function loss: 12.8204
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 462.87
               Mean episode length: 241.18
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 11.54s
                        Total time: 14705.95s
                               ETA: 1194677.3s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.800s, learning 0.159s)
               Value function loss: 11.4889
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 461.17
               Mean episode length: 241.13
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 10.96s
                        Total time: 14716.91s
                               ETA: 1194573.2s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.534s, learning 0.191s)
               Value function loss: 13.1856
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 454.18
               Mean episode length: 240.36
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 11.72s
                        Total time: 14728.64s
                               ETA: 1194531.2s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.133s, learning 0.164s)
               Value function loss: 13.7386
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 467.86
               Mean episode length: 244.74
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 11.30s
                        Total time: 14739.93s
                               ETA: 1194454.6s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.359s, learning 0.162s)
               Value function loss: 11.6873
                    Surrogate loss: 0.0099
             Mean action noise std: 0.75
                       Mean reward: 475.31
               Mean episode length: 246.97
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 11.52s
                        Total time: 14751.46s
                               ETA: 1194396.3s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.967s, learning 0.173s)
               Value function loss: 13.1061
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 448.98
               Mean episode length: 244.19
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 11.14s
                        Total time: 14762.59s
                               ETA: 1194307.2s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.451s, learning 0.165s)
               Value function loss: 13.6018
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 442.10
               Mean episode length: 242.44
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 11.62s
                        Total time: 14774.21s
                               ETA: 1194256.7s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.581s, learning 0.160s)
               Value function loss: 15.1820
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 448.51
               Mean episode length: 244.41
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 11.74s
                        Total time: 14785.95s
                               ETA: 1194216.4s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.433s, learning 0.162s)
               Value function loss: 10.6080
                    Surrogate loss: 0.0010
             Mean action noise std: 0.75
                       Mean reward: 437.45
               Mean episode length: 241.02
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 11.59s
                        Total time: 14797.55s
                               ETA: 1194164.4s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.348s, learning 0.166s)
               Value function loss: 9.9923
                    Surrogate loss: 0.0079
             Mean action noise std: 0.75
                       Mean reward: 450.85
               Mean episode length: 242.99
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 11.51s
                        Total time: 14809.06s
                               ETA: 1194105.9s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.436s, learning 0.166s)
               Value function loss: 10.8838
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 452.18
               Mean episode length: 245.49
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 11.60s
                        Total time: 14820.66s
                               ETA: 1194054.5s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.286s, learning 0.162s)
               Value function loss: 9.5848
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: 443.65
               Mean episode length: 244.81
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 11.45s
                        Total time: 14832.11s
                               ETA: 1193990.9s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.286s, learning 0.166s)
               Value function loss: 10.9725
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 430.86
               Mean episode length: 244.83
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 11.45s
                        Total time: 14843.56s
                               ETA: 1193927.6s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.382s, learning 0.165s)
               Value function loss: 10.2229
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 438.00
               Mean episode length: 243.41
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 11.55s
                        Total time: 14855.11s
                               ETA: 1193872.1s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.172s, learning 0.181s)
               Value function loss: 9.4095
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 435.99
               Mean episode length: 245.60
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 11.35s
                        Total time: 14866.46s
                               ETA: 1193801.1s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.162s)
               Value function loss: 9.5442
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 433.19
               Mean episode length: 246.92
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 11.48s
                        Total time: 14877.94s
                               ETA: 1193740.1s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.498s, learning 0.162s)
               Value function loss: 10.5239
                    Surrogate loss: 0.0036
             Mean action noise std: 0.75
                       Mean reward: 416.29
               Mean episode length: 245.65
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 11.66s
                        Total time: 14889.60s
                               ETA: 1193693.9s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.103s, learning 0.169s)
               Value function loss: 8.9713
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 401.78
               Mean episode length: 244.88
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 11.27s
                        Total time: 14900.87s
                               ETA: 1193616.6s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.256s, learning 0.161s)
               Value function loss: 8.7001
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 399.72
               Mean episode length: 243.88
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 11.42s
                        Total time: 14912.29s
                               ETA: 1193551.0s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.441s, learning 0.163s)
               Value function loss: 9.8942
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 409.13
               Mean episode length: 245.18
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 11.60s
                        Total time: 14923.89s
                               ETA: 1193500.5s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.280s, learning 0.164s)
               Value function loss: 8.0889
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 395.94
               Mean episode length: 245.26
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 11.44s
                        Total time: 14935.34s
                               ETA: 1193437.3s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.155s, learning 0.167s)
               Value function loss: 9.3914
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 388.60
               Mean episode length: 246.73
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 11.32s
                        Total time: 14946.66s
                               ETA: 1193364.4s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.106s, learning 0.186s)
               Value function loss: 10.4057
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 406.93
               Mean episode length: 247.05
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 11.29s
                        Total time: 14957.95s
                               ETA: 1193289.2s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.940s, learning 0.171s)
               Value function loss: 9.1544
                    Surrogate loss: -0.0254
             Mean action noise std: 0.75
                       Mean reward: 400.45
               Mean episode length: 245.02
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 11.11s
                        Total time: 14969.06s
                               ETA: 1193199.7s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.167s, learning 0.157s)
               Value function loss: 9.0490
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 401.99
               Mean episode length: 245.75
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 11.32s
                        Total time: 14980.38s
                               ETA: 1193127.2s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.020s, learning 0.167s)
               Value function loss: 10.5049
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 404.85
               Mean episode length: 247.42
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 11.19s
                        Total time: 14991.57s
                               ETA: 1193044.0s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.842s, learning 0.159s)
               Value function loss: 9.6809
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 412.26
               Mean episode length: 246.87
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 11.00s
                        Total time: 15002.57s
                               ETA: 1192946.1s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.187s, learning 0.180s)
               Value function loss: 10.3959
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 399.19
               Mean episode length: 246.95
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 11.37s
                        Total time: 15013.94s
                               ETA: 1192877.5s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.939s, learning 0.174s)
               Value function loss: 11.1954
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 388.80
               Mean episode length: 241.74
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 11.11s
                        Total time: 15025.05s
                               ETA: 1192788.7s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.401s, learning 0.161s)
               Value function loss: 10.6864
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 412.94
               Mean episode length: 245.60
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 11.56s
                        Total time: 15036.61s
                               ETA: 1192735.6s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.159s)
               Value function loss: 11.4708
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 423.81
               Mean episode length: 243.94
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 11.51s
                        Total time: 15048.12s
                               ETA: 1192678.4s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.968s, learning 0.174s)
               Value function loss: 11.6679
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 400.47
               Mean episode length: 240.30
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 11.14s
                        Total time: 15059.26s
                               ETA: 1192592.3s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.464s, learning 0.158s)
               Value function loss: 10.8424
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 423.27
               Mean episode length: 242.12
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 11.62s
                        Total time: 15070.89s
                               ETA: 1192544.3s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.176s, learning 0.161s)
               Value function loss: 11.3866
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 434.50
               Mean episode length: 243.35
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 11.34s
                        Total time: 15082.22s
                               ETA: 1192473.7s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.087s, learning 0.163s)
               Value function loss: 11.4262
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 432.84
               Mean episode length: 245.99
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 11.25s
                        Total time: 15093.47s
                               ETA: 1192396.5s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.991s, learning 0.203s)
               Value function loss: 11.5464
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 421.49
               Mean episode length: 243.19
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 11.19s
                        Total time: 15104.67s
                               ETA: 1192314.9s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.967s, learning 0.163s)
               Value function loss: 12.1070
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 445.84
               Mean episode length: 248.58
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 11.13s
                        Total time: 15115.80s
                               ETA: 1192228.4s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.165s)
               Value function loss: 13.2029
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 425.30
               Mean episode length: 246.08
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 11.37s
                        Total time: 15127.17s
                               ETA: 1192160.8s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.837s, learning 0.186s)
               Value function loss: 14.5060
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 419.50
               Mean episode length: 247.69
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 11.02s
                        Total time: 15138.19s
                               ETA: 1192066.0s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.146s, learning 0.171s)
               Value function loss: 11.5932
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 400.27
               Mean episode length: 243.25
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 11.32s
                        Total time: 15149.51s
                               ETA: 1191994.6s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.821s, learning 0.169s)
               Value function loss: 10.7327
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 389.00
               Mean episode length: 243.52
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 10.99s
                        Total time: 15160.50s
                               ETA: 1191897.5s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.145s, learning 0.164s)
               Value function loss: 11.6246
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 392.95
               Mean episode length: 244.55
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 11.31s
                        Total time: 15171.81s
                               ETA: 1191825.6s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.049s, learning 0.173s)
               Value function loss: 11.1561
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 409.30
               Mean episode length: 245.94
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 11.22s
                        Total time: 15183.03s
                               ETA: 1191747.0s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.349s, learning 0.165s)
               Value function loss: 13.0967
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 397.78
               Mean episode length: 244.18
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 11.51s
                        Total time: 15194.54s
                               ETA: 1191691.3s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.190s, learning 0.226s)
               Value function loss: 14.7369
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 389.69
               Mean episode length: 242.44
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 11.42s
                        Total time: 15205.96s
                               ETA: 1191628.1s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.868s, learning 0.170s)
               Value function loss: 16.9621
                    Surrogate loss: -0.0229
             Mean action noise std: 0.75
                       Mean reward: 387.43
               Mean episode length: 243.63
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 11.04s
                        Total time: 15217.00s
                               ETA: 1191535.4s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.186s, learning 0.178s)
               Value function loss: 14.5584
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 397.78
               Mean episode length: 241.99
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 11.36s
                        Total time: 15228.36s
                               ETA: 1191468.3s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.999s, learning 0.199s)
               Value function loss: 13.3837
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 412.32
               Mean episode length: 246.55
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 11.20s
                        Total time: 15239.56s
                               ETA: 1191388.3s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.014s, learning 0.160s)
               Value function loss: 11.3723
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 416.60
               Mean episode length: 245.24
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 11.17s
                        Total time: 15250.73s
                               ETA: 1191306.5s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.981s, learning 0.190s)
               Value function loss: 10.3682
                    Surrogate loss: 0.0009
             Mean action noise std: 0.75
                       Mean reward: 412.61
               Mean episode length: 247.25
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 11.17s
                        Total time: 15261.90s
                               ETA: 1191224.7s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.917s, learning 0.162s)
               Value function loss: 11.9654
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 389.90
               Mean episode length: 245.14
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 11.08s
                        Total time: 15272.98s
                               ETA: 1191135.7s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.078s, learning 0.166s)
               Value function loss: 10.9712
                    Surrogate loss: -0.0252
             Mean action noise std: 0.75
                       Mean reward: 404.02
               Mean episode length: 246.73
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 11.24s
                        Total time: 15284.23s
                               ETA: 1191059.8s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.251s, learning 0.158s)
               Value function loss: 10.9703
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 419.95
               Mean episode length: 247.23
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 11.41s
                        Total time: 15295.64s
                               ETA: 1190996.8s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.169s, learning 0.160s)
               Value function loss: 12.9212
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 401.29
               Mean episode length: 249.46
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 11.33s
                        Total time: 15306.96s
                               ETA: 1190927.7s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.747s, learning 0.162s)
               Value function loss: 10.1308
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 391.66
               Mean episode length: 245.94
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 10.91s
                        Total time: 15317.87s
                               ETA: 1190826.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.939s, learning 0.163s)
               Value function loss: 9.7770
                    Surrogate loss: -0.0236
             Mean action noise std: 0.75
                       Mean reward: 381.54
               Mean episode length: 244.48
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 11.10s
                        Total time: 15328.98s
                               ETA: 1190739.4s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.216s, learning 0.182s)
               Value function loss: 11.1332
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 416.80
               Mean episode length: 248.23
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 11.40s
                        Total time: 15340.37s
                               ETA: 1190675.9s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.956s, learning 0.170s)
               Value function loss: 10.0448
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 411.14
               Mean episode length: 249.79
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 11.13s
                        Total time: 15351.50s
                               ETA: 1190591.4s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.237s, learning 0.165s)
               Value function loss: 10.8555
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 409.11
               Mean episode length: 249.11
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 11.40s
                        Total time: 15362.90s
                               ETA: 1190528.4s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.279s, learning 0.162s)
               Value function loss: 12.6760
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 411.44
               Mean episode length: 247.28
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 11.44s
                        Total time: 15374.34s
                               ETA: 1190468.5s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.402s, learning 0.180s)
               Value function loss: 11.4668
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 422.73
               Mean episode length: 248.20
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 11.58s
                        Total time: 15385.92s
                               ETA: 1190419.5s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.810s, learning 0.181s)
               Value function loss: 11.1147
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 423.48
               Mean episode length: 248.72
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 10.99s
                        Total time: 15396.91s
                               ETA: 1190325.0s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.068s, learning 0.164s)
               Value function loss: 11.2541
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 397.16
               Mean episode length: 248.59
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 11.23s
                        Total time: 15408.15s
                               ETA: 1190249.2s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.161s)
               Value function loss: 10.1992
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 419.48
               Mean episode length: 249.14
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 11.34s
                        Total time: 15419.48s
                               ETA: 1190181.4s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.917s, learning 0.169s)
               Value function loss: 9.4017
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 422.83
               Mean episode length: 248.59
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 11.09s
                        Total time: 15430.57s
                               ETA: 1190094.5s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.131s, learning 0.159s)
               Value function loss: 10.2547
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 458.56
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 11.29s
                        Total time: 15441.86s
                               ETA: 1190023.5s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.094s, learning 0.160s)
               Value function loss: 10.1681
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 458.30
               Mean episode length: 247.42
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 11.25s
                        Total time: 15453.11s
                               ETA: 1189949.8s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.231s, learning 0.160s)
               Value function loss: 10.0659
                    Surrogate loss: -0.0229
             Mean action noise std: 0.75
                       Mean reward: 453.49
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 11.39s
                        Total time: 15464.50s
                               ETA: 1189886.7s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.254s, learning 0.175s)
               Value function loss: 10.0349
                    Surrogate loss: -0.0253
             Mean action noise std: 0.75
                       Mean reward: 457.25
               Mean episode length: 249.08
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 11.43s
                        Total time: 15475.93s
                               ETA: 1189826.7s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.343s, learning 0.167s)
               Value function loss: 10.3970
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 452.62
               Mean episode length: 246.26
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 11.51s
                        Total time: 15487.44s
                               ETA: 1189772.8s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.125s, learning 0.172s)
               Value function loss: 8.9042
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 453.88
               Mean episode length: 248.87
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 11.30s
                        Total time: 15498.74s
                               ETA: 1189702.8s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.074s, learning 0.181s)
               Value function loss: 9.1540
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 454.50
               Mean episode length: 248.14
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 11.26s
                        Total time: 15509.99s
                               ETA: 1189629.7s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.167s, learning 0.176s)
               Value function loss: 7.8429
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 470.92
               Mean episode length: 248.13
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 11.34s
                        Total time: 15521.34s
                               ETA: 1189563.3s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.472s, learning 0.174s)
               Value function loss: 8.4577
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 476.24
               Mean episode length: 248.22
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 11.65s
                        Total time: 15532.98s
                               ETA: 1189520.2s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.197s, learning 0.162s)
               Value function loss: 8.8605
                    Surrogate loss: -0.0240
             Mean action noise std: 0.75
                       Mean reward: 474.85
               Mean episode length: 247.58
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 11.36s
                        Total time: 15544.34s
                               ETA: 1189455.2s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.253s, learning 0.160s)
               Value function loss: 11.5439
                    Surrogate loss: -0.0254
             Mean action noise std: 0.75
                       Mean reward: 467.82
               Mean episode length: 245.26
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 11.41s
                        Total time: 15555.75s
                               ETA: 1189394.5s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.614s, learning 0.161s)
               Value function loss: 10.6462
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 454.66
               Mean episode length: 244.79
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 11.77s
                        Total time: 15567.53s
                               ETA: 1189361.5s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.776s, learning 0.163s)
               Value function loss: 10.5713
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 428.48
               Mean episode length: 241.13
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 11.94s
                        Total time: 15579.47s
                               ETA: 1189341.0s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.500s, learning 0.196s)
               Value function loss: 11.6774
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 434.06
               Mean episode length: 239.46
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 11.70s
                        Total time: 15591.16s
                               ETA: 1189302.0s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.165s)
               Value function loss: 10.5451
                    Surrogate loss: -0.0246
             Mean action noise std: 0.75
                       Mean reward: 439.33
               Mean episode length: 240.77
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 11.50s
                        Total time: 15602.66s
                               ETA: 1189248.2s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.282s, learning 0.159s)
               Value function loss: 11.4848
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 435.00
               Mean episode length: 238.14
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 11.44s
                        Total time: 15614.10s
                               ETA: 1189189.9s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.161s)
               Value function loss: 12.1794
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 430.53
               Mean episode length: 236.76
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 11.24s
                        Total time: 15625.34s
                               ETA: 1189116.0s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.474s, learning 0.160s)
               Value function loss: 12.0142
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 419.66
               Mean episode length: 232.90
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 11.63s
                        Total time: 15636.97s
                               ETA: 1189072.6s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.375s, learning 0.184s)
               Value function loss: 12.3512
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 419.60
               Mean episode length: 234.39
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 11.56s
                        Total time: 15648.53s
                               ETA: 1189023.5s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.311s, learning 0.160s)
               Value function loss: 16.7984
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 447.77
               Mean episode length: 240.42
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 11.47s
                        Total time: 15660.00s
                               ETA: 1188967.7s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.406s, learning 0.163s)
               Value function loss: 14.8452
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 433.52
               Mean episode length: 235.91
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 11.57s
                        Total time: 15671.57s
                               ETA: 1188919.4s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.991s, learning 0.177s)
               Value function loss: 16.8563
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 423.46
               Mean episode length: 234.01
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 11.17s
                        Total time: 15682.74s
                               ETA: 1188840.9s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.507s, learning 0.164s)
               Value function loss: 13.9897
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 389.43
               Mean episode length: 229.20
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 11.67s
                        Total time: 15694.41s
                               ETA: 1188800.5s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.955s, learning 0.163s)
               Value function loss: 14.6442
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 397.69
               Mean episode length: 228.77
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 11.12s
                        Total time: 15705.53s
                               ETA: 1188718.2s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.033s, learning 0.165s)
               Value function loss: 13.4923
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 421.54
               Mean episode length: 229.69
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 11.20s
                        Total time: 15716.73s
                               ETA: 1188642.2s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.547s, learning 0.161s)
               Value function loss: 16.1659
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 408.35
               Mean episode length: 234.07
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 11.71s
                        Total time: 15728.44s
                               ETA: 1188604.8s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.100s, learning 0.164s)
               Value function loss: 14.4024
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 419.50
               Mean episode length: 233.82
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.26s
                        Total time: 15739.70s
                               ETA: 1188534.0s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.352s, learning 0.189s)
               Value function loss: 13.7396
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 434.66
               Mean episode length: 236.27
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 11.54s
                        Total time: 15751.24s
                               ETA: 1188484.1s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.032s, learning 0.169s)
               Value function loss: 16.3566
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 451.15
               Mean episode length: 241.64
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 11.20s
                        Total time: 15762.44s
                               ETA: 1188408.5s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.743s, learning 0.173s)
               Value function loss: 18.0519
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 427.90
               Mean episode length: 237.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 10.92s
                        Total time: 15773.36s
                               ETA: 1188311.7s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.236s, learning 0.174s)
               Value function loss: 14.3967
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 432.33
               Mean episode length: 235.55
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 11.41s
                        Total time: 15784.77s
                               ETA: 1188252.2s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.108s, learning 0.163s)
               Value function loss: 14.0004
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 422.77
               Mean episode length: 238.03
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 11.27s
                        Total time: 15796.04s
                               ETA: 1188182.3s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.045s, learning 0.168s)
               Value function loss: 17.5480
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 434.09
               Mean episode length: 238.64
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 11.21s
                        Total time: 15807.25s
                               ETA: 1188108.1s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.299s, learning 0.162s)
               Value function loss: 18.6142
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 427.02
               Mean episode length: 239.30
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.46s
                        Total time: 15818.71s
                               ETA: 1188052.7s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.256s, learning 0.169s)
               Value function loss: 18.0071
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 411.49
               Mean episode length: 234.58
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 11.43s
                        Total time: 15830.14s
                               ETA: 1187994.6s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.274s, learning 0.187s)
               Value function loss: 17.5269
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 415.38
               Mean episode length: 235.95
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 11.46s
                        Total time: 15841.60s
                               ETA: 1187939.3s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.280s, learning 0.161s)
               Value function loss: 19.0340
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 431.71
               Mean episode length: 237.49
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 11.44s
                        Total time: 15853.04s
                               ETA: 1187882.5s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.914s, learning 0.160s)
               Value function loss: 13.7906
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 431.92
               Mean episode length: 239.97
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.07s
                        Total time: 15864.11s
                               ETA: 1187798.4s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.218s, learning 0.161s)
               Value function loss: 13.4749
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 433.34
               Mean episode length: 239.52
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 11.38s
                        Total time: 15875.49s
                               ETA: 1187737.1s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.515s, learning 0.252s)
               Value function loss: 13.9032
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 416.33
               Mean episode length: 235.78
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 11.77s
                        Total time: 15887.26s
                               ETA: 1187705.0s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.163s)
               Value function loss: 14.8679
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 415.78
               Mean episode length: 235.22
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 11.43s
                        Total time: 15898.69s
                               ETA: 1187647.6s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.683s, learning 0.172s)
               Value function loss: 15.3215
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 428.23
               Mean episode length: 237.35
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 11.86s
                        Total time: 15910.54s
                               ETA: 1187622.1s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.138s, learning 0.163s)
               Value function loss: 18.0791
                    Surrogate loss: 0.0024
             Mean action noise std: 0.75
                       Mean reward: 437.90
               Mean episode length: 239.95
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.30s
                        Total time: 15921.84s
                               ETA: 1187555.2s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.420s, learning 0.189s)
               Value function loss: 16.5736
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 433.14
               Mean episode length: 239.41
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 11.61s
                        Total time: 15933.45s
                               ETA: 1187511.5s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.300s, learning 0.192s)
               Value function loss: 16.5205
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 428.18
               Mean episode length: 235.55
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 11.49s
                        Total time: 15944.94s
                               ETA: 1187459.1s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.028s, learning 0.162s)
               Value function loss: 15.2259
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 418.29
               Mean episode length: 233.87
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 11.19s
                        Total time: 15956.13s
                               ETA: 1187384.3s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.201s, learning 0.170s)
               Value function loss: 13.8825
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 417.09
               Mean episode length: 236.45
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 11.37s
                        Total time: 15967.51s
                               ETA: 1187323.0s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.130s, learning 0.161s)
               Value function loss: 17.2450
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 430.15
               Mean episode length: 237.77
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 11.29s
                        Total time: 15978.80s
                               ETA: 1187255.9s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.140s, learning 0.166s)
               Value function loss: 16.6742
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 395.08
               Mean episode length: 230.55
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 11.31s
                        Total time: 15990.10s
                               ETA: 1187189.8s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.352s, learning 0.185s)
               Value function loss: 17.4027
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 421.68
               Mean episode length: 236.80
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 11.54s
                        Total time: 16001.64s
                               ETA: 1187141.1s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.079s, learning 0.260s)
               Value function loss: 17.6453
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 441.51
               Mean episode length: 240.91
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 11.34s
                        Total time: 16012.98s
                               ETA: 1187077.7s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.586s, learning 0.168s)
               Value function loss: 20.4468
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 439.95
               Mean episode length: 239.57
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 11.75s
                        Total time: 16024.73s
                               ETA: 1187045.2s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.300s, learning 0.166s)
               Value function loss: 13.2483
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 449.72
               Mean episode length: 241.07
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 11.47s
                        Total time: 16036.20s
                               ETA: 1186991.4s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.801s, learning 0.180s)
               Value function loss: 11.5978
                    Surrogate loss: -0.0274
             Mean action noise std: 0.75
                       Mean reward: 409.72
               Mean episode length: 237.97
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 10.98s
                        Total time: 16047.18s
                               ETA: 1186901.7s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.948s, learning 0.166s)
               Value function loss: 12.6015
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 408.47
               Mean episode length: 235.23
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 11.11s
                        Total time: 16058.29s
                               ETA: 1186822.0s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.457s, learning 0.175s)
               Value function loss: 11.0482
                    Surrogate loss: -0.0221
             Mean action noise std: 0.75
                       Mean reward: 411.49
               Mean episode length: 237.96
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 11.63s
                        Total time: 16069.92s
                               ETA: 1186780.7s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.962s, learning 0.189s)
               Value function loss: 12.6415
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 402.97
               Mean episode length: 239.72
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 11.15s
                        Total time: 16081.07s
                               ETA: 1186703.9s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.191s, learning 0.170s)
               Value function loss: 10.8126
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 392.72
               Mean episode length: 234.97
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 11.36s
                        Total time: 16092.44s
                               ETA: 1186642.7s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.103s, learning 0.168s)
               Value function loss: 11.1240
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 376.72
               Mean episode length: 228.50
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 11.27s
                        Total time: 16103.71s
                               ETA: 1186574.9s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.445s, learning 0.162s)
               Value function loss: 11.1091
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 411.87
               Mean episode length: 233.85
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 11.61s
                        Total time: 16115.31s
                               ETA: 1186532.0s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.335s, learning 0.174s)
               Value function loss: 8.7082
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 406.54
               Mean episode length: 235.23
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 11.51s
                        Total time: 16126.82s
                               ETA: 1186481.9s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.100s, learning 0.167s)
               Value function loss: 9.8783
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 390.59
               Mean episode length: 234.11
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 11.27s
                        Total time: 16138.09s
                               ETA: 1186414.1s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.613s, learning 0.173s)
               Value function loss: 9.0321
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 382.82
               Mean episode length: 233.25
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 11.79s
                        Total time: 16149.87s
                               ETA: 1186384.5s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.975s, learning 0.191s)
               Value function loss: 10.0520
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 374.07
               Mean episode length: 231.65
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 11.17s
                        Total time: 16161.04s
                               ETA: 1186309.4s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.172s, learning 0.188s)
               Value function loss: 10.5832
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 363.79
               Mean episode length: 228.24
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 11.36s
                        Total time: 16172.40s
                               ETA: 1186248.6s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.831s, learning 0.162s)
               Value function loss: 10.4118
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 379.66
               Mean episode length: 231.23
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 10.99s
                        Total time: 16183.39s
                               ETA: 1186161.0s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.422s, learning 0.164s)
               Value function loss: 9.5511
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 370.87
               Mean episode length: 232.67
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 11.59s
                        Total time: 16194.98s
                               ETA: 1186117.0s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.383s, learning 0.196s)
               Value function loss: 11.0594
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 385.27
               Mean episode length: 238.60
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 11.58s
                        Total time: 16206.56s
                               ETA: 1186072.5s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.200s, learning 0.167s)
               Value function loss: 8.6371
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 386.50
               Mean episode length: 236.62
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 11.37s
                        Total time: 16217.93s
                               ETA: 1186012.4s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.382s, learning 0.190s)
               Value function loss: 8.5647
                    Surrogate loss: -0.0230
             Mean action noise std: 0.75
                       Mean reward: 347.36
               Mean episode length: 225.54
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 11.57s
                        Total time: 16229.50s
                               ETA: 1185967.5s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.987s, learning 0.160s)
               Value function loss: 9.2676
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 347.14
               Mean episode length: 224.32
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 12.15s
                        Total time: 16241.64s
                               ETA: 1185964.6s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.086s, learning 0.167s)
               Value function loss: 9.1682
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 368.37
               Mean episode length: 228.99
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 11.25s
                        Total time: 16252.90s
                               ETA: 1185896.5s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.399s, learning 0.160s)
               Value function loss: 9.4843
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 382.47
               Mean episode length: 232.92
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 11.56s
                        Total time: 16264.46s
                               ETA: 1185850.7s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.119s, learning 0.165s)
               Value function loss: 10.4981
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 368.54
               Mean episode length: 230.52
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 11.28s
                        Total time: 16275.74s
                               ETA: 1185785.0s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.412s, learning 0.158s)
               Value function loss: 12.7746
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 393.67
               Mean episode length: 233.19
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 11.57s
                        Total time: 16287.31s
                               ETA: 1185740.2s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.350s, learning 0.165s)
               Value function loss: 11.9675
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 408.58
               Mean episode length: 236.95
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 11.51s
                        Total time: 16298.82s
                               ETA: 1185691.4s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.591s, learning 0.159s)
               Value function loss: 12.6177
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 408.80
               Mean episode length: 237.57
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 11.75s
                        Total time: 16310.57s
                               ETA: 1185659.8s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.998s, learning 0.161s)
               Value function loss: 10.2798
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 385.49
               Mean episode length: 234.97
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 11.16s
                        Total time: 16321.73s
                               ETA: 1185585.2s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.051s, learning 0.189s)
               Value function loss: 11.8356
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 390.33
               Mean episode length: 236.68
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 11.24s
                        Total time: 16332.97s
                               ETA: 1185516.7s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.352s, learning 0.174s)
               Value function loss: 12.1708
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 393.24
               Mean episode length: 236.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 11.53s
                        Total time: 16344.50s
                               ETA: 1185468.9s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.621s, learning 0.170s)
               Value function loss: 10.8358
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 403.18
               Mean episode length: 238.63
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 11.79s
                        Total time: 16356.29s
                               ETA: 1185440.4s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.421s, learning 0.160s)
               Value function loss: 11.2320
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 394.54
               Mean episode length: 234.62
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 11.58s
                        Total time: 16367.87s
                               ETA: 1185396.8s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.190s, learning 0.181s)
               Value function loss: 14.3764
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 424.77
               Mean episode length: 242.15
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 11.37s
                        Total time: 16379.24s
                               ETA: 1185337.9s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.160s)
               Value function loss: 12.0742
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 431.62
               Mean episode length: 242.78
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 11.55s
                        Total time: 16390.79s
                               ETA: 1185291.9s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.189s, learning 0.188s)
               Value function loss: 10.9131
                    Surrogate loss: -0.0257
             Mean action noise std: 0.75
                       Mean reward: 419.35
               Mean episode length: 239.35
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 11.38s
                        Total time: 16402.16s
                               ETA: 1185233.6s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.491s, learning 0.170s)
               Value function loss: 13.2660
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 413.16
               Mean episode length: 239.16
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 11.66s
                        Total time: 16413.83s
                               ETA: 1185196.0s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.911s, learning 0.169s)
               Value function loss: 12.0307
                    Surrogate loss: -0.0217
             Mean action noise std: 0.75
                       Mean reward: 408.52
               Mean episode length: 237.71
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 11.08s
                        Total time: 16424.91s
                               ETA: 1185116.5s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.382s, learning 0.200s)
               Value function loss: 14.8766
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 427.55
               Mean episode length: 239.64
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 11.58s
                        Total time: 16436.49s
                               ETA: 1185073.2s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.281s, learning 0.264s)
               Value function loss: 15.2949
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 431.89
               Mean episode length: 241.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 11.54s
                        Total time: 16448.03s
                               ETA: 1185027.3s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.175s, learning 0.191s)
               Value function loss: 13.4216
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 422.67
               Mean episode length: 239.79
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 11.37s
                        Total time: 16459.40s
                               ETA: 1184968.6s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.663s, learning 0.177s)
               Value function loss: 12.4233
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 432.38
               Mean episode length: 239.79
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 11.84s
                        Total time: 16471.24s
                               ETA: 1184944.0s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.084s, learning 0.180s)
               Value function loss: 11.4460
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 442.94
               Mean episode length: 241.49
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 11.26s
                        Total time: 16482.50s
                               ETA: 1184878.0s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.817s, learning 0.168s)
               Value function loss: 13.9364
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 424.03
               Mean episode length: 238.38
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 11.99s
                        Total time: 16494.49s
                               ETA: 1184864.0s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.165s)
               Value function loss: 12.6401
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 452.89
               Mean episode length: 242.31
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 11.51s
                        Total time: 16505.99s
                               ETA: 1184815.7s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.483s, learning 0.163s)
               Value function loss: 11.9350
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 437.54
               Mean episode length: 239.62
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 11.65s
                        Total time: 16517.64s
                               ETA: 1184777.3s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.351s, learning 0.178s)
               Value function loss: 13.2535
                    Surrogate loss: 0.0010
             Mean action noise std: 0.75
                       Mean reward: 433.79
               Mean episode length: 238.64
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 11.53s
                        Total time: 16529.17s
                               ETA: 1184730.6s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.098s, learning 0.160s)
               Value function loss: 12.9660
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 444.75
               Mean episode length: 241.97
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 11.26s
                        Total time: 16540.43s
                               ETA: 1184664.6s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.161s)
               Value function loss: 14.1684
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 455.47
               Mean episode length: 243.96
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 11.04s
                        Total time: 16551.47s
                               ETA: 1184583.3s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.164s)
               Value function loss: 19.0678
                    Surrogate loss: 0.0014
             Mean action noise std: 0.75
                       Mean reward: 446.82
               Mean episode length: 243.75
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 11.43s
                        Total time: 16562.91s
                               ETA: 1184530.0s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.025s, learning 0.160s)
               Value function loss: 12.1953
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 425.39
               Mean episode length: 240.13
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 11.18s
                        Total time: 16574.09s
                               ETA: 1184459.0s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.580s, learning 0.160s)
               Value function loss: 11.7255
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 427.68
               Mean episode length: 240.12
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 11.74s
                        Total time: 16585.83s
                               ETA: 1184427.6s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.949s, learning 0.172s)
               Value function loss: 11.5066
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 441.72
               Mean episode length: 242.50
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 11.12s
                        Total time: 16596.95s
                               ETA: 1184352.2s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.164s)
               Value function loss: 12.6916
                    Surrogate loss: 0.0016
             Mean action noise std: 0.75
                       Mean reward: 419.95
               Mean episode length: 238.15
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 11.57s
                        Total time: 16608.52s
                               ETA: 1184308.8s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.191s, learning 0.164s)
               Value function loss: 12.2542
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 407.27
               Mean episode length: 237.69
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 11.36s
                        Total time: 16619.88s
                               ETA: 1184250.2s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.005s, learning 0.159s)
               Value function loss: 16.0313
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 431.88
               Mean episode length: 241.88
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 11.16s
                        Total time: 16631.04s
                               ETA: 1184178.1s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.122s, learning 0.159s)
               Value function loss: 18.1254
                    Surrogate loss: 0.0016
             Mean action noise std: 0.75
                       Mean reward: 448.39
               Mean episode length: 242.37
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 11.28s
                        Total time: 16642.32s
                               ETA: 1184114.4s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.487s, learning 0.164s)
               Value function loss: 13.4138
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 435.32
               Mean episode length: 241.70
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 11.65s
                        Total time: 16653.97s
                               ETA: 1184077.0s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.157s, learning 0.184s)
               Value function loss: 13.9999
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 422.07
               Mean episode length: 242.16
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 11.34s
                        Total time: 16665.31s
                               ETA: 1184017.6s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.442s, learning 0.160s)
               Value function loss: 16.6428
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 419.00
               Mean episode length: 242.08
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 11.60s
                        Total time: 16676.91s
                               ETA: 1183976.9s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.127s, learning 0.166s)
               Value function loss: 17.2323
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 419.40
               Mean episode length: 241.56
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 11.29s
                        Total time: 16688.21s
                               ETA: 1183914.2s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.314s, learning 0.168s)
               Value function loss: 14.9029
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 423.49
               Mean episode length: 242.33
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 11.48s
                        Total time: 16699.69s
                               ETA: 1183865.1s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.167s)
               Value function loss: 14.6673
                    Surrogate loss: 0.0213
             Mean action noise std: 0.75
                       Mean reward: 422.51
               Mean episode length: 243.14
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 11.34s
                        Total time: 16711.03s
                               ETA: 1183806.2s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.990s, learning 0.189s)
               Value function loss: 12.7547
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 396.50
               Mean episode length: 240.21
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 11.18s
                        Total time: 16722.21s
                               ETA: 1183735.7s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.167s)
               Value function loss: 16.8630
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 406.68
               Mean episode length: 240.04
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 11.27s
                        Total time: 16733.48s
                               ETA: 1183672.0s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.199s, learning 0.186s)
               Value function loss: 16.3331
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 395.14
               Mean episode length: 241.34
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 11.38s
                        Total time: 16744.87s
                               ETA: 1183616.2s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.307s, learning 0.169s)
               Value function loss: 17.4291
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 394.46
               Mean episode length: 241.28
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 11.48s
                        Total time: 16756.35s
                               ETA: 1183566.9s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.420s, learning 0.164s)
               Value function loss: 17.0913
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 399.54
               Mean episode length: 241.99
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 11.58s
                        Total time: 16767.93s
                               ETA: 1183525.3s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.133s, learning 0.159s)
               Value function loss: 17.2367
                    Surrogate loss: 0.0072
             Mean action noise std: 0.75
                       Mean reward: 421.73
               Mean episode length: 244.73
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 11.29s
                        Total time: 16779.22s
                               ETA: 1183463.2s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.170s, learning 0.162s)
               Value function loss: 16.5924
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 416.65
               Mean episode length: 243.52
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 11.33s
                        Total time: 16790.55s
                               ETA: 1183403.9s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.161s)
               Value function loss: 16.6820
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 416.20
               Mean episode length: 245.85
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 11.34s
                        Total time: 16801.89s
                               ETA: 1183345.0s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.167s)
               Value function loss: 16.6005
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 414.95
               Mean episode length: 244.76
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 11.43s
                        Total time: 16813.32s
                               ETA: 1183292.6s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.162s, learning 0.163s)
               Value function loss: 19.5355
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 418.38
               Mean episode length: 245.36
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 11.33s
                        Total time: 16824.64s
                               ETA: 1183233.1s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.991s, learning 0.163s)
               Value function loss: 12.9425
                    Surrogate loss: 0.0011
             Mean action noise std: 0.75
                       Mean reward: 411.97
               Mean episode length: 247.16
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 11.15s
                        Total time: 16835.80s
                               ETA: 1183161.6s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.166s)
               Value function loss: 17.1050
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 411.03
               Mean episode length: 247.52
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 11.15s
                        Total time: 16846.95s
                               ETA: 1183090.1s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.379s, learning 0.162s)
               Value function loss: 13.1549
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 396.60
               Mean episode length: 246.81
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 11.54s
                        Total time: 16858.49s
                               ETA: 1183046.0s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.539s, learning 0.163s)
               Value function loss: 12.8938
                    Surrogate loss: 0.0024
             Mean action noise std: 0.75
                       Mean reward: 383.83
               Mean episode length: 246.54
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 11.70s
                        Total time: 16870.19s
                               ETA: 1183013.2s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.033s, learning 0.167s)
               Value function loss: 12.1802
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 388.41
               Mean episode length: 246.34
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 11.20s
                        Total time: 16881.39s
                               ETA: 1182945.2s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.183s)
               Value function loss: 11.7012
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 393.70
               Mean episode length: 247.69
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 11.25s
                        Total time: 16892.65s
                               ETA: 1182881.1s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.320s, learning 0.160s)
               Value function loss: 12.5946
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 383.04
               Mean episode length: 247.92
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 11.48s
                        Total time: 16904.13s
                               ETA: 1182832.9s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.381s, learning 0.175s)
               Value function loss: 11.9255
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 385.37
               Mean episode length: 247.74
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 11.56s
                        Total time: 16915.68s
                               ETA: 1182790.0s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.268s, learning 0.167s)
               Value function loss: 11.7582
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 380.01
               Mean episode length: 246.24
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 11.44s
                        Total time: 16927.12s
                               ETA: 1182738.8s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.286s, learning 0.239s)
               Value function loss: 14.5022
                    Surrogate loss: 0.0105
             Mean action noise std: 0.75
                       Mean reward: 370.97
               Mean episode length: 244.77
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 11.52s
                        Total time: 16938.64s
                               ETA: 1182693.9s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.139s, learning 0.170s)
               Value function loss: 10.6104
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 377.32
               Mean episode length: 247.03
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 11.31s
                        Total time: 16949.95s
                               ETA: 1182633.9s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.163s, learning 0.178s)
               Value function loss: 12.7913
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 386.46
               Mean episode length: 248.66
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 11.34s
                        Total time: 16961.29s
                               ETA: 1182576.2s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.186s, learning 0.234s)
               Value function loss: 10.0928
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 373.51
               Mean episode length: 248.39
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 11.42s
                        Total time: 16972.71s
                               ETA: 1182524.2s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.364s, learning 0.162s)
               Value function loss: 11.8122
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 370.72
               Mean episode length: 248.97
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 11.53s
                        Total time: 16984.24s
                               ETA: 1182479.5s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.496s, learning 0.193s)
               Value function loss: 9.4395
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 363.29
               Mean episode length: 249.30
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 11.69s
                        Total time: 16995.93s
                               ETA: 1182446.3s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.745s, learning 0.168s)
               Value function loss: 10.0726
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 370.17
               Mean episode length: 249.25
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 11.91s
                        Total time: 17007.84s
                               ETA: 1182428.6s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.152s, learning 0.198s)
               Value function loss: 10.8487
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 356.88
               Mean episode length: 249.29
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 11.35s
                        Total time: 17019.19s
                               ETA: 1182371.8s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.426s, learning 0.162s)
               Value function loss: 11.8799
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 363.80
               Mean episode length: 249.19
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 11.59s
                        Total time: 17030.78s
                               ETA: 1182331.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.179s, learning 0.186s)
               Value function loss: 11.2843
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 394.32
               Mean episode length: 249.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 11.36s
                        Total time: 17042.14s
                               ETA: 1182276.0s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.585s, learning 0.167s)
               Value function loss: 12.5435
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 398.89
               Mean episode length: 249.41
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 10.75s
                        Total time: 17052.89s
                               ETA: 1182178.0s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.312s, learning 0.162s)
               Value function loss: 13.6487
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 397.31
               Mean episode length: 249.64
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 11.47s
                        Total time: 17064.37s
                               ETA: 1182130.1s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.160s, learning 0.160s)
               Value function loss: 14.4582
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 378.64
               Mean episode length: 249.36
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 11.32s
                        Total time: 17075.69s
                               ETA: 1182071.5s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.311s, learning 0.173s)
               Value function loss: 15.5883
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 400.66
               Mean episode length: 248.72
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 11.48s
                        Total time: 17087.17s
                               ETA: 1182024.4s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.288s, learning 0.160s)
               Value function loss: 15.7003
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 403.65
               Mean episode length: 248.69
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 11.45s
                        Total time: 17098.62s
                               ETA: 1181974.9s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.434s, learning 0.160s)
               Value function loss: 12.9319
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 406.17
               Mean episode length: 249.12
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 11.59s
                        Total time: 17110.21s
                               ETA: 1181935.6s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.109s, learning 0.159s)
               Value function loss: 15.6963
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 401.55
               Mean episode length: 249.40
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 11.27s
                        Total time: 17121.48s
                               ETA: 1181873.8s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.602s, learning 0.163s)
               Value function loss: 15.9779
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 418.76
               Mean episode length: 248.96
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 11.76s
                        Total time: 17133.25s
                               ETA: 1181846.2s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.651s, learning 0.163s)
               Value function loss: 15.8585
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 445.80
               Mean episode length: 249.36
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 11.81s
                        Total time: 17145.06s
                               ETA: 1181822.1s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.540s, learning 0.159s)
               Value function loss: 16.6404
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 468.48
               Mean episode length: 249.82
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 11.70s
                        Total time: 17156.76s
                               ETA: 1181790.0s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.359s, learning 0.163s)
               Value function loss: 13.8915
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: 448.87
               Mean episode length: 249.65
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 11.52s
                        Total time: 17168.28s
                               ETA: 1181745.9s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.165s)
               Value function loss: 17.6922
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 439.60
               Mean episode length: 248.44
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 11.34s
                        Total time: 17179.61s
                               ETA: 1181688.9s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.031s, learning 0.159s)
               Value function loss: 12.4264
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 456.15
               Mean episode length: 248.83
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 11.19s
                        Total time: 17190.81s
                               ETA: 1181622.1s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.032s, learning 0.159s)
               Value function loss: 18.0575
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 442.02
               Mean episode length: 249.65
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 11.19s
                        Total time: 17202.00s
                               ETA: 1181555.4s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.160s)
               Value function loss: 14.3859
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 445.22
               Mean episode length: 247.83
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 11.57s
                        Total time: 17213.56s
                               ETA: 1181514.4s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.202s, learning 0.162s)
               Value function loss: 15.1017
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 445.00
               Mean episode length: 247.52
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 11.36s
                        Total time: 17224.93s
                               ETA: 1181459.7s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.415s, learning 0.167s)
               Value function loss: 16.7282
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 449.89
               Mean episode length: 247.98
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 11.58s
                        Total time: 17236.51s
                               ETA: 1181419.9s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.086s, learning 0.178s)
               Value function loss: 14.6354
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 437.48
               Mean episode length: 244.09
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 11.26s
                        Total time: 17247.77s
                               ETA: 1181358.5s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.368s, learning 0.220s)
               Value function loss: 13.9258
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 457.50
               Mean episode length: 247.26
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 11.59s
                        Total time: 17259.36s
                               ETA: 1181319.2s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.009s, learning 0.168s)
               Value function loss: 13.9891
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 452.92
               Mean episode length: 248.11
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 11.18s
                        Total time: 17270.54s
                               ETA: 1181251.9s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.313s, learning 0.168s)
               Value function loss: 19.4480
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 439.44
               Mean episode length: 247.39
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 11.48s
                        Total time: 17282.02s
                               ETA: 1181205.5s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.623s, learning 0.167s)
               Value function loss: 21.0889
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 456.29
               Mean episode length: 248.49
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 11.79s
                        Total time: 17293.81s
                               ETA: 1181180.2s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.268s, learning 0.164s)
               Value function loss: 28.7945
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 461.35
               Mean episode length: 248.99
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 11.43s
                        Total time: 17305.24s
                               ETA: 1181130.5s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.158s, learning 0.162s)
               Value function loss: 20.9754
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 447.05
               Mean episode length: 248.27
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 11.32s
                        Total time: 17316.56s
                               ETA: 1181073.3s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.497s, learning 0.174s)
               Value function loss: 21.4503
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 453.79
               Mean episode length: 248.03
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 11.67s
                        Total time: 17328.23s
                               ETA: 1181039.9s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.306s, learning 0.169s)
               Value function loss: 19.5485
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 447.30
               Mean episode length: 247.70
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 11.48s
                        Total time: 17339.71s
                               ETA: 1180993.3s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.252s, learning 0.167s)
               Value function loss: 20.6328
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 433.25
               Mean episode length: 247.06
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 11.42s
                        Total time: 17351.12s
                               ETA: 1180942.9s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.237s, learning 0.178s)
               Value function loss: 18.0421
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 437.81
               Mean episode length: 247.91
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 11.41s
                        Total time: 17362.54s
                               ETA: 1180892.3s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.301s, learning 0.160s)
               Value function loss: 23.4586
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 436.74
               Mean episode length: 248.48
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 11.46s
                        Total time: 17374.00s
                               ETA: 1180844.8s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.167s, learning 0.260s)
               Value function loss: 23.8511
                    Surrogate loss: 0.0038
             Mean action noise std: 0.75
                       Mean reward: 447.70
               Mean episode length: 248.32
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 11.43s
                        Total time: 17385.43s
                               ETA: 1180795.2s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.535s, learning 0.162s)
               Value function loss: 19.4232
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 457.53
               Mean episode length: 248.34
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 11.70s
                        Total time: 17397.12s
                               ETA: 1180763.9s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.161s)
               Value function loss: 21.0607
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 469.87
               Mean episode length: 249.20
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 11.36s
                        Total time: 17408.48s
                               ETA: 1180709.4s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.165s)
               Value function loss: 17.9810
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 475.01
               Mean episode length: 249.30
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 11.43s
                        Total time: 17419.91s
                               ETA: 1180660.4s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.164s)
               Value function loss: 21.0640
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 465.34
               Mean episode length: 249.21
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 11.39s
                        Total time: 17431.30s
                               ETA: 1180608.2s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.520s, learning 0.168s)
               Value function loss: 18.2659
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 451.49
               Mean episode length: 248.76
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 11.69s
                        Total time: 17442.99s
                               ETA: 1180576.5s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.083s, learning 0.160s)
               Value function loss: 24.5188
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 452.36
               Mean episode length: 248.04
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 11.24s
                        Total time: 17454.23s
                               ETA: 1180514.7s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.405s, learning 0.167s)
               Value function loss: 17.1390
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 451.35
               Mean episode length: 248.24
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 11.57s
                        Total time: 17465.80s
                               ETA: 1180475.2s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.163s)
               Value function loss: 17.0193
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 450.80
               Mean episode length: 248.91
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 11.48s
                        Total time: 17477.28s
                               ETA: 1180429.4s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.381s, learning 0.165s)
               Value function loss: 13.4815
                    Surrogate loss: -0.0274
             Mean action noise std: 0.75
                       Mean reward: 448.10
               Mean episode length: 247.98
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 11.55s
                        Total time: 17488.83s
                               ETA: 1180388.2s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.164s)
               Value function loss: 15.4378
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 462.71
               Mean episode length: 248.71
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 11.46s
                        Total time: 17500.29s
                               ETA: 1180341.4s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.419s, learning 0.259s)
               Value function loss: 12.9445
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 457.86
               Mean episode length: 248.87
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 11.68s
                        Total time: 17511.97s
                               ETA: 1180309.2s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.785s, learning 0.178s)
               Value function loss: 11.8414
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 454.84
               Mean episode length: 248.76
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 11.96s
                        Total time: 17523.93s
                               ETA: 1180296.2s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.165s)
               Value function loss: 15.3972
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 450.14
               Mean episode length: 248.17
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.36s
                        Total time: 17535.29s
                               ETA: 1180242.6s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.188s, learning 0.161s)
               Value function loss: 12.5402
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 455.05
               Mean episode length: 248.63
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 11.35s
                        Total time: 17546.64s
                               ETA: 1180188.3s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.162s)
               Value function loss: 13.5338
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 455.74
               Mean episode length: 248.77
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 11.33s
                        Total time: 17557.97s
                               ETA: 1180133.1s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.548s, learning 0.193s)
               Value function loss: 13.3595
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 453.17
               Mean episode length: 248.74
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 11.74s
                        Total time: 17569.72s
                               ETA: 1180105.3s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.157s, learning 0.165s)
               Value function loss: 13.1033
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 450.57
               Mean episode length: 248.39
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 11.32s
                        Total time: 17581.04s
                               ETA: 1180049.4s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.163s)
               Value function loss: 13.0122
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 452.29
               Mean episode length: 249.04
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 11.55s
                        Total time: 17592.59s
                               ETA: 1180008.7s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.169s, learning 0.159s)
               Value function loss: 15.3551
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 460.83
               Mean episode length: 248.94
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 11.33s
                        Total time: 17603.91s
                               ETA: 1179953.3s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.406s, learning 0.162s)
               Value function loss: 12.0989
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 452.84
               Mean episode length: 248.30
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 11.57s
                        Total time: 17615.48s
                               ETA: 1179914.0s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.200s, learning 0.169s)
               Value function loss: 13.5673
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 470.33
               Mean episode length: 249.18
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 11.37s
                        Total time: 17626.85s
                               ETA: 1179861.5s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.369s, learning 0.223s)
               Value function loss: 16.0127
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 475.69
               Mean episode length: 248.76
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 11.59s
                        Total time: 17638.44s
                               ETA: 1179824.0s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.286s, learning 0.166s)
               Value function loss: 10.5930
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 473.71
               Mean episode length: 248.89
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 11.45s
                        Total time: 17649.90s
                               ETA: 1179777.1s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.169s)
               Value function loss: 13.9346
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 466.19
               Mean episode length: 248.11
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 11.53s
                        Total time: 17661.43s
                               ETA: 1179735.4s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.151s, learning 0.162s)
               Value function loss: 13.2657
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 450.21
               Mean episode length: 246.67
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 11.31s
                        Total time: 17672.74s
                               ETA: 1179679.4s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.437s, learning 0.163s)
               Value function loss: 12.4936
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 436.06
               Mean episode length: 246.64
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 11.60s
                        Total time: 17684.34s
                               ETA: 1179642.5s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.202s, learning 0.166s)
               Value function loss: 17.8538
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 451.60
               Mean episode length: 247.06
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 11.37s
                        Total time: 17695.71s
                               ETA: 1179590.2s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.066s, learning 0.159s)
               Value function loss: 17.7124
                    Surrogate loss: 0.0007
             Mean action noise std: 0.75
                       Mean reward: 468.49
               Mean episode length: 248.17
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 11.22s
                        Total time: 17706.93s
                               ETA: 1179528.4s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.447s, learning 0.164s)
               Value function loss: 14.5390
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 464.20
               Mean episode length: 248.24
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 11.61s
                        Total time: 17718.54s
                               ETA: 1179492.4s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.065s, learning 0.193s)
               Value function loss: 16.6110
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 460.65
               Mean episode length: 248.01
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 11.26s
                        Total time: 17729.80s
                               ETA: 1179432.9s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.646s, learning 0.180s)
               Value function loss: 20.9901
                    Surrogate loss: 0.0055
             Mean action noise std: 0.75
                       Mean reward: 479.14
               Mean episode length: 247.63
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 10.83s
                        Total time: 17740.63s
                               ETA: 1179344.8s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.150s, learning 0.174s)
               Value function loss: 18.1201
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 480.38
               Mean episode length: 247.75
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 11.32s
                        Total time: 17751.95s
                               ETA: 1179289.9s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.465s, learning 0.185s)
               Value function loss: 15.3962
                    Surrogate loss: 0.0051
             Mean action noise std: 0.75
                       Mean reward: 477.85
               Mean episode length: 248.56
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 11.65s
                        Total time: 17763.60s
                               ETA: 1179256.6s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.631s, learning 0.167s)
               Value function loss: 15.8788
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 464.24
               Mean episode length: 247.85
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 11.80s
                        Total time: 17775.40s
                               ETA: 1179233.2s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.109s, learning 0.172s)
               Value function loss: 16.1653
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 470.84
               Mean episode length: 248.32
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 11.28s
                        Total time: 17786.68s
                               ETA: 1179175.6s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.161s)
               Value function loss: 18.0908
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 469.16
               Mean episode length: 248.42
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 11.25s
                        Total time: 17797.93s
                               ETA: 1179115.7s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.162s)
               Value function loss: 20.9248
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 475.64
               Mean episode length: 248.74
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 11.17s
                        Total time: 17809.10s
                               ETA: 1179051.1s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.093s, learning 0.167s)
               Value function loss: 17.1065
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 472.31
               Mean episode length: 247.84
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 11.26s
                        Total time: 17820.36s
                               ETA: 1178992.3s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.166s, learning 0.258s)
               Value function loss: 18.3373
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 468.98
               Mean episode length: 248.96
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 11.42s
                        Total time: 17831.79s
                               ETA: 1178944.4s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.303s, learning 0.158s)
               Value function loss: 15.0228
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 462.73
               Mean episode length: 248.03
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 11.46s
                        Total time: 17843.25s
                               ETA: 1178899.0s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.160s)
               Value function loss: 16.2199
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 473.22
               Mean episode length: 248.66
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 11.15s
                        Total time: 17854.39s
                               ETA: 1178832.8s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.533s, learning 0.160s)
               Value function loss: 15.1676
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 468.72
               Mean episode length: 248.42
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 11.69s
                        Total time: 17866.09s
                               ETA: 1178802.8s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.194s, learning 0.188s)
               Value function loss: 13.4702
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 461.61
               Mean episode length: 246.35
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 11.38s
                        Total time: 17877.47s
                               ETA: 1178752.2s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.166s)
               Value function loss: 16.5704
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 470.35
               Mean episode length: 246.67
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 11.52s
                        Total time: 17888.99s
                               ETA: 1178710.9s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.131s, learning 0.168s)
               Value function loss: 14.8049
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 486.95
               Mean episode length: 249.22
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 11.30s
                        Total time: 17900.29s
                               ETA: 1178655.0s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.255s, learning 0.162s)
               Value function loss: 15.1772
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 489.40
               Mean episode length: 247.91
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 11.42s
                        Total time: 17911.71s
                               ETA: 1178607.0s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.162s)
               Value function loss: 17.3190
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 481.96
               Mean episode length: 248.79
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 11.40s
                        Total time: 17923.10s
                               ETA: 1178557.7s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.095s, learning 0.185s)
               Value function loss: 16.8324
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 485.21
               Mean episode length: 248.59
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 11.28s
                        Total time: 17934.38s
                               ETA: 1178500.8s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.586s, learning 0.161s)
               Value function loss: 17.1195
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 480.17
               Mean episode length: 246.67
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 11.75s
                        Total time: 17946.13s
                               ETA: 1178474.5s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.112s, learning 0.179s)
               Value function loss: 22.3854
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 476.99
               Mean episode length: 246.86
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 11.29s
                        Total time: 17957.42s
                               ETA: 1178418.4s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.081s, learning 0.187s)
               Value function loss: 16.0142
                    Surrogate loss: 0.0277
             Mean action noise std: 0.75
                       Mean reward: 479.72
               Mean episode length: 249.31
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 11.27s
                        Total time: 17968.69s
                               ETA: 1178360.8s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.418s, learning 0.167s)
               Value function loss: 18.1756
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 481.40
               Mean episode length: 248.86
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.58s
                        Total time: 17980.27s
                               ETA: 1178324.0s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.585s, learning 0.165s)
               Value function loss: 19.7159
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 498.64
               Mean episode length: 249.21
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 11.75s
                        Total time: 17992.02s
                               ETA: 1178298.1s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.163s)
               Value function loss: 15.9847
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 493.72
               Mean episode length: 247.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 11.45s
                        Total time: 18003.48s
                               ETA: 1178252.8s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.884s, learning 0.185s)
               Value function loss: 21.2449
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 493.88
               Mean episode length: 248.19
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 11.07s
                        Total time: 18014.55s
                               ETA: 1178182.4s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.390s, learning 0.165s)
               Value function loss: 21.9301
                    Surrogate loss: 0.0141
             Mean action noise std: 0.75
                       Mean reward: 489.72
               Mean episode length: 248.51
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 11.55s
                        Total time: 18026.10s
                               ETA: 1178143.9s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.428s, learning 0.169s)
               Value function loss: 21.8904
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 494.76
               Mean episode length: 247.81
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 11.60s
                        Total time: 18037.70s
                               ETA: 1178108.0s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.200s, learning 0.227s)
               Value function loss: 16.4868
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 502.98
               Mean episode length: 248.41
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 11.43s
                        Total time: 18049.12s
                               ETA: 1178061.2s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.166s)
               Value function loss: 17.7223
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 499.24
               Mean episode length: 248.83
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 11.59s
                        Total time: 18060.72s
                               ETA: 1178025.4s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.343s, learning 0.175s)
               Value function loss: 16.6293
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 492.41
               Mean episode length: 248.42
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 11.52s
                        Total time: 18072.24s
                               ETA: 1177984.6s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.283s, learning 0.159s)
               Value function loss: 18.3203
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: 496.46
               Mean episode length: 249.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 11.44s
                        Total time: 18083.68s
                               ETA: 1177938.9s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.620s, learning 0.160s)
               Value function loss: 15.7512
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 489.35
               Mean episode length: 248.10
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 11.78s
                        Total time: 18095.46s
                               ETA: 1177915.2s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.310s, learning 0.266s)
               Value function loss: 17.2488
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 491.16
               Mean episode length: 247.91
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 11.58s
                        Total time: 18107.04s
                               ETA: 1177878.3s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.162s)
               Value function loss: 15.6386
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 492.60
               Mean episode length: 248.98
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 11.32s
                        Total time: 18118.36s
                               ETA: 1177824.9s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.971s, learning 0.164s)
               Value function loss: 21.4547
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 499.47
               Mean episode length: 247.91
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 11.14s
                        Total time: 18129.49s
                               ETA: 1177759.4s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.447s, learning 0.169s)
               Value function loss: 19.7337
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 493.11
               Mean episode length: 247.17
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 11.62s
                        Total time: 18141.11s
                               ETA: 1177725.2s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.165s)
               Value function loss: 20.7554
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 486.07
               Mean episode length: 246.65
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 11.43s
                        Total time: 18152.54s
                               ETA: 1177678.9s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.169s, learning 0.174s)
               Value function loss: 23.2454
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 507.78
               Mean episode length: 249.02
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 11.34s
                        Total time: 18163.88s
                               ETA: 1177627.0s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.272s, learning 0.185s)
               Value function loss: 21.3054
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 493.22
               Mean episode length: 248.92
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 11.46s
                        Total time: 18175.34s
                               ETA: 1177582.7s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.072s, learning 0.166s)
               Value function loss: 15.4217
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 499.43
               Mean episode length: 247.32
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 11.24s
                        Total time: 18186.58s
                               ETA: 1177524.1s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.156s, learning 0.186s)
               Value function loss: 16.9777
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 491.59
               Mean episode length: 246.73
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 11.34s
                        Total time: 18197.92s
                               ETA: 1177472.4s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.022s, learning 0.167s)
               Value function loss: 17.3881
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 491.13
               Mean episode length: 248.71
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 11.19s
                        Total time: 18209.11s
                               ETA: 1177410.8s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.087s, learning 0.168s)
               Value function loss: 16.8812
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 500.36
               Mean episode length: 249.28
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 11.25s
                        Total time: 18220.36s
                               ETA: 1177353.5s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.379s, learning 0.169s)
               Value function loss: 18.2372
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 509.24
               Mean episode length: 249.22
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 11.55s
                        Total time: 18231.91s
                               ETA: 1177315.3s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.286s, learning 0.164s)
               Value function loss: 18.4700
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 504.11
               Mean episode length: 248.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 11.45s
                        Total time: 18243.36s
                               ETA: 1177270.7s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.999s, learning 0.159s)
               Value function loss: 19.2960
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 508.60
               Mean episode length: 248.34
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 11.16s
                        Total time: 18254.52s
                               ETA: 1177207.3s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.364s, learning 0.171s)
               Value function loss: 13.2513
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 516.72
               Mean episode length: 249.19
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 11.54s
                        Total time: 18266.06s
                               ETA: 1177168.4s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.578s, learning 0.169s)
               Value function loss: 16.5289
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 507.85
               Mean episode length: 248.04
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 11.75s
                        Total time: 18277.80s
                               ETA: 1177143.1s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.003s, learning 0.159s)
               Value function loss: 11.9545
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 513.84
               Mean episode length: 249.20
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 11.16s
                        Total time: 18288.96s
                               ETA: 1177080.1s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.238s, learning 0.160s)
               Value function loss: 17.7877
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 511.30
               Mean episode length: 248.71
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 11.40s
                        Total time: 18300.36s
                               ETA: 1177032.4s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.354s, learning 0.170s)
               Value function loss: 15.0763
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 494.92
               Mean episode length: 248.89
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 11.52s
                        Total time: 18311.88s
                               ETA: 1176992.8s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.427s, learning 0.164s)
               Value function loss: 13.8606
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 492.48
               Mean episode length: 248.90
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 10.59s
                        Total time: 18322.48s
                               ETA: 1176893.3s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.404s, learning 0.161s)
               Value function loss: 14.6761
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 501.91
               Mean episode length: 249.19
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 11.57s
                        Total time: 18334.04s
                               ETA: 1176856.6s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.184s, learning 0.158s)
               Value function loss: 17.2062
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 519.84
               Mean episode length: 249.92
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 11.34s
                        Total time: 18345.38s
                               ETA: 1176805.5s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.390s, learning 0.167s)
               Value function loss: 13.8841
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 514.92
               Mean episode length: 249.63
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 11.56s
                        Total time: 18356.94s
                               ETA: 1176768.3s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.098s, learning 0.160s)
               Value function loss: 17.0250
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 519.98
               Mean episode length: 249.65
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 11.26s
                        Total time: 18368.20s
                               ETA: 1176712.0s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.388s, learning 0.160s)
               Value function loss: 14.7669
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 534.48
               Mean episode length: 249.61
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 11.55s
                        Total time: 18379.75s
                               ETA: 1176674.3s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.139s, learning 0.159s)
               Value function loss: 17.4893
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 536.78
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 11.30s
                        Total time: 18391.05s
                               ETA: 1176620.6s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.293s, learning 0.163s)
               Value function loss: 14.7503
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 525.04
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 11.46s
                        Total time: 18402.50s
                               ETA: 1176577.1s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.284s, learning 0.157s)
               Value function loss: 19.6452
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 513.37
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 11.44s
                        Total time: 18413.94s
                               ETA: 1176532.7s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.214s, learning 0.159s)
               Value function loss: 18.2584
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 514.62
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 11.37s
                        Total time: 18425.32s
                               ETA: 1176484.0s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.259s, learning 0.188s)
               Value function loss: 17.8101
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 522.58
               Mean episode length: 249.75
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 11.45s
                        Total time: 18436.76s
                               ETA: 1176440.0s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.093s, learning 0.180s)
               Value function loss: 19.9464
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 518.43
               Mean episode length: 249.75
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 11.27s
                        Total time: 18448.04s
                               ETA: 1176385.0s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.991s, learning 0.164s)
               Value function loss: 20.7134
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 526.55
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 11.15s
                        Total time: 18459.19s
                               ETA: 1176322.5s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.537s, learning 0.159s)
               Value function loss: 21.3600
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 517.47
               Mean episode length: 249.84
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 11.70s
                        Total time: 18470.89s
                               ETA: 1176294.5s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.026s, learning 0.164s)
               Value function loss: 23.0277
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 525.50
               Mean episode length: 249.75
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 11.19s
                        Total time: 18482.08s
                               ETA: 1176234.3s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.092s, learning 0.162s)
               Value function loss: 22.4881
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 526.86
               Mean episode length: 249.50
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 11.25s
                        Total time: 18493.33s
                               ETA: 1176178.3s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.164s)
               Value function loss: 22.6199
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 533.14
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 11.36s
                        Total time: 18504.69s
                               ETA: 1176128.9s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.556s, learning 0.188s)
               Value function loss: 27.7464
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 532.61
               Mean episode length: 249.76
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 11.74s
                        Total time: 18516.43s
                               ETA: 1176104.1s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.145s, learning 0.171s)
               Value function loss: 34.9735
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 542.27
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 11.32s
                        Total time: 18527.75s
                               ETA: 1176052.1s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.126s, learning 0.173s)
               Value function loss: 28.3813
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 542.40
               Mean episode length: 249.74
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 11.30s
                        Total time: 18539.05s
                               ETA: 1175999.2s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.196s, learning 0.192s)
               Value function loss: 23.8508
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 544.09
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 11.39s
                        Total time: 18550.44s
                               ETA: 1175951.9s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.198s, learning 0.165s)
               Value function loss: 28.0667
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 537.91
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 11.36s
                        Total time: 18561.80s
                               ETA: 1175903.1s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.078s, learning 0.171s)
               Value function loss: 25.8530
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 544.28
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 11.25s
                        Total time: 18573.05s
                               ETA: 1175847.1s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.325s, learning 0.193s)
               Value function loss: 25.0100
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 540.58
               Mean episode length: 249.67
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 11.52s
                        Total time: 18584.56s
                               ETA: 1175808.1s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.111s, learning 0.169s)
               Value function loss: 23.5746
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 539.33
               Mean episode length: 249.52
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 11.28s
                        Total time: 18595.84s
                               ETA: 1175754.2s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.797s, learning 0.182s)
               Value function loss: 31.1800
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 548.20
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 10.98s
                        Total time: 18606.82s
                               ETA: 1175681.3s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.500s, learning 0.164s)
               Value function loss: 18.0886
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 541.14
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 11.66s
                        Total time: 18618.49s
                               ETA: 1175651.8s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.960s, learning 0.169s)
               Value function loss: 32.6149
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 540.54
               Mean episode length: 249.67
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 11.13s
                        Total time: 18629.62s
                               ETA: 1175588.5s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.259s, learning 0.157s)
               Value function loss: 25.3493
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 545.70
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 11.42s
                        Total time: 18641.03s
                               ETA: 1175543.4s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.040s, learning 0.167s)
               Value function loss: 28.1542
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 546.68
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 11.21s
                        Total time: 18652.24s
                               ETA: 1175485.1s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.874s, learning 0.162s)
               Value function loss: 33.1762
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 543.11
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 12.04s
                        Total time: 18664.28s
                               ETA: 1175479.2s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.149s, learning 0.171s)
               Value function loss: 27.4235
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 543.45
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 11.32s
                        Total time: 18675.59s
                               ETA: 1175428.1s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.069s, learning 0.181s)
               Value function loss: 27.5547
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 546.33
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 11.25s
                        Total time: 18686.84s
                               ETA: 1175372.7s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.295s, learning 0.191s)
               Value function loss: 27.6329
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 542.63
               Mean episode length: 249.55
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 11.49s
                        Total time: 18698.33s
                               ETA: 1175332.1s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.664s, learning 0.172s)
               Value function loss: 24.3027
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 546.51
               Mean episode length: 249.75
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 11.84s
                        Total time: 18710.17s
                               ETA: 1175313.7s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.165s)
               Value function loss: 27.2161
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 540.53
               Mean episode length: 249.49
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 11.50s
                        Total time: 18721.67s
                               ETA: 1175274.2s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.168s)
               Value function loss: 26.0483
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 540.80
               Mean episode length: 249.80
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 11.55s
                        Total time: 18733.21s
                               ETA: 1175237.6s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.638s, learning 0.166s)
               Value function loss: 25.7187
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 534.67
               Mean episode length: 249.25
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 11.80s
                        Total time: 18745.02s
                               ETA: 1175217.2s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.242s, learning 0.186s)
               Value function loss: 23.2565
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 535.82
               Mean episode length: 248.97
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 11.43s
                        Total time: 18756.45s
                               ETA: 1175173.2s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.451s, learning 0.179s)
               Value function loss: 27.9201
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 545.78
               Mean episode length: 249.69
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 11.63s
                        Total time: 18768.08s
                               ETA: 1175141.9s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.165s, learning 0.192s)
               Value function loss: 29.7402
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 538.34
               Mean episode length: 249.35
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 11.36s
                        Total time: 18779.43s
                               ETA: 1175093.5s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.466s, learning 0.166s)
               Value function loss: 24.1158
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 534.53
               Mean episode length: 248.31
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 11.63s
                        Total time: 18791.07s
                               ETA: 1175062.5s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.384s, learning 0.176s)
               Value function loss: 26.2913
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 539.78
               Mean episode length: 248.51
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 11.56s
                        Total time: 18802.63s
                               ETA: 1175026.9s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.365s, learning 0.172s)
               Value function loss: 27.7515
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 549.60
               Mean episode length: 249.76
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 11.54s
                        Total time: 18814.16s
                               ETA: 1174989.9s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.110s, learning 0.160s)
               Value function loss: 27.3480
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 548.48
               Mean episode length: 249.58
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 11.27s
                        Total time: 18825.43s
                               ETA: 1174936.3s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.063s, learning 0.171s)
               Value function loss: 31.6297
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 544.04
               Mean episode length: 248.60
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 11.23s
                        Total time: 18836.67s
                               ETA: 1174880.5s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.409s, learning 0.179s)
               Value function loss: 26.0029
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 536.69
               Mean episode length: 247.21
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 11.59s
                        Total time: 18848.26s
                               ETA: 1174846.8s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.228s, learning 0.167s)
               Value function loss: 29.7231
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 544.13
               Mean episode length: 248.66
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 11.40s
                        Total time: 18859.65s
                               ETA: 1174801.1s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.251s, learning 0.271s)
               Value function loss: 33.5664
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 553.11
               Mean episode length: 249.63
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 11.52s
                        Total time: 18871.17s
                               ETA: 1174763.4s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.120s, learning 0.160s)
               Value function loss: 41.8929
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 537.60
               Mean episode length: 248.07
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 11.28s
                        Total time: 18882.45s
                               ETA: 1174710.7s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.405s, learning 0.177s)
               Value function loss: 27.6231
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 550.50
               Mean episode length: 249.57
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 11.58s
                        Total time: 18894.04s
                               ETA: 1174676.7s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.163s)
               Value function loss: 27.6307
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 549.66
               Mean episode length: 249.37
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 11.45s
                        Total time: 18905.48s
                               ETA: 1174634.3s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.367s, learning 0.173s)
               Value function loss: 30.9500
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 552.60
               Mean episode length: 249.65
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 11.54s
                        Total time: 18917.02s
                               ETA: 1174597.9s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.248s, learning 0.181s)
               Value function loss: 26.6540
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 545.40
               Mean episode length: 248.41
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 11.43s
                        Total time: 18928.45s
                               ETA: 1174554.5s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.200s, learning 0.190s)
               Value function loss: 22.4868
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 544.51
               Mean episode length: 247.67
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 11.39s
                        Total time: 18939.84s
                               ETA: 1174508.8s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.309s, learning 0.164s)
               Value function loss: 19.2003
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 544.15
               Mean episode length: 248.06
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 11.47s
                        Total time: 18951.31s
                               ETA: 1174468.3s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.012s, learning 0.185s)
               Value function loss: 25.5090
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 536.56
               Mean episode length: 246.83
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 11.20s
                        Total time: 18962.51s
                               ETA: 1174410.8s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.423s, learning 0.160s)
               Value function loss: 23.2821
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 537.46
               Mean episode length: 247.34
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 11.58s
                        Total time: 18974.10s
                               ETA: 1174377.1s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.357s, learning 0.188s)
               Value function loss: 20.8122
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 544.28
               Mean episode length: 248.07
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 11.54s
                        Total time: 18985.64s
                               ETA: 1174341.2s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.088s, learning 0.186s)
               Value function loss: 20.9806
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 548.04
               Mean episode length: 248.92
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 11.27s
                        Total time: 18996.91s
                               ETA: 1174288.5s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.151s, learning 0.167s)
               Value function loss: 21.4608
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 542.31
               Mean episode length: 247.61
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 11.32s
                        Total time: 19008.23s
                               ETA: 1174238.6s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.479s, learning 0.184s)
               Value function loss: 20.5139
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 543.93
               Mean episode length: 247.84
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 11.66s
                        Total time: 19019.90s
                               ETA: 1174210.1s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.295s, learning 0.170s)
               Value function loss: 21.0049
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 556.07
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 11.47s
                        Total time: 19031.36s
                               ETA: 1174169.3s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.999s, learning 0.159s)
               Value function loss: 17.6972
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 549.44
               Mean episode length: 249.13
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 11.16s
                        Total time: 19042.52s
                               ETA: 1174109.6s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.119s, learning 0.179s)
               Value function loss: 15.2276
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 547.24
               Mean episode length: 248.41
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 11.30s
                        Total time: 19053.82s
                               ETA: 1174058.7s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.394s, learning 0.159s)
               Value function loss: 17.6583
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 543.78
               Mean episode length: 247.52
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 11.55s
                        Total time: 19065.37s
                               ETA: 1174023.5s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.145s, learning 0.190s)
               Value function loss: 12.4848
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 545.42
               Mean episode length: 247.63
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 11.34s
                        Total time: 19076.70s
                               ETA: 1173974.9s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.406s, learning 0.174s)
               Value function loss: 15.6359
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 546.49
               Mean episode length: 247.96
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 11.58s
                        Total time: 19088.28s
                               ETA: 1173941.4s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.169s)
               Value function loss: 16.9514
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 534.22
               Mean episode length: 245.91
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 11.27s
                        Total time: 19099.56s
                               ETA: 1173889.0s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.159s)
               Value function loss: 16.8808
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 529.65
               Mean episode length: 245.99
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 11.39s
                        Total time: 19110.94s
                               ETA: 1173843.7s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.182s, learning 0.167s)
               Value function loss: 14.1361
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 541.57
               Mean episode length: 247.89
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 11.35s
                        Total time: 19122.29s
                               ETA: 1173796.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.243s, learning 0.168s)
               Value function loss: 19.0081
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 540.95
               Mean episode length: 247.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 11.41s
                        Total time: 19133.70s
                               ETA: 1173752.5s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.990s, learning 0.187s)
               Value function loss: 15.7002
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 533.03
               Mean episode length: 245.63
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 11.18s
                        Total time: 19144.88s
                               ETA: 1173694.4s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.985s, learning 0.171s)
               Value function loss: 14.2615
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 538.66
               Mean episode length: 246.99
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 11.16s
                        Total time: 19156.03s
                               ETA: 1173635.1s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.180s, learning 0.172s)
               Value function loss: 13.0274
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 547.12
               Mean episode length: 248.59
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 11.35s
                        Total time: 19167.39s
                               ETA: 1173587.9s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.344s, learning 0.195s)
               Value function loss: 12.7279
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 534.37
               Mean episode length: 246.51
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 11.54s
                        Total time: 19178.92s
                               ETA: 1173552.2s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.228s, learning 0.179s)
               Value function loss: 15.7520
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 541.32
               Mean episode length: 247.57
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 11.41s
                        Total time: 19190.33s
                               ETA: 1173508.5s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.160s)
               Value function loss: 21.5644
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 552.53
               Mean episode length: 249.02
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 11.33s
                        Total time: 19201.66s
                               ETA: 1173459.8s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.497s, learning 0.188s)
               Value function loss: 18.3936
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 550.18
               Mean episode length: 249.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 11.68s
                        Total time: 19213.34s
                               ETA: 1173433.1s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.281s, learning 0.159s)
               Value function loss: 19.0761
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 544.96
               Mean episode length: 247.69
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 11.44s
                        Total time: 19224.78s
                               ETA: 1173391.5s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.255s, learning 0.163s)
               Value function loss: 20.7973
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 545.09
               Mean episode length: 248.09
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 11.42s
                        Total time: 19236.20s
                               ETA: 1173348.5s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.439s, learning 0.161s)
               Value function loss: 16.5612
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 556.01
               Mean episode length: 249.30
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 11.60s
                        Total time: 19247.80s
                               ETA: 1173316.8s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.543s, learning 0.167s)
               Value function loss: 21.9397
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 545.45
               Mean episode length: 248.06
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 11.71s
                        Total time: 19259.51s
                               ETA: 1173291.7s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.031s, learning 0.159s)
               Value function loss: 17.0261
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 541.89
               Mean episode length: 247.53
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 11.19s
                        Total time: 19270.70s
                               ETA: 1173235.0s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.214s, learning 0.189s)
               Value function loss: 19.7855
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 554.32
               Mean episode length: 249.36
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 11.40s
                        Total time: 19282.10s
                               ETA: 1173191.3s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.009s, learning 0.166s)
               Value function loss: 17.3539
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 549.72
               Mean episode length: 248.31
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 11.17s
                        Total time: 19293.28s
                               ETA: 1173133.8s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.158s, learning 0.168s)
               Value function loss: 15.3252
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 557.03
               Mean episode length: 248.96
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 11.33s
                        Total time: 19304.60s
                               ETA: 1173085.5s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.172s)
               Value function loss: 19.4486
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 559.01
               Mean episode length: 249.29
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 11.38s
                        Total time: 19315.99s
                               ETA: 1173040.9s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.582s, learning 0.159s)
               Value function loss: 16.4247
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 554.77
               Mean episode length: 248.96
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 11.74s
                        Total time: 19327.73s
                               ETA: 1173017.9s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.329s, learning 0.175s)
               Value function loss: 15.4780
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 561.28
               Mean episode length: 249.69
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 11.50s
                        Total time: 19339.23s
                               ETA: 1172980.5s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.009s, learning 0.217s)
               Value function loss: 18.6726
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 562.63
               Mean episode length: 249.35
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 11.23s
                        Total time: 19350.46s
                               ETA: 1172926.4s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.163s)
               Value function loss: 17.0983
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 562.35
               Mean episode length: 249.35
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 11.51s
                        Total time: 19361.97s
                               ETA: 1172889.6s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.810s, learning 0.176s)
               Value function loss: 19.7292
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 559.63
               Mean episode length: 249.11
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 10.99s
                        Total time: 19372.96s
                               ETA: 1172821.0s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1460 steps/s (collection: 10.976s, learning 0.243s)
               Value function loss: 18.1158
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 558.32
               Mean episode length: 248.75
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 11.22s
                        Total time: 19384.18s
                               ETA: 1172766.5s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.335s, learning 0.163s)
               Value function loss: 15.7090
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 554.33
               Mean episode length: 247.78
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 11.50s
                        Total time: 19395.67s
                               ETA: 1172729.0s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.939s, learning 0.159s)
               Value function loss: 14.0809
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 552.72
               Mean episode length: 248.11
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 11.10s
                        Total time: 19406.77s
                               ETA: 1172667.3s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.801s, learning 0.271s)
               Value function loss: 16.4849
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 559.85
               Mean episode length: 249.58
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 11.07s
                        Total time: 19417.84s
                               ETA: 1172604.2s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.886s, learning 0.179s)
               Value function loss: 12.8031
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 564.54
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 11.07s
                        Total time: 19428.91s
                               ETA: 1172540.7s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.186s, learning 0.178s)
               Value function loss: 15.6295
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 558.42
               Mean episode length: 249.24
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 11.36s
                        Total time: 19440.27s
                               ETA: 1172495.3s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.263s, learning 0.260s)
               Value function loss: 11.4728
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 550.70
               Mean episode length: 248.51
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 11.52s
                        Total time: 19451.80s
                               ETA: 1172459.5s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.408s, learning 0.184s)
               Value function loss: 15.0230
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 549.28
               Mean episode length: 248.89
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 11.59s
                        Total time: 19463.39s
                               ETA: 1172427.9s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.556s, learning 0.163s)
               Value function loss: 12.1007
                    Surrogate loss: -0.0217
             Mean action noise std: 0.74
                       Mean reward: 540.29
               Mean episode length: 247.36
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 11.72s
                        Total time: 19475.11s
                               ETA: 1172403.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.237s, learning 0.162s)
               Value function loss: 14.1096
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 553.64
               Mean episode length: 249.30
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 11.40s
                        Total time: 19486.51s
                               ETA: 1172360.7s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.165s)
               Value function loss: 13.1909
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 552.76
               Mean episode length: 248.78
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 11.52s
                        Total time: 19498.02s
                               ETA: 1172324.6s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.159s)
               Value function loss: 11.6397
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 550.70
               Mean episode length: 248.46
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 11.33s
                        Total time: 19509.36s
                               ETA: 1172277.6s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.162s)
               Value function loss: 11.4477
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 559.24
               Mean episode length: 249.46
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 11.41s
                        Total time: 19520.76s
                               ETA: 1172234.9s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.289s, learning 0.163s)
               Value function loss: 17.2313
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 551.01
               Mean episode length: 248.76
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 11.45s
                        Total time: 19532.21s
                               ETA: 1172195.1s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.080s, learning 0.164s)
               Value function loss: 11.8598
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: 544.19
               Mean episode length: 247.77
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 11.24s
                        Total time: 19543.46s
                               ETA: 1172142.8s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.225s, learning 0.163s)
               Value function loss: 14.7854
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 553.07
               Mean episode length: 249.52
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 11.39s
                        Total time: 19554.85s
                               ETA: 1172099.2s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.758s, learning 0.170s)
               Value function loss: 14.2094
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 547.19
               Mean episode length: 249.41
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 11.93s
                        Total time: 19566.78s
                               ETA: 1172088.0s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.456s, learning 0.204s)
               Value function loss: 13.3417
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 539.45
               Mean episode length: 248.13
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 11.66s
                        Total time: 19578.43s
                               ETA: 1172060.7s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.294s, learning 0.159s)
               Value function loss: 15.6488
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 544.75
               Mean episode length: 249.31
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 11.45s
                        Total time: 19589.89s
                               ETA: 1172021.0s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.395s, learning 0.188s)
               Value function loss: 13.9010
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 543.24
               Mean episode length: 248.57
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 11.58s
                        Total time: 19601.47s
                               ETA: 1171989.2s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.929s, learning 0.162s)
               Value function loss: 11.5154
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 538.71
               Mean episode length: 248.27
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 11.09s
                        Total time: 19612.56s
                               ETA: 1171928.0s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.420s, learning 0.162s)
               Value function loss: 12.8113
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 543.39
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 11.58s
                        Total time: 19624.14s
                               ETA: 1171896.2s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.130s, learning 0.173s)
               Value function loss: 13.5526
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 544.99
               Mean episode length: 249.40
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 11.30s
                        Total time: 19635.45s
                               ETA: 1171847.7s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.068s, learning 0.167s)
               Value function loss: 10.0536
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 540.29
               Mean episode length: 249.02
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 11.23s
                        Total time: 19646.68s
                               ETA: 1171795.3s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.136s, learning 0.190s)
               Value function loss: 9.6621
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 535.85
               Mean episode length: 249.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 11.33s
                        Total time: 19658.01s
                               ETA: 1171748.3s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.950s, learning 0.165s)
               Value function loss: 10.2033
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 531.90
               Mean episode length: 249.20
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 11.11s
                        Total time: 19669.12s
                               ETA: 1171688.8s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.164s)
               Value function loss: 9.1262
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 530.89
               Mean episode length: 248.61
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 11.33s
                        Total time: 19680.45s
                               ETA: 1171642.1s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.391s, learning 0.190s)
               Value function loss: 7.4359
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 525.64
               Mean episode length: 248.78
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 11.58s
                        Total time: 19692.03s
                               ETA: 1171610.5s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.836s, learning 0.167s)
               Value function loss: 8.8775
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 520.16
               Mean episode length: 247.99
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 11.00s
                        Total time: 19703.04s
                               ETA: 1171544.5s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.492s, learning 0.273s)
               Value function loss: 9.2846
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 527.47
               Mean episode length: 248.89
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 11.77s
                        Total time: 19714.80s
                               ETA: 1171523.8s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.953s, learning 0.169s)
               Value function loss: 9.8778
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 530.15
               Mean episode length: 249.58
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 11.12s
                        Total time: 19725.92s
                               ETA: 1171464.9s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.438s, learning 0.163s)
               Value function loss: 10.9035
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 530.31
               Mean episode length: 249.58
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 11.60s
                        Total time: 19737.52s
                               ETA: 1171434.6s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.164s)
               Value function loss: 8.7364
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 532.18
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 11.52s
                        Total time: 19749.04s
                               ETA: 1171399.3s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.390s, learning 0.186s)
               Value function loss: 8.8409
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 531.32
               Mean episode length: 249.84
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 11.58s
                        Total time: 19760.62s
                               ETA: 1171367.5s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.688s, learning 0.176s)
               Value function loss: 10.3046
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 532.19
               Mean episode length: 249.07
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 11.86s
                        Total time: 19772.48s
                               ETA: 1171352.8s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.064s, learning 0.179s)
               Value function loss: 9.8465
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 530.66
               Mean episode length: 248.17
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 11.24s
                        Total time: 19783.72s
                               ETA: 1171301.3s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.453s, learning 0.169s)
               Value function loss: 11.2706
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 542.44
               Mean episode length: 249.27
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 11.62s
                        Total time: 19795.35s
                               ETA: 1171272.3s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.013s, learning 0.159s)
               Value function loss: 9.3558
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 538.53
               Mean episode length: 249.33
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 11.17s
                        Total time: 19806.52s
                               ETA: 1171216.7s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.561s, learning 0.164s)
               Value function loss: 11.5810
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 530.90
               Mean episode length: 248.20
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 11.72s
                        Total time: 19818.24s
                               ETA: 1171193.9s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.345s, learning 0.159s)
               Value function loss: 10.6136
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 528.34
               Mean episode length: 246.87
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 11.50s
                        Total time: 19829.75s
                               ETA: 1171158.0s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.972s, learning 0.167s)
               Value function loss: 11.3451
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 529.34
               Mean episode length: 248.31
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 11.14s
                        Total time: 19840.89s
                               ETA: 1171100.5s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.481s, learning 0.170s)
               Value function loss: 12.5973
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 532.67
               Mean episode length: 249.84
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 11.65s
                        Total time: 19852.54s
                               ETA: 1171073.4s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.435s, learning 0.183s)
               Value function loss: 11.6109
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 531.28
               Mean episode length: 249.68
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 11.62s
                        Total time: 19864.16s
                               ETA: 1171044.3s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.909s, learning 0.160s)
               Value function loss: 11.8422
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 537.72
               Mean episode length: 249.84
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 11.07s
                        Total time: 19875.22s
                               ETA: 1170982.9s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.926s, learning 0.168s)
               Value function loss: 16.7076
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 532.41
               Mean episode length: 248.82
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 11.09s
                        Total time: 19886.32s
                               ETA: 1170923.1s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.863s, learning 0.186s)
               Value function loss: 15.6149
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 532.25
               Mean episode length: 248.36
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 11.05s
                        Total time: 19897.37s
                               ETA: 1170860.6s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.160s)
               Value function loss: 18.9056
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 537.38
               Mean episode length: 249.23
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 11.23s
                        Total time: 19908.60s
                               ETA: 1170809.1s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.430s, learning 0.168s)
               Value function loss: 19.2641
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 533.08
               Mean episode length: 248.24
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 11.60s
                        Total time: 19920.20s
                               ETA: 1170779.0s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.170s)
               Value function loss: 21.1065
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 535.54
               Mean episode length: 248.54
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 11.51s
                        Total time: 19931.71s
                               ETA: 1170743.9s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.644s, learning 0.219s)
               Value function loss: 23.9508
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 534.31
               Mean episode length: 246.89
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 11.86s
                        Total time: 19943.57s
                               ETA: 1170729.4s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.516s, learning 0.186s)
               Value function loss: 28.5059
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 557.74
               Mean episode length: 249.47
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 11.70s
                        Total time: 19955.28s
                               ETA: 1170705.6s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.248s, learning 0.197s)
               Value function loss: 18.1712
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 545.39
               Mean episode length: 249.49
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 11.45s
                        Total time: 19966.72s
                               ETA: 1170666.6s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.920s, learning 0.163s)
               Value function loss: 20.2703
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 547.05
               Mean episode length: 249.06
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 11.08s
                        Total time: 19977.80s
                               ETA: 1170606.5s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.165s)
               Value function loss: 19.5877
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 536.06
               Mean episode length: 247.84
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 11.53s
                        Total time: 19989.33s
                               ETA: 1170572.6s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.032s, learning 0.196s)
               Value function loss: 15.6670
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 548.50
               Mean episode length: 249.28
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 11.23s
                        Total time: 20000.56s
                               ETA: 1170521.1s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.163s)
               Value function loss: 15.6652
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 555.69
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 11.50s
                        Total time: 20012.07s
                               ETA: 1170485.6s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.894s, learning 0.172s)
               Value function loss: 14.4215
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 541.51
               Mean episode length: 249.40
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 11.07s
                        Total time: 20023.13s
                               ETA: 1170424.7s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.136s, learning 0.164s)
               Value function loss: 22.7637
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 554.09
               Mean episode length: 249.69
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 11.30s
                        Total time: 20034.43s
                               ETA: 1170377.5s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.328s, learning 0.226s)
               Value function loss: 13.7813
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 558.16
               Mean episode length: 249.69
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 11.55s
                        Total time: 20045.99s
                               ETA: 1170345.2s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.159s, learning 0.160s)
               Value function loss: 17.4578
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 557.04
               Mean episode length: 249.02
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 11.32s
                        Total time: 20057.31s
                               ETA: 1170299.1s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.935s, learning 0.178s)
               Value function loss: 15.5131
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 559.25
               Mean episode length: 249.43
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 11.11s
                        Total time: 20068.42s
                               ETA: 1170241.1s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.209s, learning 0.232s)
               Value function loss: 16.0105
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 556.05
               Mean episode length: 249.83
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 11.44s
                        Total time: 20079.86s
                               ETA: 1170202.3s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.160s)
               Value function loss: 19.3038
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 556.35
               Mean episode length: 249.47
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 11.50s
                        Total time: 20091.36s
                               ETA: 1170166.8s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.165s)
               Value function loss: 18.1764
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 549.74
               Mean episode length: 249.45
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 11.43s
                        Total time: 20102.78s
                               ETA: 1170127.2s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.142s, learning 0.186s)
               Value function loss: 17.2158
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 552.41
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 11.33s
                        Total time: 20114.11s
                               ETA: 1170081.9s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.164s)
               Value function loss: 21.5932
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 555.92
               Mean episode length: 249.70
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 11.50s
                        Total time: 20125.61s
                               ETA: 1170046.6s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.960s, learning 0.165s)
               Value function loss: 14.2994
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 551.55
               Mean episode length: 249.45
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 11.13s
                        Total time: 20136.74s
                               ETA: 1169989.6s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.252s, learning 0.181s)
               Value function loss: 17.1511
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 549.51
               Mean episode length: 249.75
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 11.43s
                        Total time: 20148.17s
                               ETA: 1169950.5s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.167s)
               Value function loss: 20.7323
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 555.10
               Mean episode length: 249.53
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 11.53s
                        Total time: 20159.70s
                               ETA: 1169916.9s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.431s, learning 0.168s)
               Value function loss: 20.5862
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 544.00
               Mean episode length: 247.61
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 11.60s
                        Total time: 20171.30s
                               ETA: 1169887.5s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.256s, learning 0.183s)
               Value function loss: 19.6458
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 544.07
               Mean episode length: 248.67
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 11.44s
                        Total time: 20182.73s
                               ETA: 1169848.9s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.164s, learning 0.170s)
               Value function loss: 23.7320
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 565.65
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 11.33s
                        Total time: 20194.07s
                               ETA: 1169804.1s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.192s, learning 0.161s)
               Value function loss: 27.9226
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 558.58
               Mean episode length: 249.30
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 11.35s
                        Total time: 20205.42s
                               ETA: 1169760.6s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.957s, learning 0.166s)
               Value function loss: 19.5781
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 561.75
               Mean episode length: 249.30
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 11.12s
                        Total time: 20216.54s
                               ETA: 1169703.8s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.442s, learning 0.165s)
               Value function loss: 21.0940
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 567.71
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 11.61s
                        Total time: 20228.15s
                               ETA: 1169675.0s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.247s, learning 0.171s)
               Value function loss: 24.3612
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 570.02
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 11.42s
                        Total time: 20239.57s
                               ETA: 1169635.3s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.355s, learning 0.191s)
               Value function loss: 25.0927
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 567.24
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 11.55s
                        Total time: 20251.12s
                               ETA: 1169603.0s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.161s)
               Value function loss: 28.5185
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 562.61
               Mean episode length: 249.94
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 11.18s
                        Total time: 20262.29s
                               ETA: 1169549.5s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.041s, learning 0.171s)
               Value function loss: 26.0726
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 568.31
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 11.21s
                        Total time: 20273.50s
                               ETA: 1169498.1s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.234s, learning 0.171s)
               Value function loss: 31.9774
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 567.08
               Mean episode length: 248.99
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 11.41s
                        Total time: 20284.91s
                               ETA: 1169457.8s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.098s, learning 0.159s)
               Value function loss: 32.3376
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 564.21
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 11.26s
                        Total time: 20296.17s
                               ETA: 1169409.0s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.907s, learning 0.160s)
               Value function loss: 36.2910
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 566.75
               Mean episode length: 249.63
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 11.07s
                        Total time: 20307.23s
                               ETA: 1169349.3s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.205s, learning 0.163s)
               Value function loss: 20.6519
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 566.88
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 11.37s
                        Total time: 20318.60s
                               ETA: 1169307.0s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.767s, learning 0.168s)
               Value function loss: 28.3904
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 570.91
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 10.94s
                        Total time: 20329.54s
                               ETA: 1169239.8s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.164s)
               Value function loss: 33.7815
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 567.65
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 11.53s
                        Total time: 20341.06s
                               ETA: 1169206.7s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.172s, learning 0.184s)
               Value function loss: 30.2967
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 561.90
               Mean episode length: 249.43
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 11.36s
                        Total time: 20352.42s
                               ETA: 1169163.7s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.586s, learning 0.170s)
               Value function loss: 25.8020
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 570.68
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 11.76s
                        Total time: 20364.17s
                               ETA: 1169143.9s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.400s, learning 0.171s)
               Value function loss: 25.9889
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 567.19
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 11.57s
                        Total time: 20375.74s
                               ETA: 1169113.4s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.265s, learning 0.180s)
               Value function loss: 33.9148
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 574.62
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 11.45s
                        Total time: 20387.19s
                               ETA: 1169075.7s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.134s, learning 0.162s)
               Value function loss: 27.2741
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 572.01
               Mean episode length: 249.54
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 11.30s
                        Total time: 20398.49s
                               ETA: 1169029.5s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.546s, learning 0.169s)
               Value function loss: 31.4696
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 567.87
               Mean episode length: 249.54
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 11.72s
                        Total time: 20410.20s
                               ETA: 1169007.4s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.972s, learning 0.182s)
               Value function loss: 29.2037
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 572.53
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 11.15s
                        Total time: 20421.36s
                               ETA: 1168953.2s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.165s)
               Value function loss: 29.2917
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 569.89
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 11.17s
                        Total time: 20432.53s
                               ETA: 1168900.1s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.059s, learning 0.164s)
               Value function loss: 29.4516
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 573.63
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 11.22s
                        Total time: 20443.75s
                               ETA: 1168849.8s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.162s)
               Value function loss: 25.7155
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 574.71
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 11.33s
                        Total time: 20455.09s
                               ETA: 1168806.0s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.961s, learning 0.169s)
               Value function loss: 26.0554
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 574.64
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 11.13s
                        Total time: 20466.22s
                               ETA: 1168750.5s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.394s, learning 0.163s)
               Value function loss: 28.1746
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 573.26
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 11.56s
                        Total time: 20477.77s
                               ETA: 1168719.5s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.162s)
               Value function loss: 31.4081
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 572.11
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 11.46s
                        Total time: 20489.23s
                               ETA: 1168683.0s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.467s, learning 0.164s)
               Value function loss: 18.6768
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 573.15
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 11.63s
                        Total time: 20500.86s
                               ETA: 1168656.2s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.163s)
               Value function loss: 27.2478
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 575.92
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 11.03s
                        Total time: 20511.89s
                               ETA: 1168595.1s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.453s, learning 0.166s)
               Value function loss: 30.6707
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 576.29
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 11.62s
                        Total time: 20523.51s
                               ETA: 1168567.6s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.168s)
               Value function loss: 32.6499
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 575.54
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 11.52s
                        Total time: 20535.03s
                               ETA: 1168534.7s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.600s, learning 0.157s)
               Value function loss: 33.9154
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 574.54
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 11.76s
                        Total time: 20546.79s
                               ETA: 1168515.2s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.219s, learning 0.160s)
               Value function loss: 35.6296
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 572.09
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 11.38s
                        Total time: 20558.16s
                               ETA: 1168474.2s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.781s, learning 0.164s)
               Value function loss: 27.7971
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 572.98
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 11.95s
                        Total time: 20570.11s
                               ETA: 1168465.4s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.031s, learning 0.167s)
               Value function loss: 23.1331
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 574.96
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 11.20s
                        Total time: 20581.31s
                               ETA: 1168414.3s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.319s, learning 0.171s)
               Value function loss: 26.4882
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 575.58
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 11.49s
                        Total time: 20592.80s
                               ETA: 1168379.7s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.235s, learning 0.158s)
               Value function loss: 30.2963
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 569.93
               Mean episode length: 249.55
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 11.39s
                        Total time: 20604.19s
                               ETA: 1168339.7s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.262s, learning 0.160s)
               Value function loss: 22.9791
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 567.80
               Mean episode length: 249.53
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 11.42s
                        Total time: 20615.61s
                               ETA: 1168301.3s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.543s, learning 0.163s)
               Value function loss: 33.2633
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 562.02
               Mean episode length: 249.84
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 11.71s
                        Total time: 20627.32s
                               ETA: 1168279.1s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.012s, learning 0.172s)
               Value function loss: 30.9262
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 563.98
               Mean episode length: 249.87
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 11.18s
                        Total time: 20638.50s
                               ETA: 1168227.3s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.152s, learning 0.170s)
               Value function loss: 33.6091
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 567.31
               Mean episode length: 249.86
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 11.32s
                        Total time: 20649.83s
                               ETA: 1168183.4s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.258s, learning 0.164s)
               Value function loss: 40.3349
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 570.32
               Mean episode length: 249.97
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 11.42s
                        Total time: 20661.25s
                               ETA: 1168145.2s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.540s, learning 0.160s)
               Value function loss: 29.6289
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 571.74
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 11.70s
                        Total time: 20672.95s
                               ETA: 1168122.6s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.224s, learning 0.172s)
               Value function loss: 30.7734
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 564.15
               Mean episode length: 249.81
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 11.40s
                        Total time: 20684.34s
                               ETA: 1168083.0s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.830s, learning 0.175s)
               Value function loss: 31.1151
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 569.48
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 11.01s
                        Total time: 20695.35s
                               ETA: 1168021.3s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.909s, learning 0.180s)
               Value function loss: 31.7185
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 571.72
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 11.09s
                        Total time: 20706.44s
                               ETA: 1167964.4s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.509s, learning 0.166s)
               Value function loss: 27.4369
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 564.15
               Mean episode length: 249.89
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 11.67s
                        Total time: 20718.11s
                               ETA: 1167940.5s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.251s, learning 0.183s)
               Value function loss: 26.4505
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 576.05
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 11.43s
                        Total time: 20729.55s
                               ETA: 1167903.2s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.302s, learning 0.164s)
               Value function loss: 29.9881
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 571.53
               Mean episode length: 249.95
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 11.47s
                        Total time: 20741.01s
                               ETA: 1167867.7s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.429s, learning 0.170s)
               Value function loss: 28.5285
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 571.31
               Mean episode length: 249.89
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 11.60s
                        Total time: 20752.61s
                               ETA: 1167839.6s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.464s, learning 0.205s)
               Value function loss: 24.8479
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 571.95
               Mean episode length: 249.89
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 11.67s
                        Total time: 20764.28s
                               ETA: 1167815.6s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.163s)
               Value function loss: 29.0780
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 568.13
               Mean episode length: 249.75
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 11.51s
                        Total time: 20775.79s
                               ETA: 1167782.3s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.573s, learning 0.160s)
               Value function loss: 23.1396
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 571.48
               Mean episode length: 249.75
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 11.73s
                        Total time: 20787.52s
                               ETA: 1167761.9s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.631s, learning 0.167s)
               Value function loss: 25.6788
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 568.88
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 11.80s
                        Total time: 20799.32s
                               ETA: 1167745.1s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.358s, learning 0.159s)
               Value function loss: 29.7134
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 570.26
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 11.52s
                        Total time: 20810.84s
                               ETA: 1167712.5s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.365s, learning 0.171s)
               Value function loss: 19.7005
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 564.02
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 11.54s
                        Total time: 20822.37s
                               ETA: 1167681.0s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.549s, learning 0.166s)
               Value function loss: 24.4407
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 565.79
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 11.71s
                        Total time: 20834.09s
                               ETA: 1167659.6s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.102s, learning 0.160s)
               Value function loss: 29.8296
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 568.84
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 11.26s
                        Total time: 20845.35s
                               ETA: 1167612.9s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.162s)
               Value function loss: 21.4577
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 570.76
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 11.29s
                        Total time: 20856.64s
                               ETA: 1167567.8s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.662s, learning 0.168s)
               Value function loss: 28.0953
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 571.21
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 11.83s
                        Total time: 20868.47s
                               ETA: 1167553.0s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.280s, learning 0.175s)
               Value function loss: 27.4174
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 571.81
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 11.45s
                        Total time: 20879.93s
                               ETA: 1167517.1s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.196s, learning 0.159s)
               Value function loss: 31.6309
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 571.35
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 11.36s
                        Total time: 20891.28s
                               ETA: 1167475.7s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.296s, learning 0.166s)
               Value function loss: 28.3913
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 569.70
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 11.46s
                        Total time: 20902.74s
                               ETA: 1167440.2s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.241s, learning 0.195s)
               Value function loss: 31.1148
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 570.65
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 11.44s
                        Total time: 20914.18s
                               ETA: 1167403.4s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.164s)
               Value function loss: 26.9102
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 572.60
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 11.55s
                        Total time: 20925.73s
                               ETA: 1167372.9s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.922s, learning 0.165s)
               Value function loss: 24.8488
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 570.06
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 11.09s
                        Total time: 20936.82s
                               ETA: 1167316.6s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.178s, learning 0.161s)
               Value function loss: 25.0636
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 570.77
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 11.34s
                        Total time: 20948.15s
                               ETA: 1167274.4s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.158s)
               Value function loss: 32.9138
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 569.19
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 11.33s
                        Total time: 20959.49s
                               ETA: 1167231.9s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.409s, learning 0.174s)
               Value function loss: 24.6740
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 567.96
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 11.58s
                        Total time: 20971.07s
                               ETA: 1167203.4s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.358s, learning 0.168s)
               Value function loss: 29.6222
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 569.06
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 11.53s
                        Total time: 20982.60s
                               ETA: 1167171.8s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.160s)
               Value function loss: 31.7268
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 571.92
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 11.41s
                        Total time: 20994.01s
                               ETA: 1167133.7s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.534s, learning 0.159s)
               Value function loss: 32.5585
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 570.16
               Mean episode length: 248.37
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 11.69s
                        Total time: 21005.70s
                               ETA: 1167111.3s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.486s, learning 0.256s)
               Value function loss: 38.1187
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 577.92
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 11.74s
                        Total time: 21017.44s
                               ETA: 1167091.7s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.161s)
               Value function loss: 34.5633
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 575.71
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 11.53s
                        Total time: 21028.97s
                               ETA: 1167060.5s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.238s, learning 0.170s)
               Value function loss: 31.3751
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 577.50
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 11.41s
                        Total time: 21040.38s
                               ETA: 1167022.4s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.164s)
               Value function loss: 29.4500
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 578.83
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 11.35s
                        Total time: 21051.73s
                               ETA: 1166981.3s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.196s, learning 0.160s)
               Value function loss: 36.1256
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 573.30
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 11.36s
                        Total time: 21063.09s
                               ETA: 1166940.3s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.377s, learning 0.164s)
               Value function loss: 28.6298
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 573.70
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 11.54s
                        Total time: 21074.63s
                               ETA: 1166909.6s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.703s, learning 0.169s)
               Value function loss: 29.6881
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 577.35
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 10.87s
                        Total time: 21085.50s
                               ETA: 1166842.0s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.323s, learning 0.168s)
               Value function loss: 32.8825
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 574.03
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 11.49s
                        Total time: 21096.99s
                               ETA: 1166808.7s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.146s, learning 0.170s)
               Value function loss: 30.6454
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 574.28
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 11.32s
                        Total time: 21108.31s
                               ETA: 1166765.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.055s, learning 0.164s)
               Value function loss: 24.1341
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 575.20
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 11.22s
                        Total time: 21119.53s
                               ETA: 1166717.4s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.606s, learning 0.192s)
               Value function loss: 30.7030
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 577.60
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 11.80s
                        Total time: 21131.33s
                               ETA: 1166701.1s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.569s, learning 0.189s)
               Value function loss: 25.1783
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 580.01
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 11.76s
                        Total time: 21143.08s
                               ETA: 1166682.5s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.498s, learning 0.173s)
               Value function loss: 25.1327
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 580.73
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 11.67s
                        Total time: 21154.76s
                               ETA: 1166659.2s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.707s, learning 0.165s)
               Value function loss: 24.6693
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 578.89
               Mean episode length: 249.98
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 10.87s
                        Total time: 21165.63s
                               ETA: 1166591.9s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.238s, learning 0.161s)
               Value function loss: 23.0141
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 577.30
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 11.40s
                        Total time: 21177.03s
                               ETA: 1166553.6s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.071s, learning 0.170s)
               Value function loss: 27.5914
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 580.37
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 11.24s
                        Total time: 21188.27s
                               ETA: 1166506.8s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.106s, learning 0.162s)
               Value function loss: 28.2936
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 575.94
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 11.27s
                        Total time: 21199.54s
                               ETA: 1166461.4s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.300s, learning 0.163s)
               Value function loss: 25.1192
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 577.55
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 11.46s
                        Total time: 21211.00s
                               ETA: 1166426.8s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.046s, learning 0.178s)
               Value function loss: 31.4402
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 582.32
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 11.22s
                        Total time: 21222.22s
                               ETA: 1166379.1s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.211s, learning 0.168s)
               Value function loss: 26.7939
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 583.35
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 11.38s
                        Total time: 21233.60s
                               ETA: 1166340.0s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.852s, learning 0.164s)
               Value function loss: 36.3624
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 582.98
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 11.02s
                        Total time: 21244.62s
                               ETA: 1166280.9s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.267s, learning 0.163s)
               Value function loss: 28.5751
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 583.11
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 11.43s
                        Total time: 21256.05s
                               ETA: 1166244.6s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.347s, learning 0.176s)
               Value function loss: 36.7298
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 583.26
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 11.52s
                        Total time: 21267.57s
                               ETA: 1166213.5s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.952s, learning 0.184s)
               Value function loss: 32.1727
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 571.10
               Mean episode length: 249.14
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 11.14s
                        Total time: 21278.71s
                               ETA: 1166161.1s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.164s)
               Value function loss: 25.7971
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 573.16
               Mean episode length: 249.14
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 11.27s
                        Total time: 21289.98s
                               ETA: 1166116.1s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.119s, learning 0.172s)
               Value function loss: 27.2547
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 581.71
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 11.29s
                        Total time: 21301.27s
                               ETA: 1166072.4s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.162s)
               Value function loss: 31.7453
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 582.29
               Mean episode length: 249.97
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 11.28s
                        Total time: 21312.55s
                               ETA: 1166028.2s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.434s, learning 0.165s)
               Value function loss: 28.9909
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 585.42
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 11.60s
                        Total time: 21324.15s
                               ETA: 1166001.4s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.115s, learning 0.186s)
               Value function loss: 29.1740
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 584.46
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 11.30s
                        Total time: 21335.45s
                               ETA: 1165958.3s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.875s, learning 0.162s)
               Value function loss: 34.0599
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 583.44
               Mean episode length: 249.96
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 11.04s
                        Total time: 21346.49s
                               ETA: 1165900.7s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.404s, learning 0.162s)
               Value function loss: 31.3109
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 582.74
               Mean episode length: 249.85
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 11.57s
                        Total time: 21358.06s
                               ETA: 1165872.1s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.162s, learning 0.162s)
               Value function loss: 36.1430
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 589.83
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 11.32s
                        Total time: 21369.38s
                               ETA: 1165830.3s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.240s, learning 0.164s)
               Value function loss: 38.8848
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 587.42
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 11.40s
                        Total time: 21380.78s
                               ETA: 1165792.9s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.026s, learning 0.166s)
               Value function loss: 31.3385
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 585.06
               Mean episode length: 249.93
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 11.19s
                        Total time: 21391.98s
                               ETA: 1165744.0s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.689s, learning 0.171s)
               Value function loss: 25.9302
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 587.00
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 10.86s
                        Total time: 21402.84s
                               ETA: 1165677.1s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.275s, learning 0.167s)
               Value function loss: 30.8804
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 587.55
               Mean episode length: 249.39
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 11.44s
                        Total time: 21414.28s
                               ETA: 1165641.9s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.071s, learning 0.172s)
               Value function loss: 26.7285
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 590.58
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 11.24s
                        Total time: 21425.52s
                               ETA: 1165595.9s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.980s, learning 0.167s)
               Value function loss: 25.2831
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 591.53
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 11.15s
                        Total time: 21436.67s
                               ETA: 1165544.7s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.216s, learning 0.267s)
               Value function loss: 24.9810
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 583.69
               Mean episode length: 248.54
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 11.48s
                        Total time: 21448.15s
                               ETA: 1165511.8s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.338s, learning 0.165s)
               Value function loss: 29.2363
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 588.00
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 11.50s
                        Total time: 21459.66s
                               ETA: 1165480.0s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.489s, learning 0.164s)
               Value function loss: 20.8171
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 584.42
               Mean episode length: 249.29
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 11.65s
                        Total time: 21471.31s
                               ETA: 1165456.4s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.271s, learning 0.176s)
               Value function loss: 30.2093
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 586.13
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 11.45s
                        Total time: 21482.75s
                               ETA: 1165421.6s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.922s, learning 0.173s)
               Value function loss: 29.0760
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 586.78
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 11.09s
                        Total time: 21493.85s
                               ETA: 1165367.8s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.318s, learning 0.189s)
               Value function loss: 25.7592
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 586.78
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 11.51s
                        Total time: 21505.36s
                               ETA: 1165336.4s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.184s, learning 0.169s)
               Value function loss: 33.5529
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 587.73
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 11.35s
                        Total time: 21516.71s
                               ETA: 1165296.6s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.091s, learning 0.159s)
               Value function loss: 24.3956
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 583.47
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 11.25s
                        Total time: 21527.96s
                               ETA: 1165251.2s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.441s, learning 0.173s)
               Value function loss: 27.4210
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 580.90
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 11.61s
                        Total time: 21539.57s
                               ETA: 1165225.6s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.959s, learning 0.161s)
               Value function loss: 28.0019
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 587.02
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 11.12s
                        Total time: 21550.69s
                               ETA: 1165173.4s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.400s, learning 0.171s)
               Value function loss: 25.3285
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 579.96
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 11.57s
                        Total time: 21562.26s
                               ETA: 1165145.5s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.101s, learning 0.268s)
               Value function loss: 24.9227
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 581.22
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 11.37s
                        Total time: 21573.63s
                               ETA: 1165106.7s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.163s, learning 0.176s)
               Value function loss: 27.4397
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 583.87
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 11.34s
                        Total time: 21584.97s
                               ETA: 1165066.4s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.300s, learning 0.192s)
               Value function loss: 27.7984
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 575.40
               Mean episode length: 249.56
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 11.49s
                        Total time: 21596.46s
                               ETA: 1165034.3s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.740s, learning 0.171s)
               Value function loss: 27.9025
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 584.09
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 10.91s
                        Total time: 21607.37s
                               ETA: 1164970.9s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.324s, learning 0.161s)
               Value function loss: 30.0043
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 580.75
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 11.48s
                        Total time: 21618.86s
                               ETA: 1164938.5s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.186s, learning 0.194s)
               Value function loss: 37.1754
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 580.49
               Mean episode length: 249.88
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 11.38s
                        Total time: 21630.24s
                               ETA: 1164900.5s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.998s, learning 0.167s)
               Value function loss: 26.5465
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 579.61
               Mean episode length: 249.49
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 11.17s
                        Total time: 21641.40s
                               ETA: 1164850.9s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.070s, learning 0.161s)
               Value function loss: 27.5779
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 577.08
               Mean episode length: 249.37
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 11.23s
                        Total time: 21652.63s
                               ETA: 1164805.0s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.220s, learning 0.188s)
               Value function loss: 29.9263
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 580.94
               Mean episode length: 249.76
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 11.41s
                        Total time: 21664.04s
                               ETA: 1164768.6s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.207s, learning 0.167s)
               Value function loss: 31.8027
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 582.98
               Mean episode length: 249.84
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 11.37s
                        Total time: 21675.42s
                               ETA: 1164730.3s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.332s, learning 0.165s)
               Value function loss: 31.5900
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 588.81
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 11.50s
                        Total time: 21686.91s
                               ETA: 1164698.8s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.181s, learning 0.159s)
               Value function loss: 30.9124
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 584.04
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 11.34s
                        Total time: 21698.25s
                               ETA: 1164658.8s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.141s, learning 0.172s)
               Value function loss: 34.3779
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 580.06
               Mean episode length: 249.83
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 11.31s
                        Total time: 21709.57s
                               ETA: 1164617.3s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.274s, learning 0.182s)
               Value function loss: 36.6386
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 585.50
               Mean episode length: 249.83
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 11.46s
                        Total time: 21721.02s
                               ETA: 1164583.7s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.118s, learning 0.171s)
               Value function loss: 48.0821
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 587.93
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 11.29s
                        Total time: 21732.31s
                               ETA: 1164541.1s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.414s, learning 0.194s)
               Value function loss: 28.2265
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 584.52
               Mean episode length: 249.86
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 11.61s
                        Total time: 21743.92s
                               ETA: 1164515.6s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.163s, learning 0.201s)
               Value function loss: 32.9684
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 580.63
               Mean episode length: 249.66
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 11.36s
                        Total time: 21755.28s
                               ETA: 1164477.1s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.977s, learning 0.160s)
               Value function loss: 37.6652
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 588.23
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 11.14s
                        Total time: 21766.42s
                               ETA: 1164426.4s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.149s, learning 0.162s)
               Value function loss: 30.8880
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 587.95
               Mean episode length: 249.95
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 11.31s
                        Total time: 21777.73s
                               ETA: 1164385.0s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.159s, learning 0.162s)
               Value function loss: 27.8660
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 586.62
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 11.32s
                        Total time: 21789.05s
                               ETA: 1164344.3s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.896s, learning 0.174s)
               Value function loss: 25.5791
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 585.19
               Mean episode length: 249.89
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 11.07s
                        Total time: 21800.12s
                               ETA: 1164290.1s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.650s, learning 0.163s)
               Value function loss: 29.6103
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 585.96
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 10.81s
                        Total time: 21810.93s
                               ETA: 1164222.3s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.904s, learning 0.166s)
               Value function loss: 28.0835
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 590.56
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.07s
                        Total time: 21822.00s
                               ETA: 1164168.3s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.847s, learning 0.170s)
               Value function loss: 23.7894
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 586.66
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 11.02s
                        Total time: 21833.02s
                               ETA: 1164111.5s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.188s)
               Value function loss: 25.9731
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 582.14
               Mean episode length: 249.88
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 11.47s
                        Total time: 21844.49s
                               ETA: 1164078.9s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.691s, learning 0.162s)
               Value function loss: 21.4361
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 581.90
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 10.85s
                        Total time: 21855.34s
                               ETA: 1164013.4s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1134 steps/s (collection: 14.278s, learning 0.167s)
               Value function loss: 28.5692
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 583.25
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 14.44s
                        Total time: 21869.79s
                               ETA: 1164139.2s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.768s, learning 0.182s)
               Value function loss: 22.0782
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 581.09
               Mean episode length: 249.72
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 21.95s
                        Total time: 21891.74s
                               ETA: 1164664.2s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.970s, learning 0.172s)
               Value function loss: 23.4876
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 582.25
               Mean episode length: 249.72
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 22.14s
                        Total time: 21913.88s
                               ETA: 1165198.7s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.732s, learning 0.182s)
               Value function loss: 18.4342
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 584.78
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 21.91s
                        Total time: 21935.79s
                               ETA: 1165720.5s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 748 steps/s (collection: 21.673s, learning 0.206s)
               Value function loss: 23.8946
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 580.68
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 21.88s
                        Total time: 21957.67s
                               ETA: 1166239.9s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 737 steps/s (collection: 22.059s, learning 0.168s)
               Value function loss: 17.3883
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 575.14
               Mean episode length: 248.68
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 22.23s
                        Total time: 21979.90s
                               ETA: 1166777.2s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.866s, learning 0.167s)
               Value function loss: 23.6863
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 572.28
               Mean episode length: 248.36
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 22.03s
                        Total time: 22001.93s
                               ETA: 1167303.6s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.336s, learning 0.178s)
               Value function loss: 23.0998
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 580.93
               Mean episode length: 249.72
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 22.51s
                        Total time: 22024.44s
                               ETA: 1167854.8s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.117s, learning 0.172s)
               Value function loss: 25.3086
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 575.19
               Mean episode length: 249.35
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 22.29s
                        Total time: 22046.73s
                               ETA: 1168393.6s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 741 steps/s (collection: 21.932s, learning 0.175s)
               Value function loss: 29.6515
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 568.93
               Mean episode length: 249.20
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 22.11s
                        Total time: 22068.84s
                               ETA: 1168922.1s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 736 steps/s (collection: 22.064s, learning 0.167s)
               Value function loss: 29.2840
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 574.84
               Mean episode length: 249.83
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 22.23s
                        Total time: 22091.07s
                               ETA: 1169456.5s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.731s, learning 0.191s)
               Value function loss: 19.7075
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 575.28
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 21.92s
                        Total time: 22112.99s
                               ETA: 1169974.1s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.391s, learning 0.182s)
               Value function loss: 15.7449
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 579.15
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 22.57s
                        Total time: 22135.57s
                               ETA: 1170525.4s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.775s, learning 0.174s)
               Value function loss: 19.3518
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 581.13
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 21.95s
                        Total time: 22157.52s
                               ETA: 1171043.2s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 738 steps/s (collection: 22.032s, learning 0.165s)
               Value function loss: 22.1727
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 574.08
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 22.20s
                        Total time: 22179.71s
                               ETA: 1171573.5s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 744 steps/s (collection: 21.847s, learning 0.165s)
               Value function loss: 18.3764
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 575.18
               Mean episode length: 249.88
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 22.01s
                        Total time: 22201.72s
                               ETA: 1172093.4s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.992s, learning 0.160s)
               Value function loss: 25.6528
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 572.47
               Mean episode length: 248.98
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 22.15s
                        Total time: 22223.88s
                               ETA: 1172620.1s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.804s, learning 0.172s)
               Value function loss: 20.6298
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 573.03
               Mean episode length: 249.10
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 21.98s
                        Total time: 22245.85s
                               ETA: 1173136.9s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 759 steps/s (collection: 21.410s, learning 0.172s)
               Value function loss: 24.6141
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 576.08
               Mean episode length: 249.56
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 21.58s
                        Total time: 22267.43s
                               ETA: 1173632.4s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.987s, learning 0.162s)
               Value function loss: 22.1286
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 579.02
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 22.15s
                        Total time: 22289.58s
                               ETA: 1174157.3s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.431s, learning 0.169s)
               Value function loss: 20.5775
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 577.49
               Mean episode length: 249.93
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 21.60s
                        Total time: 22311.18s
                               ETA: 1174652.6s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 763 steps/s (collection: 21.296s, learning 0.160s)
               Value function loss: 21.6930
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 575.58
               Mean episode length: 249.72
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 21.46s
                        Total time: 22332.64s
                               ETA: 1175139.9s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.406s, learning 0.185s)
               Value function loss: 21.0556
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 565.27
               Mean episode length: 249.28
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 22.59s
                        Total time: 22355.23s
                               ETA: 1175686.2s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.609s, learning 0.164s)
               Value function loss: 22.5894
                    Surrogate loss: 0.0102
             Mean action noise std: 0.74
                       Mean reward: 567.21
               Mean episode length: 249.21
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 21.77s
                        Total time: 22377.00s
                               ETA: 1176189.0s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 731 steps/s (collection: 22.223s, learning 0.168s)
               Value function loss: 16.4182
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 576.81
               Mean episode length: 249.92
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 22.39s
                        Total time: 22399.39s
                               ETA: 1176723.6s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 724 steps/s (collection: 22.412s, learning 0.189s)
               Value function loss: 15.9932
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 576.54
               Mean episode length: 249.97
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 22.60s
                        Total time: 22421.99s
                               ETA: 1177268.7s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 740 steps/s (collection: 21.973s, learning 0.166s)
               Value function loss: 20.1269
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 582.82
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 22.14s
                        Total time: 22444.13s
                               ETA: 1177788.9s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.359s, learning 0.177s)
               Value function loss: 19.5228
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 581.19
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 21.54s
                        Total time: 22465.67s
                               ETA: 1178276.9s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.800s, learning 0.162s)
               Value function loss: 17.3741
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 579.74
               Mean episode length: 249.73
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 21.96s
                        Total time: 22487.63s
                               ETA: 1178786.7s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 729 steps/s (collection: 22.298s, learning 0.164s)
               Value function loss: 24.7823
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 584.27
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 22.46s
                        Total time: 22510.09s
                               ETA: 1179322.2s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.879s, learning 0.161s)
               Value function loss: 19.1255
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 584.07
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 22.04s
                        Total time: 22532.13s
                               ETA: 1179834.9s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 733 steps/s (collection: 22.174s, learning 0.164s)
               Value function loss: 21.3800
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 586.82
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 22.34s
                        Total time: 22554.47s
                               ETA: 1180362.7s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 751 steps/s (collection: 21.619s, learning 0.171s)
               Value function loss: 25.8791
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 585.93
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 21.79s
                        Total time: 22576.26s
                               ETA: 1180861.2s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 742 steps/s (collection: 21.881s, learning 0.174s)
               Value function loss: 18.1468
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 586.24
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 22.06s
                        Total time: 22598.32s
                               ETA: 1181373.0s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.756s, learning 0.168s)
               Value function loss: 17.9427
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 589.15
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 21.92s
                        Total time: 22620.24s
                               ETA: 1181877.4s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.467s, learning 0.164s)
               Value function loss: 18.7684
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 590.01
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 21.63s
                        Total time: 22641.87s
                               ETA: 1182366.0s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 740 steps/s (collection: 21.957s, learning 0.172s)
               Value function loss: 12.8838
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 590.83
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 22.13s
                        Total time: 22664.00s
                               ETA: 1182880.0s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 736 steps/s (collection: 22.072s, learning 0.166s)
               Value function loss: 21.1146
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 588.84
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 22.24s
                        Total time: 22686.24s
                               ETA: 1183399.1s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1102 steps/s (collection: 14.673s, learning 0.189s)
               Value function loss: 19.3005
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 587.92
               Mean episode length: 249.91
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 14.86s
                        Total time: 22701.10s
                               ETA: 1183533.1s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.161s)
               Value function loss: 26.2699
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 588.92
               Mean episode length: 249.91
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 11.24s
                        Total time: 22712.34s
                               ETA: 1183477.9s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.932s, learning 0.161s)
               Value function loss: 21.1375
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 592.39
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 11.09s
                        Total time: 22723.43s
                               ETA: 1183415.4s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.868s, learning 0.165s)
               Value function loss: 23.5665
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 590.84
               Mean episode length: 248.81
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 11.03s
                        Total time: 22734.46s
                               ETA: 1183349.8s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.109s, learning 0.165s)
               Value function loss: 21.0765
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 595.80
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 11.27s
                        Total time: 22745.73s
                               ETA: 1183296.8s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.267s, learning 0.257s)
               Value function loss: 20.5835
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 593.87
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 11.52s
                        Total time: 22757.26s
                               ETA: 1183256.9s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.938s, learning 0.173s)
               Value function loss: 20.2057
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 587.00
               Mean episode length: 249.79
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 11.11s
                        Total time: 22768.37s
                               ETA: 1183195.5s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.170s, learning 0.163s)
               Value function loss: 26.2404
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 589.17
               Mean episode length: 249.79
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 11.33s
                        Total time: 22779.70s
                               ETA: 1183145.7s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.396s, learning 0.160s)
               Value function loss: 22.5198
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 594.72
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 11.56s
                        Total time: 22791.26s
                               ETA: 1183107.5s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.161s)
               Value function loss: 24.8676
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 595.99
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 11.57s
                        Total time: 22802.83s
                               ETA: 1183070.2s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.936s, learning 0.193s)
               Value function loss: 25.4054
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 594.58
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 11.13s
                        Total time: 22813.96s
                               ETA: 1183009.9s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.751s, learning 0.162s)
               Value function loss: 26.1931
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 592.09
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 11.91s
                        Total time: 22825.87s
                               ETA: 1182990.4s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.418s, learning 0.163s)
               Value function loss: 29.1284
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 593.16
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 11.58s
                        Total time: 22837.45s
                               ETA: 1182953.6s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.124s, learning 0.169s)
               Value function loss: 31.3980
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 592.76
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 11.29s
                        Total time: 22848.75s
                               ETA: 1182902.0s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.464s, learning 0.163s)
               Value function loss: 23.0356
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 593.18
               Mean episode length: 249.83
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 11.63s
                        Total time: 22860.38s
                               ETA: 1182867.7s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.369s, learning 0.162s)
               Value function loss: 27.8898
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 585.23
               Mean episode length: 249.73
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 11.53s
                        Total time: 22871.91s
                               ETA: 1182828.4s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.735s, learning 0.164s)
               Value function loss: 29.9274
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 587.93
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 10.90s
                        Total time: 22882.80s
                               ETA: 1182756.5s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.868s, learning 0.171s)
               Value function loss: 20.6551
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 586.49
               Mean episode length: 249.64
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 11.04s
                        Total time: 22893.84s
                               ETA: 1182691.8s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.533s, learning 0.163s)
               Value function loss: 26.3444
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 583.47
               Mean episode length: 249.52
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 11.70s
                        Total time: 22905.54s
                               ETA: 1182661.1s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.649s, learning 0.159s)
               Value function loss: 28.7967
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 586.48
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 11.81s
                        Total time: 22917.35s
                               ETA: 1182636.3s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.319s, learning 0.166s)
               Value function loss: 32.2467
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 588.59
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 11.48s
                        Total time: 22928.83s
                               ETA: 1182594.8s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.269s, learning 0.162s)
               Value function loss: 19.3897
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 588.48
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 11.43s
                        Total time: 22940.26s
                               ETA: 1182550.6s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.161s)
               Value function loss: 27.4757
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 585.70
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 11.37s
                        Total time: 22951.63s
                               ETA: 1182503.1s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.174s, learning 0.174s)
               Value function loss: 25.9520
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 590.75
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 11.35s
                        Total time: 22962.98s
                               ETA: 1182454.7s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.248s, learning 0.174s)
               Value function loss: 22.6353
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 589.04
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 11.42s
                        Total time: 22974.40s
                               ETA: 1182410.1s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.355s, learning 0.160s)
               Value function loss: 26.0279
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 588.04
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 11.51s
                        Total time: 22985.91s
                               ETA: 1182370.4s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.084s, learning 0.163s)
               Value function loss: 18.2313
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 587.16
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 11.25s
                        Total time: 22997.16s
                               ETA: 1182316.8s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.521s, learning 0.256s)
               Value function loss: 22.5396
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 590.64
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 11.78s
                        Total time: 23008.94s
                               ETA: 1182290.6s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.574s, learning 0.161s)
               Value function loss: 28.2413
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 585.13
               Mean episode length: 249.94
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 11.74s
                        Total time: 23020.67s
                               ETA: 1182262.2s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.971s, learning 0.164s)
               Value function loss: 20.5553
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 586.89
               Mean episode length: 249.94
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 11.14s
                        Total time: 23031.81s
                               ETA: 1182203.1s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.273s, learning 0.264s)
               Value function loss: 29.0172
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 593.43
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 11.54s
                        Total time: 23043.34s
                               ETA: 1182164.5s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.998s, learning 0.163s)
               Value function loss: 25.7461
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 583.99
               Mean episode length: 248.94
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 11.16s
                        Total time: 23054.51s
                               ETA: 1182106.8s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.323s, learning 0.196s)
               Value function loss: 29.3960
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 589.82
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 11.52s
                        Total time: 23066.02s
                               ETA: 1182067.4s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.251s, learning 0.273s)
               Value function loss: 21.8180
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 590.28
               Mean episode length: 249.90
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 11.52s
                        Total time: 23077.55s
                               ETA: 1182028.4s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.372s, learning 0.169s)
               Value function loss: 25.4264
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 589.56
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 11.54s
                        Total time: 23089.09s
                               ETA: 1181990.2s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.411s, learning 0.169s)
               Value function loss: 25.9120
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 589.89
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 11.58s
                        Total time: 23100.67s
                               ETA: 1181954.1s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.179s, learning 0.260s)
               Value function loss: 20.2836
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 591.35
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 11.44s
                        Total time: 23112.11s
                               ETA: 1181910.8s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.307s, learning 0.171s)
               Value function loss: 22.2822
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 591.48
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 11.48s
                        Total time: 23123.59s
                               ETA: 1181869.5s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.178s)
               Value function loss: 23.0355
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 584.33
               Mean episode length: 248.88
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 11.31s
                        Total time: 23134.90s
                               ETA: 1181819.7s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.509s, learning 0.164s)
               Value function loss: 23.5419
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 583.22
               Mean episode length: 248.88
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 11.67s
                        Total time: 23146.57s
                               ETA: 1181788.5s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.996s, learning 0.179s)
               Value function loss: 25.1741
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 589.39
               Mean episode length: 249.99
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 11.17s
                        Total time: 23157.74s
                               ETA: 1181731.8s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.881s, learning 0.176s)
               Value function loss: 25.6604
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 588.19
               Mean episode length: 249.99
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 11.06s
                        Total time: 23168.80s
                               ETA: 1181669.1s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.251s, learning 0.170s)
               Value function loss: 23.2685
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 588.53
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 11.42s
                        Total time: 23180.22s
                               ETA: 1181625.1s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.115s, learning 0.187s)
               Value function loss: 27.9847
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 586.48
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 11.30s
                        Total time: 23191.53s
                               ETA: 1181575.1s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.940s, learning 0.158s)
               Value function loss: 30.1429
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 581.63
               Mean episode length: 248.81
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 11.10s
                        Total time: 23202.62s
                               ETA: 1181514.7s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.022s, learning 0.188s)
               Value function loss: 23.8858
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 585.38
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 11.21s
                        Total time: 23213.83s
                               ETA: 1181460.1s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.439s, learning 0.161s)
               Value function loss: 19.1697
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 588.90
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 11.60s
                        Total time: 23225.43s
                               ETA: 1181425.3s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.876s, learning 0.161s)
               Value function loss: 23.7008
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 589.91
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 11.04s
                        Total time: 23236.47s
                               ETA: 1181361.9s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.094s, learning 0.241s)
               Value function loss: 27.1069
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 589.58
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 11.33s
                        Total time: 23247.81s
                               ETA: 1181313.7s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.146s, learning 0.160s)
               Value function loss: 20.2957
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 586.35
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 11.31s
                        Total time: 23259.11s
                               ETA: 1181264.1s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.553s, learning 0.159s)
               Value function loss: 22.5782
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 591.12
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 11.71s
                        Total time: 23270.82s
                               ETA: 1181235.2s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.105s, learning 0.168s)
               Value function loss: 25.0355
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 591.32
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 11.27s
                        Total time: 23282.10s
                               ETA: 1181184.0s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.317s, learning 0.170s)
               Value function loss: 15.6509
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 588.94
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 11.49s
                        Total time: 23293.58s
                               ETA: 1181143.7s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.168s)
               Value function loss: 23.6723
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 594.92
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 11.73s
                        Total time: 23305.32s
                               ETA: 1181115.8s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.395s, learning 0.168s)
               Value function loss: 21.2748
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 595.28
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 11.56s
                        Total time: 23316.88s
                               ETA: 1181079.4s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.164s, learning 0.167s)
               Value function loss: 17.8549
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 593.58
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 11.33s
                        Total time: 23328.21s
                               ETA: 1181031.3s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.473s, learning 0.162s)
               Value function loss: 23.9012
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 595.68
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 11.63s
                        Total time: 23339.85s
                               ETA: 1180998.6s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.162s)
               Value function loss: 22.3922
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 593.86
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 11.18s
                        Total time: 23351.02s
                               ETA: 1180942.8s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.084s, learning 0.167s)
               Value function loss: 20.6019
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 589.21
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 11.25s
                        Total time: 23362.27s
                               ETA: 1180890.7s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.284s, learning 0.173s)
               Value function loss: 22.8500
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 593.51
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 11.46s
                        Total time: 23373.73s
                               ETA: 1180849.1s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.065s, learning 0.163s)
               Value function loss: 22.6414
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 593.67
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 11.23s
                        Total time: 23384.96s
                               ETA: 1180795.9s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.089s, learning 0.174s)
               Value function loss: 20.7694
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 592.63
               Mean episode length: 249.19
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 11.26s
                        Total time: 23396.22s
                               ETA: 1180744.6s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.991s, learning 0.159s)
               Value function loss: 22.5824
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 592.98
               Mean episode length: 249.19
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 11.15s
                        Total time: 23407.37s
                               ETA: 1180687.6s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.024s, learning 0.179s)
               Value function loss: 19.9527
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 592.16
               Mean episode length: 249.55
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 11.20s
                        Total time: 23418.58s
                               ETA: 1180633.3s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.317s, learning 0.165s)
               Value function loss: 24.3485
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 591.92
               Mean episode length: 249.55
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 11.48s
                        Total time: 23430.06s
                               ETA: 1180593.2s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.193s, learning 0.183s)
               Value function loss: 22.0126
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 592.38
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 11.38s
                        Total time: 23441.43s
                               ETA: 1180547.7s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.318s, learning 0.181s)
               Value function loss: 22.3683
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 591.33
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 11.50s
                        Total time: 23452.93s
                               ETA: 1180508.5s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.099s, learning 0.159s)
               Value function loss: 18.3152
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 594.96
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 11.26s
                        Total time: 23464.19s
                               ETA: 1180457.1s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.164s)
               Value function loss: 22.1104
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 595.16
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 11.10s
                        Total time: 23475.29s
                               ETA: 1180397.8s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.239s, learning 0.173s)
               Value function loss: 19.7748
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 585.74
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 11.41s
                        Total time: 23486.70s
                               ETA: 1180354.2s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.006s, learning 0.159s)
               Value function loss: 25.0516
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 581.39
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 11.16s
                        Total time: 23497.87s
                               ETA: 1180298.3s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.963s, learning 0.171s)
               Value function loss: 24.0693
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 588.75
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 11.13s
                        Total time: 23509.00s
                               ETA: 1180240.9s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.716s, learning 0.186s)
               Value function loss: 21.9684
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 587.02
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 10.90s
                        Total time: 23519.90s
                               ETA: 1180171.9s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.788s, learning 0.171s)
               Value function loss: 26.8684
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 591.00
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 10.96s
                        Total time: 23530.86s
                               ETA: 1180105.7s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.866s, learning 0.190s)
               Value function loss: 29.4206
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 588.87
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 11.06s
                        Total time: 23541.92s
                               ETA: 1180044.6s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.219s, learning 0.160s)
               Value function loss: 33.9874
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 589.16
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 11.38s
                        Total time: 23553.30s
                               ETA: 1179999.6s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.398s, learning 0.163s)
               Value function loss: 21.5583
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 591.48
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 11.56s
                        Total time: 23564.86s
                               ETA: 1179963.9s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.921s, learning 0.168s)
               Value function loss: 23.9117
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 591.36
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 11.09s
                        Total time: 23575.95s
                               ETA: 1179904.5s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.761s, learning 0.157s)
               Value function loss: 27.2133
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 592.11
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 10.92s
                        Total time: 23586.86s
                               ETA: 1179836.5s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.056s, learning 0.171s)
               Value function loss: 23.6360
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 592.73
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 11.23s
                        Total time: 23598.09s
                               ETA: 1179784.1s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.167s)
               Value function loss: 20.8918
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 593.90
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 11.49s
                        Total time: 23609.58s
                               ETA: 1179744.8s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.328s, learning 0.163s)
               Value function loss: 21.7772
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 592.67
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 11.49s
                        Total time: 23621.07s
                               ETA: 1179705.7s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.872s, learning 0.161s)
               Value function loss: 25.8603
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 590.66
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 11.03s
                        Total time: 23632.10s
                               ETA: 1179643.7s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.953s, learning 0.162s)
               Value function loss: 23.2951
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 589.95
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 11.12s
                        Total time: 23643.22s
                               ETA: 1179585.9s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.713s, learning 0.184s)
               Value function loss: 23.6764
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 590.18
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 11.90s
                        Total time: 23655.11s
                               ETA: 1179567.1s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.162s)
               Value function loss: 23.9921
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 593.61
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 11.07s
                        Total time: 23666.19s
                               ETA: 1179507.3s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.160s)
               Value function loss: 25.2251
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 592.89
               Mean episode length: 249.63
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 11.40s
                        Total time: 23677.58s
                               ETA: 1179463.6s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.762s, learning 0.159s)
               Value function loss: 23.9425
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 593.22
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 10.92s
                        Total time: 23688.50s
                               ETA: 1179396.3s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.880s, learning 0.162s)
               Value function loss: 23.6256
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 591.57
               Mean episode length: 249.65
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 11.04s
                        Total time: 23699.55s
                               ETA: 1179335.1s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.372s, learning 0.201s)
               Value function loss: 21.4632
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 592.38
               Mean episode length: 249.65
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 11.57s
                        Total time: 23711.12s
                               ETA: 1179300.3s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.140s, learning 0.167s)
               Value function loss: 20.5280
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 591.61
               Mean episode length: 248.77
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 11.31s
                        Total time: 23722.42s
                               ETA: 1179252.3s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.041s, learning 0.160s)
               Value function loss: 23.9650
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 594.74
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 11.20s
                        Total time: 23733.63s
                               ETA: 1179199.2s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.879s, learning 0.161s)
               Value function loss: 19.5503
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 595.62
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 11.04s
                        Total time: 23744.67s
                               ETA: 1179138.0s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.339s, learning 0.159s)
               Value function loss: 21.3705
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 594.80
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 11.50s
                        Total time: 23756.16s
                               ETA: 1179099.7s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.229s, learning 0.159s)
               Value function loss: 25.4386
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 593.77
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 11.39s
                        Total time: 23767.55s
                               ETA: 1179055.8s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.210s, learning 0.163s)
               Value function loss: 24.5669
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 588.75
               Mean episode length: 249.47
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 11.37s
                        Total time: 23778.92s
                               ETA: 1179011.3s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.079s, learning 0.162s)
               Value function loss: 30.8394
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 593.52
               Mean episode length: 249.47
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 11.24s
                        Total time: 23790.17s
                               ETA: 1178960.3s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.160s)
               Value function loss: 28.6956
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 596.88
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 11.30s
                        Total time: 23801.47s
                               ETA: 1178912.2s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.544s, learning 0.244s)
               Value function loss: 18.1797
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 597.54
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 11.79s
                        Total time: 23813.25s
                               ETA: 1178888.4s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.158s, learning 0.160s)
               Value function loss: 16.2673
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 598.40
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 11.32s
                        Total time: 23824.57s
                               ETA: 1178841.3s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.163s, learning 0.159s)
               Value function loss: 20.8738
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 596.64
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 11.32s
                        Total time: 23835.89s
                               ETA: 1178794.4s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.196s, learning 0.212s)
               Value function loss: 26.3912
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 595.99
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 11.41s
                        Total time: 23847.30s
                               ETA: 1178751.8s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.253s, learning 0.181s)
               Value function loss: 20.4408
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 595.76
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 11.43s
                        Total time: 23858.74s
                               ETA: 1178710.5s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.974s, learning 0.170s)
               Value function loss: 22.3210
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 598.99
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 11.14s
                        Total time: 23869.88s
                               ETA: 1178655.0s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.107s, learning 0.160s)
               Value function loss: 29.2149
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 593.55
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 11.27s
                        Total time: 23881.15s
                               ETA: 1178605.6s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.397s, learning 0.196s)
               Value function loss: 30.1146
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 594.89
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 11.59s
                        Total time: 23892.74s
                               ETA: 1178572.3s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.231s, learning 0.207s)
               Value function loss: 33.5757
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 597.61
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 11.44s
                        Total time: 23904.18s
                               ETA: 1178531.4s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.159s)
               Value function loss: 27.5149
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 597.88
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 11.55s
                        Total time: 23915.73s
                               ETA: 1178495.9s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.530s, learning 0.170s)
               Value function loss: 30.3821
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 596.83
               Mean episode length: 249.04
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 11.70s
                        Total time: 23927.43s
                               ETA: 1178467.9s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.202s, learning 0.173s)
               Value function loss: 29.7409
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 591.26
               Mean episode length: 249.04
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 11.37s
                        Total time: 23938.80s
                               ETA: 1178424.0s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.168s)
               Value function loss: 27.1974
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 593.84
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 11.18s
                        Total time: 23949.98s
                               ETA: 1178370.2s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.251s, learning 0.164s)
               Value function loss: 20.5571
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 598.14
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 11.42s
                        Total time: 23961.39s
                               ETA: 1178328.3s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.238s, learning 0.163s)
               Value function loss: 19.8012
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 595.97
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 11.40s
                        Total time: 23972.79s
                               ETA: 1178285.7s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.326s, learning 0.162s)
               Value function loss: 23.5055
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 598.25
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 11.49s
                        Total time: 23984.28s
                               ETA: 1178247.4s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.520s, learning 0.172s)
               Value function loss: 22.0887
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 596.73
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 11.69s
                        Total time: 23995.98s
                               ETA: 1178219.2s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.094s, learning 0.192s)
               Value function loss: 16.9511
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 597.64
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 11.29s
                        Total time: 24007.26s
                               ETA: 1178171.1s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.198s, learning 0.218s)
               Value function loss: 28.0424
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 594.53
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 11.42s
                        Total time: 24018.68s
                               ETA: 1178129.4s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.166s)
               Value function loss: 23.0282
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 595.42
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 11.43s
                        Total time: 24030.11s
                               ETA: 1178088.4s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.169s)
               Value function loss: 25.8025
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 597.46
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 11.49s
                        Total time: 24041.60s
                               ETA: 1178050.3s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.170s)
               Value function loss: 28.1108
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 598.36
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 11.35s
                        Total time: 24052.94s
                               ETA: 1178005.2s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.267s, learning 0.157s)
               Value function loss: 16.7872
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 598.39
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 11.42s
                        Total time: 24064.37s
                               ETA: 1177964.0s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.219s, learning 0.162s)
               Value function loss: 19.6203
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 598.28
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 11.38s
                        Total time: 24075.75s
                               ETA: 1177920.7s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.162s)
               Value function loss: 24.7487
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 596.70
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 11.13s
                        Total time: 24086.87s
                               ETA: 1177865.0s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.510s, learning 0.169s)
               Value function loss: 16.3980
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 596.12
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 11.68s
                        Total time: 24098.55s
                               ETA: 1177836.3s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.063s, learning 0.179s)
               Value function loss: 23.7217
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 593.29
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 11.24s
                        Total time: 24109.80s
                               ETA: 1177786.4s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.565s, learning 0.163s)
               Value function loss: 26.1376
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 593.43
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 11.73s
                        Total time: 24121.52s
                               ETA: 1177760.1s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.468s, learning 0.168s)
               Value function loss: 26.1220
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 596.90
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 11.64s
                        Total time: 24133.16s
                               ETA: 1177729.4s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.697s, learning 0.164s)
               Value function loss: 25.2680
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 596.49
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 11.86s
                        Total time: 24145.02s
                               ETA: 1177709.7s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.241s, learning 0.160s)
               Value function loss: 26.5537
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 592.97
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 11.40s
                        Total time: 24156.42s
                               ETA: 1177667.6s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.028s, learning 0.184s)
               Value function loss: 21.9914
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 595.93
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 11.21s
                        Total time: 24167.63s
                               ETA: 1177616.4s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.165s, learning 0.170s)
               Value function loss: 16.7184
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 596.08
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 11.34s
                        Total time: 24178.97s
                               ETA: 1177571.1s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.268s, learning 0.159s)
               Value function loss: 18.1695
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 594.22
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 11.43s
                        Total time: 24190.40s
                               ETA: 1177530.4s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.692s, learning 0.172s)
               Value function loss: 25.4162
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 595.86
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 10.86s
                        Total time: 24201.26s
                               ETA: 1177462.2s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.274s, learning 0.174s)
               Value function loss: 19.3524
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 599.71
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 11.45s
                        Total time: 24212.71s
                               ETA: 1177422.5s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.228s, learning 0.158s)
               Value function loss: 26.9125
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 596.22
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 11.39s
                        Total time: 24224.09s
                               ETA: 1177379.9s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.202s, learning 0.190s)
               Value function loss: 25.6806
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 597.42
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 11.39s
                        Total time: 24235.49s
                               ETA: 1177337.6s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.333s, learning 0.162s)
               Value function loss: 24.0505
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 595.27
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 11.50s
                        Total time: 24246.98s
                               ETA: 1177300.3s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.169s, learning 0.171s)
               Value function loss: 30.3730
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 593.21
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 11.34s
                        Total time: 24258.32s
                               ETA: 1177255.5s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.163s, learning 0.162s)
               Value function loss: 26.5256
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 595.90
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 11.33s
                        Total time: 24269.65s
                               ETA: 1177210.1s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.206s, learning 0.256s)
               Value function loss: 29.3193
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 597.80
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 11.46s
                        Total time: 24281.11s
                               ETA: 1177171.2s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.169s)
               Value function loss: 21.2909
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 596.32
               Mean episode length: 249.96
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 11.33s
                        Total time: 24292.44s
                               ETA: 1177126.0s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.202s, learning 0.159s)
               Value function loss: 27.3322
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 596.50
               Mean episode length: 249.96
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 11.36s
                        Total time: 24303.80s
                               ETA: 1177082.4s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.165s)
               Value function loss: 24.1798
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 595.58
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 11.37s
                        Total time: 24315.17s
                               ETA: 1177039.2s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.489s, learning 0.162s)
               Value function loss: 20.2024
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 589.76
               Mean episode length: 249.78
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 11.65s
                        Total time: 24326.82s
                               ETA: 1177009.6s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.416s, learning 0.168s)
               Value function loss: 25.6891
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 593.34
               Mean episode length: 249.78
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 11.58s
                        Total time: 24338.40s
                               ETA: 1176976.8s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.239s, learning 0.162s)
               Value function loss: 28.0831
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 590.71
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 11.40s
                        Total time: 24349.80s
                               ETA: 1176935.2s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.078s, learning 0.159s)
               Value function loss: 20.1260
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 593.02
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 11.24s
                        Total time: 24361.04s
                               ETA: 1176885.7s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.163s)
               Value function loss: 26.4631
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 595.19
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 11.34s
                        Total time: 24372.38s
                               ETA: 1176841.1s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.432s, learning 0.170s)
               Value function loss: 20.6152
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 593.81
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 11.60s
                        Total time: 24383.98s
                               ETA: 1176809.3s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.811s, learning 0.171s)
               Value function loss: 25.4597
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 583.01
               Mean episode length: 248.38
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 10.98s
                        Total time: 24394.96s
                               ETA: 1176747.6s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.159s)
               Value function loss: 24.8523
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 586.66
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 11.63s
                        Total time: 24406.59s
                               ETA: 1176717.3s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.227s, learning 0.170s)
               Value function loss: 18.1804
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 591.03
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 11.40s
                        Total time: 24417.99s
                               ETA: 1176675.7s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.493s, learning 0.165s)
               Value function loss: 22.8075
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 589.30
               Mean episode length: 249.67
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 11.66s
                        Total time: 24429.65s
                               ETA: 1176646.7s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.991s, learning 0.169s)
               Value function loss: 24.9022
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 591.57
               Mean episode length: 249.67
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 11.16s
                        Total time: 24440.81s
                               ETA: 1176593.8s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.072s, learning 0.162s)
               Value function loss: 17.1797
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 589.22
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 11.23s
                        Total time: 24452.04s
                               ETA: 1176544.4s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.022s, learning 0.171s)
               Value function loss: 22.6193
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 593.27
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 11.19s
                        Total time: 24463.24s
                               ETA: 1176493.1s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.613s, learning 0.172s)
               Value function loss: 20.9990
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 593.18
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 11.79s
                        Total time: 24475.02s
                               ETA: 1176470.3s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.987s, learning 0.170s)
               Value function loss: 25.8647
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 590.21
               Mean episode length: 249.44
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 11.16s
                        Total time: 24486.18s
                               ETA: 1176417.3s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.127s, learning 0.162s)
               Value function loss: 22.9300
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 590.64
               Mean episode length: 249.71
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 11.29s
                        Total time: 24497.47s
                               ETA: 1176370.8s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.236s, learning 0.174s)
               Value function loss: 25.0912
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 595.48
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 11.41s
                        Total time: 24508.88s
                               ETA: 1176330.0s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.164s)
               Value function loss: 24.5835
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 592.87
               Mean episode length: 249.73
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 11.64s
                        Total time: 24520.51s
                               ETA: 1176300.2s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.092s, learning 0.168s)
               Value function loss: 16.3312
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 592.16
               Mean episode length: 249.73
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 11.26s
                        Total time: 24531.77s
                               ETA: 1176252.2s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.053s, learning 0.199s)
               Value function loss: 21.1872
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 589.85
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 11.25s
                        Total time: 24543.02s
                               ETA: 1176204.0s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.367s, learning 0.164s)
               Value function loss: 28.0964
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 589.57
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 11.53s
                        Total time: 24554.55s
                               ETA: 1176169.2s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.997s, learning 0.170s)
               Value function loss: 22.8520
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 591.83
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 11.17s
                        Total time: 24565.72s
                               ETA: 1176116.9s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.974s, learning 0.165s)
               Value function loss: 27.6848
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 592.02
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 11.14s
                        Total time: 24576.86s
                               ETA: 1176063.4s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.162s)
               Value function loss: 26.8595
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 591.01
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 11.45s
                        Total time: 24588.31s
                               ETA: 1176024.7s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.162s)
               Value function loss: 28.0158
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 587.33
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 11.51s
                        Total time: 24599.81s
                               ETA: 1175988.8s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.939s, learning 0.165s)
               Value function loss: 36.2849
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 588.55
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 11.10s
                        Total time: 24610.92s
                               ETA: 1175933.7s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.327s, learning 0.162s)
               Value function loss: 30.5514
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 590.14
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 11.49s
                        Total time: 24622.41s
                               ETA: 1175897.1s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.319s, learning 0.164s)
               Value function loss: 30.3066
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 587.65
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 11.48s
                        Total time: 24633.89s
                               ETA: 1175860.2s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.301s, learning 0.165s)
               Value function loss: 22.8732
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 589.63
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 11.47s
                        Total time: 24645.36s
                               ETA: 1175822.5s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.330s, learning 0.158s)
               Value function loss: 28.5502
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 585.29
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 11.49s
                        Total time: 24656.85s
                               ETA: 1175785.8s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.270s, learning 0.162s)
               Value function loss: 26.7329
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 580.58
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 11.43s
                        Total time: 24668.28s
                               ETA: 1175746.6s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.268s, learning 0.169s)
               Value function loss: 23.8038
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 583.11
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 11.44s
                        Total time: 24679.72s
                               ETA: 1175707.5s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.004s, learning 0.173s)
               Value function loss: 25.5817
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 587.68
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 11.18s
                        Total time: 24690.89s
                               ETA: 1175656.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.158s)
               Value function loss: 31.8075
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 584.54
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 11.33s
                        Total time: 24702.22s
                               ETA: 1175612.2s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.572s, learning 0.165s)
               Value function loss: 16.3722
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 580.21
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 11.74s
                        Total time: 24713.96s
                               ETA: 1175587.6s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.115s, learning 0.188s)
               Value function loss: 30.7066
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 576.34
               Mean episode length: 248.48
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 11.30s
                        Total time: 24725.26s
                               ETA: 1175542.3s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.274s, learning 0.202s)
               Value function loss: 22.1577
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 572.37
               Mean episode length: 248.07
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 11.48s
                        Total time: 24736.74s
                               ETA: 1175505.3s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.286s, learning 0.199s)
               Value function loss: 23.6577
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 577.75
               Mean episode length: 249.68
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 11.48s
                        Total time: 24748.23s
                               ETA: 1175468.7s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.327s, learning 0.162s)
               Value function loss: 28.1089
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 570.50
               Mean episode length: 249.68
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 11.49s
                        Total time: 24759.71s
                               ETA: 1175432.3s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.922s, learning 0.260s)
               Value function loss: 18.9784
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 571.37
               Mean episode length: 249.63
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 11.18s
                        Total time: 24770.90s
                               ETA: 1175381.4s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.683s, learning 0.168s)
               Value function loss: 19.1202
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 565.65
               Mean episode length: 249.23
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 11.85s
                        Total time: 24782.75s
                               ETA: 1175362.3s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.454s, learning 0.160s)
               Value function loss: 22.5599
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 563.92
               Mean episode length: 249.60
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 11.61s
                        Total time: 24794.36s
                               ETA: 1175331.9s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.060s, learning 0.186s)
               Value function loss: 21.4621
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 562.84
               Mean episode length: 249.97
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 11.25s
                        Total time: 24805.61s
                               ETA: 1175284.1s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.166s)
               Value function loss: 21.1330
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 562.89
               Mean episode length: 249.97
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 11.43s
                        Total time: 24817.04s
                               ETA: 1175245.1s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.143s, learning 0.161s)
               Value function loss: 21.5014
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 562.98
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 11.30s
                        Total time: 24828.34s
                               ETA: 1175200.1s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.026s, learning 0.198s)
               Value function loss: 20.7472
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 556.40
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 11.22s
                        Total time: 24839.56s
                               ETA: 1175151.4s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.848s, learning 0.172s)
               Value function loss: 25.1652
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 562.39
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 12.02s
                        Total time: 24851.59s
                               ETA: 1175140.4s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.165s)
               Value function loss: 26.7452
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 562.46
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 11.44s
                        Total time: 24863.03s
                               ETA: 1175102.0s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.614s, learning 0.187s)
               Value function loss: 24.6631
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 561.81
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 11.80s
                        Total time: 24874.83s
                               ETA: 1175080.6s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.259s, learning 0.189s)
               Value function loss: 18.5090
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 564.32
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 11.45s
                        Total time: 24886.28s
                               ETA: 1175042.6s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.163s)
               Value function loss: 23.4196
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 563.16
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 11.52s
                        Total time: 24897.80s
                               ETA: 1175008.2s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.298s, learning 0.175s)
               Value function loss: 27.6339
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 566.85
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 11.47s
                        Total time: 24909.27s
                               ETA: 1174971.4s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.176s, learning 0.174s)
               Value function loss: 25.5360
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 566.72
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 11.35s
                        Total time: 24920.62s
                               ETA: 1174928.7s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.498s, learning 0.162s)
               Value function loss: 26.4826
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 568.08
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 11.66s
                        Total time: 24932.28s
                               ETA: 1174900.8s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.162s)
               Value function loss: 26.1879
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 569.58
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 11.50s
                        Total time: 24943.78s
                               ETA: 1174865.3s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.256s, learning 0.171s)
               Value function loss: 28.3363
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 570.17
               Mean episode length: 249.88
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 11.43s
                        Total time: 24955.21s
                               ETA: 1174826.4s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.403s, learning 0.184s)
               Value function loss: 33.1234
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 565.14
               Mean episode length: 249.71
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 11.59s
                        Total time: 24966.80s
                               ETA: 1174795.1s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.168s)
               Value function loss: 35.1083
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 569.47
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 11.12s
                        Total time: 24977.91s
                               ETA: 1174741.8s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.111s, learning 0.169s)
               Value function loss: 24.6707
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 575.84
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 11.28s
                        Total time: 24989.19s
                               ETA: 1174696.1s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.269s, learning 0.255s)
               Value function loss: 24.0723
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 573.87
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 11.52s
                        Total time: 25000.72s
                               ETA: 1174661.9s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.161s)
               Value function loss: 29.1756
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 570.16
               Mean episode length: 249.86
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 11.31s
                        Total time: 25012.02s
                               ETA: 1174617.4s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.376s, learning 0.176s)
               Value function loss: 24.6341
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 569.70
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 11.55s
                        Total time: 25023.58s
                               ETA: 1174584.6s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.082s, learning 0.191s)
               Value function loss: 21.1468
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 569.72
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 11.27s
                        Total time: 25034.85s
                               ETA: 1174538.7s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.392s, learning 0.190s)
               Value function loss: 22.0877
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 565.40
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 11.58s
                        Total time: 25046.43s
                               ETA: 1174507.2s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.199s, learning 0.169s)
               Value function loss: 27.1518
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 567.25
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 11.37s
                        Total time: 25057.80s
                               ETA: 1174465.9s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.161s)
               Value function loss: 23.1127
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 570.53
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 11.36s
                        Total time: 25069.16s
                               ETA: 1174424.1s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.164s)
               Value function loss: 23.2747
                    Surrogate loss: 0.0036
             Mean action noise std: 0.74
                       Mean reward: 565.27
               Mean episode length: 249.80
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 11.33s
                        Total time: 25080.49s
                               ETA: 1174381.0s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.162s)
               Value function loss: 24.4319
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 568.09
               Mean episode length: 249.92
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 11.28s
                        Total time: 25091.77s
                               ETA: 1174335.7s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.162s, learning 0.254s)
               Value function loss: 21.0975
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 564.14
               Mean episode length: 249.92
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 11.42s
                        Total time: 25103.19s
                               ETA: 1174296.7s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.671s, learning 0.172s)
               Value function loss: 25.1979
                    Surrogate loss: 0.0011
             Mean action noise std: 0.74
                       Mean reward: 564.01
               Mean episode length: 250.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 11.84s
                        Total time: 25115.03s
                               ETA: 1174277.6s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.169s, learning 0.178s)
               Value function loss: 19.7585
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 561.83
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 11.35s
                        Total time: 25126.38s
                               ETA: 1174235.4s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.162s)
               Value function loss: 19.2347
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 555.79
               Mean episode length: 249.95
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 11.36s
                        Total time: 25137.74s
                               ETA: 1174193.8s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.185s, learning 0.168s)
               Value function loss: 16.2635
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 549.81
               Mean episode length: 249.95
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 11.35s
                        Total time: 25149.09s
                               ETA: 1174151.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.161s)
               Value function loss: 16.9609
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 542.96
               Mean episode length: 250.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 11.36s
                        Total time: 25160.46s
                               ETA: 1174110.6s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.157s, learning 0.173s)
               Value function loss: 12.0428
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 532.04
               Mean episode length: 249.68
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 11.33s
                        Total time: 25171.79s
                               ETA: 1174067.7s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.603s, learning 0.170s)
               Value function loss: 12.0416
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 526.16
               Mean episode length: 249.68
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 11.77s
                        Total time: 25183.56s
                               ETA: 1174045.5s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.204s, learning 0.170s)
               Value function loss: 15.2863
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 519.06
               Mean episode length: 249.97
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 11.37s
                        Total time: 25194.93s
                               ETA: 1174004.7s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.501s, learning 0.158s)
               Value function loss: 17.3469
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 514.28
               Mean episode length: 249.97
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 11.66s
                        Total time: 25206.59s
                               ETA: 1173977.2s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.772s, learning 0.168s)
               Value function loss: 15.3137
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 514.13
               Mean episode length: 250.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 11.94s
                        Total time: 25218.53s
                               ETA: 1173962.8s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.393s, learning 0.269s)
               Value function loss: 18.7148
                    Surrogate loss: 0.0029
             Mean action noise std: 0.74
                       Mean reward: 501.99
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 11.66s
                        Total time: 25230.19s
                               ETA: 1173935.5s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.511s, learning 0.174s)
               Value function loss: 13.2798
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 507.06
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 11.69s
                        Total time: 25241.88s
                               ETA: 1173909.3s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.820s, learning 0.163s)
               Value function loss: 14.3728
                    Surrogate loss: -0.0021
             Mean action noise std: 0.74
                       Mean reward: 508.55
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 10.98s
                        Total time: 25252.86s
                               ETA: 1173850.4s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.222s, learning 0.158s)
               Value function loss: 15.2246
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 513.17
               Mean episode length: 249.68
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 11.38s
                        Total time: 25264.24s
                               ETA: 1173810.0s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.826s, learning 0.162s)
               Value function loss: 17.5713
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 501.05
               Mean episode length: 249.68
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 10.99s
                        Total time: 25275.23s
                               ETA: 1173751.5s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.381s, learning 0.165s)
               Value function loss: 14.3299
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 494.69
               Mean episode length: 250.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 11.55s
                        Total time: 25286.78s
                               ETA: 1173718.8s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1457 steps/s (collection: 10.986s, learning 0.257s)
               Value function loss: 19.5820
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 504.46
               Mean episode length: 250.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 11.24s
                        Total time: 25298.02s
                               ETA: 1173672.2s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.311s, learning 0.175s)
               Value function loss: 20.6043
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 499.97
               Mean episode length: 249.76
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 11.49s
                        Total time: 25309.50s
                               ETA: 1173636.9s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.350s, learning 0.160s)
               Value function loss: 23.3828
                    Surrogate loss: 0.0130
             Mean action noise std: 0.74
                       Mean reward: 497.61
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 11.51s
                        Total time: 25321.01s
                               ETA: 1173602.6s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.997s, learning 0.160s)
               Value function loss: 22.1255
                    Surrogate loss: 0.0073
             Mean action noise std: 0.74
                       Mean reward: 500.21
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 11.16s
                        Total time: 25332.17s
                               ETA: 1173552.1s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.767s, learning 0.189s)
               Value function loss: 18.0650
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 499.99
               Mean episode length: 249.57
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 11.96s
                        Total time: 25344.13s
                               ETA: 1173538.6s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.437s, learning 0.193s)
               Value function loss: 19.2709
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 491.90
               Mean episode length: 249.15
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 11.63s
                        Total time: 25355.76s
                               ETA: 1173510.0s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.171s)
               Value function loss: 18.3466
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 495.27
               Mean episode length: 249.60
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 11.38s
                        Total time: 25367.14s
                               ETA: 1173470.0s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.162s)
               Value function loss: 16.8436
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 496.48
               Mean episode length: 249.92
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 11.55s
                        Total time: 25378.69s
                               ETA: 1173437.7s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.852s, learning 0.163s)
               Value function loss: 14.4469
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 487.61
               Mean episode length: 249.37
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 11.01s
                        Total time: 25389.70s
                               ETA: 1173380.7s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.133s, learning 0.173s)
               Value function loss: 14.1829
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 479.72
               Mean episode length: 248.73
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 11.31s
                        Total time: 25401.01s
                               ETA: 1173337.2s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.702s, learning 0.168s)
               Value function loss: 16.2712
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 474.85
               Mean episode length: 249.27
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 10.87s
                        Total time: 25411.88s
                               ETA: 1173273.7s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.386s, learning 0.160s)
               Value function loss: 16.3586
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 490.02
               Mean episode length: 248.01
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 11.55s
                        Total time: 25423.42s
                               ETA: 1173241.3s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.054s, learning 0.165s)
               Value function loss: 16.1420
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 492.78
               Mean episode length: 247.26
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 11.22s
                        Total time: 25434.64s
                               ETA: 1173193.9s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.163s, learning 0.186s)
               Value function loss: 17.4265
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 470.62
               Mean episode length: 247.90
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 11.35s
                        Total time: 25445.99s
                               ETA: 1173152.6s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.227s, learning 0.182s)
               Value function loss: 15.2350
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 473.69
               Mean episode length: 247.61
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 11.41s
                        Total time: 25457.40s
                               ETA: 1173114.0s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.361s, learning 0.175s)
               Value function loss: 14.1263
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 471.19
               Mean episode length: 247.53
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 11.54s
                        Total time: 25468.94s
                               ETA: 1173081.3s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.285s, learning 0.163s)
               Value function loss: 16.0731
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 455.03
               Mean episode length: 246.94
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 11.45s
                        Total time: 25480.39s
                               ETA: 1173044.6s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.178s, learning 0.170s)
               Value function loss: 10.6804
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 441.16
               Mean episode length: 246.06
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 11.35s
                        Total time: 25491.73s
                               ETA: 1173003.3s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.074s, learning 0.159s)
               Value function loss: 12.9201
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 421.96
               Mean episode length: 244.05
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 11.23s
                        Total time: 25502.97s
                               ETA: 1172956.7s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.531s, learning 0.162s)
               Value function loss: 13.9802
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 424.93
               Mean episode length: 243.93
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 11.69s
                        Total time: 25514.66s
                               ETA: 1172931.3s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.617s, learning 0.205s)
               Value function loss: 9.6197
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 425.93
               Mean episode length: 245.03
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 11.82s
                        Total time: 25526.48s
                               ETA: 1172911.8s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.164s)
               Value function loss: 11.7449
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 418.94
               Mean episode length: 245.49
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 11.58s
                        Total time: 25538.06s
                               ETA: 1172881.1s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.205s, learning 0.183s)
               Value function loss: 9.8660
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: 425.07
               Mean episode length: 245.78
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 11.39s
                        Total time: 25549.44s
                               ETA: 1172841.8s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.156s, learning 0.171s)
               Value function loss: 14.1225
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 451.22
               Mean episode length: 247.81
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 11.33s
                        Total time: 25560.77s
                               ETA: 1172799.7s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.458s, learning 0.186s)
               Value function loss: 15.3205
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 441.21
               Mean episode length: 247.04
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 11.64s
                        Total time: 25572.42s
                               ETA: 1172772.1s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.403s, learning 0.163s)
               Value function loss: 13.4836
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 452.90
               Mean episode length: 247.83
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 11.57s
                        Total time: 25583.98s
                               ETA: 1172741.0s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.141s, learning 0.171s)
               Value function loss: 13.7708
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 457.94
               Mean episode length: 249.02
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 11.31s
                        Total time: 25595.29s
                               ETA: 1172698.3s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.013s, learning 0.165s)
               Value function loss: 13.3075
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 455.10
               Mean episode length: 248.44
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 11.18s
                        Total time: 25606.47s
                               ETA: 1172649.4s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.250s, learning 0.187s)
               Value function loss: 15.9904
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 470.90
               Mean episode length: 248.77
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 11.44s
                        Total time: 25617.91s
                               ETA: 1172612.5s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.011s, learning 0.170s)
               Value function loss: 18.4452
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 477.10
               Mean episode length: 249.50
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 11.18s
                        Total time: 25629.09s
                               ETA: 1172563.8s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.036s, learning 0.165s)
               Value function loss: 18.0537
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 480.96
               Mean episode length: 248.80
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 11.20s
                        Total time: 25640.29s
                               ETA: 1172516.1s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.979s, learning 0.191s)
               Value function loss: 16.8068
                    Surrogate loss: 0.0009
             Mean action noise std: 0.74
                       Mean reward: 469.68
               Mean episode length: 248.80
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 11.17s
                        Total time: 25651.46s
                               ETA: 1172467.0s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.121s, learning 0.175s)
               Value function loss: 19.2054
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 477.96
               Mean episode length: 249.74
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 11.30s
                        Total time: 25662.76s
                               ETA: 1172423.8s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.165s)
               Value function loss: 21.9930
                    Surrogate loss: 0.0011
             Mean action noise std: 0.74
                       Mean reward: 479.43
               Mean episode length: 250.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 11.46s
                        Total time: 25674.22s
                               ETA: 1172388.1s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.159s)
               Value function loss: 25.1702
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 487.33
               Mean episode length: 248.94
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 11.57s
                        Total time: 25685.79s
                               ETA: 1172357.4s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.579s, learning 0.170s)
               Value function loss: 22.3562
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 473.68
               Mean episode length: 249.70
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 11.75s
                        Total time: 25697.54s
                               ETA: 1172334.9s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.422s, learning 0.249s)
               Value function loss: 20.6112
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 475.96
               Mean episode length: 249.95
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 11.67s
                        Total time: 25709.21s
                               ETA: 1172308.8s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.079s, learning 0.161s)
               Value function loss: 20.9246
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 473.43
               Mean episode length: 249.83
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 11.24s
                        Total time: 25720.45s
                               ETA: 1172263.2s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.486s, learning 0.167s)
               Value function loss: 22.5534
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 487.43
               Mean episode length: 250.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 11.65s
                        Total time: 25732.10s
                               ETA: 1172236.2s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.073s, learning 0.162s)
               Value function loss: 19.1713
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 478.15
               Mean episode length: 249.91
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 11.24s
                        Total time: 25743.34s
                               ETA: 1172190.4s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.342s, learning 0.161s)
               Value function loss: 25.2122
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 465.76
               Mean episode length: 249.83
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 11.50s
                        Total time: 25754.84s
                               ETA: 1172156.7s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.048s, learning 0.172s)
               Value function loss: 20.3018
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 465.13
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 11.22s
                        Total time: 25766.06s
                               ETA: 1172110.2s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.283s, learning 0.187s)
               Value function loss: 23.6192
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 464.02
               Mean episode length: 250.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 11.47s
                        Total time: 25777.53s
                               ETA: 1172075.1s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.331s, learning 0.172s)
               Value function loss: 18.9972
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 458.66
               Mean episode length: 250.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 11.50s
                        Total time: 25789.03s
                               ETA: 1172041.5s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.648s, learning 0.168s)
               Value function loss: 26.9580
                    Surrogate loss: -0.0019
             Mean action noise std: 0.74
                       Mean reward: 469.85
               Mean episode length: 249.78
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 11.82s
                        Total time: 25800.85s
                               ETA: 1172022.1s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.093s, learning 0.161s)
               Value function loss: 21.6137
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 469.67
               Mean episode length: 249.78
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 11.25s
                        Total time: 25812.10s
                               ETA: 1171977.3s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.331s, learning 0.165s)
               Value function loss: 23.5569
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 468.47
               Mean episode length: 249.97
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 11.50s
                        Total time: 25823.60s
                               ETA: 1171943.5s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.267s, learning 0.167s)
               Value function loss: 24.5652
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: 473.45
               Mean episode length: 249.84
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 11.43s
                        Total time: 25835.03s
                               ETA: 1171906.8s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.817s, learning 0.176s)
               Value function loss: 20.4489
                    Surrogate loss: 0.0004
             Mean action noise std: 0.74
                       Mean reward: 469.47
               Mean episode length: 249.84
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 10.99s
                        Total time: 25846.03s
                               ETA: 1171850.2s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.096s, learning 0.166s)
               Value function loss: 18.3826
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 476.49
               Mean episode length: 250.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 11.26s
                        Total time: 25857.29s
                               ETA: 1171805.9s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.617s, learning 0.178s)
               Value function loss: 19.7491
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 479.82
               Mean episode length: 249.75
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 11.80s
                        Total time: 25869.08s
                               ETA: 1171785.7s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.138s, learning 0.169s)
               Value function loss: 16.0146
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 480.91
               Mean episode length: 248.22
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 11.31s
                        Total time: 25880.39s
                               ETA: 1171743.4s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.700s, learning 0.169s)
               Value function loss: 15.6445
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 485.38
               Mean episode length: 248.08
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 11.87s
                        Total time: 25892.26s
                               ETA: 1171726.6s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.263s, learning 0.161s)
               Value function loss: 13.5610
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 491.23
               Mean episode length: 249.65
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 11.42s
                        Total time: 25903.68s
                               ETA: 1171689.6s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.570s, learning 0.161s)
               Value function loss: 17.5490
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 504.50
               Mean episode length: 249.80
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 11.73s
                        Total time: 25915.42s
                               ETA: 1171666.6s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.162s)
               Value function loss: 17.8073
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 509.72
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 11.37s
                        Total time: 25926.79s
                               ETA: 1171627.3s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.603s, learning 0.180s)
               Value function loss: 17.5219
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 509.88
               Mean episode length: 249.78
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 11.78s
                        Total time: 25938.57s
                               ETA: 1171606.6s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.380s, learning 0.159s)
               Value function loss: 16.7081
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 505.12
               Mean episode length: 249.78
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 11.54s
                        Total time: 25950.11s
                               ETA: 1171574.9s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.835s, learning 0.164s)
               Value function loss: 14.0919
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 503.45
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 11.00s
                        Total time: 25961.11s
                               ETA: 1171518.9s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.382s, learning 0.177s)
               Value function loss: 17.0719
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 507.30
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 11.56s
                        Total time: 25972.67s
                               ETA: 1171488.2s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.823s, learning 0.176s)
               Value function loss: 18.5740
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 502.78
               Mean episode length: 249.95
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 11.00s
                        Total time: 25983.67s
                               ETA: 1171432.3s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.632s, learning 0.183s)
               Value function loss: 19.6692
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 515.92
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 11.82s
                        Total time: 25995.48s
                               ETA: 1171413.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.093s, learning 0.165s)
               Value function loss: 21.5583
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 516.81
               Mean episode length: 249.75
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 11.26s
                        Total time: 26006.74s
                               ETA: 1171368.9s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.318s, learning 0.169s)
               Value function loss: 20.6541
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 517.09
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 11.49s
                        Total time: 26018.23s
                               ETA: 1171335.0s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.122s, learning 0.214s)
               Value function loss: 18.9649
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 522.82
               Mean episode length: 249.69
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 11.34s
                        Total time: 26029.56s
                               ETA: 1171294.4s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.416s, learning 0.157s)
               Value function loss: 23.0233
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 521.56
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 11.57s
                        Total time: 26041.13s
                               ETA: 1171264.4s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.413s, learning 0.163s)
               Value function loss: 22.1912
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 529.86
               Mean episode length: 249.91
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 11.58s
                        Total time: 26052.71s
                               ETA: 1171234.6s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.503s, learning 0.162s)
               Value function loss: 16.0991
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 528.66
               Mean episode length: 249.94
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 11.67s
                        Total time: 26064.38s
                               ETA: 1171208.8s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.150s, learning 0.157s)
               Value function loss: 17.1499
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 525.19
               Mean episode length: 249.91
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 11.31s
                        Total time: 26075.68s
                               ETA: 1171166.9s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.161s)
               Value function loss: 19.9889
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 515.78
               Mean episode length: 249.89
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 11.48s
                        Total time: 26087.16s
                               ETA: 1171132.8s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.151s, learning 0.164s)
               Value function loss: 16.2034
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 530.27
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 11.31s
                        Total time: 26098.48s
                               ETA: 1171091.3s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.636s, learning 0.191s)
               Value function loss: 17.4591
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 531.40
               Mean episode length: 249.84
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 11.83s
                        Total time: 26110.30s
                               ETA: 1171072.8s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.563s, learning 0.166s)
               Value function loss: 16.0732
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 533.58
               Mean episode length: 249.84
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 11.73s
                        Total time: 26122.03s
                               ETA: 1171049.9s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.168s)
               Value function loss: 18.8405
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 522.10
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.55s
                        Total time: 26133.58s
                               ETA: 1171019.2s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.888s, learning 0.161s)
               Value function loss: 16.2833
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 524.51
               Mean episode length: 249.60
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 12.05s
                        Total time: 26145.63s
                               ETA: 1171010.7s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.554s, learning 0.179s)
               Value function loss: 21.7692
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 522.92
               Mean episode length: 249.84
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 11.73s
                        Total time: 26157.37s
                               ETA: 1170988.1s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.220s, learning 0.165s)
               Value function loss: 18.4612
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 534.88
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 11.38s
                        Total time: 26168.75s
                               ETA: 1170949.9s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.160s)
               Value function loss: 14.5463
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 539.69
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 11.06s
                        Total time: 26179.81s
                               ETA: 1170897.0s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.270s, learning 0.168s)
               Value function loss: 20.7131
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 535.35
               Mean episode length: 249.70
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 11.44s
                        Total time: 26191.25s
                               ETA: 1170861.2s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.950s, learning 0.176s)
               Value function loss: 20.0626
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 548.11
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 11.13s
                        Total time: 26202.37s
                               ETA: 1170811.5s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.569s, learning 0.164s)
               Value function loss: 18.4695
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 547.60
               Mean episode length: 249.73
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 11.73s
                        Total time: 26214.11s
                               ETA: 1170789.0s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.993s, learning 0.164s)
               Value function loss: 21.2346
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 553.95
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 11.16s
                        Total time: 26225.26s
                               ETA: 1170740.7s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.004s, learning 0.185s)
               Value function loss: 17.4330
                    Surrogate loss: -0.0008
             Mean action noise std: 0.74
                       Mean reward: 551.03
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 11.19s
                        Total time: 26236.45s
                               ETA: 1170693.9s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.297s, learning 0.159s)
               Value function loss: 22.0027
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 550.23
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 11.46s
                        Total time: 26247.91s
                               ETA: 1170659.1s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.713s, learning 0.177s)
               Value function loss: 14.7879
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 553.11
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 11.89s
                        Total time: 26259.80s
                               ETA: 1170643.6s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.610s, learning 0.166s)
               Value function loss: 17.8322
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 553.34
               Mean episode length: 249.94
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 11.78s
                        Total time: 26271.57s
                               ETA: 1170623.1s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.593s, learning 0.178s)
               Value function loss: 24.3802
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 556.24
               Mean episode length: 248.61
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 11.77s
                        Total time: 26283.35s
                               ETA: 1170602.3s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.164s)
               Value function loss: 21.7035
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 556.00
               Mean episode length: 248.21
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 11.52s
                        Total time: 26294.86s
                               ETA: 1170570.1s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.300s, learning 0.166s)
               Value function loss: 21.3290
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 560.72
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 11.47s
                        Total time: 26306.33s
                               ETA: 1170535.8s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.274s, learning 0.167s)
               Value function loss: 18.8937
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 561.14
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 11.44s
                        Total time: 26317.77s
                               ETA: 1170500.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.381s, learning 0.167s)
               Value function loss: 21.5489
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 554.90
               Mean episode length: 249.69
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 11.55s
                        Total time: 26329.32s
                               ETA: 1170469.8s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.895s, learning 0.196s)
               Value function loss: 22.5629
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 560.06
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 11.09s
                        Total time: 26340.41s
                               ETA: 1170418.8s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.067s, learning 0.160s)
               Value function loss: 28.4925
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 567.46
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 11.23s
                        Total time: 26351.64s
                               ETA: 1170374.0s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.215s, learning 0.162s)
               Value function loss: 25.8422
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 554.64
               Mean episode length: 248.95
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 11.38s
                        Total time: 26363.01s
                               ETA: 1170335.8s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.551s, learning 0.189s)
               Value function loss: 24.6703
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 560.01
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 11.74s
                        Total time: 26374.75s
                               ETA: 1170313.8s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.933s, learning 0.164s)
               Value function loss: 27.0240
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 558.09
               Mean episode length: 249.60
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 11.10s
                        Total time: 26385.85s
                               ETA: 1170263.2s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.476s, learning 0.166s)
               Value function loss: 37.2590
                    Surrogate loss: 0.0005
             Mean action noise std: 0.74
                       Mean reward: 567.07
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 11.64s
                        Total time: 26397.49s
                               ETA: 1170236.9s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.122s, learning 0.188s)
               Value function loss: 39.6345
                    Surrogate loss: -0.0012
             Mean action noise std: 0.74
                       Mean reward: 564.57
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 11.31s
                        Total time: 26408.80s
                               ETA: 1170195.9s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.385s, learning 0.189s)
               Value function loss: 24.3755
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 559.78
               Mean episode length: 249.80
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 11.57s
                        Total time: 26420.38s
                               ETA: 1170166.5s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.161s)
               Value function loss: 24.4156
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 555.36
               Mean episode length: 249.80
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 11.36s
                        Total time: 26431.73s
                               ETA: 1170127.6s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.584s, learning 0.163s)
               Value function loss: 31.9243
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 542.15
               Mean episode length: 247.80
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 11.75s
                        Total time: 26443.48s
                               ETA: 1170106.0s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.168s)
               Value function loss: 21.5006
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 555.10
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 11.53s
                        Total time: 26455.01s
                               ETA: 1170074.7s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.248s, learning 0.192s)
               Value function loss: 25.0649
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 567.85
               Mean episode length: 249.59
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 11.44s
                        Total time: 26466.45s
                               ETA: 1170039.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.243s, learning 0.189s)
               Value function loss: 21.6261
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 567.74
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 11.43s
                        Total time: 26477.88s
                               ETA: 1170004.0s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.575s, learning 0.254s)
               Value function loss: 24.8305
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 554.41
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 11.83s
                        Total time: 26489.71s
                               ETA: 1169986.0s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.590s, learning 0.190s)
               Value function loss: 24.0908
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 554.07
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 11.78s
                        Total time: 26501.49s
                               ETA: 1169965.9s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.642s, learning 0.162s)
               Value function loss: 27.7964
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 551.83
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 10.80s
                        Total time: 26512.29s
                               ETA: 1169902.8s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.447s, learning 0.183s)
               Value function loss: 30.9515
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 555.52
               Mean episode length: 249.83
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.63s
                        Total time: 26523.92s
                               ETA: 1169876.1s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.728s, learning 0.161s)
               Value function loss: 27.4187
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 559.87
               Mean episode length: 249.81
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 11.89s
                        Total time: 26535.81s
                               ETA: 1169860.8s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.983s, learning 0.162s)
               Value function loss: 30.8151
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 550.62
               Mean episode length: 249.49
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 11.15s
                        Total time: 26546.96s
                               ETA: 1169812.8s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.486s, learning 0.184s)
               Value function loss: 27.5951
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 566.21
               Mean episode length: 249.71
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 11.67s
                        Total time: 26558.63s
                               ETA: 1169787.9s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.169s)
               Value function loss: 26.7881
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 569.66
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 11.08s
                        Total time: 26569.71s
                               ETA: 1169737.0s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.183s, learning 0.174s)
               Value function loss: 27.6629
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 568.72
               Mean episode length: 249.61
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 11.36s
                        Total time: 26581.06s
                               ETA: 1169698.4s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.310s, learning 0.192s)
               Value function loss: 29.3021
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 561.90
               Mean episode length: 248.28
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 11.50s
                        Total time: 26592.56s
                               ETA: 1169666.1s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.234s, learning 0.192s)
               Value function loss: 19.4582
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 557.65
               Mean episode length: 248.67
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 11.43s
                        Total time: 26603.99s
                               ETA: 1169630.6s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.012s, learning 0.191s)
               Value function loss: 19.8648
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 562.34
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 11.20s
                        Total time: 26615.19s
                               ETA: 1169585.3s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.893s, learning 0.184s)
               Value function loss: 25.2520
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 569.06
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 11.08s
                        Total time: 26626.27s
                               ETA: 1169534.5s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.372s, learning 0.186s)
               Value function loss: 26.5754
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 577.89
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 11.56s
                        Total time: 26637.83s
                               ETA: 1169504.8s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.398s, learning 0.179s)
               Value function loss: 33.3258
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 573.18
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 11.58s
                        Total time: 26649.41s
                               ETA: 1169475.9s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.133s, learning 0.160s)
               Value function loss: 25.2213
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 571.27
               Mean episode length: 248.97
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 11.29s
                        Total time: 26660.70s
                               ETA: 1169434.7s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.117s, learning 0.186s)
               Value function loss: 21.7168
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 581.57
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 11.30s
                        Total time: 26672.00s
                               ETA: 1169393.9s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.550s, learning 0.197s)
               Value function loss: 23.0471
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 572.22
               Mean episode length: 248.84
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 11.75s
                        Total time: 26683.75s
                               ETA: 1169372.6s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.756s, learning 0.169s)
               Value function loss: 23.5713
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 572.13
               Mean episode length: 249.31
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 10.92s
                        Total time: 26694.67s
                               ETA: 1169315.3s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.097s, learning 0.161s)
               Value function loss: 27.4987
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 583.63
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 11.26s
                        Total time: 26705.93s
                               ETA: 1169272.6s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.991s, learning 0.159s)
               Value function loss: 24.9635
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 584.38
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 11.15s
                        Total time: 26717.08s
                               ETA: 1169225.2s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.039s, learning 0.164s)
               Value function loss: 30.6734
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 580.14
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 11.20s
                        Total time: 26728.29s
                               ETA: 1169180.1s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.105s, learning 0.175s)
               Value function loss: 25.9301
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 577.80
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 11.28s
                        Total time: 26739.57s
                               ETA: 1169138.5s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.215s, learning 0.192s)
               Value function loss: 34.6487
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 565.52
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 11.41s
                        Total time: 26750.97s
                               ETA: 1169102.4s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.377s, learning 0.171s)
               Value function loss: 39.1775
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 575.56
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 11.55s
                        Total time: 26762.52s
                               ETA: 1169072.5s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.196s, learning 0.166s)
               Value function loss: 37.1625
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 584.77
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 11.36s
                        Total time: 26773.88s
                               ETA: 1169034.5s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.369s, learning 0.176s)
               Value function loss: 37.9721
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 583.07
               Mean episode length: 249.96
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 11.54s
                        Total time: 26785.43s
                               ETA: 1169004.5s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.081s, learning 0.161s)
               Value function loss: 34.8797
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 586.88
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 11.24s
                        Total time: 26796.67s
                               ETA: 1168961.4s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.229s, learning 0.160s)
               Value function loss: 28.9416
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 579.42
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 11.39s
                        Total time: 26808.06s
                               ETA: 1168924.7s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.963s, learning 0.178s)
               Value function loss: 27.2127
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 580.96
               Mean episode length: 248.60
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 11.14s
                        Total time: 26819.20s
                               ETA: 1168877.1s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.298s, learning 0.159s)
               Value function loss: 27.3215
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 587.93
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 11.46s
                        Total time: 26830.66s
                               ETA: 1168843.4s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.235s, learning 0.185s)
               Value function loss: 30.9649
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 581.42
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 11.42s
                        Total time: 26842.08s
                               ETA: 1168808.1s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.006s, learning 0.160s)
               Value function loss: 26.5786
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 591.65
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 11.17s
                        Total time: 26853.24s
                               ETA: 1168761.7s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.165s)
               Value function loss: 26.2402
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 594.26
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 11.70s
                        Total time: 26864.95s
                               ETA: 1168738.8s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.162s)
               Value function loss: 34.0448
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 589.62
               Mean episode length: 249.98
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 11.36s
                        Total time: 26876.31s
                               ETA: 1168701.0s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.488s, learning 0.164s)
               Value function loss: 29.2849
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 583.67
               Mean episode length: 249.98
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 11.65s
                        Total time: 26887.96s
                               ETA: 1168675.9s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1468 steps/s (collection: 11.001s, learning 0.159s)
               Value function loss: 27.8605
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 580.09
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 11.16s
                        Total time: 26899.12s
                               ETA: 1168629.4s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.192s, learning 0.162s)
               Value function loss: 35.7872
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 581.86
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 11.35s
                        Total time: 26910.48s
                               ETA: 1168591.3s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.782s, learning 0.158s)
               Value function loss: 23.1245
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 585.10
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 10.94s
                        Total time: 26921.42s
                               ETA: 1168535.3s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.974s, learning 0.161s)
               Value function loss: 31.3897
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 575.93
               Mean episode length: 249.54
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 11.13s
                        Total time: 26932.55s
                               ETA: 1168487.8s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.635s, learning 0.166s)
               Value function loss: 34.5672
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 577.88
               Mean episode length: 248.53
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 11.80s
                        Total time: 26944.35s
                               ETA: 1168469.2s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.067s, learning 0.192s)
               Value function loss: 21.7070
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 581.69
               Mean episode length: 248.79
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 11.26s
                        Total time: 26955.61s
                               ETA: 1168427.1s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.489s, learning 0.171s)
               Value function loss: 24.2499
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 586.24
               Mean episode length: 249.80
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 10.66s
                        Total time: 26966.27s
                               ETA: 1168359.1s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.216s, learning 0.160s)
               Value function loss: 21.7562
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 589.16
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 11.38s
                        Total time: 26977.65s
                               ETA: 1168322.2s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.857s, learning 0.166s)
               Value function loss: 25.6998
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 578.66
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 11.02s
                        Total time: 26988.67s
                               ETA: 1168270.0s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.862s, learning 0.165s)
               Value function loss: 31.2779
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 587.46
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 11.03s
                        Total time: 26999.70s
                               ETA: 1168218.0s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.345s, learning 0.274s)
               Value function loss: 26.9957
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 588.22
               Mean episode length: 249.95
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 11.62s
                        Total time: 27011.32s
                               ETA: 1168191.7s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.348s, learning 0.170s)
               Value function loss: 25.2218
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 586.93
               Mean episode length: 249.69
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.52s
                        Total time: 27022.84s
                               ETA: 1168161.0s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.247s, learning 0.189s)
               Value function loss: 23.4628
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 583.91
               Mean episode length: 249.69
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 11.44s
                        Total time: 27034.27s
                               ETA: 1168126.7s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.430s, learning 0.188s)
               Value function loss: 25.7970
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 585.97
               Mean episode length: 249.75
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 11.62s
                        Total time: 27045.89s
                               ETA: 1168100.4s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.201s, learning 0.160s)
               Value function loss: 28.8607
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 580.56
               Mean episode length: 248.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 11.36s
                        Total time: 27057.25s
                               ETA: 1168062.9s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.470s, learning 0.159s)
               Value function loss: 28.3988
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 579.98
               Mean episode length: 249.10
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 11.63s
                        Total time: 27068.88s
                               ETA: 1168037.1s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.869s, learning 0.161s)
               Value function loss: 32.0756
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 577.42
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 11.03s
                        Total time: 27079.91s
                               ETA: 1167985.4s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.545s, learning 0.163s)
               Value function loss: 28.6337
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 582.23
               Mean episode length: 249.90
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 11.71s
                        Total time: 27091.62s
                               ETA: 1167963.0s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.599s, learning 0.166s)
               Value function loss: 36.4279
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 584.26
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 11.77s
                        Total time: 27103.38s
                               ETA: 1167943.1s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.119s, learning 0.168s)
               Value function loss: 40.1073
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 581.94
               Mean episode length: 249.95
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 11.29s
                        Total time: 27114.67s
                               ETA: 1167902.6s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.675s, learning 0.168s)
               Value function loss: 44.4208
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 576.11
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 11.84s
                        Total time: 27126.51s
                               ETA: 1167886.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.314s, learning 0.164s)
               Value function loss: 38.2723
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 578.84
               Mean episode length: 249.97
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 11.48s
                        Total time: 27137.99s
                               ETA: 1167853.8s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.508s, learning 0.166s)
               Value function loss: 34.4551
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 582.13
               Mean episode length: 249.97
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 11.67s
                        Total time: 27149.66s
                               ETA: 1167829.9s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.323s, learning 0.168s)
               Value function loss: 30.6082
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 577.70
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 11.49s
                        Total time: 27161.16s
                               ETA: 1167798.2s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.590s, learning 0.162s)
               Value function loss: 27.3372
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 577.98
               Mean episode length: 249.55
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 11.75s
                        Total time: 27172.91s
                               ETA: 1167777.8s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.605s, learning 0.159s)
               Value function loss: 33.0313
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 580.43
               Mean episode length: 249.72
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 11.76s
                        Total time: 27184.67s
                               ETA: 1167757.9s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.511s, learning 0.254s)
               Value function loss: 29.5476
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 580.79
               Mean episode length: 249.72
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 11.76s
                        Total time: 27196.44s
                               ETA: 1167738.0s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.042s, learning 0.170s)
               Value function loss: 36.1113
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 584.65
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 11.21s
                        Total time: 27207.65s
                               ETA: 1167694.4s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.162s)
               Value function loss: 26.2491
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 589.62
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 11.43s
                        Total time: 27219.07s
                               ETA: 1167660.0s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.212s, learning 0.161s)
               Value function loss: 36.4420
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 587.29
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 11.37s
                        Total time: 27230.45s
                               ETA: 1167623.4s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.164s)
               Value function loss: 28.5700
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 586.58
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 11.46s
                        Total time: 27241.91s
                               ETA: 1167590.6s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.047s, learning 0.167s)
               Value function loss: 31.0908
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 583.96
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 11.21s
                        Total time: 27253.12s
                               ETA: 1167547.2s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.062s, learning 0.160s)
               Value function loss: 32.9731
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 584.54
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 11.22s
                        Total time: 27264.34s
                               ETA: 1167504.1s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.116s, learning 0.164s)
               Value function loss: 26.5717
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 582.34
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 11.28s
                        Total time: 27275.62s
                               ETA: 1167463.6s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.082s, learning 0.164s)
               Value function loss: 32.8259
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 586.49
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 11.25s
                        Total time: 27286.87s
                               ETA: 1167421.6s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.513s, learning 0.164s)
               Value function loss: 34.4282
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 587.83
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 11.68s
                        Total time: 27298.55s
                               ETA: 1167398.1s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.620s, learning 0.175s)
               Value function loss: 20.7625
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 578.38
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 11.80s
                        Total time: 27310.34s
                               ETA: 1167379.7s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.377s, learning 0.213s)
               Value function loss: 27.8928
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 581.14
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 11.59s
                        Total time: 27321.93s
                               ETA: 1167352.5s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.163s)
               Value function loss: 23.4149
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 587.30
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 11.74s
                        Total time: 27333.67s
                               ETA: 1167331.8s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.872s, learning 0.166s)
               Value function loss: 29.5367
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 585.66
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 11.04s
                        Total time: 27344.71s
                               ETA: 1167281.1s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.161s)
               Value function loss: 29.9597
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 582.67
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 11.17s
                        Total time: 27355.88s
                               ETA: 1167235.8s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.048s, learning 0.165s)
               Value function loss: 32.4712
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 585.84
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 11.21s
                        Total time: 27367.09s
                               ETA: 1167192.6s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.158s)
               Value function loss: 30.1575
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 585.83
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 11.09s
                        Total time: 27378.18s
                               ETA: 1167144.3s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.718s, learning 0.170s)
               Value function loss: 24.1144
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 590.99
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 10.89s
                        Total time: 27389.07s
                               ETA: 1167087.3s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.343s, learning 0.161s)
               Value function loss: 30.6352
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 591.30
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 11.50s
                        Total time: 27400.57s
                               ETA: 1167056.6s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.563s, learning 0.175s)
               Value function loss: 33.5504
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 588.08
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 11.74s
                        Total time: 27412.31s
                               ETA: 1167035.9s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.018s, learning 0.159s)
               Value function loss: 32.1061
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 587.87
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 11.18s
                        Total time: 27423.49s
                               ETA: 1166991.3s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.197s, learning 0.176s)
               Value function loss: 35.4559
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 587.18
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 11.37s
                        Total time: 27434.86s
                               ETA: 1166955.1s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.190s, learning 0.165s)
               Value function loss: 34.4440
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 580.61
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 11.36s
                        Total time: 27446.22s
                               ETA: 1166918.1s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.859s, learning 0.227s)
               Value function loss: 33.7893
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 582.46
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 11.09s
                        Total time: 27457.30s
                               ETA: 1166869.7s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.098s, learning 0.165s)
               Value function loss: 44.0521
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 588.05
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 11.26s
                        Total time: 27468.57s
                               ETA: 1166828.8s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.781s, learning 0.160s)
               Value function loss: 48.1678
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 583.12
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 10.94s
                        Total time: 27479.51s
                               ETA: 1166774.4s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.163s)
               Value function loss: 36.6211
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 586.90
               Mean episode length: 249.93
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 11.53s
                        Total time: 27491.04s
                               ETA: 1166745.1s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.182s, learning 0.158s)
               Value function loss: 25.8593
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 591.69
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 11.34s
                        Total time: 27502.38s
                               ETA: 1166707.6s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.159s)
               Value function loss: 33.5576
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 586.54
               Mean episode length: 249.43
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 11.43s
                        Total time: 27513.81s
                               ETA: 1166673.8s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.163s)
               Value function loss: 29.8955
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 586.60
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 11.33s
                        Total time: 27525.14s
                               ETA: 1166636.1s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.897s, learning 0.185s)
               Value function loss: 34.6816
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 589.39
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 11.08s
                        Total time: 27536.22s
                               ETA: 1166587.8s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.343s, learning 0.161s)
               Value function loss: 27.0454
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 593.82
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 11.50s
                        Total time: 27547.73s
                               ETA: 1166557.4s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.345s, learning 0.182s)
               Value function loss: 36.4867
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 585.09
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 11.53s
                        Total time: 27559.26s
                               ETA: 1166527.9s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.072s, learning 0.161s)
               Value function loss: 23.6416
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 588.78
               Mean episode length: 249.89
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 11.23s
                        Total time: 27570.49s
                               ETA: 1166486.0s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.104s, learning 0.161s)
               Value function loss: 34.6489
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 594.98
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 11.27s
                        Total time: 27581.76s
                               ETA: 1166445.6s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.579s, learning 0.166s)
               Value function loss: 29.8818
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 588.42
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 11.74s
                        Total time: 27593.50s
                               ETA: 1166425.3s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.471s, learning 0.167s)
               Value function loss: 26.3433
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 586.42
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 11.64s
                        Total time: 27605.14s
                               ETA: 1166400.6s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.508s, learning 0.169s)
               Value function loss: 33.6124
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 593.71
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 11.68s
                        Total time: 27616.81s
                               ETA: 1166377.6s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.255s, learning 0.167s)
               Value function loss: 24.3363
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 587.87
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 11.42s
                        Total time: 27628.24s
                               ETA: 1166343.8s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.166s)
               Value function loss: 28.1945
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 585.11
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 11.33s
                        Total time: 27639.56s
                               ETA: 1166306.0s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.161s)
               Value function loss: 27.0500
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 590.76
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 11.22s
                        Total time: 27650.79s
                               ETA: 1166263.8s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.192s, learning 0.159s)
               Value function loss: 19.4276
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 585.35
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 11.35s
                        Total time: 27662.14s
                               ETA: 1166227.1s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.232s, learning 0.161s)
               Value function loss: 21.9169
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 584.57
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 11.39s
                        Total time: 27673.53s
                               ETA: 1166192.1s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.133s, learning 0.169s)
               Value function loss: 19.9096
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 586.24
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 11.30s
                        Total time: 27684.83s
                               ETA: 1166153.4s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.990s, learning 0.161s)
               Value function loss: 20.2863
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 583.88
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 11.15s
                        Total time: 27695.98s
                               ETA: 1166108.4s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.167s)
               Value function loss: 22.6425
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 588.48
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 11.46s
                        Total time: 27707.44s
                               ETA: 1166076.2s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.228s, learning 0.172s)
               Value function loss: 27.3242
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 588.16
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 11.40s
                        Total time: 27718.84s
                               ETA: 1166041.6s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.167s, learning 0.167s)
               Value function loss: 25.8170
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 588.63
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 11.33s
                        Total time: 27730.17s
                               ETA: 1166004.3s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.410s, learning 0.187s)
               Value function loss: 21.2074
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 583.61
               Mean episode length: 248.48
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.60s
                        Total time: 27741.77s
                               ETA: 1165978.1s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.189s, learning 0.167s)
               Value function loss: 22.5888
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 577.21
               Mean episode length: 248.48
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 11.36s
                        Total time: 27753.13s
                               ETA: 1165941.7s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.261s, learning 0.160s)
               Value function loss: 25.6240
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 583.80
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 11.42s
                        Total time: 27764.55s
                               ETA: 1165908.1s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.112s, learning 0.187s)
               Value function loss: 25.9748
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 587.01
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 11.30s
                        Total time: 27775.85s
                               ETA: 1165869.4s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.166s)
               Value function loss: 28.7018
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 583.04
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 11.58s
                        Total time: 27787.42s
                               ETA: 1165842.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.395s, learning 0.164s)
               Value function loss: 24.6949
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 581.65
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 11.56s
                        Total time: 27798.98s
                               ETA: 1165814.7s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.759s, learning 0.173s)
               Value function loss: 27.3827
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 584.13
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 10.93s
                        Total time: 27809.92s
                               ETA: 1165760.6s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.117s, learning 0.164s)
               Value function loss: 30.2423
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 582.27
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 11.28s
                        Total time: 27821.20s
                               ETA: 1165721.3s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.095s, learning 0.167s)
               Value function loss: 37.4139
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 580.64
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 11.26s
                        Total time: 27832.46s
                               ETA: 1165681.2s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.162s)
               Value function loss: 23.9667
                    Surrogate loss: -0.0219
             Mean action noise std: 0.74
                       Mean reward: 583.96
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 11.41s
                        Total time: 27843.87s
                               ETA: 1165647.1s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.160s)
               Value function loss: 23.2832
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 581.64
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 11.42s
                        Total time: 27855.29s
                               ETA: 1165613.6s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.285s, learning 0.163s)
               Value function loss: 31.2435
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 589.35
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 11.45s
                        Total time: 27866.73s
                               ETA: 1165581.3s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.253s, learning 0.167s)
               Value function loss: 24.6975
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 580.74
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 11.42s
                        Total time: 27878.15s
                               ETA: 1165547.9s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.371s, learning 0.183s)
               Value function loss: 26.0998
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 578.84
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 11.55s
                        Total time: 27889.71s
                               ETA: 1165520.1s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.301s, learning 0.167s)
               Value function loss: 23.0188
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 585.55
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 11.47s
                        Total time: 27901.18s
                               ETA: 1165488.7s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.930s, learning 0.180s)
               Value function loss: 27.8320
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 583.29
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 11.11s
                        Total time: 27912.28s
                               ETA: 1165442.3s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.937s, learning 0.166s)
               Value function loss: 27.1874
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 576.45
               Mean episode length: 249.97
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 11.10s
                        Total time: 27923.39s
                               ETA: 1165395.7s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.381s, learning 0.189s)
               Value function loss: 23.9854
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 577.47
               Mean episode length: 249.97
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 11.57s
                        Total time: 27934.96s
                               ETA: 1165368.6s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.371s, learning 0.169s)
               Value function loss: 25.2696
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 579.82
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 11.54s
                        Total time: 27946.50s
                               ETA: 1165340.3s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.988s, learning 0.167s)
               Value function loss: 20.8803
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 576.69
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 11.16s
                        Total time: 27957.65s
                               ETA: 1165295.9s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.315s, learning 0.164s)
               Value function loss: 27.5860
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 573.24
               Mean episode length: 248.36
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 11.48s
                        Total time: 27969.13s
                               ETA: 1165265.1s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.180s, learning 0.162s)
               Value function loss: 20.5436
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 572.17
               Mean episode length: 248.36
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 11.34s
                        Total time: 27980.47s
                               ETA: 1165228.6s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.363s, learning 0.170s)
               Value function loss: 19.8824
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: 574.35
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 11.53s
                        Total time: 27992.01s
                               ETA: 1165200.0s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.094s, learning 0.165s)
               Value function loss: 23.2494
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 571.99
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 11.26s
                        Total time: 28003.26s
                               ETA: 1165160.1s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.435s, learning 0.163s)
               Value function loss: 23.2118
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 568.55
               Mean episode length: 249.41
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 11.60s
                        Total time: 28014.86s
                               ETA: 1165134.3s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.141s, learning 0.176s)
               Value function loss: 18.0572
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 565.08
               Mean episode length: 249.41
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 11.32s
                        Total time: 28026.18s
                               ETA: 1165096.8s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.985s, learning 0.165s)
               Value function loss: 17.8374
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 572.12
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 11.15s
                        Total time: 28037.33s
                               ETA: 1165052.4s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.907s, learning 0.168s)
               Value function loss: 20.9934
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 574.34
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 11.08s
                        Total time: 28048.40s
                               ETA: 1165005.0s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.167s)
               Value function loss: 24.7359
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 577.30
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 11.51s
                        Total time: 28059.91s
                               ETA: 1164975.5s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.163s)
               Value function loss: 23.1415
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 566.49
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 11.36s
                        Total time: 28071.28s
                               ETA: 1164940.1s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.163s)
               Value function loss: 23.9196
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 566.22
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 11.24s
                        Total time: 28082.52s
                               ETA: 1164899.5s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.981s, learning 0.161s)
               Value function loss: 20.2882
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 572.41
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 11.14s
                        Total time: 28093.66s
                               ETA: 1164854.9s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.161s)
               Value function loss: 20.3330
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 566.64
               Mean episode length: 249.99
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 10.76s
                        Total time: 28104.42s
                               ETA: 1164794.5s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.912s, learning 0.161s)
               Value function loss: 21.4201
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 562.12
               Mean episode length: 249.99
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 11.07s
                        Total time: 28115.49s
                               ETA: 1164747.2s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.174s)
               Value function loss: 24.2300
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 564.45
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 11.31s
                        Total time: 28126.80s
                               ETA: 1164709.5s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.110s, learning 0.161s)
               Value function loss: 24.0326
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 562.43
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 11.27s
                        Total time: 28138.07s
                               ETA: 1164670.4s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.826s, learning 0.168s)
               Value function loss: 24.5026
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 566.56
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 10.99s
                        Total time: 28149.07s
                               ETA: 1164619.9s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.164s)
               Value function loss: 24.8343
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 564.08
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 11.48s
                        Total time: 28160.55s
                               ETA: 1164589.6s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.202s, learning 0.260s)
               Value function loss: 32.3922
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 556.30
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 11.46s
                        Total time: 28172.01s
                               ETA: 1164558.5s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.937s, learning 0.177s)
               Value function loss: 35.3809
                    Surrogate loss: 0.0049
             Mean action noise std: 0.74
                       Mean reward: 560.36
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 11.11s
                        Total time: 28183.13s
                               ETA: 1164513.0s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.228s, learning 0.166s)
               Value function loss: 26.6308
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 554.67
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 11.39s
                        Total time: 28194.52s
                               ETA: 1164479.0s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.482s, learning 0.167s)
               Value function loss: 27.2492
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 548.81
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 11.65s
                        Total time: 28206.17s
                               ETA: 1164455.7s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.800s, learning 0.158s)
               Value function loss: 29.2622
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 560.27
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 10.96s
                        Total time: 28217.13s
                               ETA: 1164403.8s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.082s, learning 0.160s)
               Value function loss: 27.8178
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 554.09
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 11.24s
                        Total time: 28228.37s
                               ETA: 1164363.6s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.330s, learning 0.159s)
               Value function loss: 23.7565
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 549.67
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 11.49s
                        Total time: 28239.86s
                               ETA: 1164333.7s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.665s, learning 0.159s)
               Value function loss: 24.3344
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 554.39
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 11.82s
                        Total time: 28251.68s
                               ETA: 1164317.5s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.392s, learning 0.166s)
               Value function loss: 25.9659
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 548.92
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 11.56s
                        Total time: 28263.24s
                               ETA: 1164290.5s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.481s, learning 0.170s)
               Value function loss: 21.1503
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 552.22
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 11.65s
                        Total time: 28274.89s
                               ETA: 1164267.2s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.146s, learning 0.173s)
               Value function loss: 20.1768
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 555.27
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 11.32s
                        Total time: 28286.21s
                               ETA: 1164230.3s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.222s, learning 0.161s)
               Value function loss: 24.7395
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 556.55
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 11.38s
                        Total time: 28297.59s
                               ETA: 1164196.1s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.183s, learning 0.265s)
               Value function loss: 22.0148
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 551.85
               Mean episode length: 249.68
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 11.45s
                        Total time: 28309.04s
                               ETA: 1164164.6s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.337s, learning 0.185s)
               Value function loss: 21.8189
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 552.76
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 11.52s
                        Total time: 28320.56s
                               ETA: 1164136.1s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.140s, learning 0.191s)
               Value function loss: 20.1356
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 551.22
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 11.33s
                        Total time: 28331.89s
                               ETA: 1164099.8s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.323s, learning 0.167s)
               Value function loss: 16.0491
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 542.50
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 11.49s
                        Total time: 28343.38s
                               ETA: 1164070.1s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.832s, learning 0.158s)
               Value function loss: 20.2500
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 544.22
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 10.99s
                        Total time: 28354.37s
                               ETA: 1164019.8s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.028s, learning 0.161s)
               Value function loss: 19.9432
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 533.97
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 11.19s
                        Total time: 28365.56s
                               ETA: 1163977.7s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.162s)
               Value function loss: 14.9181
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 540.69
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 10.85s
                        Total time: 28376.42s
                               ETA: 1163921.9s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.844s, learning 0.187s)
               Value function loss: 16.5401
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 549.27
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 11.03s
                        Total time: 28387.45s
                               ETA: 1163873.5s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.299s, learning 0.166s)
               Value function loss: 12.3940
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 552.44
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 11.47s
                        Total time: 28398.91s
                               ETA: 1163842.8s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.937s, learning 0.189s)
               Value function loss: 17.4048
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 542.86
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 11.13s
                        Total time: 28410.04s
                               ETA: 1163798.3s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.219s, learning 0.169s)
               Value function loss: 20.9710
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 542.78
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 11.39s
                        Total time: 28421.43s
                               ETA: 1163764.5s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.305s, learning 0.164s)
               Value function loss: 17.4191
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 542.52
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 11.47s
                        Total time: 28432.90s
                               ETA: 1163734.0s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.245s, learning 0.230s)
               Value function loss: 19.3945
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 551.03
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 11.48s
                        Total time: 28444.37s
                               ETA: 1163703.8s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.908s, learning 0.164s)
               Value function loss: 13.4995
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 545.20
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 11.07s
                        Total time: 28455.44s
                               ETA: 1163657.2s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.362s, learning 0.184s)
               Value function loss: 15.0891
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 547.85
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 11.55s
                        Total time: 28466.99s
                               ETA: 1163629.9s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.933s, learning 0.169s)
               Value function loss: 19.0740
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 541.06
               Mean episode length: 249.60
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 11.10s
                        Total time: 28478.09s
                               ETA: 1163584.6s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.168s)
               Value function loss: 16.5766
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 539.59
               Mean episode length: 249.60
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 11.07s
                        Total time: 28489.17s
                               ETA: 1163538.1s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.855s, learning 0.158s)
               Value function loss: 19.0476
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 550.43
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 11.01s
                        Total time: 28500.18s
                               ETA: 1163489.1s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.281s, learning 0.160s)
               Value function loss: 17.8983
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 544.56
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 11.44s
                        Total time: 28511.62s
                               ETA: 1163457.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.259s, learning 0.168s)
               Value function loss: 16.8906
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 539.93
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 11.43s
                        Total time: 28523.05s
                               ETA: 1163425.7s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.523s, learning 0.187s)
               Value function loss: 23.5352
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 547.07
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 11.71s
                        Total time: 28534.76s
                               ETA: 1163405.2s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.159s)
               Value function loss: 21.4133
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 548.17
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 11.30s
                        Total time: 28546.06s
                               ETA: 1163368.2s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.914s, learning 0.164s)
               Value function loss: 17.1847
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 540.08
               Mean episode length: 249.92
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 11.08s
                        Total time: 28557.14s
                               ETA: 1163322.0s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.380s, learning 0.163s)
               Value function loss: 16.1724
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 539.90
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 11.54s
                        Total time: 28568.68s
                               ETA: 1163294.8s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.732s, learning 0.170s)
               Value function loss: 15.7395
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 536.17
               Mean episode length: 249.76
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 11.90s
                        Total time: 28580.58s
                               ETA: 1163282.2s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.423s, learning 0.162s)
               Value function loss: 13.6637
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 533.30
               Mean episode length: 249.76
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 11.58s
                        Total time: 28592.17s
                               ETA: 1163256.7s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.384s, learning 0.160s)
               Value function loss: 17.7330
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 537.26
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 11.54s
                        Total time: 28603.71s
                               ETA: 1163229.6s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.163s)
               Value function loss: 13.4308
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 537.34
               Mean episode length: 249.70
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 11.17s
                        Total time: 28614.88s
                               ETA: 1163187.1s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.415s, learning 0.184s)
               Value function loss: 17.8182
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 546.30
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 11.60s
                        Total time: 28626.48s
                               ETA: 1163162.2s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.779s, learning 0.169s)
               Value function loss: 13.1315
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 541.88
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 11.95s
                        Total time: 28638.43s
                               ETA: 1163151.6s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.332s, learning 0.185s)
               Value function loss: 20.5153
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 543.82
               Mean episode length: 249.91
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 11.52s
                        Total time: 28649.94s
                               ETA: 1163123.4s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.333s, learning 0.163s)
               Value function loss: 12.7888
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 537.32
               Mean episode length: 249.91
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 11.50s
                        Total time: 28661.44s
                               ETA: 1163094.4s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.465s, learning 0.248s)
               Value function loss: 19.2484
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 540.89
               Mean episode length: 249.99
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 11.71s
                        Total time: 28673.15s
                               ETA: 1163074.1s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.168s)
               Value function loss: 21.7248
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 538.81
               Mean episode length: 249.99
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 11.38s
                        Total time: 28684.53s
                               ETA: 1163040.5s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.485s, learning 0.169s)
               Value function loss: 15.7107
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 542.30
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 11.65s
                        Total time: 28696.19s
                               ETA: 1163017.9s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.161s)
               Value function loss: 18.1229
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 538.64
               Mean episode length: 249.86
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 11.53s
                        Total time: 28707.72s
                               ETA: 1162990.4s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.163s, learning 0.167s)
               Value function loss: 20.0015
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 545.17
               Mean episode length: 249.86
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 11.33s
                        Total time: 28719.05s
                               ETA: 1162954.7s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.374s, learning 0.160s)
               Value function loss: 12.1025
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 537.69
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 11.53s
                        Total time: 28730.58s
                               ETA: 1162927.3s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.668s, learning 0.174s)
               Value function loss: 18.9041
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 539.12
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 11.84s
                        Total time: 28742.43s
                               ETA: 1162912.4s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.133s, learning 0.161s)
               Value function loss: 13.4825
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 544.20
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 11.29s
                        Total time: 28753.72s
                               ETA: 1162875.3s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.087s, learning 0.162s)
               Value function loss: 20.8188
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 539.62
               Mean episode length: 249.98
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 11.25s
                        Total time: 28764.97s
                               ETA: 1162836.4s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.326s, learning 0.190s)
               Value function loss: 23.3677
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 540.27
               Mean episode length: 249.89
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 11.52s
                        Total time: 28776.49s
                               ETA: 1162808.3s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.345s, learning 0.159s)
               Value function loss: 18.8525
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 534.75
               Mean episode length: 249.89
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 11.50s
                        Total time: 28787.99s
                               ETA: 1162779.8s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.283s, learning 0.167s)
               Value function loss: 18.1103
                    Surrogate loss: -0.0009
             Mean action noise std: 0.74
                       Mean reward: 533.56
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 11.45s
                        Total time: 28799.44s
                               ETA: 1162749.1s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.045s, learning 0.184s)
               Value function loss: 15.1414
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 538.66
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 11.23s
                        Total time: 28810.67s
                               ETA: 1162709.4s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.788s, learning 0.172s)
               Value function loss: 18.9894
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 534.82
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 10.96s
                        Total time: 28821.63s
                               ETA: 1162659.0s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.630s, learning 0.161s)
               Value function loss: 19.8035
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 533.08
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 11.79s
                        Total time: 28833.42s
                               ETA: 1162642.1s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.114s, learning 0.193s)
               Value function loss: 19.0626
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 529.18
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 11.31s
                        Total time: 28844.73s
                               ETA: 1162605.7s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.735s, learning 0.160s)
               Value function loss: 22.5816
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 529.37
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 11.90s
                        Total time: 28856.62s
                               ETA: 1162593.0s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.166s)
               Value function loss: 22.4806
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 536.64
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 11.38s
                        Total time: 28868.00s
                               ETA: 1162559.4s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.436s, learning 0.168s)
               Value function loss: 22.9251
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 533.77
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 11.60s
                        Total time: 28879.60s
                               ETA: 1162535.0s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.365s, learning 0.164s)
               Value function loss: 28.9459
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 530.56
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 11.53s
                        Total time: 28891.13s
                               ETA: 1162507.6s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.566s, learning 0.163s)
               Value function loss: 28.9450
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 518.37
               Mean episode length: 249.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 11.73s
                        Total time: 28902.86s
                               ETA: 1162488.3s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.286s, learning 0.160s)
               Value function loss: 28.6711
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 526.83
               Mean episode length: 249.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 11.45s
                        Total time: 28914.31s
                               ETA: 1162457.6s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.191s, learning 0.162s)
               Value function loss: 25.0930
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 532.56
               Mean episode length: 249.93
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 11.35s
                        Total time: 28925.66s
                               ETA: 1162423.1s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.241s, learning 0.174s)
               Value function loss: 25.7600
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 533.78
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 11.41s
                        Total time: 28937.07s
                               ETA: 1162391.2s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.355s, learning 0.163s)
               Value function loss: 17.6012
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 533.29
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 11.52s
                        Total time: 28948.59s
                               ETA: 1162363.4s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.321s, learning 0.160s)
               Value function loss: 20.4744
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 524.54
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 11.48s
                        Total time: 28960.07s
                               ETA: 1162334.1s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.368s, learning 0.160s)
               Value function loss: 19.0014
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 516.90
               Mean episode length: 249.89
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 11.53s
                        Total time: 28971.60s
                               ETA: 1162306.8s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.129s, learning 0.182s)
               Value function loss: 27.4962
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 518.87
               Mean episode length: 249.84
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 11.31s
                        Total time: 28982.91s
                               ETA: 1162270.7s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.897s, learning 0.171s)
               Value function loss: 18.1573
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 521.38
               Mean episode length: 249.91
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 11.07s
                        Total time: 28993.98s
                               ETA: 1162225.0s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.212s, learning 0.185s)
               Value function loss: 27.5450
                    Surrogate loss: 0.0053
             Mean action noise std: 0.74
                       Mean reward: 536.31
               Mean episode length: 249.91
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 11.40s
                        Total time: 29005.38s
                               ETA: 1162192.4s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.307s, learning 0.192s)
               Value function loss: 20.2373
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 531.51
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 11.50s
                        Total time: 29016.87s
                               ETA: 1162163.9s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.259s, learning 0.179s)
               Value function loss: 18.5677
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 527.81
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 11.44s
                        Total time: 29028.31s
                               ETA: 1162133.1s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.163s)
               Value function loss: 22.2208
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 532.21
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 11.39s
                        Total time: 29039.70s
                               ETA: 1162100.3s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.289s, learning 0.181s)
               Value function loss: 20.3590
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 544.21
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 11.47s
                        Total time: 29051.17s
                               ETA: 1162070.7s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.168s)
               Value function loss: 25.0959
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 544.67
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 11.51s
                        Total time: 29062.68s
                               ETA: 1162042.7s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.571s, learning 0.162s)
               Value function loss: 23.1925
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 544.69
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 10.73s
                        Total time: 29073.41s
                               ETA: 1161983.7s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.164s)
               Value function loss: 15.4501
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 536.41
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 11.52s
                        Total time: 29084.94s
                               ETA: 1161956.4s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.192s, learning 0.167s)
               Value function loss: 16.3894
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 543.81
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 11.36s
                        Total time: 29096.30s
                               ETA: 1161922.5s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.163s)
               Value function loss: 15.8060
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 545.78
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 11.57s
                        Total time: 29107.87s
                               ETA: 1161896.9s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.138s, learning 0.160s)
               Value function loss: 17.7054
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 546.96
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 11.30s
                        Total time: 29119.16s
                               ETA: 1161860.6s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.416s, learning 0.164s)
               Value function loss: 17.4477
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 545.09
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 11.58s
                        Total time: 29130.74s
                               ETA: 1161835.5s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.163s)
               Value function loss: 18.9985
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 522.97
               Mean episode length: 249.34
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 11.41s
                        Total time: 29142.16s
                               ETA: 1161803.8s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.026s, learning 0.160s)
               Value function loss: 18.7735
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 520.65
               Mean episode length: 249.34
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 11.19s
                        Total time: 29153.34s
                               ETA: 1161763.1s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.474s, learning 0.247s)
               Value function loss: 16.1092
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 542.21
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 11.72s
                        Total time: 29165.06s
                               ETA: 1161743.7s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.229s, learning 0.165s)
               Value function loss: 16.7789
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 541.26
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 11.39s
                        Total time: 29176.46s
                               ETA: 1161711.3s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.164s)
               Value function loss: 19.5347
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 541.95
               Mean episode length: 249.92
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 11.64s
                        Total time: 29188.09s
                               ETA: 1161688.5s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.111s, learning 0.176s)
               Value function loss: 19.8840
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 543.31
               Mean episode length: 249.28
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 11.29s
                        Total time: 29199.38s
                               ETA: 1161651.9s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.366s, learning 0.164s)
               Value function loss: 21.9153
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 547.45
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 11.53s
                        Total time: 29210.91s
                               ETA: 1161624.9s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.515s, learning 0.194s)
               Value function loss: 17.1069
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 551.00
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 11.71s
                        Total time: 29222.62s
                               ETA: 1161605.1s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.420s, learning 0.166s)
               Value function loss: 24.1296
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 544.40
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 11.59s
                        Total time: 29234.20s
                               ETA: 1161580.3s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.972s, learning 0.163s)
               Value function loss: 26.0615
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 536.17
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 11.13s
                        Total time: 29245.34s
                               ETA: 1161537.7s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.163s)
               Value function loss: 29.3351
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 540.66
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 11.37s
                        Total time: 29256.71s
                               ETA: 1161504.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.397s, learning 0.164s)
               Value function loss: 19.0030
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 541.99
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 11.56s
                        Total time: 29268.27s
                               ETA: 1161478.7s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.271s, learning 0.192s)
               Value function loss: 21.0695
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 553.26
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 11.46s
                        Total time: 29279.73s
                               ETA: 1161449.2s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.625s, learning 0.191s)
               Value function loss: 28.5499
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 544.31
               Mean episode length: 248.26
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 11.82s
                        Total time: 29291.55s
                               ETA: 1161433.7s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.382s, learning 0.192s)
               Value function loss: 21.1237
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 543.60
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 11.57s
                        Total time: 29303.12s
                               ETA: 1161408.6s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.165s)
               Value function loss: 24.1667
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 544.20
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 11.55s
                        Total time: 29314.68s
                               ETA: 1161382.7s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.225s, learning 0.159s)
               Value function loss: 24.4941
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 561.50
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 11.38s
                        Total time: 29326.06s
                               ETA: 1161350.1s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.001s, learning 0.185s)
               Value function loss: 25.5614
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 566.21
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 11.19s
                        Total time: 29337.25s
                               ETA: 1161309.6s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.360s, learning 0.162s)
               Value function loss: 26.3680
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 566.45
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 11.52s
                        Total time: 29348.77s
                               ETA: 1161282.5s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.107s, learning 0.163s)
               Value function loss: 25.7476
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 568.09
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 11.27s
                        Total time: 29360.04s
                               ETA: 1161245.4s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.128s, learning 0.165s)
               Value function loss: 27.9424
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 574.81
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 11.29s
                        Total time: 29371.33s
                               ETA: 1161209.3s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.487s, learning 0.168s)
               Value function loss: 23.8952
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 567.07
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 11.65s
                        Total time: 29382.98s
                               ETA: 1161187.5s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.992s, learning 0.174s)
               Value function loss: 28.5227
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 565.71
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 11.17s
                        Total time: 29394.15s
                               ETA: 1161146.4s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.245s, learning 0.164s)
               Value function loss: 25.6089
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 568.60
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 11.41s
                        Total time: 29405.56s
                               ETA: 1161114.9s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.166s)
               Value function loss: 22.3635
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 573.13
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 11.33s
                        Total time: 29416.89s
                               ETA: 1161080.1s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.160s)
               Value function loss: 30.5276
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 582.03
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 11.37s
                        Total time: 29428.25s
                               ETA: 1161047.0s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.115s, learning 0.162s)
               Value function loss: 24.4034
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 583.94
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 11.28s
                        Total time: 29439.53s
                               ETA: 1161010.3s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.137s, learning 0.168s)
               Value function loss: 20.7481
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 575.08
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 11.30s
                        Total time: 29450.83s
                               ETA: 1160974.7s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.152s, learning 0.188s)
               Value function loss: 18.2272
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 572.45
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 11.34s
                        Total time: 29462.17s
                               ETA: 1160940.6s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.059s, learning 0.160s)
               Value function loss: 24.5771
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 582.07
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 11.22s
                        Total time: 29473.39s
                               ETA: 1160901.8s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.896s, learning 0.170s)
               Value function loss: 26.3322
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 578.68
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 11.07s
                        Total time: 29484.46s
                               ETA: 1160856.9s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.945s, learning 0.172s)
               Value function loss: 32.9818
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 577.22
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 11.12s
                        Total time: 29495.58s
                               ETA: 1160814.1s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.924s, learning 0.162s)
               Value function loss: 31.3170
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 577.38
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 11.09s
                        Total time: 29506.66s
                               ETA: 1160770.0s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.235s, learning 0.163s)
               Value function loss: 21.8555
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 585.28
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 11.40s
                        Total time: 29518.06s
                               ETA: 1160738.3s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.957s, learning 0.165s)
               Value function loss: 22.2191
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 585.71
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 11.12s
                        Total time: 29529.18s
                               ETA: 1160695.7s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.160s)
               Value function loss: 29.4366
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 584.15
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 11.57s
                        Total time: 29540.75s
                               ETA: 1160670.8s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.301s, learning 0.170s)
               Value function loss: 28.2454
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 583.63
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 11.47s
                        Total time: 29552.23s
                               ETA: 1160641.9s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.167s)
               Value function loss: 24.1939
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 578.32
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 11.64s
                        Total time: 29563.86s
                               ETA: 1160619.7s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.161s)
               Value function loss: 23.0409
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 577.69
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 11.33s
                        Total time: 29575.20s
                               ETA: 1160585.6s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.524s, learning 0.237s)
               Value function loss: 24.9126
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 577.36
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 11.76s
                        Total time: 29586.96s
                               ETA: 1160568.2s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.262s, learning 0.168s)
               Value function loss: 34.1235
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 575.73
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 11.43s
                        Total time: 29598.39s
                               ETA: 1160537.8s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.280s, learning 0.185s)
               Value function loss: 30.0006
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 580.16
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 11.47s
                        Total time: 29609.86s
                               ETA: 1160508.8s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.159s, learning 0.163s)
               Value function loss: 27.2520
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 582.92
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 11.32s
                        Total time: 29621.18s
                               ETA: 1160474.2s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.602s, learning 0.162s)
               Value function loss: 24.1410
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 583.29
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 11.76s
                        Total time: 29632.94s
                               ETA: 1160456.9s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.236s, learning 0.172s)
               Value function loss: 22.8921
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 583.06
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 11.41s
                        Total time: 29644.35s
                               ETA: 1160425.7s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.795s, learning 0.163s)
               Value function loss: 23.1125
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 576.24
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 10.96s
                        Total time: 29655.31s
                               ETA: 1160376.9s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.162s)
               Value function loss: 19.0058
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 571.40
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 11.55s
                        Total time: 29666.86s
                               ETA: 1160351.3s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.479s, learning 0.163s)
               Value function loss: 19.0059
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 571.20
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 11.64s
                        Total time: 29678.50s
                               ETA: 1160329.3s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.695s, learning 0.263s)
               Value function loss: 17.4713
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 570.64
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 11.96s
                        Total time: 29690.45s
                               ETA: 1160319.6s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.907s, learning 0.171s)
               Value function loss: 21.1723
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 570.50
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 12.08s
                        Total time: 29702.53s
                               ETA: 1160314.7s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.438s, learning 0.165s)
               Value function loss: 16.3986
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 575.09
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 11.60s
                        Total time: 29714.13s
                               ETA: 1160291.1s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.228s, learning 0.171s)
               Value function loss: 21.4315
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 569.98
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 11.40s
                        Total time: 29725.53s
                               ETA: 1160259.7s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.997s, learning 0.161s)
               Value function loss: 14.8443
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 566.55
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 11.16s
                        Total time: 29736.69s
                               ETA: 1160218.9s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.431s, learning 0.169s)
               Value function loss: 16.7438
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 569.77
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 11.60s
                        Total time: 29748.29s
                               ETA: 1160195.3s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.353s, learning 0.185s)
               Value function loss: 18.7579
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 572.15
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 11.54s
                        Total time: 29759.83s
                               ETA: 1160169.4s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.165s)
               Value function loss: 12.2487
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 573.58
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 11.51s
                        Total time: 29771.34s
                               ETA: 1160142.3s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.721s, learning 0.180s)
               Value function loss: 17.9848
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 564.68
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 10.90s
                        Total time: 29782.24s
                               ETA: 1160091.6s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.167s)
               Value function loss: 16.5834
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 559.52
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 11.14s
                        Total time: 29793.39s
                               ETA: 1160050.3s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.147s, learning 0.188s)
               Value function loss: 12.8066
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 561.18
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 11.34s
                        Total time: 29804.72s
                               ETA: 1160016.5s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.524s, learning 0.161s)
               Value function loss: 11.9337
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 567.50
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 11.69s
                        Total time: 29816.41s
                               ETA: 1159996.3s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.390s, learning 0.176s)
               Value function loss: 11.1766
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 566.14
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 11.57s
                        Total time: 29827.97s
                               ETA: 1159971.5s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.048s, learning 0.176s)
               Value function loss: 18.4580
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 570.86
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 11.22s
                        Total time: 29839.20s
                               ETA: 1159933.4s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.110s, learning 0.164s)
               Value function loss: 20.2132
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 570.86
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 11.27s
                        Total time: 29850.47s
                               ETA: 1159897.3s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.164s, learning 0.168s)
               Value function loss: 17.0718
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 570.03
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 11.33s
                        Total time: 29861.80s
                               ETA: 1159863.4s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.278s, learning 0.161s)
               Value function loss: 15.4462
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 570.46
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 11.44s
                        Total time: 29873.24s
                               ETA: 1159833.8s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.163s)
               Value function loss: 14.2973
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 566.28
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 11.34s
                        Total time: 29884.58s
                               ETA: 1159800.2s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.128s, learning 0.165s)
               Value function loss: 14.0545
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 569.86
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 11.29s
                        Total time: 29895.88s
                               ETA: 1159764.9s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.653s, learning 0.167s)
               Value function loss: 21.0039
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 574.89
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 10.82s
                        Total time: 29906.70s
                               ETA: 1159711.3s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.406s, learning 0.162s)
               Value function loss: 16.5314
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 571.57
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 11.57s
                        Total time: 29918.27s
                               ETA: 1159686.7s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.087s, learning 0.166s)
               Value function loss: 21.0108
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 564.62
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 11.25s
                        Total time: 29929.52s
                               ETA: 1159649.9s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.160s)
               Value function loss: 14.3466
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 566.43
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 11.32s
                        Total time: 29940.84s
                               ETA: 1159615.9s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.228s, learning 0.163s)
               Value function loss: 19.1287
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 568.84
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 11.39s
                        Total time: 29952.23s
                               ETA: 1159584.5s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.892s, learning 0.160s)
               Value function loss: 20.5339
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 569.54
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 11.05s
                        Total time: 29963.29s
                               ETA: 1159540.0s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.042s, learning 0.159s)
               Value function loss: 23.1003
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 579.34
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 11.20s
                        Total time: 29974.49s
                               ETA: 1159501.2s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.197s, learning 0.188s)
               Value function loss: 21.2669
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 578.65
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 11.39s
                        Total time: 29985.87s
                               ETA: 1159469.7s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.430s, learning 0.167s)
               Value function loss: 19.8859
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 576.87
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 11.60s
                        Total time: 29997.47s
                               ETA: 1159446.3s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.764s, learning 0.193s)
               Value function loss: 17.1985
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 573.87
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 10.96s
                        Total time: 30008.43s
                               ETA: 1159398.2s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.220s, learning 0.170s)
               Value function loss: 12.7642
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 568.74
               Mean episode length: 249.98
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 11.39s
                        Total time: 30019.82s
                               ETA: 1159366.8s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.215s, learning 0.168s)
               Value function loss: 18.2201
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 562.66
               Mean episode length: 249.14
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 11.38s
                        Total time: 30031.20s
                               ETA: 1159335.2s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.005s, learning 0.169s)
               Value function loss: 20.7169
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 559.19
               Mean episode length: 249.11
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 11.17s
                        Total time: 30042.38s
                               ETA: 1159295.6s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.984s, learning 0.168s)
               Value function loss: 21.3746
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 570.84
               Mean episode length: 249.95
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 11.15s
                        Total time: 30053.53s
                               ETA: 1159255.1s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.845s, learning 0.200s)
               Value function loss: 16.3178
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 570.19
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 11.05s
                        Total time: 30064.57s
                               ETA: 1159210.5s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.050s, learning 0.161s)
               Value function loss: 20.0829
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 571.70
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 11.21s
                        Total time: 30075.78s
                               ETA: 1159172.4s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.028s, learning 0.168s)
               Value function loss: 15.6611
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 573.82
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 11.20s
                        Total time: 30086.98s
                               ETA: 1159133.6s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.191s, learning 0.165s)
               Value function loss: 17.8242
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 576.38
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 11.36s
                        Total time: 30098.34s
                               ETA: 1159101.1s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.697s, learning 0.162s)
               Value function loss: 27.7203
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 575.33
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 10.86s
                        Total time: 30109.19s
                               ETA: 1159049.4s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.975s, learning 0.177s)
               Value function loss: 15.7249
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 580.22
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 11.15s
                        Total time: 30120.35s
                               ETA: 1159009.0s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.177s, learning 0.164s)
               Value function loss: 22.0355
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 573.85
               Mean episode length: 249.84
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 11.34s
                        Total time: 30131.69s
                               ETA: 1158976.0s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.966s, learning 0.189s)
               Value function loss: 22.2284
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 577.44
               Mean episode length: 249.84
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 11.15s
                        Total time: 30142.84s
                               ETA: 1158935.8s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.362s, learning 0.163s)
               Value function loss: 13.9620
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 573.44
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 11.53s
                        Total time: 30154.37s
                               ETA: 1158909.8s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.812s, learning 0.165s)
               Value function loss: 16.7732
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 568.03
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 10.98s
                        Total time: 30165.34s
                               ETA: 1158862.9s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.164s)
               Value function loss: 16.8812
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 566.72
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 11.34s
                        Total time: 30176.68s
                               ETA: 1158829.7s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.009s, learning 0.161s)
               Value function loss: 21.0855
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 570.41
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 11.17s
                        Total time: 30187.85s
                               ETA: 1158790.2s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.275s, learning 0.186s)
               Value function loss: 20.2020
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 578.38
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 11.46s
                        Total time: 30199.31s
                               ETA: 1158761.8s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.164s)
               Value function loss: 21.6154
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 575.05
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 11.53s
                        Total time: 30210.85s
                               ETA: 1158736.3s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.217s, learning 0.159s)
               Value function loss: 20.4580
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 574.42
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 11.38s
                        Total time: 30222.22s
                               ETA: 1158704.7s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.851s, learning 0.160s)
               Value function loss: 13.6778
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 578.89
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 11.01s
                        Total time: 30233.23s
                               ETA: 1158659.2s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.527s, learning 0.160s)
               Value function loss: 15.6394
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 576.61
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 11.69s
                        Total time: 30244.92s
                               ETA: 1158639.6s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.448s, learning 0.188s)
               Value function loss: 17.4797
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 573.10
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 10.64s
                        Total time: 30255.55s
                               ETA: 1158579.7s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.540s, learning 0.172s)
               Value function loss: 19.8067
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 569.75
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 11.71s
                        Total time: 30267.27s
                               ETA: 1158561.1s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.469s, learning 0.164s)
               Value function loss: 20.6740
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 568.62
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 11.63s
                        Total time: 30278.90s
                               ETA: 1158539.4s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.493s, learning 0.161s)
               Value function loss: 19.6517
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 570.86
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 11.65s
                        Total time: 30290.55s
                               ETA: 1158518.6s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.784s, learning 0.186s)
               Value function loss: 17.4552
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 567.38
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 10.97s
                        Total time: 30301.52s
                               ETA: 1158471.6s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.793s, learning 0.164s)
               Value function loss: 24.6558
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 567.34
               Mean episode length: 249.71
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 10.96s
                        Total time: 30312.48s
                               ETA: 1158424.1s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.392s, learning 0.161s)
               Value function loss: 28.7792
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 566.75
               Mean episode length: 249.68
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 11.55s
                        Total time: 30324.03s
                               ETA: 1158399.5s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.398s, learning 0.180s)
               Value function loss: 22.7800
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 557.69
               Mean episode length: 249.68
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 11.58s
                        Total time: 30335.61s
                               ETA: 1158375.8s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.412s, learning 0.185s)
               Value function loss: 16.0128
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 565.04
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 11.60s
                        Total time: 30347.21s
                               ETA: 1158352.9s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.855s, learning 0.165s)
               Value function loss: 21.5602
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 563.45
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 11.02s
                        Total time: 30358.23s
                               ETA: 1158307.9s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.192s, learning 0.159s)
               Value function loss: 18.6667
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: 564.34
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 11.35s
                        Total time: 30369.58s
                               ETA: 1158275.6s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.426s, learning 0.162s)
               Value function loss: 21.3963
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 564.20
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 11.59s
                        Total time: 30381.17s
                               ETA: 1158252.3s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.738s, learning 0.162s)
               Value function loss: 16.9551
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 565.72
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 10.90s
                        Total time: 30392.07s
                               ETA: 1158202.9s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.160s)
               Value function loss: 22.5713
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 562.07
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 11.41s
                        Total time: 30403.48s
                               ETA: 1158172.9s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.105s, learning 0.158s)
               Value function loss: 14.1909
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: 548.82
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 11.26s
                        Total time: 30414.74s
                               ETA: 1158137.3s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.549s, learning 0.170s)
               Value function loss: 19.4908
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 549.33
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 11.72s
                        Total time: 30426.46s
                               ETA: 1158119.1s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.160s)
               Value function loss: 16.7267
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 550.11
               Mean episode length: 249.56
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 11.40s
                        Total time: 30437.87s
                               ETA: 1158088.9s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.572s, learning 0.161s)
               Value function loss: 15.1176
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 549.70
               Mean episode length: 249.16
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 11.73s
                        Total time: 30449.60s
                               ETA: 1158071.2s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.954s, learning 0.158s)
               Value function loss: 17.7108
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 554.17
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 11.11s
                        Total time: 30460.71s
                               ETA: 1158029.9s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.286s, learning 0.161s)
               Value function loss: 16.5917
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 552.89
               Mean episode length: 249.63
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 11.45s
                        Total time: 30472.16s
                               ETA: 1158001.4s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.244s, learning 0.173s)
               Value function loss: 16.9892
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 555.63
               Mean episode length: 249.63
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 11.42s
                        Total time: 30483.57s
                               ETA: 1157971.7s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.391s, learning 0.162s)
               Value function loss: 17.0935
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 555.15
               Mean episode length: 249.47
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 11.55s
                        Total time: 30495.13s
                               ETA: 1157947.3s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.695s, learning 0.188s)
               Value function loss: 13.1393
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 555.42
               Mean episode length: 249.47
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 11.88s
                        Total time: 30507.01s
                               ETA: 1157935.3s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.496s, learning 0.209s)
               Value function loss: 17.9187
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 551.80
               Mean episode length: 249.65
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 11.71s
                        Total time: 30518.71s
                               ETA: 1157916.6s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.890s, learning 0.160s)
               Value function loss: 14.5696
                    Surrogate loss: 0.0009
             Mean action noise std: 0.73
                       Mean reward: 549.94
               Mean episode length: 249.65
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 11.05s
                        Total time: 30529.77s
                               ETA: 1157873.1s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.720s, learning 0.184s)
               Value function loss: 12.3736
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 556.11
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 10.90s
                        Total time: 30540.67s
                               ETA: 1157824.1s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.275s, learning 0.161s)
               Value function loss: 15.6219
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 549.70
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 11.44s
                        Total time: 30552.11s
                               ETA: 1157795.3s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.524s, learning 0.161s)
               Value function loss: 19.0562
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 542.02
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 11.68s
                        Total time: 30563.79s
                               ETA: 1157775.9s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.974s, learning 0.160s)
               Value function loss: 16.5742
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 559.76
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 11.13s
                        Total time: 30574.92s
                               ETA: 1157735.6s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.252s, learning 0.255s)
               Value function loss: 13.0161
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: 557.25
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 11.51s
                        Total time: 30586.43s
                               ETA: 1157709.5s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.161s)
               Value function loss: 15.1481
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 554.56
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 11.22s
                        Total time: 30597.65s
                               ETA: 1157672.6s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.168s, learning 0.162s)
               Value function loss: 16.8348
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 555.45
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 11.33s
                        Total time: 30608.98s
                               ETA: 1157639.9s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.231s, learning 0.252s)
               Value function loss: 15.1088
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 550.10
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 11.48s
                        Total time: 30620.47s
                               ETA: 1157612.9s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.626s, learning 0.161s)
               Value function loss: 18.3648
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 533.74
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 10.79s
                        Total time: 30631.25s
                               ETA: 1157559.6s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.941s, learning 0.193s)
               Value function loss: 13.7798
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 532.98
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 11.13s
                        Total time: 30642.39s
                               ETA: 1157519.5s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.577s, learning 0.263s)
               Value function loss: 18.7313
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 533.12
               Mean episode length: 249.65
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 11.84s
                        Total time: 30654.23s
                               ETA: 1157506.0s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.125s, learning 0.163s)
               Value function loss: 19.4266
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 542.48
               Mean episode length: 249.32
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 11.29s
                        Total time: 30665.52s
                               ETA: 1157471.7s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.081s, learning 0.162s)
               Value function loss: 21.3835
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 536.59
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 11.24s
                        Total time: 30676.76s
                               ETA: 1157435.8s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.579s, learning 0.160s)
               Value function loss: 18.2200
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 528.63
               Mean episode length: 249.53
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 11.74s
                        Total time: 30688.50s
                               ETA: 1157418.6s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.962s, learning 0.170s)
               Value function loss: 15.8367
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 549.17
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 11.13s
                        Total time: 30699.63s
                               ETA: 1157378.5s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.284s, learning 0.168s)
               Value function loss: 22.8814
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 550.31
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 11.45s
                        Total time: 30711.08s
                               ETA: 1157350.4s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.164s)
               Value function loss: 18.9956
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 553.89
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 11.14s
                        Total time: 30722.23s
                               ETA: 1157310.7s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.219s, learning 0.157s)
               Value function loss: 17.3244
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 546.45
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 11.38s
                        Total time: 30733.60s
                               ETA: 1157279.9s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.379s, learning 0.172s)
               Value function loss: 16.9392
                    Surrogate loss: 0.0029
             Mean action noise std: 0.73
                       Mean reward: 541.53
               Mean episode length: 247.88
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 11.55s
                        Total time: 30745.15s
                               ETA: 1157255.6s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.638s, learning 0.160s)
               Value function loss: 18.5949
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 557.35
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 11.80s
                        Total time: 30756.95s
                               ETA: 1157240.6s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.586s, learning 0.174s)
               Value function loss: 16.8953
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 562.07
               Mean episode length: 249.61
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 11.76s
                        Total time: 30768.71s
                               ETA: 1157224.3s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.248s, learning 0.208s)
               Value function loss: 16.8299
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 567.23
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 11.46s
                        Total time: 30780.17s
                               ETA: 1157196.5s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.329s, learning 0.167s)
               Value function loss: 19.5893
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 558.92
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 11.50s
                        Total time: 30791.66s
                               ETA: 1157170.2s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.045s, learning 0.160s)
               Value function loss: 18.0088
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: 558.92
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 11.21s
                        Total time: 30802.87s
                               ETA: 1157133.0s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.581s, learning 0.188s)
               Value function loss: 21.7313
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 558.12
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 11.77s
                        Total time: 30814.64s
                               ETA: 1157117.0s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.167s)
               Value function loss: 17.7096
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 565.35
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 11.53s
                        Total time: 30826.16s
                               ETA: 1157091.9s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.243s, learning 0.192s)
               Value function loss: 20.2966
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 561.31
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 11.43s
                        Total time: 30837.60s
                               ETA: 1157063.3s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.890s, learning 0.176s)
               Value function loss: 23.0171
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 566.37
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 11.07s
                        Total time: 30848.66s
                               ETA: 1157020.9s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.114s, learning 0.189s)
               Value function loss: 21.0120
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 569.12
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 11.30s
                        Total time: 30859.97s
                               ETA: 1156987.5s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.086s, learning 0.171s)
               Value function loss: 17.7245
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 565.39
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 11.26s
                        Total time: 30871.23s
                               ETA: 1156952.3s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.040s, learning 0.201s)
               Value function loss: 15.1370
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 563.41
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 11.24s
                        Total time: 30882.47s
                               ETA: 1156916.6s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.085s, learning 0.169s)
               Value function loss: 19.4387
                    Surrogate loss: 0.0045
             Mean action noise std: 0.73
                       Mean reward: 555.78
               Mean episode length: 249.54
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 11.25s
                        Total time: 30893.72s
                               ETA: 1156881.4s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.181s, learning 0.169s)
               Value function loss: 19.2160
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 557.00
               Mean episode length: 249.54
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 11.35s
                        Total time: 30905.07s
                               ETA: 1156849.7s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.364s, learning 0.158s)
               Value function loss: 21.6713
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 556.10
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 11.52s
                        Total time: 30916.59s
                               ETA: 1156824.6s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.137s, learning 0.168s)
               Value function loss: 17.1445
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 548.83
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 11.31s
                        Total time: 30927.90s
                               ETA: 1156791.3s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.329s, learning 0.170s)
               Value function loss: 17.3956
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 546.34
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 11.50s
                        Total time: 30939.40s
                               ETA: 1156765.3s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.970s, learning 0.162s)
               Value function loss: 18.2244
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 550.66
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 11.13s
                        Total time: 30950.53s
                               ETA: 1156725.6s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.102s, learning 0.168s)
               Value function loss: 19.5942
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 548.20
               Mean episode length: 249.73
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 11.27s
                        Total time: 30961.80s
                               ETA: 1156691.0s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.165s)
               Value function loss: 20.9736
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 537.61
               Mean episode length: 249.61
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 11.45s
                        Total time: 30973.25s
                               ETA: 1156663.1s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.168s, learning 0.157s)
               Value function loss: 19.0084
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 536.35
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 11.32s
                        Total time: 30984.57s
                               ETA: 1156630.7s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.819s, learning 0.161s)
               Value function loss: 20.1146
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 532.81
               Mean episode length: 249.97
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 10.98s
                        Total time: 30995.55s
                               ETA: 1156585.3s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.313s, learning 0.193s)
               Value function loss: 22.1693
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 516.04
               Mean episode length: 249.84
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 11.51s
                        Total time: 31007.06s
                               ETA: 1156559.7s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.080s, learning 0.171s)
               Value function loss: 27.0983
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 513.78
               Mean episode length: 249.87
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 11.25s
                        Total time: 31018.31s
                               ETA: 1156524.5s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.303s, learning 0.158s)
               Value function loss: 26.7101
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 535.50
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 11.46s
                        Total time: 31029.77s
                               ETA: 1156497.2s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.305s, learning 0.178s)
               Value function loss: 25.0904
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 524.85
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 11.48s
                        Total time: 31041.25s
                               ETA: 1156470.7s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.358s, learning 0.166s)
               Value function loss: 27.6921
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 519.09
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 11.52s
                        Total time: 31052.78s
                               ETA: 1156445.8s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.980s, learning 0.165s)
               Value function loss: 26.0586
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 523.86
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 11.14s
                        Total time: 31063.92s
                               ETA: 1156406.7s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.906s, learning 0.238s)
               Value function loss: 22.4057
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: 528.47
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 11.14s
                        Total time: 31075.06s
                               ETA: 1156367.6s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.401s, learning 0.169s)
               Value function loss: 20.8874
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 538.28
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 11.57s
                        Total time: 31086.64s
                               ETA: 1156344.5s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.564s, learning 0.183s)
               Value function loss: 21.2029
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 544.16
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 11.75s
                        Total time: 31098.38s
                               ETA: 1156327.9s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.168s)
               Value function loss: 21.6388
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 561.81
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 11.41s
                        Total time: 31109.79s
                               ETA: 1156298.7s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.993s, learning 0.176s)
               Value function loss: 19.4492
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 564.02
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 11.17s
                        Total time: 31120.96s
                               ETA: 1156260.6s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.390s, learning 0.160s)
               Value function loss: 18.7967
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 559.01
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 11.55s
                        Total time: 31132.51s
                               ETA: 1156236.7s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.057s, learning 0.181s)
               Value function loss: 22.9434
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 556.97
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 11.24s
                        Total time: 31143.75s
                               ETA: 1156201.2s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.162s)
               Value function loss: 19.4216
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 552.71
               Mean episode length: 249.98
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 11.57s
                        Total time: 31155.32s
                               ETA: 1156178.1s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.934s, learning 0.168s)
               Value function loss: 21.3948
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 562.98
               Mean episode length: 249.98
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 11.10s
                        Total time: 31166.42s
                               ETA: 1156137.6s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.195s, learning 0.174s)
               Value function loss: 23.5749
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 570.67
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 11.37s
                        Total time: 31177.79s
                               ETA: 1156107.1s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.097s, learning 0.180s)
               Value function loss: 16.6370
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 575.60
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 11.28s
                        Total time: 31189.07s
                               ETA: 1156073.1s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.232s, learning 0.171s)
               Value function loss: 21.3806
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 575.33
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 11.40s
                        Total time: 31200.47s
                               ETA: 1156043.8s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.011s, learning 0.185s)
               Value function loss: 19.4487
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 571.63
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 11.20s
                        Total time: 31211.66s
                               ETA: 1156006.9s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.669s, learning 0.174s)
               Value function loss: 14.1223
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 570.05
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 11.84s
                        Total time: 31223.51s
                               ETA: 1155993.9s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.185s, learning 0.253s)
               Value function loss: 18.6822
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 572.69
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 11.44s
                        Total time: 31234.94s
                               ETA: 1155966.0s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.225s, learning 0.186s)
               Value function loss: 15.5518
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 577.15
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 11.41s
                        Total time: 31246.36s
                               ETA: 1155937.1s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.232s, learning 0.189s)
               Value function loss: 17.3648
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 581.06
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 11.42s
                        Total time: 31257.78s
                               ETA: 1155908.5s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.345s, learning 0.190s)
               Value function loss: 23.2756
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 578.30
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 11.53s
                        Total time: 31269.31s
                               ETA: 1155884.2s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.339s, learning 0.188s)
               Value function loss: 18.5177
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 576.59
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 11.53s
                        Total time: 31280.84s
                               ETA: 1155859.6s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.244s, learning 0.185s)
               Value function loss: 16.7096
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 577.77
               Mean episode length: 249.43
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 11.43s
                        Total time: 31292.27s
                               ETA: 1155831.4s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.140s, learning 0.175s)
               Value function loss: 15.9949
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 579.77
               Mean episode length: 249.43
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 11.32s
                        Total time: 31303.58s
                               ETA: 1155799.0s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.280s, learning 0.177s)
               Value function loss: 16.7955
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 580.58
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 11.46s
                        Total time: 31315.04s
                               ETA: 1155771.9s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.308s, learning 0.175s)
               Value function loss: 23.4069
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 582.97
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 11.48s
                        Total time: 31326.52s
                               ETA: 1155745.7s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.132s, learning 0.193s)
               Value function loss: 18.1673
                    Surrogate loss: 0.0088
             Mean action noise std: 0.73
                       Mean reward: 578.55
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 11.33s
                        Total time: 31337.85s
                               ETA: 1155713.7s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.276s, learning 0.200s)
               Value function loss: 18.7452
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 583.04
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 11.48s
                        Total time: 31349.32s
                               ETA: 1155687.3s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.880s, learning 0.162s)
               Value function loss: 19.1792
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 580.07
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 11.04s
                        Total time: 31360.37s
                               ETA: 1155644.9s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.238s, learning 0.159s)
               Value function loss: 19.7310
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 583.88
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 11.40s
                        Total time: 31371.76s
                               ETA: 1155615.6s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.228s, learning 0.161s)
               Value function loss: 25.1483
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 580.95
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 11.39s
                        Total time: 31383.15s
                               ETA: 1155586.0s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.860s, learning 0.177s)
               Value function loss: 23.5544
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 586.67
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 11.04s
                        Total time: 31394.19s
                               ETA: 1155543.5s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.068s, learning 0.175s)
               Value function loss: 19.4201
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 581.84
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 11.24s
                        Total time: 31405.43s
                               ETA: 1155508.6s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.262s, learning 0.175s)
               Value function loss: 17.8395
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 582.91
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 11.44s
                        Total time: 31416.87s
                               ETA: 1155480.8s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.362s, learning 0.228s)
               Value function loss: 17.8311
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 579.75
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 11.59s
                        Total time: 31428.46s
                               ETA: 1155458.7s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.320s, learning 0.193s)
               Value function loss: 14.6510
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 583.87
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 11.51s
                        Total time: 31439.97s
                               ETA: 1155433.7s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.892s, learning 0.163s)
               Value function loss: 20.2696
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 580.75
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 11.05s
                        Total time: 31451.02s
                               ETA: 1155392.0s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.112s, learning 0.170s)
               Value function loss: 18.7445
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 582.36
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 11.28s
                        Total time: 31462.31s
                               ETA: 1155358.6s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.189s, learning 0.172s)
               Value function loss: 18.5257
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 582.52
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 11.36s
                        Total time: 31473.67s
                               ETA: 1155328.1s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.153s, learning 0.159s)
               Value function loss: 12.9786
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 585.25
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 11.31s
                        Total time: 31484.98s
                               ETA: 1155295.8s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.066s, learning 0.173s)
               Value function loss: 21.8241
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 587.26
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 11.24s
                        Total time: 31496.22s
                               ETA: 1155260.9s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.457s, learning 0.169s)
               Value function loss: 14.3306
                    Surrogate loss: 0.0045
             Mean action noise std: 0.73
                       Mean reward: 584.97
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 11.63s
                        Total time: 31507.85s
                               ETA: 1155240.2s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.166s)
               Value function loss: 18.7416
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 583.66
               Mean episode length: 249.78
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 11.39s
                        Total time: 31519.24s
                               ETA: 1155210.9s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.164s)
               Value function loss: 21.8282
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 585.41
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 11.85s
                        Total time: 31531.08s
                               ETA: 1155198.2s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.102s, learning 0.158s)
               Value function loss: 17.2212
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 584.93
               Mean episode length: 249.94
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 11.26s
                        Total time: 31542.34s
                               ETA: 1155164.1s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.145s, learning 0.170s)
               Value function loss: 18.1140
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 584.18
               Mean episode length: 249.94
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 11.32s
                        Total time: 31553.66s
                               ETA: 1155132.1s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.143s, learning 0.171s)
               Value function loss: 15.8114
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 583.11
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 11.31s
                        Total time: 31564.97s
                               ETA: 1155100.0s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.441s, learning 0.163s)
               Value function loss: 11.8462
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 586.18
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 11.60s
                        Total time: 31576.57s
                               ETA: 1155078.5s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.469s, learning 0.162s)
               Value function loss: 15.6496
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 584.92
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 11.63s
                        Total time: 31588.21s
                               ETA: 1155058.0s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.251s, learning 0.166s)
               Value function loss: 12.6896
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 581.44
               Mean episode length: 249.76
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 11.42s
                        Total time: 31599.62s
                               ETA: 1155029.7s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.901s, learning 0.159s)
               Value function loss: 24.5194
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 581.50
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 11.06s
                        Total time: 31610.68s
                               ETA: 1154988.4s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.885s, learning 0.162s)
               Value function loss: 23.4627
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 583.01
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 11.05s
                        Total time: 31621.73s
                               ETA: 1154946.6s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.594s, learning 0.163s)
               Value function loss: 16.1504
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 581.69
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 11.76s
                        Total time: 31633.49s
                               ETA: 1154930.8s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.354s, learning 0.175s)
               Value function loss: 17.9045
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 589.08
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 11.53s
                        Total time: 31645.02s
                               ETA: 1154906.7s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.162s)
               Value function loss: 15.0357
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 590.97
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 11.44s
                        Total time: 31656.46s
                               ETA: 1154879.4s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.607s, learning 0.160s)
               Value function loss: 18.5461
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 585.46
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 11.77s
                        Total time: 31668.23s
                               ETA: 1154864.0s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.224s, learning 0.174s)
               Value function loss: 19.3767
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 588.48
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 11.40s
                        Total time: 31679.63s
                               ETA: 1154835.1s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.165s)
               Value function loss: 18.8093
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 593.47
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 11.40s
                        Total time: 31691.03s
                               ETA: 1154806.3s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.153s, learning 0.162s)
               Value function loss: 20.0446
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 590.03
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 11.32s
                        Total time: 31702.34s
                               ETA: 1154774.4s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.325s, learning 0.177s)
               Value function loss: 25.6294
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 586.96
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 11.50s
                        Total time: 31713.84s
                               ETA: 1154749.4s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.677s, learning 0.181s)
               Value function loss: 21.6052
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 586.20
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 11.86s
                        Total time: 31725.70s
                               ETA: 1154737.3s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.302s, learning 0.200s)
               Value function loss: 35.2785
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 591.21
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 11.50s
                        Total time: 31737.20s
                               ETA: 1154712.2s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.300s, learning 0.167s)
               Value function loss: 34.9779
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 583.09
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 11.47s
                        Total time: 31748.67s
                               ETA: 1154685.9s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.053s, learning 0.166s)
               Value function loss: 30.9118
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 590.39
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 11.22s
                        Total time: 31759.89s
                               ETA: 1154650.6s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.152s, learning 0.165s)
               Value function loss: 24.3719
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 588.72
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 11.32s
                        Total time: 31771.21s
                               ETA: 1154618.8s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.164s)
               Value function loss: 33.7241
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 590.90
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 11.37s
                        Total time: 31782.58s
                               ETA: 1154589.2s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.190s, learning 0.171s)
               Value function loss: 29.9129
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 588.09
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 11.36s
                        Total time: 31793.94s
                               ETA: 1154559.0s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.296s, learning 0.184s)
               Value function loss: 32.8215
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 588.71
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 11.48s
                        Total time: 31805.42s
                               ETA: 1154533.3s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.960s, learning 0.172s)
               Value function loss: 26.4912
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 592.21
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 11.13s
                        Total time: 31816.55s
                               ETA: 1154494.9s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.185s, learning 0.167s)
               Value function loss: 31.1288
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 590.16
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 11.35s
                        Total time: 31827.91s
                               ETA: 1154464.5s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.911s, learning 0.165s)
               Value function loss: 25.8529
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 583.26
               Mean episode length: 248.79
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 11.08s
                        Total time: 31838.98s
                               ETA: 1154424.1s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.617s, learning 0.206s)
               Value function loss: 34.6205
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 599.96
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 11.82s
                        Total time: 31850.80s
                               ETA: 1154410.8s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.465s, learning 0.173s)
               Value function loss: 25.6006
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 590.29
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 11.64s
                        Total time: 31862.44s
                               ETA: 1154390.7s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.266s, learning 0.170s)
               Value function loss: 20.3310
                    Surrogate loss: -0.0209
             Mean action noise std: 0.73
                       Mean reward: 591.45
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 11.44s
                        Total time: 31873.88s
                               ETA: 1154363.4s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.406s, learning 0.168s)
               Value function loss: 29.1656
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 594.57
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 11.57s
                        Total time: 31885.45s
                               ETA: 1154341.1s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.087s, learning 0.184s)
               Value function loss: 21.7238
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 589.05
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 11.27s
                        Total time: 31896.72s
                               ETA: 1154307.9s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.215s, learning 0.168s)
               Value function loss: 23.6398
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 589.77
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 11.38s
                        Total time: 31908.11s
                               ETA: 1154278.7s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.111s, learning 0.166s)
               Value function loss: 23.3277
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 595.22
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 11.28s
                        Total time: 31919.38s
                               ETA: 1154245.7s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.596s, learning 0.170s)
               Value function loss: 16.9968
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 594.41
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 11.77s
                        Total time: 31931.15s
                               ETA: 1154230.4s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.525s, learning 0.174s)
               Value function loss: 22.6253
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 593.92
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 11.70s
                        Total time: 31942.85s
                               ETA: 1154212.6s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.073s, learning 0.179s)
               Value function loss: 17.4619
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 596.70
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 11.25s
                        Total time: 31954.10s
                               ETA: 1154178.8s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.522s, learning 0.167s)
               Value function loss: 20.9481
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 598.91
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 11.69s
                        Total time: 31965.79s
                               ETA: 1154160.7s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.178s, learning 0.169s)
               Value function loss: 21.0410
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 597.82
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 11.35s
                        Total time: 31977.14s
                               ETA: 1154130.3s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.160s)
               Value function loss: 24.6338
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 600.57
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 10.93s
                        Total time: 31988.07s
                               ETA: 1154085.0s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.969s, learning 0.183s)
               Value function loss: 22.7940
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 599.37
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 11.15s
                        Total time: 31999.22s
                               ETA: 1154047.6s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.219s, learning 0.169s)
               Value function loss: 16.9933
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 602.42
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 11.39s
                        Total time: 32010.61s
                               ETA: 1154018.7s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.170s)
               Value function loss: 17.6976
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 600.18
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 11.38s
                        Total time: 32021.99s
                               ETA: 1153989.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.164s)
               Value function loss: 24.5040
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 597.40
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 11.24s
                        Total time: 32033.23s
                               ETA: 1153955.4s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.519s, learning 0.165s)
               Value function loss: 25.9481
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 595.81
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 11.68s
                        Total time: 32044.92s
                               ETA: 1153937.2s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.209s, learning 0.159s)
               Value function loss: 25.2971
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 594.33
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 11.37s
                        Total time: 32056.29s
                               ETA: 1153907.7s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.012s, learning 0.166s)
               Value function loss: 22.4355
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 591.74
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 11.18s
                        Total time: 32067.46s
                               ETA: 1153871.3s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.148s, learning 0.161s)
               Value function loss: 26.8783
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 598.82
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 11.31s
                        Total time: 32078.77s
                               ETA: 1153839.7s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.384s, learning 0.161s)
               Value function loss: 32.6287
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 593.80
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 11.54s
                        Total time: 32090.32s
                               ETA: 1153816.5s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.879s, learning 0.168s)
               Value function loss: 40.2975
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 593.78
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 11.05s
                        Total time: 32101.36s
                               ETA: 1153775.4s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.053s, learning 0.158s)
               Value function loss: 25.6860
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 593.60
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 11.21s
                        Total time: 32112.57s
                               ETA: 1153740.3s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.425s, learning 0.170s)
               Value function loss: 23.8062
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 595.04
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 11.60s
                        Total time: 32124.17s
                               ETA: 1153719.0s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.223s, learning 0.159s)
               Value function loss: 32.6459
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 598.26
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 11.38s
                        Total time: 32135.55s
                               ETA: 1153690.1s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.967s, learning 0.160s)
               Value function loss: 24.7364
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 597.90
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 11.13s
                        Total time: 32146.68s
                               ETA: 1153652.0s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.976s, learning 0.174s)
               Value function loss: 24.5831
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 601.03
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 11.15s
                        Total time: 32157.83s
                               ETA: 1153614.7s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.442s, learning 0.164s)
               Value function loss: 24.3151
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 600.77
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 11.61s
                        Total time: 32169.44s
                               ETA: 1153593.8s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.531s, learning 0.166s)
               Value function loss: 27.7966
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 598.65
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 11.70s
                        Total time: 32181.13s
                               ETA: 1153576.2s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.296s, learning 0.165s)
               Value function loss: 27.3924
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 597.31
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 11.46s
                        Total time: 32192.59s
                               ETA: 1153550.2s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.162s)
               Value function loss: 23.6235
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 597.77
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 11.55s
                        Total time: 32204.15s
                               ETA: 1153527.4s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.234s, learning 0.163s)
               Value function loss: 29.7737
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 593.86
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 11.40s
                        Total time: 32215.54s
                               ETA: 1153499.0s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.288s, learning 0.164s)
               Value function loss: 27.1647
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 593.13
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 11.45s
                        Total time: 32226.99s
                               ETA: 1153472.6s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.050s, learning 0.253s)
               Value function loss: 28.6952
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 594.16
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 11.30s
                        Total time: 32238.30s
                               ETA: 1153440.9s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.161s)
               Value function loss: 26.3771
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 586.91
               Mean episode length: 249.01
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 11.18s
                        Total time: 32249.47s
                               ETA: 1153404.8s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.417s, learning 0.162s)
               Value function loss: 22.6750
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 589.53
               Mean episode length: 249.01
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 11.58s
                        Total time: 32261.05s
                               ETA: 1153383.0s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.609s, learning 0.189s)
               Value function loss: 31.1314
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 593.28
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 11.80s
                        Total time: 32272.85s
                               ETA: 1153369.1s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.192s, learning 0.175s)
               Value function loss: 25.0955
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 593.62
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 11.37s
                        Total time: 32284.22s
                               ETA: 1153339.7s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.162s)
               Value function loss: 21.0223
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 595.76
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 11.14s
                        Total time: 32295.36s
                               ETA: 1153302.3s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.731s, learning 0.186s)
               Value function loss: 18.7882
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 596.98
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 10.92s
                        Total time: 32306.27s
                               ETA: 1153256.9s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.123s, learning 0.191s)
               Value function loss: 25.0842
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 596.96
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 11.31s
                        Total time: 32317.59s
                               ETA: 1153225.8s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.295s, learning 0.161s)
               Value function loss: 24.3779
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 597.71
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 11.46s
                        Total time: 32329.05s
                               ETA: 1153199.7s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.011s, learning 0.158s)
               Value function loss: 26.7940
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 595.20
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 11.17s
                        Total time: 32340.21s
                               ETA: 1153163.4s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.320s, learning 0.171s)
               Value function loss: 23.2846
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 600.93
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 11.49s
                        Total time: 32351.70s
                               ETA: 1153138.5s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.488s, learning 0.180s)
               Value function loss: 19.4536
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 603.26
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 10.67s
                        Total time: 32362.37s
                               ETA: 1153084.4s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.862s, learning 0.174s)
               Value function loss: 19.7754
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 598.75
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 11.04s
                        Total time: 32373.41s
                               ETA: 1153043.4s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.067s, learning 0.164s)
               Value function loss: 23.5973
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 595.43
               Mean episode length: 250.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 11.23s
                        Total time: 32384.64s
                               ETA: 1153009.3s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.612s, learning 0.183s)
               Value function loss: 24.6855
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 600.23
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 11.80s
                        Total time: 32396.43s
                               ETA: 1152995.4s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.166s, learning 0.174s)
               Value function loss: 18.8034
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 603.00
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 11.34s
                        Total time: 32407.77s
                               ETA: 1152965.2s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.921s, learning 0.184s)
               Value function loss: 23.8042
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 597.83
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 11.11s
                        Total time: 32418.88s
                               ETA: 1152926.8s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.469s, learning 0.189s)
               Value function loss: 24.2344
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 601.35
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 11.66s
                        Total time: 32430.54s
                               ETA: 1152908.0s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.111s, learning 0.161s)
               Value function loss: 28.4628
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 604.42
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 11.27s
                        Total time: 32441.81s
                               ETA: 1152875.5s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.164s)
               Value function loss: 27.0204
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 601.54
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 11.51s
                        Total time: 32453.32s
                               ETA: 1152851.4s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.924s, learning 0.164s)
               Value function loss: 21.9505
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 603.19
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 11.09s
                        Total time: 32464.41s
                               ETA: 1152812.4s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.458s, learning 0.171s)
               Value function loss: 24.5164
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 602.61
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 11.63s
                        Total time: 32476.04s
                               ETA: 1152792.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.670s, learning 0.166s)
               Value function loss: 18.1981
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 601.77
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 10.84s
                        Total time: 32486.87s
                               ETA: 1152744.7s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.375s, learning 0.168s)
               Value function loss: 20.6527
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 602.69
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 11.54s
                        Total time: 32498.42s
                               ETA: 1152721.9s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.443s, learning 0.175s)
               Value function loss: 16.0708
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 604.70
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 11.62s
                        Total time: 32510.03s
                               ETA: 1152701.8s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.898s, learning 0.172s)
               Value function loss: 20.2898
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 605.08
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 11.07s
                        Total time: 32521.10s
                               ETA: 1152662.2s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.833s, learning 0.170s)
               Value function loss: 21.2935
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 603.65
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 11.00s
                        Total time: 32532.11s
                               ETA: 1152620.3s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.478s, learning 0.164s)
               Value function loss: 20.4308
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 602.67
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 11.64s
                        Total time: 32543.75s
                               ETA: 1152601.0s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.441s, learning 0.162s)
               Value function loss: 17.2717
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 601.98
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 11.60s
                        Total time: 32555.35s
                               ETA: 1152580.4s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.140s, learning 0.171s)
               Value function loss: 24.5956
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 602.80
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 11.31s
                        Total time: 32566.66s
                               ETA: 1152549.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.497s, learning 0.184s)
               Value function loss: 18.3812
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 604.57
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 11.68s
                        Total time: 32578.34s
                               ETA: 1152531.5s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.261s, learning 0.257s)
               Value function loss: 18.4330
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 605.99
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 11.52s
                        Total time: 32589.86s
                               ETA: 1152507.9s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.161s)
               Value function loss: 21.1999
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 600.48
               Mean episode length: 249.28
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 11.45s
                        Total time: 32601.31s
                               ETA: 1152481.8s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.570s, learning 0.160s)
               Value function loss: 14.7925
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 597.41
               Mean episode length: 248.71
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 11.73s
                        Total time: 32613.04s
                               ETA: 1152465.7s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.697s, learning 0.157s)
               Value function loss: 17.4250
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 605.70
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 11.85s
                        Total time: 32624.89s
                               ETA: 1152453.9s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.230s, learning 0.159s)
               Value function loss: 18.8088
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 603.66
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 11.39s
                        Total time: 32636.28s
                               ETA: 1152425.8s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.129s, learning 0.159s)
               Value function loss: 15.1504
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 603.96
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 11.29s
                        Total time: 32647.57s
                               ETA: 1152394.1s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.253s, learning 0.189s)
               Value function loss: 18.2727
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 602.78
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 11.44s
                        Total time: 32659.01s
                               ETA: 1152367.8s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.843s, learning 0.159s)
               Value function loss: 13.9492
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 601.68
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 12.00s
                        Total time: 32671.02s
                               ETA: 1152361.3s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.189s)
               Value function loss: 19.6321
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 602.31
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 11.47s
                        Total time: 32682.49s
                               ETA: 1152336.1s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.216s, learning 0.175s)
               Value function loss: 23.8379
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 600.73
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 11.39s
                        Total time: 32693.88s
                               ETA: 1152308.1s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.685s, learning 0.182s)
               Value function loss: 16.3885
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 602.18
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 10.87s
                        Total time: 32704.74s
                               ETA: 1152261.6s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.359s, learning 0.178s)
               Value function loss: 16.5342
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 599.83
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 11.54s
                        Total time: 32716.28s
                               ETA: 1152238.8s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.928s, learning 0.164s)
               Value function loss: 14.5617
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 600.93
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 11.09s
                        Total time: 32727.37s
                               ETA: 1152200.2s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.043s, learning 0.165s)
               Value function loss: 18.7494
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 600.47
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 11.21s
                        Total time: 32738.58s
                               ETA: 1152165.8s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.170s, learning 0.173s)
               Value function loss: 21.0171
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 598.35
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 11.34s
                        Total time: 32749.92s
                               ETA: 1152136.2s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.398s, learning 0.159s)
               Value function loss: 18.0003
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 598.80
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 11.56s
                        Total time: 32761.48s
                               ETA: 1152114.1s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.115s, learning 0.167s)
               Value function loss: 23.0874
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 599.79
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 11.28s
                        Total time: 32772.76s
                               ETA: 1152082.3s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.703s, learning 0.161s)
               Value function loss: 16.6581
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 601.46
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 11.86s
                        Total time: 32784.63s
                               ETA: 1152071.0s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.022s, learning 0.184s)
               Value function loss: 23.0905
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 597.46
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 11.21s
                        Total time: 32795.83s
                               ETA: 1152036.6s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.114s, learning 0.193s)
               Value function loss: 25.3106
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 595.10
               Mean episode length: 248.73
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 11.31s
                        Total time: 32807.14s
                               ETA: 1152005.7s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.990s, learning 0.165s)
               Value function loss: 23.8857
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 596.33
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 11.16s
                        Total time: 32818.30s
                               ETA: 1151969.6s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.090s, learning 0.163s)
               Value function loss: 20.5807
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 598.63
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 11.25s
                        Total time: 32829.55s
                               ETA: 1151936.9s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.163s)
               Value function loss: 19.0982
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 599.58
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 11.14s
                        Total time: 32840.69s
                               ETA: 1151900.3s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.288s, learning 0.170s)
               Value function loss: 16.8940
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 600.17
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 11.46s
                        Total time: 32852.15s
                               ETA: 1151874.8s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.144s, learning 0.170s)
               Value function loss: 15.0585
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 597.74
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 11.31s
                        Total time: 32863.46s
                               ETA: 1151844.2s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.441s, learning 0.168s)
               Value function loss: 22.8591
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 595.44
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 11.61s
                        Total time: 32875.07s
                               ETA: 1151824.0s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.127s, learning 0.162s)
               Value function loss: 20.9783
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 588.82
               Mean episode length: 248.02
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 11.29s
                        Total time: 32886.36s
                               ETA: 1151792.6s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.230s, learning 0.165s)
               Value function loss: 27.0663
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 593.26
               Mean episode length: 248.02
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 11.39s
                        Total time: 32897.75s
                               ETA: 1151765.0s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.238s, learning 0.179s)
               Value function loss: 16.4848
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 598.95
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 11.42s
                        Total time: 32909.17s
                               ETA: 1151738.1s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.221s, learning 0.176s)
               Value function loss: 26.5484
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 596.99
               Mean episode length: 249.49
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 11.40s
                        Total time: 32920.57s
                               ETA: 1151710.5s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.969s, learning 0.182s)
               Value function loss: 21.8008
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 598.76
               Mean episode length: 249.49
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 11.15s
                        Total time: 32931.72s
                               ETA: 1151674.4s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.909s, learning 0.172s)
               Value function loss: 21.7685
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 603.76
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 11.08s
                        Total time: 32942.80s
                               ETA: 1151635.8s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 836 steps/s (collection: 19.415s, learning 0.170s)
               Value function loss: 29.9553
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 601.56
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 19.59s
                        Total time: 32962.39s
                               ETA: 1151894.4s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 753 steps/s (collection: 21.572s, learning 0.173s)
               Value function loss: 22.1190
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 600.91
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 21.75s
                        Total time: 32984.13s
                               ETA: 1152228.3s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.411s, learning 0.159s)
               Value function loss: 27.7945
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 600.95
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 22.57s
                        Total time: 33006.70s
                               ETA: 1152590.7s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 749 steps/s (collection: 21.686s, learning 0.172s)
               Value function loss: 24.3171
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 601.06
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 21.86s
                        Total time: 33028.56s
                               ETA: 1152927.9s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 758 steps/s (collection: 21.423s, learning 0.187s)
               Value function loss: 16.4994
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 601.69
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 21.61s
                        Total time: 33050.17s
                               ETA: 1153256.3s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 730 steps/s (collection: 22.270s, learning 0.161s)
               Value function loss: 22.4456
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 598.00
               Mean episode length: 249.81
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 22.43s
                        Total time: 33072.60s
                               ETA: 1153613.1s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.674s, learning 0.160s)
               Value function loss: 20.3733
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 598.49
               Mean episode length: 249.81
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 21.83s
                        Total time: 33094.43s
                               ETA: 1153948.7s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.665s, learning 0.164s)
               Value function loss: 21.9199
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 601.19
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 21.83s
                        Total time: 33116.26s
                               ETA: 1154284.0s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.808s, learning 0.171s)
               Value function loss: 26.4254
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 591.99
               Mean episode length: 248.75
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 21.98s
                        Total time: 33138.24s
                               ETA: 1154624.2s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.112s, learning 0.166s)
               Value function loss: 27.2930
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: 594.14
               Mean episode length: 248.75
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 22.28s
                        Total time: 33160.52s
                               ETA: 1154974.6s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.281s, learning 0.161s)
               Value function loss: 25.4020
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 598.01
               Mean episode length: 249.35
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 21.44s
                        Total time: 33181.96s
                               ETA: 1155295.6s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.136s, learning 0.174s)
               Value function loss: 21.3580
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 602.74
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 22.31s
                        Total time: 33204.27s
                               ETA: 1155646.6s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.172s, learning 0.184s)
               Value function loss: 26.8115
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 601.07
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 22.36s
                        Total time: 33226.63s
                               ETA: 1155998.9s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.619s, learning 0.167s)
               Value function loss: 27.0587
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 600.52
               Mean episode length: 250.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 21.79s
                        Total time: 33248.41s
                               ETA: 1156331.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 749 steps/s (collection: 21.703s, learning 0.168s)
               Value function loss: 27.2294
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 601.89
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 21.87s
                        Total time: 33270.29s
                               ETA: 1156666.0s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.806s, learning 0.175s)
               Value function loss: 25.9317
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 602.67
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 21.98s
                        Total time: 33292.27s
                               ETA: 1157004.4s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.200s, learning 0.175s)
               Value function loss: 32.7508
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 602.76
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 22.37s
                        Total time: 33314.64s
                               ETA: 1157356.3s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.084s, learning 0.191s)
               Value function loss: 30.9586
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 601.74
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 22.28s
                        Total time: 33336.92s
                               ETA: 1157704.5s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.821s, learning 0.166s)
               Value function loss: 39.1148
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 599.74
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 21.99s
                        Total time: 33358.90s
                               ETA: 1158042.4s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 738 steps/s (collection: 22.029s, learning 0.167s)
               Value function loss: 42.9612
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 601.53
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 22.20s
                        Total time: 33381.10s
                               ETA: 1158387.3s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 737 steps/s (collection: 22.050s, learning 0.167s)
               Value function loss: 33.7141
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 603.26
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 22.22s
                        Total time: 33403.32s
                               ETA: 1158732.7s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 736 steps/s (collection: 22.073s, learning 0.165s)
               Value function loss: 23.2666
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 602.46
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 22.24s
                        Total time: 33425.55s
                               ETA: 1159078.5s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 749 steps/s (collection: 21.683s, learning 0.163s)
               Value function loss: 31.0031
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 600.79
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 21.85s
                        Total time: 33447.40s
                               ETA: 1159410.5s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 760 steps/s (collection: 21.365s, learning 0.174s)
               Value function loss: 25.5420
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 599.46
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 21.54s
                        Total time: 33468.94s
                               ETA: 1159731.6s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.683s, learning 0.160s)
               Value function loss: 29.5711
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 598.60
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 21.84s
                        Total time: 33490.78s
                               ETA: 1160062.9s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 736 steps/s (collection: 22.076s, learning 0.162s)
               Value function loss: 25.6117
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 601.69
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 22.24s
                        Total time: 33513.02s
                               ETA: 1160407.7s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.760s, learning 0.187s)
               Value function loss: 36.6592
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 597.37
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 21.95s
                        Total time: 33534.97s
                               ETA: 1160742.2s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.535s, learning 0.163s)
               Value function loss: 25.4635
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 596.48
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 21.70s
                        Total time: 33556.66s
                               ETA: 1161067.8s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.143s, learning 0.163s)
               Value function loss: 32.3942
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 592.62
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 22.31s
                        Total time: 33578.97s
                               ETA: 1161414.1s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.195s, learning 0.173s)
               Value function loss: 36.9107
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 583.07
               Mean episode length: 248.07
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 22.37s
                        Total time: 33601.34s
                               ETA: 1161762.4s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.146s, learning 0.164s)
               Value function loss: 28.7582
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 591.48
               Mean episode length: 249.17
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 22.31s
                        Total time: 33623.65s
                               ETA: 1162108.4s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 748 steps/s (collection: 21.700s, learning 0.189s)
               Value function loss: 31.2757
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 598.22
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 21.89s
                        Total time: 33645.54s
                               ETA: 1162439.6s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.357s, learning 0.173s)
               Value function loss: 26.0092
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 590.23
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 22.53s
                        Total time: 33668.07s
                               ETA: 1162792.7s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.412s, learning 0.178s)
               Value function loss: 26.9601
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 592.02
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 22.59s
                        Total time: 33690.66s
                               ETA: 1163147.6s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 741 steps/s (collection: 21.940s, learning 0.164s)
               Value function loss: 27.4315
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 595.45
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 22.10s
                        Total time: 33712.76s
                               ETA: 1163485.4s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.757s, learning 0.166s)
               Value function loss: 20.9426
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 592.77
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 21.92s
                        Total time: 33734.69s
                               ETA: 1163816.7s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.748s, learning 0.167s)
               Value function loss: 26.4819
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 591.35
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 21.91s
                        Total time: 33756.60s
                               ETA: 1164147.5s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 798 steps/s (collection: 20.365s, learning 0.162s)
               Value function loss: 20.8086
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: 594.30
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 20.53s
                        Total time: 33777.13s
                               ETA: 1164430.2s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.995s, learning 0.168s)
               Value function loss: 24.0199
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 596.25
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 11.16s
                        Total time: 33788.29s
                               ETA: 1164390.0s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.289s, learning 0.184s)
               Value function loss: 26.6366
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 597.83
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 11.47s
                        Total time: 33799.76s
                               ETA: 1164360.5s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.160s)
               Value function loss: 34.2860
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 601.58
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 11.36s
                        Total time: 33811.13s
                               ETA: 1164327.3s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.053s, learning 0.169s)
               Value function loss: 28.8766
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 599.74
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 11.22s
                        Total time: 33822.35s
                               ETA: 1164289.2s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.295s, learning 0.161s)
               Value function loss: 23.1652
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 593.17
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 11.46s
                        Total time: 33833.81s
                               ETA: 1164259.1s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.164s)
               Value function loss: 27.4575
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 587.94
               Mean episode length: 249.21
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 11.39s
                        Total time: 33845.20s
                               ETA: 1164226.9s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.823s, learning 0.159s)
               Value function loss: 35.5512
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 592.22
               Mean episode length: 249.21
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 10.98s
                        Total time: 33856.18s
                               ETA: 1164180.5s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.308s, learning 0.171s)
               Value function loss: 32.8014
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 598.11
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 11.48s
                        Total time: 33867.66s
                               ETA: 1164151.3s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.984s, learning 0.163s)
               Value function loss: 30.7493
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 594.21
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 11.15s
                        Total time: 33878.80s
                               ETA: 1164110.7s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.070s, learning 0.161s)
               Value function loss: 29.2395
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 590.27
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 11.23s
                        Total time: 33890.04s
                               ETA: 1164073.0s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.212s, learning 0.162s)
               Value function loss: 33.0113
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 594.62
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 11.37s
                        Total time: 33901.41s
                               ETA: 1164040.2s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.001s, learning 0.169s)
               Value function loss: 42.9017
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 591.67
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 11.17s
                        Total time: 33912.58s
                               ETA: 1164000.5s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.959s, learning 0.160s)
               Value function loss: 39.4441
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 588.61
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 11.12s
                        Total time: 33923.70s
                               ETA: 1163959.0s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.222s, learning 0.172s)
               Value function loss: 26.9087
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 594.18
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 11.39s
                        Total time: 33935.09s
                               ETA: 1163927.0s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.107s, learning 0.166s)
               Value function loss: 27.0876
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 595.02
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 11.27s
                        Total time: 33946.37s
                               ETA: 1163890.8s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.448s, learning 0.210s)
               Value function loss: 36.4928
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 598.22
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 11.66s
                        Total time: 33958.02s
                               ETA: 1163867.9s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.296s, learning 0.159s)
               Value function loss: 30.2136
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 597.47
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 11.46s
                        Total time: 33969.48s
                               ETA: 1163838.0s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.904s, learning 0.166s)
               Value function loss: 25.0820
                    Surrogate loss: 0.0066
             Mean action noise std: 0.73
                       Mean reward: 593.07
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 11.07s
                        Total time: 33980.55s
                               ETA: 1163794.9s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.202s, learning 0.188s)
               Value function loss: 25.1367
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 589.63
               Mean episode length: 249.46
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 11.39s
                        Total time: 33991.94s
                               ETA: 1163762.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.161s, learning 0.159s)
               Value function loss: 31.0022
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 595.03
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 11.32s
                        Total time: 34003.26s
                               ETA: 1163728.3s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.841s, learning 0.158s)
               Value function loss: 27.6342
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 593.91
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 11.00s
                        Total time: 34014.26s
                               ETA: 1163682.9s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.251s, learning 0.165s)
               Value function loss: 30.5131
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 596.88
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 11.42s
                        Total time: 34025.68s
                               ETA: 1163651.7s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.894s, learning 0.185s)
               Value function loss: 33.2979
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 598.51
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 11.08s
                        Total time: 34036.75s
                               ETA: 1163609.1s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.689s, learning 0.164s)
               Value function loss: 30.4869
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 596.83
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 11.85s
                        Total time: 34048.61s
                               ETA: 1163592.9s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.140s, learning 0.164s)
               Value function loss: 30.9419
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 594.29
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 11.30s
                        Total time: 34059.91s
                               ETA: 1163558.0s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.944s, learning 0.157s)
               Value function loss: 26.1061
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 594.23
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 11.10s
                        Total time: 34071.01s
                               ETA: 1163516.1s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.085s, learning 0.187s)
               Value function loss: 21.1137
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 590.93
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 11.27s
                        Total time: 34082.28s
                               ETA: 1163480.1s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.078s, learning 0.158s)
               Value function loss: 31.0823
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 593.05
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 11.24s
                        Total time: 34093.52s
                               ETA: 1163442.9s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.313s, learning 0.170s)
               Value function loss: 26.0046
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 594.33
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 11.48s
                        Total time: 34105.00s
                               ETA: 1163414.1s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.167s)
               Value function loss: 20.8790
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 586.84
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 11.41s
                        Total time: 34116.41s
                               ETA: 1163382.8s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.297s, learning 0.168s)
               Value function loss: 16.5619
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 582.45
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 11.47s
                        Total time: 34127.88s
                               ETA: 1163353.5s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.727s, learning 0.179s)
               Value function loss: 19.9600
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 587.98
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 10.91s
                        Total time: 34138.78s
                               ETA: 1163305.1s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.239s, learning 0.157s)
               Value function loss: 24.6218
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 588.85
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 11.40s
                        Total time: 34150.18s
                               ETA: 1163273.5s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.312s, learning 0.189s)
               Value function loss: 23.6594
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 588.05
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 11.50s
                        Total time: 34161.68s
                               ETA: 1163245.4s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.118s, learning 0.164s)
               Value function loss: 20.8920
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 586.43
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 11.28s
                        Total time: 34172.96s
                               ETA: 1163209.8s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.464s, learning 0.164s)
               Value function loss: 17.2131
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 583.84
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 11.63s
                        Total time: 34184.59s
                               ETA: 1163186.1s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.431s, learning 0.160s)
               Value function loss: 18.3879
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 581.65
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 11.59s
                        Total time: 34196.18s
                               ETA: 1163161.1s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.213s, learning 0.165s)
               Value function loss: 17.4652
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 574.24
               Mean episode length: 249.39
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 11.38s
                        Total time: 34207.56s
                               ETA: 1163128.9s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.173s, learning 0.162s)
               Value function loss: 20.9455
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 572.61
               Mean episode length: 249.39
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 11.34s
                        Total time: 34218.90s
                               ETA: 1163095.2s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.341s, learning 0.162s)
               Value function loss: 19.6732
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 577.57
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 11.50s
                        Total time: 34230.40s
                               ETA: 1163067.3s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.078s, learning 0.168s)
               Value function loss: 24.0449
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 577.28
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 11.25s
                        Total time: 34241.65s
                               ETA: 1163030.7s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.158s, learning 0.189s)
               Value function loss: 22.3632
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 574.61
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 11.35s
                        Total time: 34252.99s
                               ETA: 1162997.5s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.473s, learning 0.184s)
               Value function loss: 23.1994
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 578.66
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 11.66s
                        Total time: 34264.65s
                               ETA: 1162974.8s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.984s, learning 0.159s)
               Value function loss: 36.3493
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 580.16
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 11.14s
                        Total time: 34275.79s
                               ETA: 1162934.7s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.588s, learning 0.161s)
               Value function loss: 23.5445
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 581.17
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 11.75s
                        Total time: 34287.54s
                               ETA: 1162915.1s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.344s, learning 0.159s)
               Value function loss: 21.5598
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 579.74
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 11.50s
                        Total time: 34299.04s
                               ETA: 1162887.2s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.130s, learning 0.186s)
               Value function loss: 24.1320
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 582.19
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 11.32s
                        Total time: 34310.36s
                               ETA: 1162853.0s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.516s, learning 0.195s)
               Value function loss: 20.2100
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 574.86
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 11.71s
                        Total time: 34322.07s
                               ETA: 1162832.2s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.012s, learning 0.167s)
               Value function loss: 19.5087
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 583.07
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 11.18s
                        Total time: 34333.25s
                               ETA: 1162793.4s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.162s)
               Value function loss: 24.1235
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 578.10
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 11.15s
                        Total time: 34344.40s
                               ETA: 1162753.6s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.154s, learning 0.182s)
               Value function loss: 24.0053
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 576.46
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 11.34s
                        Total time: 34355.73s
                               ETA: 1162720.1s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.646s, learning 0.175s)
               Value function loss: 24.8725
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 580.42
               Mean episode length: 249.72
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 11.82s
                        Total time: 34367.56s
                               ETA: 1162703.1s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.605s, learning 0.160s)
               Value function loss: 19.0214
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 581.38
               Mean episode length: 249.72
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 11.77s
                        Total time: 34379.32s
                               ETA: 1162684.2s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.238s, learning 0.191s)
               Value function loss: 29.4902
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 584.14
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 11.43s
                        Total time: 34390.75s
                               ETA: 1162653.9s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.760s, learning 0.188s)
               Value function loss: 26.1041
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 586.35
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 10.95s
                        Total time: 34401.70s
                               ETA: 1162607.4s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.515s, learning 0.165s)
               Value function loss: 27.1552
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 590.31
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 11.68s
                        Total time: 34413.38s
                               ETA: 1162585.7s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.127s, learning 0.163s)
               Value function loss: 32.6097
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 587.02
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 11.29s
                        Total time: 34424.67s
                               ETA: 1162550.7s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.303s, learning 0.175s)
               Value function loss: 23.5250
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 584.99
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 11.48s
                        Total time: 34436.15s
                               ETA: 1162522.2s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.163s)
               Value function loss: 29.9277
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 590.34
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 11.35s
                        Total time: 34447.50s
                               ETA: 1162489.4s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.167s)
               Value function loss: 29.1489
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 590.87
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 11.43s
                        Total time: 34458.93s
                               ETA: 1162459.3s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.943s, learning 0.164s)
               Value function loss: 19.8684
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 590.32
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 11.11s
                        Total time: 34470.04s
                               ETA: 1162418.3s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.115s, learning 0.204s)
               Value function loss: 23.5693
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 592.54
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 11.32s
                        Total time: 34481.36s
                               ETA: 1162384.4s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.533s, learning 0.160s)
               Value function loss: 22.5956
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 590.99
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 11.69s
                        Total time: 34493.05s
                               ETA: 1162363.1s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.110s, learning 0.184s)
               Value function loss: 32.1685
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 589.52
               Mean episode length: 249.68
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 11.29s
                        Total time: 34504.34s
                               ETA: 1162328.4s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.129s, learning 0.156s)
               Value function loss: 34.1605
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 586.43
               Mean episode length: 249.68
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 11.29s
                        Total time: 34515.63s
                               ETA: 1162293.5s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.962s, learning 0.170s)
               Value function loss: 25.7007
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 588.77
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 11.13s
                        Total time: 34526.76s
                               ETA: 1162253.4s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.328s, learning 0.165s)
               Value function loss: 29.6357
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 594.28
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 11.49s
                        Total time: 34538.25s
                               ETA: 1162225.4s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.488s, learning 0.181s)
               Value function loss: 26.2932
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 590.49
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 11.67s
                        Total time: 34549.92s
                               ETA: 1162203.4s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.040s, learning 0.184s)
               Value function loss: 27.5114
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 588.58
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 11.22s
                        Total time: 34561.15s
                               ETA: 1162166.4s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.168s)
               Value function loss: 38.0321
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 594.29
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 11.43s
                        Total time: 34572.58s
                               ETA: 1162136.5s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.260s, learning 0.173s)
               Value function loss: 35.3250
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 592.65
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 11.43s
                        Total time: 34584.01s
                               ETA: 1162106.6s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.203s, learning 0.162s)
               Value function loss: 39.4675
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 593.80
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 11.36s
                        Total time: 34595.38s
                               ETA: 1162074.4s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.020s, learning 0.186s)
               Value function loss: 33.1577
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 593.39
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 11.21s
                        Total time: 34606.58s
                               ETA: 1162036.9s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.178s, learning 0.182s)
               Value function loss: 33.0912
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 591.17
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 11.36s
                        Total time: 34617.94s
                               ETA: 1162004.5s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.936s, learning 0.162s)
               Value function loss: 43.8348
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 587.34
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 11.10s
                        Total time: 34629.04s
                               ETA: 1161963.4s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.676s, learning 0.257s)
               Value function loss: 41.3611
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 585.71
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 11.93s
                        Total time: 34640.97s
                               ETA: 1161950.4s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.880s, learning 0.165s)
               Value function loss: 31.9015
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 588.56
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 11.05s
                        Total time: 34652.02s
                               ETA: 1161907.5s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.114s, learning 0.162s)
               Value function loss: 27.0010
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 584.17
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 11.28s
                        Total time: 34663.30s
                               ETA: 1161872.5s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.255s, learning 0.265s)
               Value function loss: 32.3625
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 585.60
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 11.52s
                        Total time: 34674.81s
                               ETA: 1161845.6s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.183s, learning 0.167s)
               Value function loss: 28.0850
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 584.38
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 11.35s
                        Total time: 34686.16s
                               ETA: 1161813.0s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.026s, learning 0.166s)
               Value function loss: 29.9543
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 586.34
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 11.19s
                        Total time: 34697.36s
                               ETA: 1161775.2s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.165s, learning 0.257s)
               Value function loss: 34.2177
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 589.01
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 11.42s
                        Total time: 34708.78s
                               ETA: 1161745.0s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.295s, learning 0.172s)
               Value function loss: 30.4381
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 590.18
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 11.47s
                        Total time: 34720.24s
                               ETA: 1161716.4s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.416s, learning 0.178s)
               Value function loss: 24.1700
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 583.81
               Mean episode length: 248.69
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 11.59s
                        Total time: 34731.84s
                               ETA: 1161692.1s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.969s, learning 0.169s)
               Value function loss: 27.0619
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 582.01
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 11.14s
                        Total time: 34742.98s
                               ETA: 1161652.5s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.164s)
               Value function loss: 22.4399
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 574.65
               Mean episode length: 249.91
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 11.22s
                        Total time: 34754.20s
                               ETA: 1161615.8s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.472s, learning 0.177s)
               Value function loss: 24.3773
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 571.77
               Mean episode length: 249.91
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 11.65s
                        Total time: 34765.85s
                               ETA: 1161593.3s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.077s, learning 0.166s)
               Value function loss: 28.0371
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 583.94
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 11.24s
                        Total time: 34777.09s
                               ETA: 1161557.3s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.081s, learning 0.170s)
               Value function loss: 21.7893
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 580.17
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 11.25s
                        Total time: 34788.34s
                               ETA: 1161521.5s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.120s, learning 0.166s)
               Value function loss: 23.5278
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 582.33
               Mean episode length: 249.86
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 11.29s
                        Total time: 34799.63s
                               ETA: 1161487.0s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.663s, learning 0.161s)
               Value function loss: 22.9740
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 583.61
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 10.82s
                        Total time: 34810.45s
                               ETA: 1161437.0s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.164s)
               Value function loss: 16.3894
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 582.35
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 11.37s
                        Total time: 34821.82s
                               ETA: 1161405.3s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.969s, learning 0.166s)
               Value function loss: 21.1880
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 583.14
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 11.13s
                        Total time: 34832.96s
                               ETA: 1161365.7s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.013s, learning 0.164s)
               Value function loss: 18.4326
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 573.72
               Mean episode length: 246.57
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 11.18s
                        Total time: 34844.13s
                               ETA: 1161327.6s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.158s, learning 0.174s)
               Value function loss: 21.0876
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 575.94
               Mean episode length: 247.96
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 11.33s
                        Total time: 34855.47s
                               ETA: 1161294.7s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.057s, learning 0.160s)
               Value function loss: 23.9455
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 581.57
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 11.22s
                        Total time: 34866.68s
                               ETA: 1161257.9s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.683s, learning 0.160s)
               Value function loss: 20.7383
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 581.70
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 10.84s
                        Total time: 34877.53s
                               ETA: 1161208.7s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.166s)
               Value function loss: 19.4140
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 585.17
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 11.42s
                        Total time: 34888.95s
                               ETA: 1161178.8s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.165s, learning 0.183s)
               Value function loss: 16.4001
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 575.72
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 11.35s
                        Total time: 34900.30s
                               ETA: 1161146.5s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.794s, learning 0.161s)
               Value function loss: 18.4982
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 579.26
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 10.96s
                        Total time: 34911.25s
                               ETA: 1161101.1s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.165s)
               Value function loss: 20.8412
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 576.96
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 11.40s
                        Total time: 34922.65s
                               ETA: 1161070.5s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.355s, learning 0.162s)
               Value function loss: 20.7226
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 577.12
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 11.52s
                        Total time: 34934.17s
                               ETA: 1161043.9s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.144s, learning 0.183s)
               Value function loss: 21.6875
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 580.09
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 11.33s
                        Total time: 34945.50s
                               ETA: 1161010.9s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.221s, learning 0.192s)
               Value function loss: 19.5862
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 575.77
               Mean episode length: 249.29
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 11.41s
                        Total time: 34956.91s
                               ETA: 1160980.8s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.986s, learning 0.160s)
               Value function loss: 19.8406
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 572.15
               Mean episode length: 249.29
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 11.15s
                        Total time: 34968.06s
                               ETA: 1160941.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.313s, learning 0.157s)
               Value function loss: 25.8333
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 572.56
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 11.47s
                        Total time: 34979.53s
                               ETA: 1160913.7s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.978s, learning 0.176s)
               Value function loss: 26.4195
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 577.51
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 11.15s
                        Total time: 34990.68s
                               ETA: 1160875.0s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.179s)
               Value function loss: 21.9640
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 572.78
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 11.31s
                        Total time: 35001.99s
                               ETA: 1160841.6s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.078s, learning 0.158s)
               Value function loss: 17.4805
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 575.30
               Mean episode length: 249.96
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 11.24s
                        Total time: 35013.23s
                               ETA: 1160805.7s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.059s, learning 0.162s)
               Value function loss: 23.3329
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 576.99
               Mean episode length: 249.96
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 11.22s
                        Total time: 35024.45s
                               ETA: 1160769.3s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.224s, learning 0.160s)
               Value function loss: 16.8810
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 571.34
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 11.38s
                        Total time: 35035.83s
                               ETA: 1160738.4s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.893s, learning 0.167s)
               Value function loss: 20.7884
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 562.86
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 11.06s
                        Total time: 35046.89s
                               ETA: 1160696.7s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.132s, learning 0.160s)
               Value function loss: 14.7565
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 566.44
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 11.29s
                        Total time: 35058.19s
                               ETA: 1160662.7s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.158s)
               Value function loss: 24.7281
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 564.24
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 11.33s
                        Total time: 35069.52s
                               ETA: 1160630.0s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.932s, learning 0.187s)
               Value function loss: 20.1267
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 565.46
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 11.12s
                        Total time: 35080.64s
                               ETA: 1160590.4s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.109s, learning 0.171s)
               Value function loss: 25.7273
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 570.47
               Mean episode length: 249.69
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 11.28s
                        Total time: 35091.92s
                               ETA: 1160556.0s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.435s, learning 0.209s)
               Value function loss: 24.4960
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 572.54
               Mean episode length: 249.69
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 11.64s
                        Total time: 35103.56s
                               ETA: 1160533.8s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.858s, learning 0.160s)
               Value function loss: 19.5932
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 570.75
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 11.02s
                        Total time: 35114.58s
                               ETA: 1160490.8s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.293s, learning 0.161s)
               Value function loss: 28.4535
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 576.72
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 11.45s
                        Total time: 35126.03s
                               ETA: 1160462.2s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.014s, learning 0.160s)
               Value function loss: 19.8212
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 581.64
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 11.17s
                        Total time: 35137.21s
                               ETA: 1160424.5s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.041s, learning 0.160s)
               Value function loss: 22.7201
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 578.33
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 11.20s
                        Total time: 35148.41s
                               ETA: 1160387.6s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.825s, learning 0.160s)
               Value function loss: 26.8433
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 583.52
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 10.98s
                        Total time: 35159.39s
                               ETA: 1160343.6s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.567s, learning 0.225s)
               Value function loss: 18.7471
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 578.39
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 11.79s
                        Total time: 35171.18s
                               ETA: 1160326.3s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.740s, learning 0.166s)
               Value function loss: 24.1538
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 581.27
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 10.91s
                        Total time: 35182.09s
                               ETA: 1160279.8s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.458s, learning 0.191s)
               Value function loss: 20.5788
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 587.79
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 11.65s
                        Total time: 35193.74s
                               ETA: 1160257.7s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.992s, learning 0.159s)
               Value function loss: 24.7547
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 583.00
               Mean episode length: 249.72
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 11.15s
                        Total time: 35204.89s
                               ETA: 1160219.3s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.642s, learning 0.171s)
               Value function loss: 23.3657
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 580.96
               Mean episode length: 249.72
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 11.81s
                        Total time: 35216.70s
                               ETA: 1160202.7s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.162s)
               Value function loss: 29.2970
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 586.10
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 11.48s
                        Total time: 35228.19s
                               ETA: 1160175.2s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.117s, learning 0.163s)
               Value function loss: 26.2113
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 589.48
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 11.28s
                        Total time: 35239.47s
                               ETA: 1160141.1s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.525s, learning 0.163s)
               Value function loss: 23.2380
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 592.43
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 11.69s
                        Total time: 35251.15s
                               ETA: 1160120.3s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.565s, learning 0.161s)
               Value function loss: 25.3503
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 592.50
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 11.73s
                        Total time: 35262.88s
                               ETA: 1160100.9s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.223s, learning 0.161s)
               Value function loss: 28.2161
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 587.53
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 11.38s
                        Total time: 35274.26s
                               ETA: 1160070.2s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.254s, learning 0.162s)
               Value function loss: 31.6147
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 592.30
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 11.42s
                        Total time: 35285.68s
                               ETA: 1160040.6s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.455s, learning 0.162s)
               Value function loss: 32.9437
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 592.23
               Mean episode length: 250.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 11.62s
                        Total time: 35297.30s
                               ETA: 1160017.6s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.165s)
               Value function loss: 27.3576
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 597.78
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 11.14s
                        Total time: 35308.44s
                               ETA: 1159979.1s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.785s, learning 0.165s)
               Value function loss: 32.5005
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 592.68
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 10.95s
                        Total time: 35319.39s
                               ETA: 1159934.2s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.463s, learning 0.176s)
               Value function loss: 41.8273
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 591.84
               Mean episode length: 250.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 11.64s
                        Total time: 35331.03s
                               ETA: 1159911.9s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.058s, learning 0.171s)
               Value function loss: 54.1790
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 595.17
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 11.23s
                        Total time: 35342.26s
                               ETA: 1159876.3s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.212s, learning 0.236s)
               Value function loss: 33.4729
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 600.99
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 11.45s
                        Total time: 35353.71s
                               ETA: 1159847.8s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.148s, learning 0.163s)
               Value function loss: 30.9284
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 596.92
               Mean episode length: 249.92
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 11.31s
                        Total time: 35365.02s
                               ETA: 1159814.8s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.164s)
               Value function loss: 38.8938
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 591.37
               Mean episode length: 249.92
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 11.38s
                        Total time: 35376.40s
                               ETA: 1159784.1s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.163s)
               Value function loss: 32.1462
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 597.15
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 11.34s
                        Total time: 35387.74s
                               ETA: 1159752.1s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.787s, learning 0.165s)
               Value function loss: 32.8025
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 603.97
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 10.95s
                        Total time: 35398.69s
                               ETA: 1159707.5s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.164s)
               Value function loss: 28.3519
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 603.76
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 11.91s
                        Total time: 35410.60s
                               ETA: 1159694.2s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.334s, learning 0.175s)
               Value function loss: 34.0622
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 603.78
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 11.51s
                        Total time: 35422.11s
                               ETA: 1159667.8s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.046s, learning 0.169s)
               Value function loss: 31.4921
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 599.27
               Mean episode length: 247.92
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 11.21s
                        Total time: 35433.33s
                               ETA: 1159631.8s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.166s)
               Value function loss: 29.9014
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 597.22
               Mean episode length: 247.92
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 11.44s
                        Total time: 35444.76s
                               ETA: 1159603.0s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.442s, learning 0.162s)
               Value function loss: 33.2305
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 603.85
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 11.60s
                        Total time: 35456.37s
                               ETA: 1159579.7s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.147s, learning 0.165s)
               Value function loss: 26.9796
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 601.08
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 11.31s
                        Total time: 35467.68s
                               ETA: 1159546.9s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.069s, learning 0.181s)
               Value function loss: 28.7409
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 603.84
               Mean episode length: 248.44
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 11.25s
                        Total time: 35478.93s
                               ETA: 1159512.0s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.346s, learning 0.172s)
               Value function loss: 27.7304
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 600.74
               Mean episode length: 248.10
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 11.52s
                        Total time: 35490.44s
                               ETA: 1159486.0s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.980s, learning 0.196s)
               Value function loss: 26.3760
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 603.73
               Mean episode length: 248.50
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 11.18s
                        Total time: 35501.62s
                               ETA: 1159448.7s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.160s)
               Value function loss: 29.6960
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 607.09
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 11.09s
                        Total time: 35512.71s
                               ETA: 1159408.9s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.118s, learning 0.163s)
               Value function loss: 26.9866
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 604.02
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 11.28s
                        Total time: 35524.00s
                               ETA: 1159375.1s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.948s, learning 0.161s)
               Value function loss: 23.9619
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 603.90
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 11.11s
                        Total time: 35535.10s
                               ETA: 1159335.8s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.954s, learning 0.161s)
               Value function loss: 23.7908
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 606.34
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 11.12s
                        Total time: 35546.22s
                               ETA: 1159296.7s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.157s, learning 0.168s)
               Value function loss: 28.2234
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 601.78
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 11.33s
                        Total time: 35557.55s
                               ETA: 1159264.4s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.857s, learning 0.161s)
               Value function loss: 28.3910
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 601.80
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 11.02s
                        Total time: 35568.56s
                               ETA: 1159222.1s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.572s, learning 0.171s)
               Value function loss: 36.2331
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 599.64
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 11.74s
                        Total time: 35580.31s
                               ETA: 1159203.5s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.071s, learning 0.162s)
               Value function loss: 34.4399
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 600.04
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 11.23s
                        Total time: 35591.54s
                               ETA: 1159168.3s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.044s, learning 0.159s)
               Value function loss: 28.6243
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 601.56
               Mean episode length: 250.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 11.20s
                        Total time: 35602.74s
                               ETA: 1159132.1s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.162s)
               Value function loss: 26.8609
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 603.40
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 11.17s
                        Total time: 35613.91s
                               ETA: 1159094.7s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.314s, learning 0.161s)
               Value function loss: 36.5412
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 603.38
               Mean episode length: 249.40
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 11.47s
                        Total time: 35625.38s
                               ETA: 1159067.3s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.490s, learning 0.158s)
               Value function loss: 36.4648
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 606.52
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 11.65s
                        Total time: 35637.03s
                               ETA: 1159045.7s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.121s, learning 0.227s)
               Value function loss: 29.6955
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 603.97
               Mean episode length: 250.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 11.35s
                        Total time: 35648.38s
                               ETA: 1159014.3s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.938s, learning 0.182s)
               Value function loss: 34.7604
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 599.29
               Mean episode length: 250.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 11.12s
                        Total time: 35659.50s
                               ETA: 1158975.5s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.892s, learning 0.161s)
               Value function loss: 39.0177
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 598.63
               Mean episode length: 249.39
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 11.05s
                        Total time: 35670.55s
                               ETA: 1158934.5s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.462s, learning 0.163s)
               Value function loss: 41.6403
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 595.01
               Mean episode length: 249.35
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 11.62s
                        Total time: 35682.18s
                               ETA: 1158912.2s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.169s)
               Value function loss: 45.4552
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: 596.63
               Mean episode length: 249.96
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 11.53s
                        Total time: 35693.70s
                               ETA: 1158886.7s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.990s, learning 0.159s)
               Value function loss: 34.5694
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 600.53
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 11.15s
                        Total time: 35704.85s
                               ETA: 1158848.9s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.166s)
               Value function loss: 38.7402
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 600.78
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 10.72s
                        Total time: 35715.58s
                               ETA: 1158797.3s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.163s)
               Value function loss: 35.0991
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 602.27
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 11.28s
                        Total time: 35726.86s
                               ETA: 1158763.9s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.325s, learning 0.164s)
               Value function loss: 35.9662
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 603.45
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 11.49s
                        Total time: 35738.35s
                               ETA: 1158737.2s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.164s)
               Value function loss: 27.5323
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 603.26
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 11.31s
                        Total time: 35749.66s
                               ETA: 1158704.5s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.433s, learning 0.173s)
               Value function loss: 34.4381
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 595.92
               Mean episode length: 249.33
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 11.61s
                        Total time: 35761.26s
                               ETA: 1158681.6s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.072s, learning 0.164s)
               Value function loss: 33.5726
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 589.96
               Mean episode length: 249.33
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 11.24s
                        Total time: 35772.50s
                               ETA: 1158646.8s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.166s)
               Value function loss: 29.3861
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 587.09
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 11.39s
                        Total time: 35783.89s
                               ETA: 1158616.9s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.031s, learning 0.256s)
               Value function loss: 27.6776
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 590.82
               Mean episode length: 249.94
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 11.29s
                        Total time: 35795.18s
                               ETA: 1158583.7s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.104s, learning 0.180s)
               Value function loss: 36.3930
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 584.70
               Mean episode length: 249.94
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 11.28s
                        Total time: 35806.46s
                               ETA: 1158550.4s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.166s)
               Value function loss: 27.6104
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 587.32
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 11.42s
                        Total time: 35817.88s
                               ETA: 1158521.6s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.167s)
               Value function loss: 30.8914
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 592.91
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 11.49s
                        Total time: 35829.37s
                               ETA: 1158494.9s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.152s, learning 0.162s)
               Value function loss: 35.0259
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 591.22
               Mean episode length: 249.60
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 11.31s
                        Total time: 35840.68s
                               ETA: 1158462.6s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.924s, learning 0.163s)
               Value function loss: 23.4939
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 587.77
               Mean episode length: 249.60
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 11.09s
                        Total time: 35851.77s
                               ETA: 1158423.0s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.281s, learning 0.169s)
               Value function loss: 32.7678
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 588.08
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 11.45s
                        Total time: 35863.22s
                               ETA: 1158395.2s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.121s, learning 0.161s)
               Value function loss: 32.5241
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 594.68
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 11.28s
                        Total time: 35874.50s
                               ETA: 1158361.9s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.812s, learning 0.159s)
               Value function loss: 20.6136
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 595.27
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 10.97s
                        Total time: 35885.47s
                               ETA: 1158318.6s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.216s, learning 0.176s)
               Value function loss: 25.6628
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 593.55
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 11.39s
                        Total time: 35896.87s
                               ETA: 1158288.9s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.161s)
               Value function loss: 23.4677
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 596.35
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 11.04s
                        Total time: 35907.91s
                               ETA: 1158248.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.123s, learning 0.162s)
               Value function loss: 28.2340
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 594.05
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 11.29s
                        Total time: 35919.20s
                               ETA: 1158214.9s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.110s, learning 0.173s)
               Value function loss: 35.7876
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 590.10
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 11.28s
                        Total time: 35930.48s
                               ETA: 1158181.8s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.569s, learning 0.176s)
               Value function loss: 26.2065
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 589.20
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 11.75s
                        Total time: 35942.22s
                               ETA: 1158163.5s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.380s, learning 0.173s)
               Value function loss: 25.8038
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 591.02
               Mean episode length: 250.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 11.55s
                        Total time: 35953.78s
                               ETA: 1158139.1s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.993s, learning 0.186s)
               Value function loss: 22.4576
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 589.69
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 11.18s
                        Total time: 35964.96s
                               ETA: 1158102.6s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.063s, learning 0.168s)
               Value function loss: 26.1168
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 584.13
               Mean episode length: 250.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 11.23s
                        Total time: 35976.19s
                               ETA: 1158067.9s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.891s, learning 0.164s)
               Value function loss: 33.2951
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 589.28
               Mean episode length: 250.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 11.05s
                        Total time: 35987.24s
                               ETA: 1158027.4s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.983s, learning 0.176s)
               Value function loss: 27.5962
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 594.30
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 11.16s
                        Total time: 35998.40s
                               ETA: 1157990.4s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.286s, learning 0.193s)
               Value function loss: 29.6956
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 588.03
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 11.48s
                        Total time: 36009.88s
                               ETA: 1157963.6s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.161s)
               Value function loss: 27.0805
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 581.08
               Mean episode length: 249.01
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 11.15s
                        Total time: 36021.03s
                               ETA: 1157926.2s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.313s, learning 0.161s)
               Value function loss: 28.1151
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 575.92
               Mean episode length: 249.01
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 11.47s
                        Total time: 36032.50s
                               ETA: 1157899.3s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.070s, learning 0.164s)
               Value function loss: 35.7119
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 583.15
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 11.23s
                        Total time: 36043.74s
                               ETA: 1157864.8s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.162s)
               Value function loss: 30.8205
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 586.57
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 11.51s
                        Total time: 36055.25s
                               ETA: 1157839.0s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.285s, learning 0.159s)
               Value function loss: 23.7000
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 581.78
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 11.44s
                        Total time: 36066.69s
                               ETA: 1157811.2s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.163s)
               Value function loss: 22.0410
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 586.42
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 11.45s
                        Total time: 36078.14s
                               ETA: 1157783.5s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.381s, learning 0.250s)
               Value function loss: 24.2869
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 588.16
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 11.63s
                        Total time: 36089.77s
                               ETA: 1157761.7s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.378s, learning 0.172s)
               Value function loss: 18.2332
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 578.67
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 11.55s
                        Total time: 36101.32s
                               ETA: 1157737.3s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.167s)
               Value function loss: 28.6511
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 579.40
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 11.12s
                        Total time: 36112.44s
                               ETA: 1157699.0s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.950s, learning 0.166s)
               Value function loss: 22.3439
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 577.31
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 11.12s
                        Total time: 36123.55s
                               ETA: 1157660.7s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.683s, learning 0.182s)
               Value function loss: 22.1129
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 581.21
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 11.87s
                        Total time: 36135.42s
                               ETA: 1157646.5s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.259s, learning 0.187s)
               Value function loss: 18.9349
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 575.12
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 11.45s
                        Total time: 36146.86s
                               ETA: 1157618.8s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.434s, learning 0.187s)
               Value function loss: 27.7258
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 573.35
               Mean episode length: 249.63
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 11.62s
                        Total time: 36158.48s
                               ETA: 1157596.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.154s, learning 0.194s)
               Value function loss: 21.7933
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 582.02
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 11.35s
                        Total time: 36169.83s
                               ETA: 1157566.0s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.561s, learning 0.170s)
               Value function loss: 24.1094
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 582.84
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.73s
                        Total time: 36181.56s
                               ETA: 1157547.4s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.620s, learning 0.161s)
               Value function loss: 26.1689
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 587.13
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 11.78s
                        Total time: 36193.35s
                               ETA: 1157530.5s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.034s, learning 0.161s)
               Value function loss: 19.5957
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 584.39
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 11.20s
                        Total time: 36204.54s
                               ETA: 1157494.9s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.410s, learning 0.165s)
               Value function loss: 22.8399
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 584.91
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 11.57s
                        Total time: 36216.12s
                               ETA: 1157471.4s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.058s, learning 0.162s)
               Value function loss: 21.0782
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 588.42
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 11.22s
                        Total time: 36227.34s
                               ETA: 1157436.5s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.171s, learning 0.169s)
               Value function loss: 15.0251
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 588.38
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 11.34s
                        Total time: 36238.68s
                               ETA: 1157405.5s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.360s, learning 0.239s)
               Value function loss: 19.3993
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 586.43
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 11.60s
                        Total time: 36250.27s
                               ETA: 1157382.8s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.284s, learning 0.167s)
               Value function loss: 16.1186
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 580.77
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 11.45s
                        Total time: 36261.73s
                               ETA: 1157355.4s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.027s, learning 0.167s)
               Value function loss: 19.2358
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 579.59
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 11.19s
                        Total time: 36272.92s
                               ETA: 1157319.8s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.217s, learning 0.221s)
               Value function loss: 21.3716
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 573.20
               Mean episode length: 249.60
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 11.44s
                        Total time: 36284.36s
                               ETA: 1157292.0s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.618s, learning 0.191s)
               Value function loss: 17.6045
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 572.77
               Mean episode length: 249.60
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 11.81s
                        Total time: 36296.17s
                               ETA: 1157276.0s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.331s, learning 0.168s)
               Value function loss: 18.0649
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 575.24
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 11.50s
                        Total time: 36307.67s
                               ETA: 1157250.1s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.185s, learning 0.163s)
               Value function loss: 12.3683
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 573.00
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 11.35s
                        Total time: 36319.01s
                               ETA: 1157219.5s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.605s, learning 0.188s)
               Value function loss: 15.9792
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 572.97
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 11.79s
                        Total time: 36330.81s
                               ETA: 1157203.0s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.152s, learning 0.201s)
               Value function loss: 18.9364
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 561.50
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 11.35s
                        Total time: 36342.16s
                               ETA: 1157172.6s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.310s, learning 0.161s)
               Value function loss: 18.9098
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 551.08
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 11.47s
                        Total time: 36353.63s
                               ETA: 1157145.9s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.480s, learning 0.190s)
               Value function loss: 19.9750
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 552.60
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 11.67s
                        Total time: 36365.30s
                               ETA: 1157125.5s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.162s)
               Value function loss: 22.0913
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 549.15
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 11.68s
                        Total time: 36376.98s
                               ETA: 1157105.3s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.162s)
               Value function loss: 19.5791
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 550.81
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 11.36s
                        Total time: 36388.33s
                               ETA: 1157075.1s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.941s, learning 0.162s)
               Value function loss: 25.8127
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 545.51
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 11.10s
                        Total time: 36399.44s
                               ETA: 1157036.7s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.254s, learning 0.159s)
               Value function loss: 25.6670
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 540.41
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 11.41s
                        Total time: 36410.85s
                               ETA: 1157008.2s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.268s, learning 0.160s)
               Value function loss: 19.6027
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 545.85
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 11.43s
                        Total time: 36422.28s
                               ETA: 1156980.2s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.330s, learning 0.194s)
               Value function loss: 16.1167
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 542.24
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 11.52s
                        Total time: 36433.80s
                               ETA: 1156955.2s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.399s, learning 0.189s)
               Value function loss: 20.1697
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 537.14
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 11.59s
                        Total time: 36445.39s
                               ETA: 1156932.3s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.165s)
               Value function loss: 14.5113
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 543.03
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 11.45s
                        Total time: 36456.84s
                               ETA: 1156904.9s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.163s)
               Value function loss: 21.7622
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 542.50
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 11.34s
                        Total time: 36468.17s
                               ETA: 1156874.1s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.296s, learning 0.183s)
               Value function loss: 15.9023
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 541.12
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 11.48s
                        Total time: 36479.65s
                               ETA: 1156847.8s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.035s, learning 0.161s)
               Value function loss: 18.7879
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 541.80
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 11.20s
                        Total time: 36490.85s
                               ETA: 1156812.5s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.847s, learning 0.161s)
               Value function loss: 15.2048
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 544.08
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 11.01s
                        Total time: 36501.86s
                               ETA: 1156771.2s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.402s, learning 0.170s)
               Value function loss: 24.6626
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 552.86
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 11.57s
                        Total time: 36513.43s
                               ETA: 1156747.8s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.161s)
               Value function loss: 20.6789
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 554.77
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 11.36s
                        Total time: 36524.79s
                               ETA: 1156717.8s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.438s, learning 0.163s)
               Value function loss: 15.4456
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 543.24
               Mean episode length: 249.66
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 11.60s
                        Total time: 36536.39s
                               ETA: 1156695.4s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.811s, learning 0.180s)
               Value function loss: 22.4528
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 551.42
               Mean episode length: 249.66
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 11.99s
                        Total time: 36548.38s
                               ETA: 1156685.3s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.359s, learning 0.171s)
               Value function loss: 16.3439
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 554.18
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 11.53s
                        Total time: 36559.91s
                               ETA: 1156660.7s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.185s, learning 0.202s)
               Value function loss: 19.4460
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 550.53
               Mean episode length: 249.72
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 11.39s
                        Total time: 36571.30s
                               ETA: 1156631.5s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.962s, learning 0.161s)
               Value function loss: 20.4615
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 552.68
               Mean episode length: 249.72
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 11.12s
                        Total time: 36582.42s
                               ETA: 1156594.0s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.847s, learning 0.163s)
               Value function loss: 16.8293
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 555.08
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 11.01s
                        Total time: 36593.43s
                               ETA: 1156552.9s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.544s, learning 0.161s)
               Value function loss: 20.8053
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 553.59
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 11.71s
                        Total time: 36605.14s
                               ETA: 1156533.8s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.167s, learning 0.160s)
               Value function loss: 17.1035
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 551.38
               Mean episode length: 249.28
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 11.33s
                        Total time: 36616.47s
                               ETA: 1156502.8s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.097s, learning 0.159s)
               Value function loss: 17.4507
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 553.84
               Mean episode length: 248.38
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 11.26s
                        Total time: 36627.72s
                               ETA: 1156469.6s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.349s, learning 0.182s)
               Value function loss: 20.1751
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 551.56
               Mean episode length: 247.61
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 11.53s
                        Total time: 36639.25s
                               ETA: 1156445.0s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.166s)
               Value function loss: 24.3350
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 544.53
               Mean episode length: 245.80
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 11.53s
                        Total time: 36650.78s
                               ETA: 1156420.3s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.070s, learning 0.172s)
               Value function loss: 18.7677
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 551.02
               Mean episode length: 247.10
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 11.24s
                        Total time: 36662.02s
                               ETA: 1156386.7s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.875s, learning 0.183s)
               Value function loss: 18.1839
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 547.93
               Mean episode length: 245.66
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 11.06s
                        Total time: 36673.08s
                               ETA: 1156347.2s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.162s)
               Value function loss: 18.9773
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 534.94
               Mean episode length: 242.78
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 11.25s
                        Total time: 36684.32s
                               ETA: 1156313.8s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.068s, learning 0.178s)
               Value function loss: 22.4181
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 535.53
               Mean episode length: 243.45
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 11.25s
                        Total time: 36695.57s
                               ETA: 1156280.3s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.161s)
               Value function loss: 21.7396
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 547.22
               Mean episode length: 245.09
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 11.15s
                        Total time: 36706.72s
                               ETA: 1156243.7s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.978s, learning 0.177s)
               Value function loss: 23.2927
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 542.81
               Mean episode length: 244.88
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 11.15s
                        Total time: 36717.87s
                               ETA: 1156207.4s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.680s, learning 0.159s)
               Value function loss: 23.3311
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 550.56
               Mean episode length: 245.60
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 11.84s
                        Total time: 36729.71s
                               ETA: 1156192.6s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.465s, learning 0.164s)
               Value function loss: 27.0728
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 553.98
               Mean episode length: 246.09
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 11.63s
                        Total time: 36741.34s
                               ETA: 1156171.2s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.029s, learning 0.173s)
               Value function loss: 35.6060
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 557.82
               Mean episode length: 245.52
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 11.20s
                        Total time: 36752.54s
                               ETA: 1156136.4s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.281s, learning 0.162s)
               Value function loss: 36.2989
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 557.16
               Mean episode length: 245.32
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 11.44s
                        Total time: 36763.98s
                               ETA: 1156109.2s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.482s, learning 0.165s)
               Value function loss: 24.1717
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 566.47
               Mean episode length: 247.19
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 11.65s
                        Total time: 36775.63s
                               ETA: 1156088.5s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.091s, learning 0.159s)
               Value function loss: 19.9819
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 559.83
               Mean episode length: 247.16
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 11.25s
                        Total time: 36786.88s
                               ETA: 1156055.2s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.737s, learning 0.166s)
               Value function loss: 26.4714
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 550.34
               Mean episode length: 245.71
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 11.90s
                        Total time: 36798.79s
                               ETA: 1156042.5s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1468 steps/s (collection: 11.003s, learning 0.156s)
               Value function loss: 27.0019
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 551.02
               Mean episode length: 243.54
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 11.16s
                        Total time: 36809.95s
                               ETA: 1156006.4s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.162s)
               Value function loss: 24.1393
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 568.06
               Mean episode length: 248.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 11.25s
                        Total time: 36821.19s
                               ETA: 1155973.1s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.160s)
               Value function loss: 24.7811
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 569.90
               Mean episode length: 249.82
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 10.84s
                        Total time: 36832.04s
                               ETA: 1155927.2s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.947s, learning 0.160s)
               Value function loss: 30.8817
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 582.98
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 11.11s
                        Total time: 36843.14s
                               ETA: 1155889.5s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.276s, learning 0.191s)
               Value function loss: 33.5496
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 576.34
               Mean episode length: 248.33
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 11.47s
                        Total time: 36854.61s
                               ETA: 1155863.2s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.129s, learning 0.190s)
               Value function loss: 30.9110
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 575.44
               Mean episode length: 246.83
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 11.32s
                        Total time: 36865.93s
                               ETA: 1155832.2s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.928s, learning 0.164s)
               Value function loss: 30.7440
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 585.46
               Mean episode length: 249.27
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 11.09s
                        Total time: 36877.02s
                               ETA: 1155794.1s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.075s, learning 0.164s)
               Value function loss: 28.1467
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 582.16
               Mean episode length: 249.26
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 11.24s
                        Total time: 36888.26s
                               ETA: 1155760.6s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.302s, learning 0.162s)
               Value function loss: 31.6547
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 571.44
               Mean episode length: 248.60
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 11.46s
                        Total time: 36899.72s
                               ETA: 1155734.2s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.162s)
               Value function loss: 27.0956
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 578.37
               Mean episode length: 248.98
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 11.52s
                        Total time: 36911.25s
                               ETA: 1155709.7s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.259s, learning 0.158s)
               Value function loss: 23.0960
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 582.90
               Mean episode length: 249.09
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 11.42s
                        Total time: 36922.66s
                               ETA: 1155681.8s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.361s, learning 0.174s)
               Value function loss: 30.4090
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 579.58
               Mean episode length: 248.54
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 11.54s
                        Total time: 36934.20s
                               ETA: 1155657.6s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.185s, learning 0.165s)
               Value function loss: 29.7859
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 582.89
               Mean episode length: 248.44
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 11.35s
                        Total time: 36945.55s
                               ETA: 1155627.7s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.227s, learning 0.189s)
               Value function loss: 21.1272
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 576.16
               Mean episode length: 247.63
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 11.42s
                        Total time: 36956.97s
                               ETA: 1155599.8s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.500s, learning 0.185s)
               Value function loss: 21.9981
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 572.89
               Mean episode length: 247.32
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 11.69s
                        Total time: 36968.65s
                               ETA: 1155580.4s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.050s, learning 0.173s)
               Value function loss: 29.9765
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 579.09
               Mean episode length: 247.26
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 11.22s
                        Total time: 36979.87s
                               ETA: 1155546.5s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.192s, learning 0.163s)
               Value function loss: 24.7941
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 583.19
               Mean episode length: 249.22
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 11.35s
                        Total time: 36991.23s
                               ETA: 1155516.8s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.180s, learning 0.162s)
               Value function loss: 33.8765
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 575.39
               Mean episode length: 248.67
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 11.34s
                        Total time: 37002.57s
                               ETA: 1155486.6s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.959s, learning 0.169s)
               Value function loss: 27.9145
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 581.35
               Mean episode length: 248.52
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 11.13s
                        Total time: 37013.70s
                               ETA: 1155449.8s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.168s)
               Value function loss: 24.2438
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: 585.10
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 11.51s
                        Total time: 37025.21s
                               ETA: 1155425.1s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.094s, learning 0.166s)
               Value function loss: 25.0938
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 586.18
               Mean episode length: 250.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 11.26s
                        Total time: 37036.47s
                               ETA: 1155392.5s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.366s, learning 0.162s)
               Value function loss: 27.1758
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 583.81
               Mean episode length: 249.09
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 11.53s
                        Total time: 37048.00s
                               ETA: 1155368.2s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.158s)
               Value function loss: 30.9326
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 571.53
               Mean episode length: 246.60
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 11.36s
                        Total time: 37059.36s
                               ETA: 1155338.6s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.294s, learning 0.171s)
               Value function loss: 27.1575
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 583.93
               Mean episode length: 248.07
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 11.46s
                        Total time: 37070.82s
                               ETA: 1155312.4s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.323s, learning 0.165s)
               Value function loss: 29.3762
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 578.05
               Mean episode length: 248.83
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 11.49s
                        Total time: 37082.31s
                               ETA: 1155286.9s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.516s, learning 0.162s)
               Value function loss: 32.1869
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 577.71
               Mean episode length: 249.79
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 11.68s
                        Total time: 37093.99s
                               ETA: 1155267.3s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.424s, learning 0.175s)
               Value function loss: 34.2968
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 583.24
               Mean episode length: 249.16
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 11.60s
                        Total time: 37105.59s
                               ETA: 1155245.3s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.266s, learning 0.187s)
               Value function loss: 36.8770
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 576.19
               Mean episode length: 248.34
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 11.45s
                        Total time: 37117.04s
                               ETA: 1155218.7s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.281s, learning 0.161s)
               Value function loss: 31.2657
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 585.91
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 11.44s
                        Total time: 37128.48s
                               ETA: 1155191.8s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.505s, learning 0.157s)
               Value function loss: 30.4639
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 583.50
               Mean episode length: 248.39
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 11.66s
                        Total time: 37140.15s
                               ETA: 1155171.8s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.166s)
               Value function loss: 33.4819
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 584.68
               Mean episode length: 249.20
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 11.34s
                        Total time: 37151.48s
                               ETA: 1155141.7s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.093s, learning 0.185s)
               Value function loss: 31.5223
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 592.45
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 11.28s
                        Total time: 37162.76s
                               ETA: 1155109.7s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.318s, learning 0.169s)
               Value function loss: 22.6347
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 590.84
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 11.49s
                        Total time: 37174.25s
                               ETA: 1155084.2s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.522s, learning 0.162s)
               Value function loss: 27.5294
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 589.06
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 11.68s
                        Total time: 37185.93s
                               ETA: 1155064.9s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.134s, learning 0.167s)
               Value function loss: 24.7678
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 582.16
               Mean episode length: 249.19
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 11.30s
                        Total time: 37197.23s
                               ETA: 1155033.7s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.407s, learning 0.173s)
               Value function loss: 29.2048
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 586.43
               Mean episode length: 249.34
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 11.58s
                        Total time: 37208.81s
                               ETA: 1155011.1s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.052s, learning 0.159s)
               Value function loss: 24.0120
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 595.07
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 11.21s
                        Total time: 37220.02s
                               ETA: 1154977.1s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.007s, learning 0.160s)
               Value function loss: 27.2839
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 595.20
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 11.17s
                        Total time: 37231.19s
                               ETA: 1154941.8s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.111s, learning 0.162s)
               Value function loss: 20.2001
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 587.22
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 11.27s
                        Total time: 37242.46s
                               ETA: 1154909.7s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.462s, learning 0.173s)
               Value function loss: 21.3853
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 582.35
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 11.64s
                        Total time: 37254.10s
                               ETA: 1154889.0s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.345s, learning 0.163s)
               Value function loss: 26.9453
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 585.91
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 11.51s
                        Total time: 37265.61s
                               ETA: 1154864.2s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.280s, learning 0.159s)
               Value function loss: 16.7417
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 580.68
               Mean episode length: 248.35
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 11.44s
                        Total time: 37277.04s
                               ETA: 1154837.4s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.055s, learning 0.186s)
               Value function loss: 23.1880
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 591.13
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 12.24s
                        Total time: 37289.29s
                               ETA: 1154835.4s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.875s, learning 0.168s)
               Value function loss: 22.7952
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 589.64
               Mean episode length: 250.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 11.04s
                        Total time: 37300.33s
                               ETA: 1154796.3s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.220s, learning 0.166s)
               Value function loss: 14.0154
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 588.45
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 11.39s
                        Total time: 37311.72s
                               ETA: 1154767.8s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.389s, learning 0.168s)
               Value function loss: 18.2417
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 583.25
               Mean episode length: 250.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 11.56s
                        Total time: 37323.27s
                               ETA: 1154744.6s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.271s, learning 0.175s)
               Value function loss: 16.7190
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 583.65
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 11.45s
                        Total time: 37334.72s
                               ETA: 1154718.0s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.924s, learning 0.174s)
               Value function loss: 22.1048
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 581.87
               Mean episode length: 248.97
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 11.10s
                        Total time: 37345.82s
                               ETA: 1154680.7s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.434s, learning 0.163s)
               Value function loss: 24.0870
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 578.19
               Mean episode length: 250.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 11.60s
                        Total time: 37357.41s
                               ETA: 1154658.8s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.352s, learning 0.161s)
               Value function loss: 20.9215
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 584.42
               Mean episode length: 250.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 11.51s
                        Total time: 37368.93s
                               ETA: 1154634.3s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.271s, learning 0.160s)
               Value function loss: 18.3859
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 581.71
               Mean episode length: 249.48
                  Mean reward/step: 2.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 11.43s
                        Total time: 37380.36s
                               ETA: 1154607.3s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.083s, learning 0.171s)
               Value function loss: 18.2956
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 584.90
               Mean episode length: 249.39
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 11.25s
                        Total time: 37391.61s
                               ETA: 1154574.8s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.145s, learning 0.174s)
               Value function loss: 18.6129
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 581.68
               Mean episode length: 249.08
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 11.32s
                        Total time: 37402.93s
                               ETA: 1154544.3s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.158s, learning 0.173s)
               Value function loss: 26.4098
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 581.71
               Mean episode length: 249.17
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 11.33s
                        Total time: 37414.26s
                               ETA: 1154514.2s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.237s, learning 0.194s)
               Value function loss: 22.4337
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 585.53
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 11.43s
                        Total time: 37425.69s
                               ETA: 1154487.2s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.167s)
               Value function loss: 21.7460
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 586.64
               Mean episode length: 250.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 11.52s
                        Total time: 37437.21s
                               ETA: 1154463.0s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.103s, learning 0.171s)
               Value function loss: 20.6972
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 577.65
               Mean episode length: 248.82
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 11.27s
                        Total time: 37448.49s
                               ETA: 1154431.2s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.164s)
               Value function loss: 22.0771
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 564.85
               Mean episode length: 248.10
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 11.29s
                        Total time: 37459.78s
                               ETA: 1154400.1s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.311s, learning 0.179s)
               Value function loss: 26.2704
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 570.81
               Mean episode length: 249.28
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 11.49s
                        Total time: 37471.27s
                               ETA: 1154374.9s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.165s)
               Value function loss: 27.8705
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 583.60
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 11.39s
                        Total time: 37482.66s
                               ETA: 1154346.7s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.126s, learning 0.165s)
               Value function loss: 23.5327
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 573.99
               Mean episode length: 249.50
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 11.29s
                        Total time: 37493.95s
                               ETA: 1154315.5s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.747s, learning 0.168s)
               Value function loss: 19.7194
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 574.21
               Mean episode length: 249.45
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 11.91s
                        Total time: 37505.86s
                               ETA: 1154303.4s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.347s, learning 0.183s)
               Value function loss: 22.8092
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 579.42
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 11.53s
                        Total time: 37517.39s
                               ETA: 1154279.6s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.099s, learning 0.164s)
               Value function loss: 16.2581
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 575.81
               Mean episode length: 249.18
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 11.26s
                        Total time: 37528.66s
                               ETA: 1154247.5s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.325s, learning 0.188s)
               Value function loss: 23.3817
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 570.14
               Mean episode length: 249.18
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 11.51s
                        Total time: 37540.17s
                               ETA: 1154223.1s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.163s)
               Value function loss: 18.9394
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 575.41
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 11.14s
                        Total time: 37551.31s
                               ETA: 1154187.4s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.043s, learning 0.165s)
               Value function loss: 20.8363
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 570.33
               Mean episode length: 249.37
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 11.21s
                        Total time: 37562.52s
                               ETA: 1154153.7s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.104s, learning 0.175s)
               Value function loss: 17.8367
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 570.46
               Mean episode length: 249.42
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 11.28s
                        Total time: 37573.80s
                               ETA: 1154122.1s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.992s, learning 0.168s)
               Value function loss: 22.3708
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 576.19
               Mean episode length: 249.42
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 11.16s
                        Total time: 37584.96s
                               ETA: 1154087.0s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.272s, learning 0.164s)
               Value function loss: 19.1259
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 580.14
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 11.44s
                        Total time: 37596.39s
                               ETA: 1154060.3s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.274s, learning 0.184s)
               Value function loss: 22.8114
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 576.73
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 11.46s
                        Total time: 37607.85s
                               ETA: 1154034.3s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.214s, learning 0.180s)
               Value function loss: 21.4250
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 579.22
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 11.39s
                        Total time: 37619.25s
                               ETA: 1154006.4s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.383s, learning 0.175s)
               Value function loss: 19.1447
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 582.67
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 11.56s
                        Total time: 37630.80s
                               ETA: 1153983.5s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.996s, learning 0.169s)
               Value function loss: 20.6889
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 577.80
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 11.16s
                        Total time: 37641.97s
                               ETA: 1153948.5s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.166s)
               Value function loss: 21.7554
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 576.15
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 11.18s
                        Total time: 37653.15s
                               ETA: 1153914.1s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.480s, learning 0.169s)
               Value function loss: 14.6942
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 576.37
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 11.65s
                        Total time: 37664.80s
                               ETA: 1153894.0s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.255s, learning 0.181s)
               Value function loss: 17.6784
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 569.27
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 11.44s
                        Total time: 37676.23s
                               ETA: 1153867.4s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.456s, learning 0.158s)
               Value function loss: 12.0075
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 569.86
               Mean episode length: 250.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 10.61s
                        Total time: 37686.85s
                               ETA: 1153815.7s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.531s, learning 0.169s)
               Value function loss: 22.1025
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 569.02
               Mean episode length: 249.68
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 11.70s
                        Total time: 37698.55s
                               ETA: 1153797.2s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.622s, learning 0.159s)
               Value function loss: 23.3166
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: 574.58
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 11.78s
                        Total time: 37710.33s
                               ETA: 1153781.2s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.856s, learning 0.161s)
               Value function loss: 19.4729
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 570.72
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 11.02s
                        Total time: 37721.35s
                               ETA: 1153741.8s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.236s, learning 0.174s)
               Value function loss: 18.0812
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 562.22
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 11.41s
                        Total time: 37732.76s
                               ETA: 1153714.5s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.953s, learning 0.164s)
               Value function loss: 14.9694
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 559.88
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 12.12s
                        Total time: 37744.87s
                               ETA: 1153708.7s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.832s, learning 0.166s)
               Value function loss: 18.6214
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 555.25
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 11.00s
                        Total time: 37755.87s
                               ETA: 1153668.8s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.052s, learning 0.163s)
               Value function loss: 20.6530
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 559.29
               Mean episode length: 249.13
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 11.22s
                        Total time: 37767.09s
                               ETA: 1153635.6s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.722s, learning 0.183s)
               Value function loss: 21.8469
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 563.54
               Mean episode length: 248.69
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 10.90s
                        Total time: 37777.99s
                               ETA: 1153592.8s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.878s, learning 0.184s)
               Value function loss: 19.7794
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 561.93
               Mean episode length: 249.06
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 11.06s
                        Total time: 37789.05s
                               ETA: 1153554.9s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.370s, learning 0.160s)
               Value function loss: 17.6537
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 557.51
               Mean episode length: 249.06
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 11.53s
                        Total time: 37800.58s
                               ETA: 1153531.3s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.166s)
               Value function loss: 21.5041
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 558.75
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 11.33s
                        Total time: 37811.91s
                               ETA: 1153501.5s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.652s, learning 0.186s)
               Value function loss: 23.8683
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 561.93
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 11.84s
                        Total time: 37823.75s
                               ETA: 1153487.3s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.246s, learning 0.163s)
               Value function loss: 27.2476
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 556.63
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 11.41s
                        Total time: 37835.16s
                               ETA: 1153460.0s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.519s, learning 0.183s)
               Value function loss: 20.7123
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 556.55
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 11.70s
                        Total time: 37846.86s
                               ETA: 1153441.7s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.341s, learning 0.174s)
               Value function loss: 14.5410
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 553.17
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 11.52s
                        Total time: 37858.37s
                               ETA: 1153417.7s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.782s, learning 0.160s)
               Value function loss: 17.6490
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 549.58
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 11.94s
                        Total time: 37870.32s
                               ETA: 1153406.7s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.167s)
               Value function loss: 17.5719
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 553.66
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 11.42s
                        Total time: 37881.73s
                               ETA: 1153379.7s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.171s, learning 0.163s)
               Value function loss: 22.4585
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 554.40
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 11.33s
                        Total time: 37893.07s
                               ETA: 1153350.1s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.339s, learning 0.241s)
               Value function loss: 15.0422
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 547.37
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 11.58s
                        Total time: 37904.65s
                               ETA: 1153328.1s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.423s, learning 0.160s)
               Value function loss: 20.7537
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 541.72
               Mean episode length: 249.74
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 11.58s
                        Total time: 37916.23s
                               ETA: 1153306.2s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.303s, learning 0.161s)
               Value function loss: 17.4703
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 544.83
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 11.46s
                        Total time: 37927.70s
                               ETA: 1153280.7s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.891s, learning 0.165s)
               Value function loss: 22.9545
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 536.41
               Mean episode length: 249.28
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 11.06s
                        Total time: 37938.75s
                               ETA: 1153242.8s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.163s)
               Value function loss: 20.0964
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 541.99
               Mean episode length: 249.68
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 11.05s
                        Total time: 37949.80s
                               ETA: 1153204.5s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.177s, learning 0.164s)
               Value function loss: 18.1662
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 533.41
               Mean episode length: 249.68
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 11.34s
                        Total time: 37961.14s
                               ETA: 1153175.3s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.934s, learning 0.169s)
               Value function loss: 21.1275
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 534.49
               Mean episode length: 248.39
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 11.10s
                        Total time: 37972.24s
                               ETA: 1153138.8s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.036s, learning 0.175s)
               Value function loss: 21.2342
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 535.04
               Mean episode length: 248.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 11.21s
                        Total time: 37983.45s
                               ETA: 1153105.7s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.445s, learning 0.162s)
               Value function loss: 19.8146
                    Surrogate loss: 0.0130
             Mean action noise std: 0.73
                       Mean reward: 535.54
               Mean episode length: 250.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 11.61s
                        Total time: 37995.06s
                               ETA: 1153084.6s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.511s, learning 0.174s)
               Value function loss: 17.1017
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 536.75
               Mean episode length: 250.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 11.69s
                        Total time: 38006.74s
                               ETA: 1153065.8s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.030s, learning 0.161s)
               Value function loss: 15.3285
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 528.96
               Mean episode length: 250.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 11.19s
                        Total time: 38017.94s
                               ETA: 1153032.1s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.939s, learning 0.161s)
               Value function loss: 17.8114
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 529.11
               Mean episode length: 250.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 11.10s
                        Total time: 38029.04s
                               ETA: 1152995.6s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.407s, learning 0.160s)
               Value function loss: 13.2268
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 523.25
               Mean episode length: 250.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 11.57s
                        Total time: 38040.60s
                               ETA: 1152973.3s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.428s, learning 0.182s)
               Value function loss: 17.4215
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 525.08
               Mean episode length: 250.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 11.61s
                        Total time: 38052.21s
                               ETA: 1152952.3s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.712s, learning 0.184s)
               Value function loss: 16.6402
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 523.01
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 11.90s
                        Total time: 38064.11s
                               ETA: 1152939.9s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.792s, learning 0.181s)
               Value function loss: 19.6243
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 523.67
               Mean episode length: 250.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 10.97s
                        Total time: 38075.08s
                               ETA: 1152899.7s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.292s, learning 0.183s)
               Value function loss: 17.3509
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 510.02
               Mean episode length: 249.26
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 11.47s
                        Total time: 38086.56s
                               ETA: 1152874.6s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.580s, learning 0.173s)
               Value function loss: 12.5990
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 503.80
               Mean episode length: 249.26
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 11.75s
                        Total time: 38098.31s
                               ETA: 1152857.9s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.224s, learning 0.169s)
               Value function loss: 14.1556
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 492.56
               Mean episode length: 250.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 11.39s
                        Total time: 38109.70s
                               ETA: 1152830.4s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.930s, learning 0.172s)
               Value function loss: 15.6115
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 496.17
               Mean episode length: 250.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 11.10s
                        Total time: 38120.80s
                               ETA: 1152794.0s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.004s, learning 0.159s)
               Value function loss: 15.1604
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 492.64
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 11.16s
                        Total time: 38131.97s
                               ETA: 1152759.6s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.942s, learning 0.192s)
               Value function loss: 15.8033
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 489.23
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 11.13s
                        Total time: 38143.10s
                               ETA: 1152724.2s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.315s, learning 0.174s)
               Value function loss: 13.5045
                    Surrogate loss: -0.0225
             Mean action noise std: 0.73
                       Mean reward: 486.20
               Mean episode length: 249.04
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 11.49s
                        Total time: 38154.59s
                               ETA: 1152699.6s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.407s, learning 0.162s)
               Value function loss: 16.0836
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 488.33
               Mean episode length: 248.56
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 11.57s
                        Total time: 38166.16s
                               ETA: 1152677.5s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.189s, learning 0.178s)
               Value function loss: 18.1066
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 479.57
               Mean episode length: 250.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 11.37s
                        Total time: 38177.52s
                               ETA: 1152649.2s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.402s, learning 0.173s)
               Value function loss: 17.5615
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 474.81
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 11.58s
                        Total time: 38189.10s
                               ETA: 1152627.3s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.026s, learning 0.163s)
               Value function loss: 13.2273
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 468.32
               Mean episode length: 250.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 11.19s
                        Total time: 38200.29s
                               ETA: 1152593.7s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.220s, learning 0.251s)
               Value function loss: 14.1465
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 472.10
               Mean episode length: 250.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 11.47s
                        Total time: 38211.76s
                               ETA: 1152568.6s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.830s, learning 0.180s)
               Value function loss: 17.3451
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 454.50
               Mean episode length: 250.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 11.01s
                        Total time: 38222.77s
                               ETA: 1152529.6s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.316s, learning 0.193s)
               Value function loss: 15.9183
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 465.85
               Mean episode length: 250.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 11.51s
                        Total time: 38234.28s
                               ETA: 1152505.7s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.429s, learning 0.202s)
               Value function loss: 16.2888
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 474.54
               Mean episode length: 250.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 11.63s
                        Total time: 38245.91s
                               ETA: 1152485.4s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.219s, learning 0.160s)
               Value function loss: 13.5059
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 478.77
               Mean episode length: 250.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 11.38s
                        Total time: 38257.29s
                               ETA: 1152457.6s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.588s, learning 0.162s)
               Value function loss: 16.9294
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 456.98
               Mean episode length: 250.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 11.75s
                        Total time: 38269.04s
                               ETA: 1152441.0s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.200s, learning 0.165s)
               Value function loss: 18.6503
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 465.46
               Mean episode length: 250.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 11.36s
                        Total time: 38280.40s
                               ETA: 1152412.8s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.163s)
               Value function loss: 17.2093
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 462.40
               Mean episode length: 250.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 11.14s
                        Total time: 38291.54s
                               ETA: 1152377.8s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.106s, learning 0.187s)
               Value function loss: 18.9238
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 476.61
               Mean episode length: 250.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 11.29s
                        Total time: 38302.84s
                               ETA: 1152347.5s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.390s, learning 0.168s)
               Value function loss: 16.5936
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 465.61
               Mean episode length: 249.78
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 11.56s
                        Total time: 38314.39s
                               ETA: 1152325.1s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.436s, learning 0.163s)
               Value function loss: 24.4095
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 471.07
               Mean episode length: 249.78
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 11.60s
                        Total time: 38325.99s
                               ETA: 1152303.9s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.608s, learning 0.168s)
               Value function loss: 19.3423
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 473.94
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 11.78s
                        Total time: 38337.77s
                               ETA: 1152288.1s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.334s, learning 0.161s)
               Value function loss: 19.1386
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 480.30
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 11.50s
                        Total time: 38349.27s
                               ETA: 1152263.9s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.166s)
               Value function loss: 24.0281
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 498.76
               Mean episode length: 249.80
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 11.51s
                        Total time: 38360.78s
                               ETA: 1152240.2s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.090s, learning 0.167s)
               Value function loss: 19.1237
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 509.00
               Mean episode length: 249.80
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 11.26s
                        Total time: 38372.04s
                               ETA: 1152208.8s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.410s, learning 0.179s)
               Value function loss: 20.7214
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 511.53
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 11.59s
                        Total time: 38383.62s
                               ETA: 1152187.4s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.641s, learning 0.165s)
               Value function loss: 21.2312
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 516.21
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 11.81s
                        Total time: 38395.43s
                               ETA: 1152172.5s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.675s, learning 0.166s)
               Value function loss: 22.3679
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 520.83
               Mean episode length: 249.89
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 11.84s
                        Total time: 38407.27s
                               ETA: 1152158.6s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.483s, learning 0.166s)
               Value function loss: 24.1752
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 520.52
               Mean episode length: 248.94
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 11.65s
                        Total time: 38418.92s
                               ETA: 1152139.0s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.225s, learning 0.163s)
               Value function loss: 33.0460
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 541.99
               Mean episode length: 249.05
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 11.39s
                        Total time: 38430.31s
                               ETA: 1152111.6s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.301s, learning 0.186s)
               Value function loss: 22.7744
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 542.28
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 11.49s
                        Total time: 38441.80s
                               ETA: 1152087.2s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.193s, learning 0.164s)
               Value function loss: 20.3128
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 541.23
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 11.36s
                        Total time: 38453.15s
                               ETA: 1152058.8s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.873s, learning 0.160s)
               Value function loss: 18.9704
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 541.35
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 11.03s
                        Total time: 38464.19s
                               ETA: 1152020.8s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.135s, learning 0.163s)
               Value function loss: 25.5647
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 538.52
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 11.30s
                        Total time: 38475.48s
                               ETA: 1151990.7s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.624s, learning 0.183s)
               Value function loss: 28.3981
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 551.49
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 11.81s
                        Total time: 38487.29s
                               ETA: 1151975.9s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.287s, learning 0.185s)
               Value function loss: 22.4135
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 551.85
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 11.47s
                        Total time: 38498.76s
                               ETA: 1151951.1s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.398s, learning 0.189s)
               Value function loss: 25.3862
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 566.87
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 11.59s
                        Total time: 38510.35s
                               ETA: 1151929.7s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.184s, learning 0.186s)
               Value function loss: 27.7471
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 547.51
               Mean episode length: 248.85
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 11.37s
                        Total time: 38521.72s
                               ETA: 1151901.8s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.333s, learning 0.164s)
               Value function loss: 30.3146
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 553.36
               Mean episode length: 249.87
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 11.50s
                        Total time: 38533.22s
                               ETA: 1151877.7s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.248s, learning 0.183s)
               Value function loss: 36.5378
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 568.62
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 11.43s
                        Total time: 38544.65s
                               ETA: 1151851.7s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.478s, learning 0.162s)
               Value function loss: 26.3812
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 558.41
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 11.64s
                        Total time: 38556.29s
                               ETA: 1151831.9s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.847s, learning 0.169s)
               Value function loss: 30.8452
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 560.27
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 11.02s
                        Total time: 38567.30s
                               ETA: 1151793.5s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.205s, learning 0.159s)
               Value function loss: 32.9262
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 566.12
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 11.36s
                        Total time: 38578.67s
                               ETA: 1151765.5s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.700s, learning 0.169s)
               Value function loss: 28.1624
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 559.53
               Mean episode length: 250.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 10.87s
                        Total time: 38589.54s
                               ETA: 1151722.7s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.487s, learning 0.171s)
               Value function loss: 21.1655
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 558.36
               Mean episode length: 248.66
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 11.66s
                        Total time: 38601.19s
                               ETA: 1151703.5s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.163s)
               Value function loss: 22.5974
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 566.31
               Mean episode length: 250.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 11.88s
                        Total time: 38613.07s
                               ETA: 1151690.8s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.886s, learning 0.168s)
               Value function loss: 21.9139
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 556.87
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 11.05s
                        Total time: 38624.13s
                               ETA: 1151653.6s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.839s, learning 0.162s)
               Value function loss: 25.0645
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 557.79
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 11.00s
                        Total time: 38635.13s
                               ETA: 1151614.8s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.072s, learning 0.172s)
               Value function loss: 19.4435
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 562.43
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 11.24s
                        Total time: 38646.37s
                               ETA: 1151583.3s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1462 steps/s (collection: 10.945s, learning 0.258s)
               Value function loss: 22.7576
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 561.44
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 11.20s
                        Total time: 38657.57s
                               ETA: 1151550.5s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.307s, learning 0.168s)
               Value function loss: 22.7633
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 570.07
               Mean episode length: 250.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 11.48s
                        Total time: 38669.05s
                               ETA: 1151525.9s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.166s)
               Value function loss: 21.5260
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 564.01
               Mean episode length: 248.85
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 11.46s
                        Total time: 38680.51s
                               ETA: 1151501.0s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1457 steps/s (collection: 10.975s, learning 0.266s)
               Value function loss: 25.3817
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 561.57
               Mean episode length: 249.81
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 11.24s
                        Total time: 38691.75s
                               ETA: 1151469.4s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.310s, learning 0.165s)
               Value function loss: 18.7039
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 567.88
               Mean episode length: 249.81
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 11.47s
                        Total time: 38703.23s
                               ETA: 1151444.8s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.511s, learning 0.166s)
               Value function loss: 23.9968
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 564.46
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 11.68s
                        Total time: 38714.90s
                               ETA: 1151426.2s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.087s, learning 0.260s)
               Value function loss: 23.3298
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 571.61
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 11.35s
                        Total time: 38726.25s
                               ETA: 1151397.9s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.394s, learning 0.164s)
               Value function loss: 15.1528
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 562.47
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 11.56s
                        Total time: 38737.81s
                               ETA: 1151375.8s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.022s, learning 0.190s)
               Value function loss: 20.7344
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 569.00
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 11.21s
                        Total time: 38749.02s
                               ETA: 1151343.4s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.395s, learning 0.182s)
               Value function loss: 18.1910
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 571.55
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 11.58s
                        Total time: 38760.60s
                               ETA: 1151321.9s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.846s, learning 0.166s)
               Value function loss: 22.3930
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 558.30
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 11.01s
                        Total time: 38771.61s
                               ETA: 1151283.6s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.834s, learning 0.162s)
               Value function loss: 28.7300
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 567.83
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 11.00s
                        Total time: 38782.61s
                               ETA: 1151244.8s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.063s, learning 0.162s)
               Value function loss: 24.8208
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 562.73
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 11.22s
                        Total time: 38793.83s
                               ETA: 1151212.9s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.342s, learning 0.178s)
               Value function loss: 18.2792
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 562.72
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 11.52s
                        Total time: 38805.35s
                               ETA: 1151189.7s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.270s, learning 0.185s)
               Value function loss: 19.2187
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 564.58
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 11.45s
                        Total time: 38816.80s
                               ETA: 1151164.6s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.836s, learning 0.178s)
               Value function loss: 25.1684
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 565.68
               Mean episode length: 249.70
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 11.01s
                        Total time: 38827.82s
                               ETA: 1151126.4s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.530s, learning 0.191s)
               Value function loss: 29.3132
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 556.22
               Mean episode length: 247.68
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 11.72s
                        Total time: 38839.54s
                               ETA: 1151109.2s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.366s, learning 0.184s)
               Value function loss: 21.7599
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 561.05
               Mean episode length: 248.96
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 11.55s
                        Total time: 38851.09s
                               ETA: 1151087.0s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.008s, learning 0.161s)
               Value function loss: 25.3084
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 549.14
               Mean episode length: 249.23
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 11.17s
                        Total time: 38862.26s
                               ETA: 1151053.4s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.056s, learning 0.195s)
               Value function loss: 22.6281
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 551.65
               Mean episode length: 249.92
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 11.25s
                        Total time: 38873.51s
                               ETA: 1151022.4s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.592s, learning 0.187s)
               Value function loss: 23.2251
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 552.85
               Mean episode length: 249.92
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 11.78s
                        Total time: 38885.29s
                               ETA: 1151006.9s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.972s, learning 0.164s)
               Value function loss: 27.2412
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 558.12
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 11.14s
                        Total time: 38896.42s
                               ETA: 1150972.5s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.208s, learning 0.162s)
               Value function loss: 23.7377
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 547.48
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 11.37s
                        Total time: 38907.79s
                               ETA: 1150944.9s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.891s, learning 0.182s)
               Value function loss: 20.0659
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 551.19
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 11.07s
                        Total time: 38918.87s
                               ETA: 1150908.6s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.526s, learning 0.171s)
               Value function loss: 21.7006
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 550.21
               Mean episode length: 248.85
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 11.70s
                        Total time: 38930.56s
                               ETA: 1150890.7s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.469s, learning 0.168s)
               Value function loss: 21.8192
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 548.81
               Mean episode length: 248.85
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 11.64s
                        Total time: 38942.20s
                               ETA: 1150871.1s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.139s, learning 0.175s)
               Value function loss: 15.4910
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 547.45
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 11.31s
                        Total time: 38953.51s
                               ETA: 1150842.0s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.812s, learning 0.193s)
               Value function loss: 20.0665
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 540.36
               Mean episode length: 249.28
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 11.01s
                        Total time: 38964.52s
                               ETA: 1150803.7s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.362s, learning 0.210s)
               Value function loss: 21.0274
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 547.20
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 11.57s
                        Total time: 38976.09s
                               ETA: 1150782.2s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.003s, learning 0.179s)
               Value function loss: 21.2968
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 555.62
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 11.18s
                        Total time: 38987.27s
                               ETA: 1150749.2s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.013s, learning 0.186s)
               Value function loss: 19.7177
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 561.23
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 11.20s
                        Total time: 38998.47s
                               ETA: 1150716.7s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.275s, learning 0.183s)
               Value function loss: 21.5903
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 561.27
               Mean episode length: 249.73
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 11.46s
                        Total time: 39009.93s
                               ETA: 1150691.8s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.886s, learning 0.182s)
               Value function loss: 16.7114
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 557.51
               Mean episode length: 249.73
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 11.07s
                        Total time: 39021.00s
                               ETA: 1150655.5s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.411s, learning 0.187s)
               Value function loss: 17.3930
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 548.36
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 11.60s
                        Total time: 39032.60s
                               ETA: 1150634.8s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.119s, learning 0.161s)
               Value function loss: 20.6013
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 543.40
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 11.28s
                        Total time: 39043.88s
                               ETA: 1150604.7s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.368s, learning 0.181s)
               Value function loss: 17.1586
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 559.18
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 11.55s
                        Total time: 39055.42s
                               ETA: 1150582.6s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.202s, learning 0.177s)
               Value function loss: 18.8442
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 558.82
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 11.38s
                        Total time: 39066.80s
                               ETA: 1150555.4s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.078s, learning 0.186s)
               Value function loss: 17.9216
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 555.21
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 11.26s
                        Total time: 39078.07s
                               ETA: 1150524.9s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.006s, learning 0.192s)
               Value function loss: 11.6984
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 548.34
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 11.20s
                        Total time: 39089.27s
                               ETA: 1150492.5s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.982s, learning 0.180s)
               Value function loss: 17.6752
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 550.13
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 11.16s
                        Total time: 39100.43s
                               ETA: 1150459.0s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.056s, learning 0.168s)
               Value function loss: 15.9594
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 555.11
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 11.22s
                        Total time: 39111.65s
                               ETA: 1150427.3s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.210s, learning 0.170s)
               Value function loss: 20.6422
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 552.41
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 11.38s
                        Total time: 39123.03s
                               ETA: 1150400.3s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.601s, learning 0.167s)
               Value function loss: 20.9237
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 556.59
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 11.77s
                        Total time: 39134.80s
                               ETA: 1150384.7s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.119s, learning 0.176s)
               Value function loss: 22.7972
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 552.56
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 11.29s
                        Total time: 39146.09s
                               ETA: 1150355.1s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.972s, learning 0.164s)
               Value function loss: 22.2602
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 543.12
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 11.14s
                        Total time: 39157.23s
                               ETA: 1150320.9s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.650s, learning 0.159s)
               Value function loss: 16.1065
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 546.28
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 11.81s
                        Total time: 39169.04s
                               ETA: 1150306.5s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.239s, learning 0.180s)
               Value function loss: 18.9272
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 543.37
               Mean episode length: 250.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 11.42s
                        Total time: 39180.46s
                               ETA: 1150280.6s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.185s, learning 0.187s)
               Value function loss: 21.9518
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 543.50
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 11.37s
                        Total time: 39191.83s
                               ETA: 1150253.4s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.142s, learning 0.188s)
               Value function loss: 19.9729
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 548.30
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 11.33s
                        Total time: 39203.16s
                               ETA: 1150224.9s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.869s, learning 0.166s)
               Value function loss: 21.7540
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 531.68
               Mean episode length: 249.75
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 11.04s
                        Total time: 39214.19s
                               ETA: 1150187.8s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.104s, learning 0.199s)
               Value function loss: 18.4166
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 529.24
               Mean episode length: 249.75
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 11.30s
                        Total time: 39225.50s
                               ETA: 1150158.6s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.130s, learning 0.167s)
               Value function loss: 20.3081
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 530.75
               Mean episode length: 250.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 11.30s
                        Total time: 39236.79s
                               ETA: 1150129.3s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.081s, learning 0.180s)
               Value function loss: 29.5197
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 537.82
               Mean episode length: 249.90
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 11.26s
                        Total time: 39248.05s
                               ETA: 1150098.8s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.280s, learning 0.176s)
               Value function loss: 30.0959
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 536.49
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 11.46s
                        Total time: 39259.51s
                               ETA: 1150074.1s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.197s, learning 0.175s)
               Value function loss: 22.9760
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 534.90
               Mean episode length: 248.85
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 11.37s
                        Total time: 39270.88s
                               ETA: 1150047.0s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.179s, learning 0.196s)
               Value function loss: 20.5543
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 530.48
               Mean episode length: 249.57
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 11.37s
                        Total time: 39282.26s
                               ETA: 1150019.9s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.695s, learning 0.170s)
               Value function loss: 24.9232
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 519.54
               Mean episode length: 248.91
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 10.87s
                        Total time: 39293.12s
                               ETA: 1149977.9s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.175s, learning 0.167s)
               Value function loss: 21.5634
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 527.30
               Mean episode length: 249.34
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 11.34s
                        Total time: 39304.46s
                               ETA: 1149949.9s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.183s, learning 0.172s)
               Value function loss: 23.3910
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 531.09
               Mean episode length: 248.22
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 11.35s
                        Total time: 39315.82s
                               ETA: 1149922.3s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.161s)
               Value function loss: 19.2879
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 526.98
               Mean episode length: 248.09
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 11.52s
                        Total time: 39327.33s
                               ETA: 1149899.4s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.252s, learning 0.182s)
               Value function loss: 24.9986
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 528.77
               Mean episode length: 248.76
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 11.43s
                        Total time: 39338.77s
                               ETA: 1149874.1s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.700s, learning 0.188s)
               Value function loss: 21.8618
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 543.80
               Mean episode length: 248.78
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 11.89s
                        Total time: 39350.66s
                               ETA: 1149862.1s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.193s, learning 0.173s)
               Value function loss: 26.9278
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 545.76
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 11.37s
                        Total time: 39362.02s
                               ETA: 1149834.9s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.979s, learning 0.173s)
               Value function loss: 23.2183
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 527.13
               Mean episode length: 250.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 11.15s
                        Total time: 39373.18s
                               ETA: 1149801.4s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.014s, learning 0.170s)
               Value function loss: 21.7825
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 526.85
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 11.18s
                        Total time: 39384.36s
                               ETA: 1149768.9s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.283s, learning 0.173s)
               Value function loss: 23.7672
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 535.24
               Mean episode length: 248.70
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 11.46s
                        Total time: 39395.82s
                               ETA: 1149744.3s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.903s, learning 0.184s)
               Value function loss: 18.4660
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 538.48
               Mean episode length: 250.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 11.09s
                        Total time: 39406.90s
                               ETA: 1149708.9s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.112s, learning 0.174s)
               Value function loss: 21.0579
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 537.61
               Mean episode length: 250.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 11.29s
                        Total time: 39418.19s
                               ETA: 1149679.4s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.165s)
               Value function loss: 18.8800
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 518.14
               Mean episode length: 250.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 11.57s
                        Total time: 39429.76s
                               ETA: 1149658.1s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.427s, learning 0.176s)
               Value function loss: 15.8592
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 522.99
               Mean episode length: 249.95
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 11.60s
                        Total time: 39441.36s
                               ETA: 1149637.9s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.236s, learning 0.167s)
               Value function loss: 20.1166
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 535.49
               Mean episode length: 249.95
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 11.40s
                        Total time: 39452.77s
                               ETA: 1149611.8s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.637s, learning 0.187s)
               Value function loss: 18.3968
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 538.70
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 11.82s
                        Total time: 39464.59s
                               ETA: 1149597.9s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.390s, learning 0.174s)
               Value function loss: 19.9198
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 521.28
               Mean episode length: 249.98
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 11.56s
                        Total time: 39476.15s
                               ETA: 1149576.5s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.164s)
               Value function loss: 21.5699
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 536.70
               Mean episode length: 249.98
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 10.87s
                        Total time: 39487.03s
                               ETA: 1149535.1s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.123s, learning 0.165s)
               Value function loss: 23.0922
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 519.44
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 11.29s
                        Total time: 39498.32s
                               ETA: 1149505.7s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.301s, learning 0.163s)
               Value function loss: 22.0947
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 519.76
               Mean episode length: 248.92
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 11.46s
                        Total time: 39509.78s
                               ETA: 1149481.4s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.957s, learning 0.175s)
               Value function loss: 16.9468
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 524.19
               Mean episode length: 248.92
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 11.13s
                        Total time: 39520.91s
                               ETA: 1149447.5s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.380s, learning 0.177s)
               Value function loss: 22.5423
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 529.31
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 11.56s
                        Total time: 39532.47s
                               ETA: 1149425.9s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.087s, learning 0.166s)
               Value function loss: 22.4689
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 537.01
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 11.25s
                        Total time: 39543.72s
                               ETA: 1149395.5s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.789s, learning 0.164s)
               Value function loss: 22.8033
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 531.32
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 11.95s
                        Total time: 39555.68s
                               ETA: 1149385.5s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.376s, learning 0.183s)
               Value function loss: 22.8961
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 525.88
               Mean episode length: 249.98
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 11.56s
                        Total time: 39567.24s
                               ETA: 1149364.0s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.360s, learning 0.255s)
               Value function loss: 21.0972
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 516.82
               Mean episode length: 249.43
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 11.61s
                        Total time: 39578.85s
                               ETA: 1149344.2s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.407s, learning 0.180s)
               Value function loss: 20.7914
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 519.21
               Mean episode length: 249.45
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 11.59s
                        Total time: 39590.44s
                               ETA: 1149323.5s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.254s, learning 0.177s)
               Value function loss: 28.2739
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 546.43
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 11.43s
                        Total time: 39601.87s
                               ETA: 1149298.3s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.199s, learning 0.166s)
               Value function loss: 29.5058
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 520.14
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 11.36s
                        Total time: 39613.23s
                               ETA: 1149271.2s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.197s, learning 0.172s)
               Value function loss: 21.3298
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 527.06
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 11.37s
                        Total time: 39624.60s
                               ETA: 1149244.3s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.331s, learning 0.166s)
               Value function loss: 21.8865
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 544.55
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 11.50s
                        Total time: 39636.10s
                               ETA: 1149221.0s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.566s, learning 0.176s)
               Value function loss: 30.8275
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 536.24
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 11.74s
                        Total time: 39647.84s
                               ETA: 1149204.9s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.720s, learning 0.177s)
               Value function loss: 24.2716
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 543.37
               Mean episode length: 250.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 11.90s
                        Total time: 39659.74s
                               ETA: 1149193.2s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.513s, learning 0.186s)
               Value function loss: 22.6172
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 548.25
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 11.70s
                        Total time: 39671.44s
                               ETA: 1149175.9s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.556s, learning 0.183s)
               Value function loss: 20.4931
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 541.65
               Mean episode length: 250.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 11.74s
                        Total time: 39683.18s
                               ETA: 1149159.7s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.286s, learning 0.177s)
               Value function loss: 24.1064
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 537.50
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 11.46s
                        Total time: 39694.64s
                               ETA: 1149135.5s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.171s)
               Value function loss: 25.1383
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 542.30
               Mean episode length: 250.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 11.38s
                        Total time: 39706.02s
                               ETA: 1149109.0s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.306s, learning 0.162s)
               Value function loss: 23.5700
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 545.59
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 11.47s
                        Total time: 39717.49s
                               ETA: 1149084.9s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.303s, learning 0.161s)
               Value function loss: 25.8393
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 542.36
               Mean episode length: 250.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 11.46s
                        Total time: 39728.96s
                               ETA: 1149060.8s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.200s, learning 0.171s)
               Value function loss: 22.0704
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 549.48
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 11.37s
                        Total time: 39740.33s
                               ETA: 1149033.9s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.460s, learning 0.161s)
               Value function loss: 27.8245
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 555.66
               Mean episode length: 250.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 11.62s
                        Total time: 39751.95s
                               ETA: 1149014.4s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.993s, learning 0.178s)
               Value function loss: 23.8782
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 546.42
               Mean episode length: 250.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 11.17s
                        Total time: 39763.12s
                               ETA: 1148981.7s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.643s, learning 0.167s)
               Value function loss: 18.8448
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 545.34
               Mean episode length: 249.73
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 10.81s
                        Total time: 39773.93s
                               ETA: 1148938.7s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.228s, learning 0.164s)
               Value function loss: 25.5138
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 547.11
               Mean episode length: 249.73
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 11.39s
                        Total time: 39785.32s
                               ETA: 1148912.5s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.243s, learning 0.171s)
               Value function loss: 24.9912
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 550.27
               Mean episode length: 250.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 11.41s
                        Total time: 39796.73s
                               ETA: 1148887.0s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.119s, learning 0.160s)
               Value function loss: 18.6932
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 547.69
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 11.28s
                        Total time: 39808.01s
                               ETA: 1148857.6s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.244s, learning 0.172s)
               Value function loss: 20.2435
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 555.44
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 11.42s
                        Total time: 39819.43s
                               ETA: 1148832.1s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.196s, learning 0.181s)
               Value function loss: 23.2711
                    Surrogate loss: 0.0057
             Mean action noise std: 0.73
                       Mean reward: 558.23
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 11.38s
                        Total time: 39830.81s
                               ETA: 1148805.5s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.037s, learning 0.163s)
               Value function loss: 22.9927
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 546.62
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 11.20s
                        Total time: 39842.01s
                               ETA: 1148773.9s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.362s, learning 0.163s)
               Value function loss: 27.9061
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 545.39
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 11.53s
                        Total time: 39853.53s
                               ETA: 1148751.6s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.950s, learning 0.177s)
               Value function loss: 24.5642
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 555.61
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 11.13s
                        Total time: 39864.66s
                               ETA: 1148717.8s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.533s, learning 0.189s)
               Value function loss: 22.9574
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 553.62
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 11.72s
                        Total time: 39876.38s
                               ETA: 1148701.3s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.645s, learning 0.170s)
               Value function loss: 19.4917
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 546.42
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 11.82s
                        Total time: 39888.20s
                               ETA: 1148687.4s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.134s, learning 0.169s)
               Value function loss: 24.8676
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 546.45
               Mean episode length: 250.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 11.30s
                        Total time: 39899.50s
                               ETA: 1148658.7s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.206s, learning 0.184s)
               Value function loss: 31.1681
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 555.68
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 11.39s
                        Total time: 39910.89s
                               ETA: 1148632.6s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.306s, learning 0.184s)
               Value function loss: 25.4140
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 547.88
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 11.49s
                        Total time: 39922.38s
                               ETA: 1148609.3s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.272s, learning 0.181s)
               Value function loss: 29.4215
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 558.18
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 11.45s
                        Total time: 39933.83s
                               ETA: 1148585.0s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.218s, learning 0.168s)
               Value function loss: 32.8615
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 559.58
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 11.39s
                        Total time: 39945.22s
                               ETA: 1148558.7s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.256s, learning 0.172s)
               Value function loss: 37.2429
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 565.35
               Mean episode length: 250.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 11.43s
                        Total time: 39956.65s
                               ETA: 1148533.7s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.952s, learning 0.223s)
               Value function loss: 40.4082
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 561.31
               Mean episode length: 249.21
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 11.17s
                        Total time: 39967.82s
                               ETA: 1148501.4s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.453s, learning 0.198s)
               Value function loss: 32.1126
                    Surrogate loss: 0.0156
             Mean action noise std: 0.73
                       Mean reward: 567.10
               Mean episode length: 249.99
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 11.65s
                        Total time: 39979.47s
                               ETA: 1148482.8s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.879s, learning 0.180s)
               Value function loss: 33.5931
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 574.66
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 11.06s
                        Total time: 39990.53s
                               ETA: 1148447.2s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.324s, learning 0.161s)
               Value function loss: 32.7430
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 561.48
               Mean episode length: 249.93
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 11.48s
                        Total time: 40002.01s
                               ETA: 1148423.8s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.167s, learning 0.164s)
               Value function loss: 30.5542
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 563.17
               Mean episode length: 250.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 11.33s
                        Total time: 40013.34s
                               ETA: 1148396.1s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.094s, learning 0.184s)
               Value function loss: 24.4831
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 574.42
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 11.28s
                        Total time: 40024.62s
                               ETA: 1148366.8s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1455 steps/s (collection: 10.997s, learning 0.259s)
               Value function loss: 28.5268
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 574.41
               Mean episode length: 250.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 11.26s
                        Total time: 40035.88s
                               ETA: 1148336.9s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.255s, learning 0.174s)
               Value function loss: 28.9711
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 573.43
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 11.43s
                        Total time: 40047.31s
                               ETA: 1148312.0s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.398s, learning 0.174s)
               Value function loss: 26.2828
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 574.30
               Mean episode length: 250.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 11.57s
                        Total time: 40058.88s
                               ETA: 1148291.2s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.986s, learning 0.184s)
               Value function loss: 23.8489
                    Surrogate loss: 0.0009
             Mean action noise std: 0.73
                       Mean reward: 576.46
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 11.17s
                        Total time: 40070.05s
                               ETA: 1148258.9s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.946s, learning 0.168s)
               Value function loss: 28.6939
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 573.94
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 11.11s
                        Total time: 40081.16s
                               ETA: 1148225.0s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.979s, learning 0.200s)
               Value function loss: 27.2347
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 576.92
               Mean episode length: 250.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 11.18s
                        Total time: 40092.34s
                               ETA: 1148192.9s
